{"version":3,"kind":"Notebook","sha256":"3b675c7742d3b78f7f4761f1f69acd326a140448e28558ae6e158d2f99538a58","slug":"llm","location":"/notebook/llm.ipynb","dependencies":[],"frontmatter":{"title":"大規模言語モデルの基本","content_includes_title":false,"kernelspec":{"name":"python3","display_name":"jupyterbook","language":"python"},"github":"https://github.com/lvzeyu/css_nlp","numbering":{"heading_1":{"enabled":true,"template":"enabled"},"heading_2":{"enabled":true,"template":"enabled"},"heading_3":{"enabled":true,"template":"enabled"},"heading_4":{"enabled":true,"template":"enabled"},"heading_5":{"enabled":true,"template":"enabled"},"heading_6":{"enabled":true,"template":"enabled"},"title":{"offset":1}},"source_url":"https://github.com/lvzeyu/css_nlp/blob/master/notebook/llm.ipynb","edit_url":"https://github.com/lvzeyu/css_nlp/edit/master/notebook/llm.ipynb","thumbnail":"/build/LLMs_parameter-ec157a4028abc601fb429210b7f4b07c.png","exports":[{"format":"ipynb","filename":"llm.ipynb","url":"/build/llm-83b731980afacb372a53c7f294eda9c0.ipynb"}]},"widgets":{},"mdast":{"type":"root","children":[{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"近年、GPTをはじめとする大規模言語モデルが自然言語処理分野において顕著な成果を挙げ、文章の生成や論理推論など高度な課題においても高い性能を示している。それに伴う、大規模言語モデルを活用することでより効率的に人間の行動や社会現象に関する理論や仮説の検証と発展し、新たな可能性をもたらすと期待されています。","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"YvRoiEpTis"}],"key":"rPrgQHGeqR"},{"type":"paragraph","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"今まで説明した通り、文脈を考慮した単語埋め込みである文脈化単語埋め込み(contextualized word embedding)を計算するTransformerを大規模コーパスで自己教師あり学習(Masked LMやNext sentence prediction)で事前学習し、そのモデルを下流タスクのテータセットを使って微調整する方法は、自然言語処理でよく用いられる手法です。","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"lmkOPGc2sS"}],"key":"DRNXPIa8Ml"},{"type":"paragraph","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"text","value":"こうした事前学習した大規模なニューラルネットワークは、大規模言語モデルのように呼ばれます。","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"dO79r6B605"}],"key":"OVfb4ciI69"},{"type":"paragraph","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"text","value":"特に、2020年以降にパラメータ数およびテキストデータをさらに大規模することで、大規模言語モデルの性能も飛躍的に向上し、人間と自然にやりとりできるような能力を身につけました。これによって、ファインチューニングは行なわずに、事前学習された大規模言語モデルをプロンプト(prompt)と呼ばれるテキストを通じて制御することで下流タスク方法も一般的になりつつあります。","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"q6PHMIMS2A"}],"key":"SVOL5mEdmU"},{"type":"paragraph","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"text","value":"ここでは、大規模言語モデルの近年(2023年まで)の進展について解説します。進展が激しい分野のため、ここでの解説に基づいて適宜最新の情報も各自調べてほしいです。","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"UDbeSwUptM"}],"key":"KqKFLjd47J"}],"key":"Pc5oQqogFc"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"モデルの大規模化とその効果","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"EOP4dcMrg1"}],"identifier":"id","label":"モデルの大規模化とその効果","html_id":"id","implicit":true,"enumerator":"1","key":"h1AQGuAs2L"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"大規模言語モデルの開発が進むにつれて、モデルに含まれるパラメータ数が飛躍的に増加してきています。","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"W2WariUXwy"}],"key":"U549dy8brD"},{"type":"paragraph","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"2018年に発表されたBERTは3.4億個、2019年のGPT-2では15億個だったパラメータ数が2020年のGPT-3では1750億、そしてGPT-4はは100兆個のパラメータを持つと言われており、加速的に増加していることがわかります。","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"mUgvKqKSe7"}],"key":"CKEUpOHiPs"}],"key":"NQYcXVEkTS"},{"type":"block","kind":"notebook-content","children":[{"type":"image","url":"/build/LLMs_parameter-ec157a4028abc601fb429210b7f4b07c.png","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"KNlwRGHfcZ","urlSource":"./Figure/LLMs_parameter.png"},{"type":"image","url":"/build/gpt_para-66d806b5b79617ddbf36df25e9e6fef1.png","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"vnvuYjnLRU","urlSource":"./Figure/gpt_para.png"}],"key":"JAofIklR5T"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"こうした大規模化が行われている背景には、モデルの規模を大きくすることで性能が比例して改善していくという経験的な法則であるスケール則があります。","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"gIR44XtklU"}],"key":"XhBUvhptJt"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"さらに、大規模言語モデルが一定の規模を超えると、タスクの性能が飛躍的に向上する現象も報告されています。こうした大規模化することで性能が改善し獲得される能力を創発的能力(emergent abilities)と呼ぶことがあります。","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"y4t2kioFVw"}],"key":"ebI8HrLdXh"},{"type":"aside","children":[{"type":"paragraph","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"text","value":"一方、大規模言語モデルが創発的能力を持つことに疑問視している声もあります。","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"awVLpex6J2"}],"key":"A9mNXxIqyp"}],"key":"hsBAceSRJO"},{"type":"image","url":"/build/emergent-d2694b9351ce41c7dfd2034adf2cb385.png","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"cYDuJj13o1","urlSource":"./Figure/emergent.png"}],"key":"pn12GHUZns"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Prompting","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"kwxL2IXRXc"}],"identifier":"prompting","label":"Prompting","html_id":"prompting","implicit":true,"enumerator":"2","key":"JqH9y0vWOp"},{"type":"heading","depth":3,"position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"文脈内学習","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"BiICJJ4smx"}],"identifier":"id","label":"文脈内学習","html_id":"id-1","implicit":true,"enumerator":"2.1","key":"JEHTweszX8"}],"key":"Lq7gXQhUXM"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"text","value":"GPTのようなパラメータが非常に大きいモデルでは、学習時に入力される文章内には、様々なサブタスクが埋め込まれると考えられます。こうした文章の生成を学習することで、内包される様々な言語タスクへの処理能力の獲得が期待できます。\nこのアプローチはGPT-2から用いられており、文脈内学習(in-context learning)と呼ばれます。","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"rlpv40qKpp"}],"key":"g7TWR7By4l"},{"type":"aside","children":[{"type":"paragraph","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"詳細はこちらの","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"pTbjRBpe2j"},{"type":"link","url":"https://arxiv.org/abs/2005.14165","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"論文:Language Models are Few-Shot Learners","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"SmsgHnE4XA"}],"urlSource":"https://arxiv.org/abs/2005.14165","key":"K9xedAatBQ"},{"type":"text","value":"を参照してください。","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"t4PWIkLgX3"}],"key":"APBNQdDOBB"}],"key":"kBbJAYC7zp"},{"type":"image","url":"/build/meta_learning-0d9335f3b146c91ca09078f39fc34491.png","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"NgF8Dx12Na","urlSource":"./Figure/meta_learning.png"},{"type":"paragraph","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"text","value":"モデルは新しいタスクをこなすための追加のトレーニングデータなしに（zero-shot learning）、または非常に少量のデータで（few-shot learning）多様なタスクを達成することが可能になります。","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"I7qouloGX2"}],"key":"RiLBtueoja"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":14,"column":1},"end":{"line":17,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Few-shot learning: 推論時に少数（10から100）のデモンストレーションを与えます","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"key":"XuFrIZit0G"}],"key":"ovxliaXHsD"}],"key":"UGEvqwts0Z"},{"type":"listItem","spread":true,"position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"One-shot learning: 推論時に一つのデモンストレーションを与えます","position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"key":"j50k6BVCsC"}],"key":"FxENiQbF5r"}],"key":"xg3tqHWXbG"},{"type":"listItem","spread":true,"position":{"start":{"line":16,"column":1},"end":{"line":17,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Zero-shot learning: 推論時にデモンストレーションは与えられず、自然言語によるタスク指示のみが与えられます","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"key":"OaQ0zdcQmx"}],"key":"BkjlOfDx8K"}],"key":"gMGdm5Pn6D"}],"key":"pzqL6DyOhq"},{"type":"image","url":"/build/shot-learning-ae6fca156f60571e643b613d4db391d0.png","position":{"start":{"line":18,"column":1},"end":{"line":18,"column":1}},"key":"BsJMjwIuan","urlSource":"./Figure/shot-learning.png"},{"type":"paragraph","position":{"start":{"line":22,"column":1},"end":{"line":22,"column":1}},"children":[{"type":"text","value":"このように、従来ファインチューニングが必要される多くのタスクは、モデルにプロンプト(prompt)と呼ばれるテキストを入力して後続けするテキストを予測するという形で解かせることが知られています。","position":{"start":{"line":22,"column":1},"end":{"line":22,"column":1}},"key":"KK9OvmbiWd"}],"key":"rKXZht4P25"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":24,"column":1},"end":{"line":27,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":24,"column":1},"end":{"line":25,"column":1}},"children":[{"type":"paragraph","position":{"start":{"line":24,"column":1},"end":{"line":24,"column":1}},"children":[{"type":"text","value":"基本的には、タスクの説明と多くのデモンストレーションを与えることでモデルの性能が向上していく傾向が見られます。","position":{"start":{"line":24,"column":1},"end":{"line":24,"column":1}},"key":"cP6h069DXP"}],"key":"QfoYhUo9cZ"}],"key":"nu0KbqA6JT"},{"type":"listItem","spread":true,"position":{"start":{"line":26,"column":1},"end":{"line":27,"column":1}},"children":[{"type":"paragraph","position":{"start":{"line":26,"column":1},"end":{"line":26,"column":1}},"children":[{"type":"text","value":"zero, one-shotでも悪くない性能、few-shotでは一部ファインチューニングを用いたSOTAモデルに匹敵する性能を得ることが確認できました。","position":{"start":{"line":26,"column":1},"end":{"line":26,"column":1}},"key":"eRAW6ege3o"}],"key":"OQsdDWEnKR"}],"key":"Hag6lwQUqA"}],"key":"sUg7EsvfD6"},{"type":"image","url":"/build/in-context-200c05c5ac8b27c8efc40deafa5de088.png","position":{"start":{"line":28,"column":1},"end":{"line":28,"column":1}},"key":"iffpVr4aQC","urlSource":"./Figure/in-context.png"},{"type":"paragraph","position":{"start":{"line":31,"column":1},"end":{"line":31,"column":1}},"children":[{"type":"text","value":"現在、一般的には、few-shot learningモデルの性能がファインチューニングによる教師あり学習モデルを超えることはまだ少ないが、ラベル付きデータの生成に費やすコストと時間を効率すると、プロンプトはファインチューニングによる教師あり学習の効率的な代替手段と考えられます。","position":{"start":{"line":31,"column":1},"end":{"line":31,"column":1}},"key":"Q88rKUJIKc"}],"key":"gGurDh4Ms5"},{"type":"image","url":"/build/shot-compare-31e1df16c91bc3dd7e1e5176a2d87a4a.png","position":{"start":{"line":34,"column":1},"end":{"line":34,"column":1}},"key":"pKbJpQ92wF","urlSource":"./Figure/shot-compare.png"}],"key":"T7dB21D2Qw"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"chain-of-thought推論","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"J9VSdIgJZf"}],"identifier":"chain-of-thought","label":"chain-of-thought推論","html_id":"chain-of-thought","implicit":true,"enumerator":"2.2","key":"ps8bobFQMQ"},{"type":"aside","children":[{"type":"paragraph","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"詳細はこちらの","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"uduX5amCu3"},{"type":"link","url":"https://arxiv.org/abs/2201.11903","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"論文:Chain-of-Thought Prompting Elicits Reasoning in Large Language Models","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"otLfkaDKDJ"}],"urlSource":"https://arxiv.org/abs/2201.11903","key":"GBMjmeYSIc"},{"type":"text","value":"を参照してください。","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"UzFDq7ctzy"}],"key":"J5JPRCDavh"}],"key":"JzQWetWQdM"},{"type":"paragraph","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"text","value":"大規模言語モデルが苦手とされるタスクの一つに他段階の推論が必要となるマルチステップ推論(multi-step reasoning)があります。","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"key":"NRVYL7jMIN"}],"key":"enizo6XiFK"},{"type":"paragraph","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"children":[{"type":"text","value":"複数の段階の推論が必要な際に、推論過程の例示を与えるchain-of-thought推論(chain-of-thought reasoning)を用いることで性能が改善することが報告されています。","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"IQpQKYhFdB"}],"key":"alWCzHrMr0"},{"type":"paragraph","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"children":[{"type":"text","value":"具体的言えば、chain-of-thought推論では、回答を加えて推論過程を示す例示を与えて、モデルが回答を行う際に推論過程を含めて出力テキストを生成するようにします。","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"hZWDVeURT5"}],"key":"BBFPMonXuj"},{"type":"image","url":"/build/chains_of_thought-6af15225868a72490f05a0a1c638f322.png","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"key":"REuo6qzNnZ","urlSource":"./Figure/chains_of_thought.png"}],"key":"inEcI0SJcN"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"さらに、chain-of-thought推論の推論過程を人間が与えるのではなく、推論過程の生成を促す「Let’s think step by step」のような文字列をプロンプトの末尾に追加して、推論過程を生成されてから回答を抽出するzero-shot chain-of-thought reasoningも提案されています。","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"qE78gOKUZf"}],"key":"WV0d9lh05A"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"この方法を使うと、プロンプトを書き換えるだけで推論の性能を改善することができます。このことから、大規模言語モデルを使うあたってはプロンプトの与え方を工夫することが重要であることがわかります。","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"jhVUYIOfHY"}],"key":"AVR77H0IK8"},{"type":"image","url":"/build/zero_CoT-2e1d99436695654f74540cb26ffc846a.png","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"ZsmzCA0ax0","urlSource":"./Figure/zero_CoT.png"},{"type":"aside","children":[{"type":"paragraph","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"text","value":"詳細はこちらの","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"iuo64xUCkJ"},{"type":"link","url":"https://arxiv.org/abs/2205.11916","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"text","value":"論文:Large Language Models are Zero-Shot Reasoners","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"InGJjxXFzM"}],"urlSource":"https://arxiv.org/abs/2205.11916","key":"ss1l6acE5E"},{"type":"text","value":"を参照してください。","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"pd1luXEKJz"}],"key":"H7OPfebwch"}],"key":"AdzHj5MNG6"}],"key":"uEs8N9Yads"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"指示チューニング","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Dq7fxeSxJ7"}],"identifier":"id","label":"指示チューニング","html_id":"id-2","implicit":true,"enumerator":"3","key":"LNtah9CDGr"}],"key":"IZzFHeAfOc"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"通常の事前学習では、モデルは膨大なデータセットから次の単語を予測する能力を学習します。しかし、そのままでは具体的なタスクを効率的にこなすことは難しい場合があります。","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"uM8X8geqyl"}],"key":"SeDUVh1hCa"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"指示チューニング（instruction tuning）とは、様々なタスクのデータを指示と理想的な回答の組で構成されるデータセットを言語モデルに与え追加学習させることで、言語モデルの性能を向上させる技術です。","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"OobiZBXElL"}],"key":"oYEpBx3BFR"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":5,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"指示チューニングデータセットの構築","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"v6zHkXzgTr"}],"key":"R4zjaCAaXg"}],"key":"wnD12AHoWO"},{"type":"listItem","spread":true,"position":{"start":{"line":6,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"ファインチューニング: 指示チューニングデータセットを用いて、既存のLLMを再トレーニングします。これにより、モデルが指示に応じた出力を生成する能力が向上します。","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"e2QmUFaZqZ"}],"key":"gdvfGVkYVW"}],"key":"l2io1gTSE7"}],"key":"Tl4n4OQFBO"},{"type":"image","url":"/build/instrust-example-5c2e99563d08457de4497eb7533aaddb.png","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"key":"ekkQwJvQ10","urlSource":"./Figure/instrust-example.png"},{"type":"paragraph","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"text","value":"Flan(Finetuned Lanaguage Net)では、大規模言語モデルをデータセットを集約してフィインチューニングした結果、多数のタスクにおけるzero-shot学習の性能が向上したことが報告されています。","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"s5KdxtW7F4"}],"key":"x3NjIQdfus"},{"type":"paragraph","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"link","url":"https://huggingface.co/google/flan-t5-xxl?text=The+square+root+of+x+is+the+cube+root+of+y.+What+is+y+to+the+power+of+2%2C+if+x+%3D+4%3F","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"text","value":"「Flan-T5」","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"szSnr5kxgk"}],"urlSource":"https://huggingface.co/google/flan-t5-xxl?text=The+square+root+of+x+is+the+cube+root+of+y.+What+is+y+to+the+power+of+2%2C+if+x+%3D+4%3F","key":"kvlyfXW5fH"},{"type":"text","value":"は、Google AI の新しいオープンソース言語モデルです。1,800 以上の言語タスクでファインチューニングされており、プロンプトとマルチステップの推論能力が劇的に向上しています。","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"JHWcDL2yFe"}],"key":"lY5eNf6GxO"},{"type":"image","url":"/build/flan-1-7a505e0e60ebed20f22312c621ec2af9.png","position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"key":"LZG6XvY00P","urlSource":"./Figure/flan-1.png"}],"key":"pvuOt4Ho3f"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"人間のフィードバックからの強化学習","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"luhMhmdn4a"}],"identifier":"id","label":"人間のフィードバックからの強化学習","html_id":"id-3","implicit":true,"enumerator":"4","key":"gRpR8VeZOM"}],"key":"HuoNoo0tKc"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"RLHF（Reinforcement Learning from Human Feedback）とは、「人間のフィードバックからの強化学習」という名前の通り、人間の価値基準に沿うように、人間のフィードバックを使ってAI（言語）モデルを強化学習で微調整（ファインチューニング）する手法である。","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"h9RouRVDpy"}],"key":"caPm9uoSku"},{"type":"heading","depth":3,"position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"強化学習","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"uTPvF3j5K5"}],"identifier":"id","label":"強化学習","html_id":"id-4","implicit":true,"enumerator":"4.1","key":"fTf9Mby7T7"},{"type":"paragraph","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"強化学習 (Reinforcement Learning) とは、機械学習の一種であり、エージェントが動的環境と、繰り返し試行錯誤のやりとりを重ねることによってタスクを実行できるようになる手法です。この学習手法により、エージェントは、タスクの報酬を最大化する一連の意思決定を行うことができます。教師付き学習とよく似た問題設定ですが、与えられた正解の出力をそのまま学習すれば良いわけではなく、もっと広い意味での「価値」を最大化する行動を学習しなければなりません。","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"ywBOgYuEL5"}],"key":"wvdN8JtsrH"},{"type":"paragraph","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"text","value":"例えば、犬を訓練すること例として考えてください。強化学習の用語を使用すると、この場合の学習の目的は、犬 (エージェント) のしつけ (学習) を行い、ある環境の中でタスクを完了させることです。これには、犬の周囲の環境や訓練士が含まれます。","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"IMgwCUHurm"}],"key":"mY5cHv7zks"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":9,"column":1},"end":{"line":14,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":9,"column":1},"end":{"line":10,"column":1}},"children":[{"type":"paragraph","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"text","value":"まず、訓練士が命令や合図を出し、それを犬が観察 (観測) します。続いて、犬は行動を起こすことで反応します。犬の行動が目的の行動に近い場合、訓練士は、おやつやおもちゃなどのごほうび (報酬) を与えますが、それ以外の場合、ごほうびは与えません。","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"fFVZp3uCXN"}],"key":"VKZPg7h5Ml"}],"key":"tIMjaNdb9b"},{"type":"listItem","spread":true,"position":{"start":{"line":11,"column":1},"end":{"line":12,"column":1}},"children":[{"type":"paragraph","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"text","value":"しつけ (学習) を始めたばかりの頃は、犬はランダムな行動を取る傾向にあります。犬は観測した特定の状況を行動やごほうび (報酬) と関連付けようとするため、与えられた指示が「おすわり」であっても、ローリングなど別の行動を取る場合があります。","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"KvptEbf5VW"}],"key":"nXykqFc1jh"}],"key":"zUQydNObo4"},{"type":"listItem","spread":true,"position":{"start":{"line":13,"column":1},"end":{"line":14,"column":1}},"children":[{"type":"paragraph","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"text","value":"犬の立場から見ると、すべての合図に正しく反応して、おやつをできるだけ多くもらえるような状況が理想的です。","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"xKZz5lxO4b"}],"key":"gZSdPpenLh"}],"key":"otkTC0y2qN"}],"key":"GtP728JzdM"},{"type":"paragraph","position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"children":[{"type":"text","value":"強化学習のしつけ (学習) とは、犬が何らかのごぼうび (報酬) を最大化する理想的な行動を学習するように、犬の方策を「調整」することを指します。学習が完了すると、犬は飼い主を観察し、獲得した方策によって、その場にふさわしい行動 (「おすわり」と命令されたらおすわりをするなど) が取れるようになります。","position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"key":"Fliz7XAbJB"}],"key":"F3V0r3ICWl"},{"type":"paragraph","position":{"start":{"line":17,"column":1},"end":{"line":17,"column":1}},"children":[{"type":"text","value":"まとめると、強化学習の目的は、与えられた「環境」における価値（あるいは「利益」と呼びます）を最大化するように「エージェント」を学習させます。","position":{"start":{"line":17,"column":1},"end":{"line":17,"column":1}},"key":"g73yXAIB46"}],"key":"jZdaUYELzW"}],"key":"L7kSTKY4Dl"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"RLHF（Reinforcement Learning from Human Feedback）","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"tjx4myBqSb"}],"identifier":"rlhf-reinforcement-learning-from-human-feedback","label":"RLHF（Reinforcement Learning from Human Feedback）","html_id":"rlhf-reinforcement-learning-from-human-feedback","implicit":true,"enumerator":"5","key":"B1ZHlD3kez"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"RLHFの役割は、人間の好みや意図といった「人間の価値基準」がAIモデルに反映されることになります。具体的には、あるプロンプトに対してAIが生成した応答文の良し悪しを人間がランク付けし、そのランク付されたデータセットを使って「より望ましい応答文とはどんな感じの文章なのか」を評価できる報酬モデルを作成するわけです。","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"i98QQFpRED"}],"key":"uj9cbyWfSU"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":5,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"教師あり学習で既存の言語モデルをfine-tuning","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"TKONjxPdc4"}],"key":"wyPpmNfWc5"}],"key":"CmnBonuwz0"},{"type":"listItem","spread":true,"position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"指示チューニング済みモデルが出力したテキストに対して人手で優劣に順位付けします","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"yWrd1nOYvt"}],"key":"s8FZIk56zr"}],"key":"x5baTJe420"},{"type":"listItem","spread":true,"position":{"start":{"line":7,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"このデータセットを使って、報酬(テキストの優劣を反映したスカラー値)を予測する報酬モデルを学習します","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"bhcd94Imjb"}],"key":"MVeBgIXQLQ"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":8,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":8,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"報酬モデルをこのように学習させることで、似たようなプロンプトが与えられた時に、より望ましいと評価された応答（＝よりランクが高かった応答）に近い応答文が、より報酬が高くなります。結果として、より好ましいと評価された応答文に近い応答文が生成される確率が高まります。","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"key":"ZfdIyE7edv"}],"key":"nLUezQIKf9"}],"key":"VpeJRkzcZW"}],"key":"MZiylpG3gA"}],"key":"v3kQBMmAvy"}],"key":"gBUhTkLipj"},{"type":"image","url":"/build/rlhf-799f02a1517ba0cc2ca803959684c685.png","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"qFkePFpnYE","urlSource":"./Figure/rlhf.png"},{"type":"image","url":"/build/rlhf-performance-73858c45a8e87668ecaab06fcb89bf36.png","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"g1LBuHZtni","urlSource":"./Figure/rlhf-performance.png"}],"key":"klNLn6GNey"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Parameter efficient fine-tuning (PEFT)","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"tNKzH3jmjS"}],"identifier":"parameter-efficient-fine-tuning-peft","label":"Parameter efficient fine-tuning (PEFT)","html_id":"parameter-efficient-fine-tuning-peft","implicit":true,"enumerator":"6","key":"aMSBbUFeBK"},{"type":"image","url":"/build/PEFT-99de48d7f1515ab4df9c25e6375bb4f1.png","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"V9Kwqp9fwW","urlSource":"./Figure/PEFT.png"}],"key":"GKRkU5OHh3"},{"type":"block","kind":"notebook-content","children":[{"type":"table","position":{"start":{"line":1,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"tableRow","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"tableCell","header":true,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"項目","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"avl45PfdTt"}],"key":"sIUhbUhunw"}],"key":"oAPvqa1QET"},{"type":"tableCell","header":true,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"PEFT","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"NEWtoeHJTP"}],"key":"aLXFo7xi0o"}],"key":"AqBHFUbMwK"},{"type":"tableCell","header":true,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Full Fine-Tuning","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"z9AILoSl8t"}],"key":"ce7M4ee68L"}],"key":"XZtuArCMbG"}],"key":"PrsxbaKc2r"},{"type":"tableRow","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"tableCell","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"strong","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"概要","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"QJGlS1lbC9"}],"key":"YwRyLC9ZMP"}],"key":"auRO0qBU8U"},{"type":"tableCell","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"モデル全体ではなく、追加的に設定したパラメータや、一部のパラメータを微調整する手法。","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"T1KsLlgpbV"}],"key":"vt6wciGApZ"},{"type":"tableCell","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"事前学習済みモデル全体のパラメータを調整する手法。","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"aWuJwXYRyh"}],"key":"J1yPjdJIcb"}],"key":"NXuLNQRez4"},{"type":"tableRow","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"tableCell","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"strong","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"text","value":"計算リソース","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"DzINzyyhBD"}],"key":"PxwAALlXXu"}],"key":"r8kUJkYGss"},{"type":"tableCell","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"text","value":"大幅に削減可能。学習中に更新するパラメータ数が少ないため、GPUメモリや計算時間が少なくて済む。 (例　GPT-3 LoRA: 350GB)","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"YVzj9DDibS"}],"key":"jA3Nx1oyeX"},{"type":"tableCell","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"text","value":"多大な計算リソースを必要とする。全パラメータを更新するため、GPUメモリや計算負荷が高い (例　GPT-3: 1.2TB)。","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"aUrVk3Rywl"}],"key":"FOY4X59WrL"}],"key":"jdDh6fFeoR"},{"type":"tableRow","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"tableCell","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"strong","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"保存領域","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"YTEjVaCk3O"}],"key":"BffbVHN42n"}],"key":"hjzhnhda9n"},{"type":"tableCell","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"調整されたパラメータのみを保存するため、小さな保存領域で十分(例　GPT-3 LoRA: 35MB)。","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"POnyUIIT2X"}],"key":"KB6a4pPClV"},{"type":"tableCell","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"微調整後のモデル全体を保存する必要があるため、元モデルと同じサイズの保存領域は必要(例　GPT-3: 350GB)。","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"WNxqd9PieT"}],"key":"VeOTmGWE5x"}],"key":"REwCWNNL6E"}],"key":"LKM9bHmQoz"}],"key":"sO3qJW9C3z"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"PEFTの手法は主に以下の2つのカテゴリに分類できます：","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"xXhLjjpSBv"}],"key":"ktXan7hWnU"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":3,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":3,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Reparameterization: モデルの一部パラメータを特定の形式に再構成し、その部分のみを微調整します。","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"B762r8Vf0A"}],"key":"fUN7fFLABz"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"LoRA (Low-Rank Adaptation)","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"I9FmbEowHg"}],"key":"lw5xTDvfTi"}],"key":"vgkAFo2D3G"}],"key":"jslxguxMSl"}],"key":"mvWCHfJglg"},{"type":"listItem","spread":true,"position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Additive: 元のモデルのパラメータに加算的に新しいパラメータ（追加の層など）を導入して微調整を行います。","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"G4C8eX6RGy"}],"key":"cDHfrI3os4"}],"key":"QtGDXtvWPO"},{"type":"listItem","spread":true,"position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Soft Prompt: 入力系列にタスクごとのベクトル(Soft Prompt)を付加し、学習を行います。","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"lPeoMhN8rk"}],"key":"F6cILg97dh"}],"key":"eiQF4OejaP"}],"key":"KqurUB7vY1"}],"key":"ydVi81jke0"},{"type":"block","kind":"notebook-content","children":[{"type":"image","url":"/build/PEFT_method-3d50c0b8874b94ca76d8d888f3213641.png","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"TMplLM1K1D","urlSource":"./Figure/PEFT_method.png"}],"key":"whrGQJMWCS"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"LoRA (Low-Rank Adaptation)","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"qsRaRVOBXO"}],"identifier":"lora-low-rank-adaptation","label":"LoRA (Low-Rank Adaptation)","html_id":"lora-low-rank-adaptation","implicit":true,"enumerator":"6.1","key":"BaLx0dFdEe"},{"type":"paragraph","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"text","value":"LoRA は、モデル内の大きな重み行列（例: 全結合層や注意メカニズムの線形変換）を低ランクの行列に分解し、その一部だけを学習することで、計算コストとストレージを大幅に削減します。","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"J4RYvVZkzP"}],"key":"iEfDYbA3h5"},{"type":"paragraph","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"inlineMath","value":"W + \\Delta W = W + AB","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>W</mi><mo>+</mo><mi mathvariant=\"normal\">Δ</mi><mi>W</mi><mo>=</mo><mi>W</mi><mo>+</mo><mi>A</mi><mi>B</mi></mrow><annotation encoding=\"application/x-tex\">W + \\Delta W = W + AB</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.7667em;vertical-align:-0.0833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">W</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord\">Δ</span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">W</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.7667em;vertical-align:-0.0833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">W</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\">A</span><span class=\"mord mathnormal\" style=\"margin-right:0.05017em;\">B</span></span></span></span>","key":"M8yOB00Cg2"}],"key":"ghiqTWujk5"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":8,"column":1},"end":{"line":10,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"paragraph","children":[{"type":"inlineMath","value":"A \\in \\mathbb{R}^{d \\times r}, \\, B \\in \\mathbb{R}^{r \\times k}","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>A</mi><mo>∈</mo><msup><mi mathvariant=\"double-struck\">R</mi><mrow><mi>d</mi><mo>×</mo><mi>r</mi></mrow></msup><mo separator=\"true\">,</mo><mtext> </mtext><mi>B</mi><mo>∈</mo><msup><mi mathvariant=\"double-struck\">R</mi><mrow><mi>r</mi><mo>×</mo><mi>k</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">A \\in \\mathbb{R}^{d \\times r}, \\, B \\in \\mathbb{R}^{r \\times k}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.7224em;vertical-align:-0.0391em;\"></span><span class=\"mord mathnormal\">A</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">∈</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.0435em;vertical-align:-0.1944em;\"></span><span class=\"mord\"><span class=\"mord mathbb\">R</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8491em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">d</span><span class=\"mbin mtight\">×</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.02778em;\">r</span></span></span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05017em;\">B</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">∈</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8491em;\"></span><span class=\"mord\"><span class=\"mord mathbb\">R</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8491em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.02778em;\">r</span><span class=\"mbin mtight\">×</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.03148em;\">k</span></span></span></span></span></span></span></span></span></span></span></span>","key":"kZesaRX5Ol"},{"type":"text","value":" は学習可能な低ランク行列。","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"key":"BkyKimJefe"}],"key":"gcKitshHsg"}],"key":"als7N6NYtw"},{"type":"listItem","spread":true,"position":{"start":{"line":9,"column":1},"end":{"line":10,"column":1}},"children":[{"type":"paragraph","children":[{"type":"inlineMath","value":"r","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>r</mi></mrow><annotation encoding=\"application/x-tex\">r</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">r</span></span></span></span>","key":"YzRM2dhWIY"},{"type":"text","value":" はランクで、通常は ","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"D4c8qIgyYc"},{"type":"inlineMath","value":"r \\ll \\min(d, k)","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>r</mi><mo>≪</mo><mi>min</mi><mo>⁡</mo><mo stretchy=\"false\">(</mo><mi>d</mi><mo separator=\"true\">,</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">r \\ll \\min(d, k)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.5782em;vertical-align:-0.0391em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">r</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">≪</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mop\">min</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">d</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03148em;\">k</span><span class=\"mclose\">)</span></span></span></span>","key":"Nkzvl3jT29"},{"type":"text","value":"。","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"jo767wWeWO"}],"key":"qINSodqr2X"}],"key":"hbr419vKuR"}],"key":"svoig0lz0L"},{"type":"paragraph","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"text","value":"この近似により、学習すべきパラメータの総数は元の ","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"Ec5XU25RYX"},{"type":"inlineMath","value":"W","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>W</mi></mrow><annotation encoding=\"application/x-tex\">W</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">W</span></span></span></span>","key":"m7FqPZfu08"},{"type":"text","value":" の ","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"QkpBcJQqvb"},{"type":"inlineMath","value":"\\mathcal{O}(d \\cdot k)","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi mathvariant=\"script\">O</mi><mo stretchy=\"false\">(</mo><mi>d</mi><mo>⋅</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\mathcal{O}(d \\cdot k)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathcal\" style=\"margin-right:0.02778em;\">O</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">d</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">⋅</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03148em;\">k</span><span class=\"mclose\">)</span></span></span></span>","key":"WtJVctIjCR"},{"type":"text","value":" から、低ランク部分のみの ","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"Y3w14sbdzr"},{"type":"inlineMath","value":"\\mathcal{O}(r \\cdot (d + k))","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi mathvariant=\"script\">O</mi><mo stretchy=\"false\">(</mo><mi>r</mi><mo>⋅</mo><mo stretchy=\"false\">(</mo><mi>d</mi><mo>+</mo><mi>k</mi><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\mathcal{O}(r \\cdot (d + k))</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathcal\" style=\"margin-right:0.02778em;\">O</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">r</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">⋅</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">d</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03148em;\">k</span><span class=\"mclose\">))</span></span></span></span>","key":"ilIxweTIfx"},{"type":"text","value":" に大幅に削減されます。","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"HPmlYZ8kXx"}],"key":"EbUnekTL28"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":12,"column":1},"end":{"line":14,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Transformerのパラメータ数 ","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"fNcNDFzyVH"},{"type":"inlineMath","value":"512 \\times 64 = 32768","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>512</mn><mo>×</mo><mn>64</mn><mo>=</mo><mn>32768</mn></mrow><annotation encoding=\"application/x-tex\">512 \\times 64 = 32768</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.7278em;vertical-align:-0.0833em;\"></span><span class=\"mord\">512</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em;\"></span><span class=\"mord\">64</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em;\"></span><span class=\"mord\">32768</span></span></span></span>","key":"DXYKOgrglz"}],"key":"xOKx70xi8f"}],"key":"kSgGpUnL2N"},{"type":"listItem","spread":true,"position":{"start":{"line":13,"column":1},"end":{"line":14,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"LoRAのパラメータ数","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"iWie6SWvG5"},{"type":"inlineMath","value":"d \\times r = 512 \\times 8 = 4096 (r=8)","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>d</mi><mo>×</mo><mi>r</mi><mo>=</mo><mn>512</mn><mo>×</mo><mn>8</mn><mo>=</mo><mn>4096</mn><mo stretchy=\"false\">(</mo><mi>r</mi><mo>=</mo><mn>8</mn><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">d \\times r = 512 \\times 8 = 4096 (r=8)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.7778em;vertical-align:-0.0833em;\"></span><span class=\"mord mathnormal\">d</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">r</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.7278em;vertical-align:-0.0833em;\"></span><span class=\"mord\">512</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em;\"></span><span class=\"mord\">8</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\">4096</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">r</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\">8</span><span class=\"mclose\">)</span></span></span></span>","key":"Vmu1vYy5nw"}],"key":"gV77hbjbBH"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"children":[{"type":"paragraph","children":[{"type":"inlineMath","value":"86 \\%","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>86</mn><mi mathvariant=\"normal\">%</mi></mrow><annotation encoding=\"application/x-tex\">86 \\%</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8056em;vertical-align:-0.0556em;\"></span><span class=\"mord\">86%</span></span></span></span>","key":"XKytuiK7Qd"},{"type":"text","value":"削減","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"key":"Wz3iYy9GVW"}],"key":"v2HcusCRM5"}],"key":"mu3c7JnfH7"}],"key":"J3Ft2cHkxa"}],"key":"EYmXfvUYBV"}],"key":"vxUOZ08qM5"}],"key":"vRg9GD0Qw4"},{"type":"block","kind":"notebook-content","children":[{"type":"image","url":"/build/lora_overview-e9c3f7498b7537cab5fa8931814013ba.png","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"uISRZlcD3k","urlSource":"./Figure/lora_overview.png"}],"key":"g17HsTbeOe"},{"type":"block","kind":"notebook-content","children":[{"type":"image","url":"/build/lora_calculation-e4c86dc99c5b589495059f01187bc3ee.png","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ntyLb7LAAK","urlSource":"./Figure/lora_calculation.png"}],"key":"LxdTRR3NgX"},{"type":"block","kind":"notebook-content","children":[{"type":"image","url":"/build/lora_performance-3d4cd142d775bfcd32a7c9a5cab6a81a.png","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"s4OGY9oriN","urlSource":"./Figure/lora_performance.png"}],"key":"ngMvzb2paW"}],"key":"xGmx0X7cUq"},"references":{"cite":{"order":[],"data":{}}},"footer":{"navigation":{"prev":{"title":"GPT","url":"/gpt","group":"大規模言語モデル"},"next":{"title":"LLMSの応用(1)：Langchainの基礎","url":"/langchain-basic","group":"大規模言語モデル"}}},"domain":"http://localhost:3000"}