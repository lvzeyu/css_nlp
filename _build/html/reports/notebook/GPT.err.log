Traceback (most recent call last):
  File "/Users/ryozawau/anaconda3/envs/jupyterbook/lib/python3.12/site-packages/jupyter_cache/executors/utils.py", line 58, in single_nb_execution
    executenb(
  File "/Users/ryozawau/anaconda3/envs/jupyterbook/lib/python3.12/site-packages/nbclient/client.py", line 1305, in execute
    return NotebookClient(nb=nb, resources=resources, km=km, **kwargs).execute()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/ryozawau/anaconda3/envs/jupyterbook/lib/python3.12/site-packages/jupyter_core/utils/__init__.py", line 165, in wrapped
    return loop.run_until_complete(inner)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/ryozawau/anaconda3/envs/jupyterbook/lib/python3.12/asyncio/base_events.py", line 684, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/Users/ryozawau/anaconda3/envs/jupyterbook/lib/python3.12/site-packages/nbclient/client.py", line 705, in async_execute
    await self.async_execute_cell(
  File "/Users/ryozawau/anaconda3/envs/jupyterbook/lib/python3.12/site-packages/nbclient/client.py", line 1058, in async_execute_cell
    await self._check_raise_for_error(cell, cell_index, exec_reply)
  File "/Users/ryozawau/anaconda3/envs/jupyterbook/lib/python3.12/site-packages/nbclient/client.py", line 914, in _check_raise_for_error
    raise CellExecutionError.from_cell_and_msg(cell, exec_reply_content)
nbclient.exceptions.CellExecutionError: An error occurred while executing the following cell:
------------------
from transformers import pipeline
#!pip install sentencepiece
#!pip install protobuf
generator = pipeline("text-generation", model="abeja/gpt2-large-japanese")

------------------

----- stderr -----
/Users/ryozawau/anaconda3/envs/jupyterbook/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html
  from .autonotebook import tqdm as notebook_tqdm
----- stderr -----
/Users/ryozawau/anaconda3/envs/jupyterbook/lib/python3.12/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
----- stderr -----
/Users/ryozawau/anaconda3/envs/jupyterbook/lib/python3.12/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
----- stderr -----
/Users/ryozawau/anaconda3/envs/jupyterbook/lib/python3.12/site-packages/transformers/modeling_utils.py:519: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  return torch.load(checkpoint_file, map_location=map_location)
------------------

[0;31m---------------------------------------------------------------------------[0m
[0;31mValueError[0m                                Traceback (most recent call last)
Cell [0;32mIn[1], line 4[0m
[1;32m      1[0m [38;5;28;01mfrom[39;00m [38;5;21;01mtransformers[39;00m [38;5;28;01mimport[39;00m pipeline
[1;32m      2[0m [38;5;66;03m#!pip install sentencepiece[39;00m
[1;32m      3[0m [38;5;66;03m#!pip install protobuf[39;00m
[0;32m----> 4[0m generator [38;5;241m=[39m [43mpipeline[49m[43m([49m[38;5;124;43m"[39;49m[38;5;124;43mtext-generation[39;49m[38;5;124;43m"[39;49m[43m,[49m[43m [49m[43mmodel[49m[38;5;241;43m=[39;49m[38;5;124;43m"[39;49m[38;5;124;43mabeja/gpt2-large-japanese[39;49m[38;5;124;43m"[39;49m[43m)[49m

File [0;32m~/anaconda3/envs/jupyterbook/lib/python3.12/site-packages/transformers/pipelines/__init__.py:967[0m, in [0;36mpipeline[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)[0m
[1;32m    964[0m             tokenizer_kwargs [38;5;241m=[39m model_kwargs[38;5;241m.[39mcopy()
[1;32m    965[0m             tokenizer_kwargs[38;5;241m.[39mpop([38;5;124m"[39m[38;5;124mtorch_dtype[39m[38;5;124m"[39m, [38;5;28;01mNone[39;00m)
[0;32m--> 967[0m         tokenizer [38;5;241m=[39m [43mAutoTokenizer[49m[38;5;241;43m.[39;49m[43mfrom_pretrained[49m[43m([49m
[1;32m    968[0m [43m            [49m[43mtokenizer_identifier[49m[43m,[49m[43m [49m[43muse_fast[49m[38;5;241;43m=[39;49m[43muse_fast[49m[43m,[49m[43m [49m[43m_from_pipeline[49m[38;5;241;43m=[39;49m[43mtask[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mhub_kwargs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mtokenizer_kwargs[49m
[1;32m    969[0m [43m        [49m[43m)[49m
[1;32m    971[0m [38;5;28;01mif[39;00m load_image_processor:
[1;32m    972[0m     [38;5;66;03m# Try to infer image processor from model or config name (if provided as str)[39;00m
[1;32m    973[0m     [38;5;28;01mif[39;00m image_processor [38;5;129;01mis[39;00m [38;5;28;01mNone[39;00m:

File [0;32m~/anaconda3/envs/jupyterbook/lib/python3.12/site-packages/transformers/models/auto/tokenization_auto.py:787[0m, in [0;36mAutoTokenizer.from_pretrained[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)[0m
[1;32m    783[0m     [38;5;28;01mif[39;00m tokenizer_class [38;5;129;01mis[39;00m [38;5;28;01mNone[39;00m:
[1;32m    784[0m         [38;5;28;01mraise[39;00m [38;5;167;01mValueError[39;00m(
[1;32m    785[0m             [38;5;124mf[39m[38;5;124m"[39m[38;5;124mTokenizer class [39m[38;5;132;01m{[39;00mtokenizer_class_candidate[38;5;132;01m}[39;00m[38;5;124m does not exist or is not currently imported.[39m[38;5;124m"[39m
[1;32m    786[0m         )
[0;32m--> 787[0m     [38;5;28;01mreturn[39;00m [43mtokenizer_class[49m[38;5;241;43m.[39;49m[43mfrom_pretrained[49m[43m([49m[43mpretrained_model_name_or_path[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[43minputs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m
[1;32m    789[0m [38;5;66;03m# Otherwise we have to be creative.[39;00m
[1;32m    790[0m [38;5;66;03m# if model is an encoder decoder, the encoder tokenizer class is used by default[39;00m
[1;32m    791[0m [38;5;28;01mif[39;00m [38;5;28misinstance[39m(config, EncoderDecoderConfig):

File [0;32m~/anaconda3/envs/jupyterbook/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2028[0m, in [0;36mPreTrainedTokenizerBase.from_pretrained[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, *init_inputs, **kwargs)[0m
[1;32m   2025[0m     [38;5;28;01melse[39;00m:
[1;32m   2026[0m         logger[38;5;241m.[39minfo([38;5;124mf[39m[38;5;124m"[39m[38;5;124mloading file [39m[38;5;132;01m{[39;00mfile_path[38;5;132;01m}[39;00m[38;5;124m from cache at [39m[38;5;132;01m{[39;00mresolved_vocab_files[file_id][38;5;132;01m}[39;00m[38;5;124m"[39m)
[0;32m-> 2028[0m [38;5;28;01mreturn[39;00m [38;5;28;43mcls[39;49m[38;5;241;43m.[39;49m[43m_from_pretrained[49m[43m([49m
[1;32m   2029[0m [43m    [49m[43mresolved_vocab_files[49m[43m,[49m
[1;32m   2030[0m [43m    [49m[43mpretrained_model_name_or_path[49m[43m,[49m
[1;32m   2031[0m [43m    [49m[43minit_configuration[49m[43m,[49m
[1;32m   2032[0m [43m    [49m[38;5;241;43m*[39;49m[43minit_inputs[49m[43m,[49m
[1;32m   2033[0m [43m    [49m[43mtoken[49m[38;5;241;43m=[39;49m[43mtoken[49m[43m,[49m
[1;32m   2034[0m [43m    [49m[43mcache_dir[49m[38;5;241;43m=[39;49m[43mcache_dir[49m[43m,[49m
[1;32m   2035[0m [43m    [49m[43mlocal_files_only[49m[38;5;241;43m=[39;49m[43mlocal_files_only[49m[43m,[49m
[1;32m   2036[0m [43m    [49m[43m_commit_hash[49m[38;5;241;43m=[39;49m[43mcommit_hash[49m[43m,[49m
[1;32m   2037[0m [43m    [49m[43m_is_local[49m[38;5;241;43m=[39;49m[43mis_local[49m[43m,[49m
[1;32m   2038[0m [43m    [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m,[49m
[1;32m   2039[0m [43m[49m[43m)[49m

File [0;32m~/anaconda3/envs/jupyterbook/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2260[0m, in [0;36mPreTrainedTokenizerBase._from_pretrained[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, *init_inputs, **kwargs)[0m
[1;32m   2258[0m [38;5;66;03m# Instantiate the tokenizer.[39;00m
[1;32m   2259[0m [38;5;28;01mtry[39;00m:
[0;32m-> 2260[0m     tokenizer [38;5;241m=[39m [38;5;28;43mcls[39;49m[43m([49m[38;5;241;43m*[39;49m[43minit_inputs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43minit_kwargs[49m[43m)[49m
[1;32m   2261[0m [38;5;28;01mexcept[39;00m [38;5;167;01mOSError[39;00m:
[1;32m   2262[0m     [38;5;28;01mraise[39;00m [38;5;167;01mOSError[39;00m(
[1;32m   2263[0m         [38;5;124m"[39m[38;5;124mUnable to load vocabulary from file. [39m[38;5;124m"[39m
[1;32m   2264[0m         [38;5;124m"[39m[38;5;124mPlease check that the provided vocabulary is accessible and not corrupted.[39m[38;5;124m"[39m
[1;32m   2265[0m     )

File [0;32m~/anaconda3/envs/jupyterbook/lib/python3.12/site-packages/transformers/models/t5/tokenization_t5_fast.py:135[0m, in [0;36mT5TokenizerFast.__init__[0;34m(self, vocab_file, tokenizer_file, eos_token, unk_token, pad_token, extra_ids, additional_special_tokens, **kwargs)[0m
[1;32m    132[0m     extra_tokens [38;5;241m=[39m [[38;5;124mf[39m[38;5;124m"[39m[38;5;124m<extra_id_[39m[38;5;132;01m{[39;00mi[38;5;132;01m}[39;00m[38;5;124m>[39m[38;5;124m"[39m [38;5;28;01mfor[39;00m i [38;5;129;01min[39;00m [38;5;28mrange[39m(extra_ids)]
[1;32m    133[0m     additional_special_tokens [38;5;241m=[39m extra_tokens
[0;32m--> 135[0m [38;5;28;43msuper[39;49m[43m([49m[43m)[49m[38;5;241;43m.[39;49m[38;5;21;43m__init__[39;49m[43m([49m
[1;32m    136[0m [43m    [49m[43mvocab_file[49m[43m,[49m
[1;32m    137[0m [43m    [49m[43mtokenizer_file[49m[38;5;241;43m=[39;49m[43mtokenizer_file[49m[43m,[49m
[1;32m    138[0m [43m    [49m[43meos_token[49m[38;5;241;43m=[39;49m[43meos_token[49m[43m,[49m
[1;32m    139[0m [43m    [49m[43munk_token[49m[38;5;241;43m=[39;49m[43munk_token[49m[43m,[49m
[1;32m    140[0m [43m    [49m[43mpad_token[49m[38;5;241;43m=[39;49m[43mpad_token[49m[43m,[49m
[1;32m    141[0m [43m    [49m[43mextra_ids[49m[38;5;241;43m=[39;49m[43mextra_ids[49m[43m,[49m
[1;32m    142[0m [43m    [49m[43madditional_special_tokens[49m[38;5;241;43m=[39;49m[43madditional_special_tokens[49m[43m,[49m
[1;32m    143[0m [43m    [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m,[49m
[1;32m    144[0m [43m[49m[43m)[49m
[1;32m    146[0m [38;5;28mself[39m[38;5;241m.[39mvocab_file [38;5;241m=[39m vocab_file
[1;32m    147[0m [38;5;28mself[39m[38;5;241m.[39m_extra_ids [38;5;241m=[39m extra_ids

File [0;32m~/anaconda3/envs/jupyterbook/lib/python3.12/site-packages/transformers/tokenization_utils_fast.py:120[0m, in [0;36mPreTrainedTokenizerFast.__init__[0;34m(self, *args, **kwargs)[0m
[1;32m    118[0m     fast_tokenizer [38;5;241m=[39m convert_slow_tokenizer(slow_tokenizer)
[1;32m    119[0m [38;5;28;01melse[39;00m:
[0;32m--> 120[0m     [38;5;28;01mraise[39;00m [38;5;167;01mValueError[39;00m(
[1;32m    121[0m         [38;5;124m"[39m[38;5;124mCouldn[39m[38;5;124m'[39m[38;5;124mt instantiate the backend tokenizer from one of: [39m[38;5;130;01m\n[39;00m[38;5;124m"[39m
[1;32m    122[0m         [38;5;124m"[39m[38;5;124m(1) a `tokenizers` library serialization file, [39m[38;5;130;01m\n[39;00m[38;5;124m"[39m
[1;32m    123[0m         [38;5;124m"[39m[38;5;124m(2) a slow tokenizer instance to convert or [39m[38;5;130;01m\n[39;00m[38;5;124m"[39m
[1;32m    124[0m         [38;5;124m"[39m[38;5;124m(3) an equivalent slow tokenizer class to instantiate and convert. [39m[38;5;130;01m\n[39;00m[38;5;124m"[39m
[1;32m    125[0m         [38;5;124m"[39m[38;5;124mYou need to have sentencepiece installed to convert a slow tokenizer to a fast one.[39m[38;5;124m"[39m
[1;32m    126[0m     )
[1;32m    128[0m [38;5;28mself[39m[38;5;241m.[39m_tokenizer [38;5;241m=[39m fast_tokenizer
[1;32m    130[0m [38;5;28;01mif[39;00m slow_tokenizer [38;5;129;01mis[39;00m [38;5;129;01mnot[39;00m [38;5;28;01mNone[39;00m:

[0;31mValueError[0m: Couldn't instantiate the backend tokenizer from one of: 
(1) a `tokenizers` library serialization file, 
(2) a slow tokenizer instance to convert or 
(3) an equivalent slow tokenizer class to instantiate and convert. 
You need to have sentencepiece installed to convert a slow tokenizer to a fast one.

