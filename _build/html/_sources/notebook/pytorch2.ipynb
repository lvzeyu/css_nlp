{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[PyTorch](https://pytorch.org/)はPythonのオープンソースの機械学習・深層学習ライブラリです。\n",
    "\n",
    "- 柔軟性を重視した設計であり、さらに、機械学習・深層学習モデルをPythonの慣用的なクラスや関数の取り扱い方で実装できるようになっています。\n",
    "- GPUを使用した計算をサポートしますので、CPU上で同じ計算を行う場合に比べて、数十倍の高速化を実現します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install torch torchvision torchaudio\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## テンソル\n",
    "\n",
    "深層学習モデルは通常、入力から出力にどのようにマッピングされるのかを対応つけるデータ構造を表します。一般的に、このようなある形式のデータから別の形式への変換は膨大な浮動小数点数の計算を通じて実現されています。\n",
    "\n",
    "データを浮動小数点数を扱うためには、Pytorchは基本的なデータ構造として「テンソル」を導入しています。\n",
    "\n",
    "深層学習の文脈でのテンソルとは、ベクトルや行列を任意の次元数に一般化したものを指します。つまり、多次元配列を扱います。\n",
    "\n",
    "```{margin}\n",
    "Tensorとの同じように、NumPyも多次元配列を扱えます。ただ、PyTorchにおいてテンソルはGPU上でも使用できるため、処理速度の向上させることも可能です。\n",
    "```\n",
    "\n",
    "![](/Users/ryozawau/css_nlp/notebook/Figure/tensor.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### テンソルの操作\n",
    "\n",
    "#### テンソルの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1312, 0.3180, 0.4280],\n",
      "        [0.9342, 0.4189, 0.7771],\n",
      "        [0.3704, 0.2448, 0.4078],\n",
      "        [0.9934, 0.3880, 0.9116],\n",
      "        [0.5787, 0.7577, 0.9173]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.5000, 3.0000])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([5.5, 3])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### テンソル要素の型\n",
    "\n",
    "テンソル要素の型は、引数に適切な```dtype```を渡すことで指定します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "double_points = torch.ones(10, 2, dtype=torch.double)\n",
    "short_points = torch.tensor([[1, 2], [3, 4]], dtype=torch.short)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### テンソルの操作（変形・変換等）\n",
    "\n",
    "PyTorchにはテンソルに対する[操作（変形・演算など）](https://torch7.readthedocs.io/en/rtd/maths/index.html)が多く用意されています。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2157, 0.9833, 1.1066],\n",
      "        [1.6697, 1.0484, 0.9558],\n",
      "        [1.4207, 0.8761, 0.4021],\n",
      "        [1.1880, 1.2514, 0.4395],\n",
      "        [1.1720, 0.5105, 0.6646]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(5, 3)\n",
    "y = torch.rand(5, 3)\n",
    "print(x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2157, 0.9833, 1.1066],\n",
      "        [1.6697, 1.0484, 0.9558],\n",
      "        [1.4207, 0.8761, 0.4021],\n",
      "        [1.1880, 1.2514, 0.4395],\n",
      "        [1.1720, 0.5105, 0.6646]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.add(x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### テンソルの一部指定や取り出し(Indexing)\n",
    "\n",
    "Pytorchテンソルは、Numpyや他のPythonの科学計算ライブラリーと同じく、テンソルの次元ごとのレンジインデックス記法で一部指定や取り出しを行えます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6081, 0.3532, 0.1895],\n",
       "        [0.2815, 0.4500, 0.5554]])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[3:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9075, 0.9293, 0.6081, 0.2815])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[1:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### テンソルの微分機能\n",
    "\n",
    "PyTorchテンソルは、テンソルに対して実行された計算を追跡し、計算結果の出力テンソルの微分を、各テンソルの要素に対して解析的に計算することができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.)\n"
     ]
    }
   ],
   "source": [
    "# 微分を計算するためのテンソルを作成\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "# 関数の定義\n",
    "y = x ** 2\n",
    "# 勾配を計算\n",
    "y.backward()\n",
    "\n",
    "# 勾配の値を表示\n",
    "print(x.grad)  # 4.0 (これは2*xの値、x=2のとき)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CUDA Tensors（CUDA テンソル）\n",
    "\n",
    "tensorは ```.to``` メソッドを使用することであらゆるデバイス上のメモリへと移動させることができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # a CUDA device object\n",
    "    y = torch.ones_like(x, device=device)  # directly create a tensor on GPU\n",
    "    x = x.to(device)                       # or just use strings ``.to(\"cuda\")``\n",
    "    z = x + y\n",
    "    print(z)\n",
    "    print(z.to(\"cpu\", torch.double))       # ``.to`` can also change dtype together!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習データセットの作成\n",
    "\n",
    "### ```Dataset```クラス\n",
    "\n",
    "\n",
    "\n",
    "Pytorchで深層学習を実装する際には、特徴量行列とラベルを```Dataset```というクラスに渡して、特徴量行列とラベルを一つのデータベース的なものにまとめる働きをします。\n",
    "\n",
    "一般的には、PyTorchの```torch.utils.data.Dataset```クラスを継承して定義します。以下に示すメソッドを定義するように指定されています。\n",
    "\n",
    "- ```__init__(self)```: 初期実行関数です。Datasetを定義する際に必要な情報を受け取ります。\n",
    "- ```__len__(self)```: データ全体の数を返す関数です。\n",
    "- ```__getitem__(self, index)```: 指定されたindexに対応するデータと正解ラベル(ターゲット)を返します。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        # データセットのサイズを返す\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # 指定されたインデックスのデータとラベルを返す\n",
    "        return self.data[index], self.labels[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# サンプルデータ\n",
    "data = torch.tensor([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
    "labels = torch.tensor([0, 1, 0])\n",
    "\n",
    "# カスタムデータセットのインスタンスを作成\n",
    "dataset = CustomDataset(data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 2.]), tensor(0))"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{margin}\n",
    "データをPyTorchテンソルに変換する前に、なんらかの加工を加えたい場合もあります。特に画像データに足して、```transforms```は色々な前処理をサポートします。\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```DataLoader```クラス\n",
    "\n",
    "機械学習のトレーニングには、データセットのサンプリング、シャッフル、バッチ分割などの操作が必要されます。これらの操作を効率化にするために```DataLoader```クラスが用意されます。\n",
    "\n",
    "- バッチ処理: 指定したバッチサイズでデータを分割します\n",
    "- シャッフル: データの順序をランダムに並べ替えられます"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# サンプルデータ\n",
    "data = torch.randn(100, 3)  # 100個のデータ、3つの特徴\n",
    "labels = torch.randint(0, 2, (100,))  # 100個のラベル (0または1)\n",
    "dataset = CustomDataset(data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x317741fa0>"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch data shape: torch.Size([10, 3])\n",
      "Batch labels shape: torch.Size([10])\n",
      "Batch data shape: torch.Size([10, 3])\n",
      "Batch labels shape: torch.Size([10])\n",
      "Batch data shape: torch.Size([10, 3])\n",
      "Batch labels shape: torch.Size([10])\n",
      "Batch data shape: torch.Size([10, 3])\n",
      "Batch labels shape: torch.Size([10])\n",
      "Batch data shape: torch.Size([10, 3])\n",
      "Batch labels shape: torch.Size([10])\n",
      "Batch data shape: torch.Size([10, 3])\n",
      "Batch labels shape: torch.Size([10])\n",
      "Batch data shape: torch.Size([10, 3])\n",
      "Batch labels shape: torch.Size([10])\n",
      "Batch data shape: torch.Size([10, 3])\n",
      "Batch labels shape: torch.Size([10])\n",
      "Batch data shape: torch.Size([10, 3])\n",
      "Batch labels shape: torch.Size([10])\n",
      "Batch data shape: torch.Size([10, 3])\n",
      "Batch labels shape: torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for batch_data, batch_labels in dataloader:\n",
    "    print(\"Batch data shape:\", batch_data.shape)\n",
    "    print(\"Batch labels shape:\", batch_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data: tensor([0.2357, 1.0437, 0.4600])\n",
      "Sample label: tensor(0)\n"
     ]
    }
   ],
   "source": [
    "# 最初のバッチを取得\n",
    "data_iter = iter(dataloader)\n",
    "sample_data, sample_label = next(data_iter)\n",
    "\n",
    "# 1つ目のサンプルのデータとラベルを確認\n",
    "print(\"Sample data:\", sample_data[0])\n",
    "print(\"Sample label:\", sample_label[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```DataLoader```は反復処理が可能なので、トレーニング中のループで直接に使用することができます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{note}\n",
    "`iter(dataloader)`は、`DataLoader`オブジェクトからイテレーターを作成します。 イテレーターは順次データを取り出すためのオブジェクトで、`for`ループや`next()`を使用して1つずつデータを取得できます。\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 深層学習モデルの構築"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[`torch.nn`](https://pytorch.org/docs/stable/nn.html)で用意されているクラス、関数は、独自のニューラルネットワークを構築するために必要な要素を網羅しています。\n",
    "\n",
    "PyTorchの全てのモジュールは、[`nn.Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html)を継承しています。\n",
    "\n",
    "\n",
    "そしてニューラルネットワークは、モジュール自体が他のモジュール（レイヤー）から構成されています。\n",
    "\n",
    "この入れ子構造により、複雑なアーキテクチャを容易に構築・管理することができます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### クラスの定義\n",
    "\n",
    "``nn.Module``を継承し、独自のネットワークモデルを定義し、その後ネットワークのレイヤーを ``__init__``で初期化します。\n",
    "\n",
    "``nn.Module`` を継承した全モジュールは、入力データの順伝搬関数である``forward``関数を持ちます。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 3),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このクラスは、PyTorchの```nn.Module```を継承した単純なニューラルネットワークの実装を示しています。入力は固定長の$512$とされており、出力は$3$の次元を持つベクトルです。\n",
    "\n",
    "```{margin}\n",
    "最大長512であるテキストに対して、センチメント(ポジティブ、中立、ネガティブ)を予測するタスクをイメージしてください。\n",
    "```\n",
    "\n",
    "- ```self.linear_relu_stack```: このシーケンシャルな層は、3つの線形層とそれぞれの後に続くReLU活性化関数から構成されています。\n",
    "    - [`nn.Sequential`](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html)にレイヤーを順に渡すだけで、数のレイヤーを順に積み重ねたモデルを簡単に定義できます。\n",
    "    - [`linear layer`](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)は、線形変換を施します。`linear layer`は重みとバイアスのパラメータを保持しています。\n",
    "    - [`nn.ReLU`](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html)という活性化関数を設置することで、ニューラルネットワークの表現力を向上させます。\n",
    "- 順伝播メソッド (```forward```): 入力テンソル```x```を受け取り、ネットワークを通して出力を生成する機能を持ちます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPUの利用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``NeuralNetwork``クラスのインスタンスを作成し、変数``device``上に移動させます。\n",
    "\n",
    "以下でネットワークの構造を出力し確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using {} device'.format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=128, out_features=3, bias=True)\n",
      "    (5): ReLU()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モデルによる計算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ニューラルネットワークの最後のlinear layerは`logits`を出力します。この`logits`は[`nn.Softmax`](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html)モジュールへと渡されます。出力ベクトルの要素の値は$[0, 1]$の範囲となり、これは各クラスである確率を示します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0484, 0.1847, 0.0000],\n",
      "        [0.0601, 0.1881, 0.0000],\n",
      "        [0.0588, 0.0933, 0.0000]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(3, 512, device=device)\n",
    "logits = model(X) \n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: tensor([1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "pred_probab = nn.Softmax(dim=1)(logits)\n",
    "y_pred = pred_probab.argmax(1)\n",
    "print(f\"Predicted class: {y_pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} おまけ：tensorboard\n",
    ":class: tip, dropdown\n",
    "\n",
    "tensorboardでニューラルネットワークの構造を確認する。\n",
    "\n",
    "```python\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "X = torch.rand(3, 28, 28)\n",
    "writer = SummaryWriter(\"torchlogs/\")\n",
    "writer.add_graph(model, X)\n",
    "writer.close()\n",
    "```\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自動微分\n",
    "\n",
    "ニューラルネットワークを訓練する際、その学習アルゴリズムとして、**バックプロパゲーション（back propagation）** がよく使用されます。\n",
    "\n",
    "バックプロパゲーションでは、モデルの重みなどの各パラメータは、損失関数に対するその変数の微分値（勾配）に応じて調整されます。\n",
    "\n",
    "これらの勾配の値を計算するために、PyTorchには``torch.autograd`` という微分エンジンが組み込まれています。\n",
    "\n",
    "autogradはPyTorchの計算グラフに対する勾配の自動計算を支援します。\n",
    "\n",
    "シンプルな1レイヤーのネットワークを想定しましょう。\n",
    "\n",
    "入力を``x``、パラメータを``w`` と ``b``、そして適切な損失関数を決めます。\n",
    "\n",
    "<br>\n",
    "\n",
    "PyTorchでは例えば以下のように実装します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.rand(5)  # input tensor\n",
    "y = torch.zeros(3)  # expected output\n",
    "w = torch.randn(5, 3, requires_grad=True)\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "z = torch.matmul(x, w)+b\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 勾配情報の保存"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "こののニューラルネットワークでは、``w``と``b``が最適したいパラメータです。\n",
    "\n",
    "そのため、これらの変数に対する損失関数の微分値を計算する必要があります。\n",
    "\n",
    "これらのパラメータで微分を可能にするために、``requires_grad``属性をこれらのテンソルに追記します。\n",
    "\n",
    "そうすると、勾配は、テンソルの ``grad_fn`` プロパティに格納されます。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient function for z = <AddBackward0 object at 0x17ff73b50>\n",
      "Gradient function for loss = <BinaryCrossEntropyWithLogitsBackward0 object at 0x17ff73b50>\n"
     ]
    }
   ],
   "source": [
    "print('Gradient function for z =',z.grad_fn)\n",
    "print('Gradient function for loss =', loss.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 勾配計算\n",
    "\n",
    "ニューラルネットワークの各パラメータを最適化するために、入力``x``と出力``y``が与えられたもとで、損失関数の各変数の偏微分値、\n",
    "\n",
    "すなわち\n",
    "\n",
    "$\\frac{\\partial loss}{\\partial w}$ 、$\\frac{\\partial loss}{\\partial b}$ \n",
    "\n",
    "を求める必要があります。\n",
    "\n",
    "\n",
    "これらの偏微分値を求めるために``loss.backward()``を実行し、``w.grad``と``b.grad``の値を導出します。\n",
    "\n",
    "逆伝搬では、``.backward()``がテンソルに対して実行されると、autogradは、\n",
    "- 各変数の ``.grad_fn``を計算する\n",
    "- 各変数の``.grad``属性に微分値を代入する\n",
    "- 微分の連鎖律を使用して、各leafのテンソルの微分値を求める\n",
    "\n",
    "を行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0838, 0.0811, 0.0192],\n",
      "        [0.0897, 0.0868, 0.0206],\n",
      "        [0.0755, 0.0731, 0.0173],\n",
      "        [0.1777, 0.1720, 0.0407],\n",
      "        [0.1117, 0.1081, 0.0256]])\n",
      "tensor([0.1940, 0.1878, 0.0445])\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最適化ループを構築し、Pytorchより自動的に逆伝播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def training_loop(n_epochs, learning_rate, model, input, target):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        # Forward pass\n",
    "        outputs = model(input)\n",
    "        \n",
    "        # Compute the loss using Binary Cross Entropy with Logits\n",
    "        loss = F.binary_cross_entropy_with_logits(outputs, target)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the parameters\n",
    "        with torch.no_grad():\n",
    "            for param in model.parameters():\n",
    "                param -= learning_rate * param.grad\n",
    "        model.zero_grad()\n",
    "        # Zero the parameter gradients after updating \n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Loss: 0.691612958908081\n",
      "Epoch 200, Loss: 0.6908579468727112\n",
      "Epoch 300, Loss: 0.6902244687080383\n",
      "Epoch 400, Loss: 0.6896253824234009\n",
      "Epoch 500, Loss: 0.6890526413917542\n"
     ]
    }
   ],
   "source": [
    "# Example usage (with dummy data)\n",
    "input = torch.rand(10, 512)  # 10 samples with 512 features each\n",
    "target = torch.rand(10, 3)  # 10 samples with 3 target values each\n",
    "\n",
    "n_epochs = 500\n",
    "learning_rate = 0.01\n",
    "model = NeuralNetwork()\n",
    "\n",
    "trained_model = training_loop(n_epochs, learning_rate, model, input, target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{note}\n",
    "\n",
    "PyTorchの勾配計算メカニズムでは、``.backward``を呼び出すと、リーフノードで導関数の計算結果が累積されます。つまり、もし``.backward``が以前にも呼び出されていた場合、損失関数が再び計算され、``.backward``も再び呼び出され、各リーフの勾配が前の反復で計算された結果の上に累積されます。その結果、勾配の値は誤ったものになります。\n",
    "\n",
    "このようなことが起こらないようにするためには、反復のルーブのたびに``model.zero_grad()``を用いて明示的に勾配をゼロに設定する必要があります。\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 最適化関数\n",
    "\n",
    "最適化は各訓練ステップにおいてモデルの誤差を小さくなるように、モデルパラメータを調整するプロセスです。\n",
    "\n",
    "ここまでの説明は、単純な勾配下降法を最適化に使用しました。これは、シンプルなケースでは問題なく機能しますが、モデルが複雑になったときのために、パラメータ学習の収束を助ける最適化の工夫が必要されます。\n",
    "\n",
    "#### Optimizer\n",
    "\n",
    "```optim```というモジュールには、様々な最適化アルゴリズムが実装されています。\n",
    "\n",
    "ここでは、確率的勾配降下法（Stochastic Gradient Descent）を例として使い方を説明します。\n",
    "\n",
    "確率的勾配降下法は、ランダムに選んだ１つのデータのみで勾配を計算してパラメータを更新し、データの数だけ繰り返す方法です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "訓練したいモデルパラメータをoptimizerに登録し、合わせて学習率をハイパーパラメータとして渡すことで初期化を行います。訓練ループ内で、最適化（optimization）は3つのステップから構成されます。\n",
    "\n",
    "- ``optimizer.zero_grad()``を実行し、モデルパラメータの勾配をリセットします。勾配の計算は蓄積されていくので、毎イテレーション、明示的にリセットします。\n",
    "\n",
    "\n",
    "- 続いて、``loss.backwards()``を実行し、バックプロパゲーションを実行します。PyTorchは損失に対する各パラメータの偏微分の値（勾配）を求めます。\n",
    "\n",
    "- 最後に、``optimizer.step()``を実行し、各パラメータの勾配を使用してパラメータの値を調整します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, learning_rate, model, input, target):\n",
    "    # Use Binary Cross Entropy with Logits as the loss function\n",
    "    \n",
    "    # Use Adam as the optimizer\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(input)\n",
    "        loss = F.binary_cross_entropy_with_logits(outputs, target)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print loss every 100 epochs\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Loss: 0.6944586038589478\n",
      "Epoch 200, Loss: 0.693421483039856\n",
      "Epoch 300, Loss: 0.6924219131469727\n",
      "Epoch 400, Loss: 0.6914820075035095\n",
      "Epoch 500, Loss: 0.6905751824378967\n",
      "Epoch 600, Loss: 0.689775824546814\n",
      "Epoch 700, Loss: 0.6889927387237549\n",
      "Epoch 800, Loss: 0.6882842183113098\n",
      "Epoch 900, Loss: 0.6877082586288452\n",
      "Epoch 1000, Loss: 0.6871109008789062\n"
     ]
    }
   ],
   "source": [
    "input = torch.rand(10, 512)  # 10 samples with 512 features each\n",
    "target = torch.rand(10, 3)  # 10 samples with 3 target values each\n",
    "\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.001\n",
    "model = NeuralNetwork()\n",
    "\n",
    "trained_model = training_loop(n_epochs, learning_rate, model, input, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorchによる深層学習の実装例\n",
    "\n",
    "### データの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train_df = pd.read_csv('./Data/Titanic/train.csv')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>892</td>\n",
       "      <td>3</td>\n",
       "      <td>Kelly, Mr. James</td>\n",
       "      <td>male</td>\n",
       "      <td>34.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>330911</td>\n",
       "      <td>7.8292</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>893</td>\n",
       "      <td>3</td>\n",
       "      <td>Wilkes, Mrs. James (Ellen Needs)</td>\n",
       "      <td>female</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>363272</td>\n",
       "      <td>7.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>894</td>\n",
       "      <td>2</td>\n",
       "      <td>Myles, Mr. Thomas Francis</td>\n",
       "      <td>male</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>240276</td>\n",
       "      <td>9.6875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>895</td>\n",
       "      <td>3</td>\n",
       "      <td>Wirz, Mr. Albert</td>\n",
       "      <td>male</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>315154</td>\n",
       "      <td>8.6625</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>896</td>\n",
       "      <td>3</td>\n",
       "      <td>Hirvonen, Mrs. Alexander (Helga E Lindqvist)</td>\n",
       "      <td>female</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3101298</td>\n",
       "      <td>12.2875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Pclass                                          Name     Sex  \\\n",
       "0          892       3                              Kelly, Mr. James    male   \n",
       "1          893       3              Wilkes, Mrs. James (Ellen Needs)  female   \n",
       "2          894       2                     Myles, Mr. Thomas Francis    male   \n",
       "3          895       3                              Wirz, Mr. Albert    male   \n",
       "4          896       3  Hirvonen, Mrs. Alexander (Helga E Lindqvist)  female   \n",
       "\n",
       "    Age  SibSp  Parch   Ticket     Fare Cabin Embarked  \n",
       "0  34.5      0      0   330911   7.8292   NaN        Q  \n",
       "1  47.0      1      0   363272   7.0000   NaN        S  \n",
       "2  62.0      0      0   240276   9.6875   NaN        Q  \n",
       "3  27.0      0      0   315154   8.6625   NaN        S  \n",
       "4  22.0      1      1  3101298  12.2875   NaN        S  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv('./Data/Titanic/test.csv')\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PassengerId      int64\n",
      "Survived         int64\n",
      "Pclass           int64\n",
      "Name            object\n",
      "Sex             object\n",
      "Age            float64\n",
      "SibSp            int64\n",
      "Parch            int64\n",
      "Ticket          object\n",
      "Fare           float64\n",
      "Cabin           object\n",
      "Embarked        object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(train_df.dtypes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データの前処理\n",
    "\n",
    "- 列の削除: 要らない列を消していきます．乗客IDや名前，チケット，港(Embarked)や部屋番号(Cabin)はは生死にあまり関係がなさそうので、削除します。\n",
    "- 欠損の補完: 平均値で欠損しているデータを補完します。\n",
    "- 文字列を数字に置換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived  Pclass  Sex   Age  SibSp  Parch     Fare\n",
       "0         0       3    0  22.0      1      0   7.2500\n",
       "1         1       1    1  38.0      1      0  71.2833\n",
       "2         1       3    1  26.0      0      0   7.9250\n",
       "3         1       1    1  35.0      1      0  53.1000\n",
       "4         0       3    0  35.0      0      0   8.0500"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process_df(df):\n",
    "    df = df.drop([\"PassengerId\", \"Name\", \"Ticket\", \"Cabin\", \"Embarked\"], axis=1)\n",
    "    df[\"Age\"] = df[\"Age\"].fillna(df[\"Age\"].mean())\n",
    "    df = df.replace(\"male\", 0)\n",
    "    df = df.replace(\"female\", 1)\n",
    "    return df\n",
    "\n",
    "train_df = process_df(train_df)\n",
    "test_df = process_df(test_df)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データセットの作成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データセットとデータローダーでミニバッチ学習ためのデータセットを作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\"]\n",
    "X = train_df[features]\n",
    "y = train_df[\"Survived\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TitanicDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        # DataFrameをNumPy配列に変換\n",
    "        self.X = torch.tensor(X.values, dtype=torch.float32)  # XをNumPy配列に変換してからテンソル化\n",
    "        self.y = torch.tensor(y.values, dtype=torch.float32)  # yも同様にテンソル化\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データローダーの作成\n",
    "train_dataset = TitanicDataset(X_train, y_train)\n",
    "val_dataset = TitanicDataset(X_val, y_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 1.0000,  0.0000, 45.5000,  0.0000,  0.0000, 28.5000]), tensor(0.))"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モデルの定義\n",
    "\n",
    "```__init__```で行列の掛け算の定義をして，```forward```でそれをどの順番で行うかを指定する感じです．\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TitanicModel(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(TitanicModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モデルの学習\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "損失関数は2乗誤差、最適化関数はAdamを使用します．\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, n_epochs, learning_rate):\n",
    "    criterion = nn.BCELoss()  # 分類の損失関数\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch).squeeze()\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * X_batch.size(0)\n",
    "\n",
    "        # 検証データでの評価\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                outputs = model(X_batch).squeeze()\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                val_loss += loss.item() * X_batch.size(0)\n",
    "\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{n_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500, Train Loss: 0.6646, Val Loss: 0.5938\n",
      "Epoch 11/500, Train Loss: 0.5614, Val Loss: 0.5699\n",
      "Epoch 21/500, Train Loss: 0.5037, Val Loss: 0.4841\n",
      "Epoch 31/500, Train Loss: 0.4624, Val Loss: 0.4706\n",
      "Epoch 41/500, Train Loss: 0.4465, Val Loss: 0.4490\n",
      "Epoch 51/500, Train Loss: 0.4260, Val Loss: 0.4356\n",
      "Epoch 61/500, Train Loss: 0.4311, Val Loss: 0.4374\n",
      "Epoch 71/500, Train Loss: 0.4341, Val Loss: 0.4378\n",
      "Epoch 81/500, Train Loss: 0.4344, Val Loss: 0.4918\n",
      "Epoch 91/500, Train Loss: 0.4252, Val Loss: 0.4548\n",
      "Epoch 101/500, Train Loss: 0.4398, Val Loss: 0.4504\n",
      "Epoch 111/500, Train Loss: 0.4205, Val Loss: 0.4573\n",
      "Epoch 121/500, Train Loss: 0.4196, Val Loss: 0.4388\n",
      "Epoch 131/500, Train Loss: 0.4149, Val Loss: 0.4306\n",
      "Epoch 141/500, Train Loss: 0.4175, Val Loss: 0.4254\n",
      "Epoch 151/500, Train Loss: 0.4165, Val Loss: 0.4350\n",
      "Epoch 161/500, Train Loss: 0.4091, Val Loss: 0.4334\n",
      "Epoch 171/500, Train Loss: 0.4040, Val Loss: 0.4267\n",
      "Epoch 181/500, Train Loss: 0.4111, Val Loss: 0.4231\n",
      "Epoch 191/500, Train Loss: 0.4006, Val Loss: 0.4474\n",
      "Epoch 201/500, Train Loss: 0.4002, Val Loss: 0.4413\n",
      "Epoch 211/500, Train Loss: 0.4043, Val Loss: 0.4638\n",
      "Epoch 221/500, Train Loss: 0.4132, Val Loss: 0.4264\n",
      "Epoch 231/500, Train Loss: 0.4093, Val Loss: 0.4401\n",
      "Epoch 241/500, Train Loss: 0.3995, Val Loss: 0.4357\n",
      "Epoch 251/500, Train Loss: 0.3881, Val Loss: 0.4294\n",
      "Epoch 261/500, Train Loss: 0.4192, Val Loss: 0.4580\n",
      "Epoch 271/500, Train Loss: 0.3964, Val Loss: 0.4634\n",
      "Epoch 281/500, Train Loss: 0.3864, Val Loss: 0.4298\n",
      "Epoch 291/500, Train Loss: 0.3852, Val Loss: 0.4621\n",
      "Epoch 301/500, Train Loss: 0.3909, Val Loss: 0.4482\n",
      "Epoch 311/500, Train Loss: 0.3789, Val Loss: 0.4582\n",
      "Epoch 321/500, Train Loss: 0.3781, Val Loss: 0.4432\n",
      "Epoch 331/500, Train Loss: 0.3830, Val Loss: 0.4486\n",
      "Epoch 341/500, Train Loss: 0.3868, Val Loss: 0.4433\n",
      "Epoch 351/500, Train Loss: 0.3837, Val Loss: 0.4553\n",
      "Epoch 361/500, Train Loss: 0.3738, Val Loss: 0.4600\n",
      "Epoch 371/500, Train Loss: 0.3692, Val Loss: 0.4543\n",
      "Epoch 381/500, Train Loss: 0.3691, Val Loss: 0.4518\n",
      "Epoch 391/500, Train Loss: 0.3683, Val Loss: 0.4545\n",
      "Epoch 401/500, Train Loss: 0.3751, Val Loss: 0.4656\n",
      "Epoch 411/500, Train Loss: 0.3841, Val Loss: 0.4673\n",
      "Epoch 421/500, Train Loss: 0.3875, Val Loss: 0.4646\n",
      "Epoch 431/500, Train Loss: 0.3901, Val Loss: 0.4791\n",
      "Epoch 441/500, Train Loss: 0.3565, Val Loss: 0.4571\n",
      "Epoch 451/500, Train Loss: 0.3571, Val Loss: 0.4694\n",
      "Epoch 461/500, Train Loss: 0.3598, Val Loss: 0.4585\n",
      "Epoch 471/500, Train Loss: 0.3536, Val Loss: 0.4727\n",
      "Epoch 481/500, Train Loss: 0.3488, Val Loss: 0.4658\n",
      "Epoch 491/500, Train Loss: 0.3582, Val Loss: 0.4824\n"
     ]
    }
   ],
   "source": [
    "model = TitanicModel(input_size=X_train.shape[1])\n",
    "train_model(model, train_loader, val_loader, n_epochs=500, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モデルの評価\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def evaluate_model(model, loader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in loader:\n",
    "            outputs = model(X_batch).squeeze()\n",
    "            # 出力を0または1に丸める\n",
    "            predicted_classes = outputs.round().numpy()\n",
    "            predictions.extend(predicted_classes)\n",
    "            true_labels.extend(y_batch.numpy())\n",
    "\n",
    "    # レポートの表示\n",
    "    print(classification_report(true_labels, predictions, target_names=[\"Did not survive\", \"Survived\"],digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "Did not survive     0.7895    0.8571    0.8219       105\n",
      "       Survived     0.7692    0.6757    0.7194        74\n",
      "\n",
      "       accuracy                         0.7821       179\n",
      "      macro avg     0.7794    0.7664    0.7707       179\n",
      "   weighted avg     0.7811    0.7821    0.7795       179\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# モデルの評価を実行\n",
    "evaluate_model(model, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````{tab-set}\n",
    "```{tab-item} 実習問題\n",
    "深層学習の実装を以下の指示に従って、改めて学習を行なってください。\n",
    "\n",
    "- バッチサイズを64に変更しなさい\n",
    "- モデルに一つ隠れ層を追加しなさい\n",
    "- ドロップアウト層を追加しなさい\n",
    "- オプティマイザはAdamWを設定しなさい\n",
    "- epochを増やしんさい\n",
    "\n",
    "```\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.5963879823684692\n",
      "Epoch 10, Loss: 0.7046562433242798\n",
      "Epoch 20, Loss: 0.7102022171020508\n",
      "Epoch 30, Loss: 0.6189422607421875\n",
      "Epoch 40, Loss: 0.6295629739761353\n",
      "Epoch 50, Loss: 0.6043649315834045\n",
      "Epoch 60, Loss: 0.7920413613319397\n",
      "Epoch 70, Loss: 0.5864745378494263\n",
      "Epoch 80, Loss: 0.5514351725578308\n",
      "Epoch 90, Loss: 0.6408214569091797\n",
      "Epoch 100, Loss: 0.2728642523288727\n",
      "Epoch 110, Loss: 0.3511948585510254\n",
      "Epoch 120, Loss: 0.5472555756568909\n",
      "Epoch 130, Loss: 0.652298092842102\n",
      "Epoch 140, Loss: 0.6173288822174072\n",
      "Epoch 150, Loss: 0.5431506633758545\n",
      "Epoch 160, Loss: 0.44707730412483215\n",
      "Epoch 170, Loss: 0.45531612634658813\n",
      "Epoch 180, Loss: 0.5403363108634949\n",
      "Epoch 190, Loss: 0.31630808115005493\n",
      "Epoch 200, Loss: 0.177628293633461\n",
      "Epoch 210, Loss: 0.341421514749527\n",
      "Epoch 220, Loss: 0.2885816693305969\n",
      "Epoch 230, Loss: 0.44160687923431396\n",
      "Epoch 240, Loss: 0.4404385983943939\n",
      "Epoch 250, Loss: 0.08747775852680206\n",
      "Epoch 260, Loss: 0.5158218741416931\n",
      "Epoch 270, Loss: 0.20541369915008545\n",
      "Epoch 280, Loss: 0.38503286242485046\n",
      "Epoch 290, Loss: 0.2064630687236786\n",
      "Epoch 300, Loss: 0.2510749399662018\n",
      "Epoch 310, Loss: 0.06898139417171478\n",
      "Epoch 320, Loss: 0.4055630564689636\n",
      "Epoch 330, Loss: 0.04109148308634758\n",
      "Epoch 340, Loss: 0.16631028056144714\n",
      "Epoch 350, Loss: 0.3222368657588959\n",
      "Epoch 360, Loss: 0.09543536603450775\n",
      "Epoch 370, Loss: 0.30374833941459656\n",
      "Epoch 380, Loss: 0.35060417652130127\n",
      "Epoch 390, Loss: 0.2410183995962143\n",
      "Epoch 400, Loss: 0.04953644797205925\n",
      "Epoch 410, Loss: 0.5792601704597473\n",
      "Epoch 420, Loss: 0.5372923612594604\n",
      "Epoch 430, Loss: 0.10680956393480301\n",
      "Epoch 440, Loss: 0.006740816403180361\n",
      "Epoch 450, Loss: 0.07592997699975967\n",
      "Epoch 460, Loss: 0.23773075640201569\n",
      "Epoch 470, Loss: 0.1513519138097763\n",
      "Epoch 480, Loss: 0.09067592769861221\n",
      "Epoch 490, Loss: 0.11920297890901566\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "Did not survive       1.00      0.93      0.97        60\n",
      "       Survived       0.91      1.00      0.95        40\n",
      "\n",
      "       accuracy                           0.96       100\n",
      "      macro avg       0.95      0.97      0.96       100\n",
      "   weighted avg       0.96      0.96      0.96       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import classification_report\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# データセットクラスの定義\n",
    "class TitanicDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X.values, dtype=torch.float32)  # NumPy配列に変換\n",
    "        self.y = torch.tensor(y.values, dtype=torch.float32)  # NumPy配列に変換\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# ネットワークの定義\n",
    "class TitanicModel(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(TitanicModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "# 訓練関数\n",
    "def train(model, dataloader, loss_func, optimizer, epochs):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for X, labels in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X).squeeze()\n",
    "            loss = loss_func(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "\n",
    "# モデル評価関数\n",
    "def evaluate_model(model, loader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in loader:\n",
    "            outputs = model(X_batch).squeeze()\n",
    "            predicted_classes = outputs.round().numpy()  # 0または1に丸める\n",
    "            predictions.extend(predicted_classes)\n",
    "            true_labels.extend(y_batch.numpy())\n",
    "\n",
    "    print(classification_report(true_labels, predictions, target_names=[\"Did not survive\", \"Survived\"]))\n",
    "\n",
    "# メインの実行部分\n",
    "if __name__ == \"__main__\":\n",
    "    # ここでX_trainとy_trainを定義または読み込む\n",
    "    # 例えば、pandasを用いてCSVから読み込むなど\n",
    "\n",
    "    # ダミーデータの生成\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "\n",
    "    # 仮のデータ\n",
    "    np.random.seed(42)\n",
    "    X_train = pd.DataFrame(np.random.rand(100, 5))  # 100サンプル、5特徴量\n",
    "    y_train = pd.Series(np.random.randint(0, 2, size=100))  # 0または1のラベル\n",
    "\n",
    "    # データセットとデータローダーの作成\n",
    "    train_dataset = TitanicDataset(X_train, y_train)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "    # モデル、損失関数、オプティマイザの定義\n",
    "    model = TitanicModel(input_size=X_train.shape[1])\n",
    "    loss_func = nn.BCELoss()  # Binary Cross Entropy Loss\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)  # AdamWオプティマイザ\n",
    "\n",
    "    # 訓練の実行\n",
    "    train(model, train_dataloader, loss_func, optimizer, epochs=500)\n",
    "\n",
    "    # モデル評価\n",
    "    evaluate_model(model, train_dataloader)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyterbook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
