
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>word2vec &#8212; 計算社会科学のための自然言語処理</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="prev" title="単語分散表現" href="word2vec_1.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/tohoku-university-logo-vector.svg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">計算社会科学のための自然言語処理</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    計算社会科学と自然言語処理
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  イントロダクション
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="introduction.html">
   ガイダンス
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  基礎知識
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="nlp_basis.html">
   自然言語処理の基礎
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ml_basis.html">
   機械学習の基本概念
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  ニューラルネットワーク
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="NN.html">
   ニューラルネットワーク
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="backpropagation.html">
   誤差逆伝播法
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  PyTorch
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="pytorch.html">
   Pytorch
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  単語分散表現
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="word2vec_1.html">
   単語分散表現
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   word2vec
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://mybinder.org/v2/gh/lvzeyu/css_nlp/master?urlpath=lab/tree/notebook/word2vec_2_embedding.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../_static/images/logo_binder.svg">
  </span>
<span class="headerbtn__text-container">Binder</span>
</a>

      </li>
      
      <li>
        <a href="https://colab.research.google.com/github/lvzeyu/css_nlp/blob/master/notebook/word2vec_2_embedding.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Colab"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../_static/images/logo_colab.png">
  </span>
<span class="headerbtn__text-container">Colab</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/lvzeyu/css_nlp/tree/master"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/lvzeyu/css_nlp/tree/master/issues/new?title=Issue%20on%20page%20%2Fnotebook/word2vec_2_embedding.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../_sources/notebook/word2vec_2_embedding.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id3">
   推論ベース手法とニューラルネットワーク
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id4">
     推論ベース手法の設計
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#one-hot">
     one-hot表現
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#cbow-continuous-bag-of-words">
     CBOW（continuous bag-of-words）モデル
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id5">
       入力層から中間層(エンコード)
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id6">
       中間層から出力層(デコード)
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id7">
       word2vecの重みと分散表現
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id8">
   Word2Vecモデルの実装
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id9">
     学習データの準備
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id10">
       コンテキストとターゲット
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#pytorchcbow">
     PytorchでCBOWモデルの実装
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#embedding">
       Embeddingレイヤ
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id11">
       ミニバッチ化データセットの作成
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#dataset">
         DataSet
        </a>
       </li>
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#dataloader">
         DataLoader
        </a>
       </li>
      </ul>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#cbow">
       CBOWモデルの構築
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>word2vec</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id3">
   推論ベース手法とニューラルネットワーク
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id4">
     推論ベース手法の設計
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#one-hot">
     one-hot表現
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#cbow-continuous-bag-of-words">
     CBOW（continuous bag-of-words）モデル
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id5">
       入力層から中間層(エンコード)
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id6">
       中間層から出力層(デコード)
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id7">
       word2vecの重みと分散表現
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id8">
   Word2Vecモデルの実装
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id9">
     学習データの準備
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id10">
       コンテキストとターゲット
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#pytorchcbow">
     PytorchでCBOWモデルの実装
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#embedding">
       Embeddingレイヤ
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id11">
       ミニバッチ化データセットの作成
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#dataset">
         DataSet
        </a>
       </li>
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#dataloader">
         DataLoader
        </a>
       </li>
      </ul>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#cbow">
       CBOWモデルの構築
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="word2vec">
<h1>word2vec<a class="headerlink" href="#word2vec" title="Permalink to this headline">#</a></h1>
<p>前章では、「カウントベースの手法」によって単語分散表現を得ました。具体的には、単語の共起行列を作り、その行列に対してSVDを適用することで、密なベクトくー 単語分散表現ーを獲得したのです。</p>
<p>しかし、カウントベースの手法にはいくつかの問題点があります。</p>
<ul class="simple">
<li><p>大規模なコーパスを扱う場合、巨大な共起行列に対してSVDを計算することが難しい。</p></li>
<li><p>コーパスの全体から一回の学習で単語分散表現を獲得していますので、新しい単語が追加される場合、再度最初から学習を行う必要があり、単語分散表現更新の効率が低い。</p></li>
</ul>
<p>「カウントベースの手法」に代わる強力な手法として「推論ベース」の手法が挙げられます。特に、Mikolov et al. <span id="id1">[<a class="reference internal" href="introduction.html#id20" title="Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. Linguistic regularities in continuous space word representations. In Lucy Vanderwende, Hal Daumé III, and Katrin Kirchhoff, editors, Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 746–751. Atlanta, Georgia, June 2013. Association for Computational Linguistics. URL: https://aclanthology.org/N13-1090.">Mikolov <em>et al.</em>, 2013</a>]</span> <span id="id2">[<a class="reference internal" href="introduction.html#id21" title="Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed representations of words and phrases and their compositionality. In C.J. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems, volume 26. Curran Associates, Inc., 2013. URL: https://proceedings.neurips.cc/paper_files/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf.">Mikolov <em>et al.</em>, 2013</a>]</span>　によって提案されたword2vecの有用性が多くの自然言語処理タスクにおいて示されてきたのです。</p>
<p>本章では、word2vecの仕組みについて説明し、それを実装することで理解を深めます。</p>
<section id="id3">
<h2>推論ベース手法とニューラルネットワーク<a class="headerlink" href="#id3" title="Permalink to this headline">#</a></h2>
<p>推論ベースの手法は、ミニバッチで学習する形で、ニューラルネットワークを用いて、重みを繰り返し更新することで単語分散表現を獲得します。</p>
<p><img alt="" src="../_images/inference.png" /></p>
<section id="id4">
<h3>推論ベース手法の設計<a class="headerlink" href="#id4" title="Permalink to this headline">#</a></h3>
<p>推論ベース手法では、<code class="docutils literal notranslate"><span class="pre">you</span> <span class="pre">【？】</span> <span class="pre">goodbye</span> <span class="pre">and</span> <span class="pre">I</span> <span class="pre">say</span> <span class="pre">hello</span> <span class="pre">.</span></code>のような、周囲の単語が与えられたときに、<code class="docutils literal notranslate"><span class="pre">【？】</span></code>にどのような単語が出現するのかを推測する推論問題を繰り返し解くことで、単語の出現バターンを学習します。</p>
<p>つまり、コンテキスト情報を入力として受け取り、各単語の出現する確率を出力する「モデル」を作成することは目標になります。ここで、正しい推測ができるように、コーパスを使って、ニューラルネットワークモデルの学習を行います。そして、その学習の結果として、単語の分散表現を得られます。</p>
<p><img alt="" src="../_images/inference2.png" /></p>
</section>
<section id="one-hot">
<h3>one-hot表現<a class="headerlink" href="#one-hot" title="Permalink to this headline">#</a></h3>
<p>ニューラルネットワークで単語を処理するには、それを「固定長のベクトル」に変換する必要があります。</p>
<p>そのための方法の一つは、単語をone-hot表現へと変換することです。one-hot表現とは、ベクトルの要素の中で一つだけが<span class="math notranslate nohighlight">\(1\)</span>で、残りは全て<span class="math notranslate nohighlight">\(0\)</span>であるようなベクトルと言います。</p>
<p>単語をone-hot表現に変換するには、語彙数分の要素を持つベクトルを用意して、単語IDの該当する箇所を<span class="math notranslate nohighlight">\(1\)</span>に、残りは全て<span class="math notranslate nohighlight">\(0\)</span>に設定します。</p>
<p><img alt="" src="../_images/one-hot.png" /></p>
</section>
<section id="cbow-continuous-bag-of-words">
<h3>CBOW（continuous bag-of-words）モデル<a class="headerlink" href="#cbow-continuous-bag-of-words" title="Permalink to this headline">#</a></h3>
<p>CBOWモデルは、コンテキストからターゲットを推測することを目的としたニューラルネットワークです。このCBOWモデルで、できるだけ正確な推測ができるように訓練することで、単語の分散表現を取得することができます。</p>
<p>ここで、例として、コンテキスト<code class="docutils literal notranslate"><span class="pre">[&quot;you&quot;,&quot;goodbye&quot;]</span></code>からターゲット<code class="docutils literal notranslate"><span class="pre">&quot;say&quot;</span></code>を予測するタスクを考えます。</p>
<p><img alt="" src="../_images/cbow.png" /></p>
<section id="id5">
<h4>入力層から中間層(エンコード)<a class="headerlink" href="#id5" title="Permalink to this headline">#</a></h4>
<p>one-hotエンコーディングで、単語を固定長のベクトルに変換するすることができます。</p>
<p>単語をベクトルで表すことができれば、そのベクトルはニューラルネットワークを構成する「レイヤ」によって処理することができるようになりました。</p>
<p>コンテキストを<span class="math notranslate nohighlight">\(\mathbf{c}\)</span>、重みを<span class="math notranslate nohighlight">\(\mathbf{W}\)</span>とし、それぞれ次の形状とします。</p>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p>イメージしやすいように、ここでは添字を対応する単語で表すことにします。ただし「.(ピリオド)」については「priod」とします。</p>
</aside>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{c}
    = \begin{pmatrix}
          c_{\mathrm{you}} &amp; c_{\mathrm{say}} &amp; c_{\mathrm{goodbye}} &amp; c_{\mathrm{and}} &amp; c_{\mathrm{I}} &amp; c_{\mathrm{hello}} &amp; c_{\mathrm{period}}
      \end{pmatrix}
,\ 
\mathbf{W}
    = \begin{pmatrix}
          w_{\mathrm{you},1} &amp; w_{\mathrm{you},2} &amp; w_{\mathrm{you},3} \\
          w_{\mathrm{say},1} &amp; w_{\mathrm{say},2} &amp; w_{\mathrm{say},3} \\
          w_{\mathrm{goodbye},1} &amp; w_{\mathrm{goodbye},2} &amp; w_{\mathrm{goodbye},3} \\
          w_{\mathrm{and},1} &amp; w_{\mathrm{and},2} &amp; w_{\mathrm{and},3} \\
          w_{\mathrm{I},1} &amp; w_{\mathrm{I},2} &amp; w_{\mathrm{I},3} \\
          w_{\mathrm{hello},1} &amp; w_{\mathrm{hello},2} &amp; w_{\mathrm{hello},3} \\
          w_{\mathrm{period},1} &amp; w_{\mathrm{period},2} &amp; w_{\mathrm{period},3} \\
      \end{pmatrix}
\end{split}\]</div>
<p>コンテキストの要素数(列数)と重みの行数が、単語の種類数に対応します。</p>
<p>コンテキスト(単語)はone-hot表現として扱うため、例えば「you」の場合は</p>
<div class="math notranslate nohighlight">
\[
\mathbf{c}_{\mathrm{you}}
    = \begin{pmatrix}
          1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0
      \end{pmatrix}
\]</div>
<p>とすることで、単語「you」を表現できます。</p>
<p>重み付き和<span class="math notranslate nohighlight">\(\mathbf{h}\)</span>は、行列の積で求められます。</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbf{h}
   &amp;= \mathbf{c}_{\mathrm{you}}
      \mathbf{W}
\\
   &amp;= \begin{pmatrix}
          h_1 &amp; h_2 &amp; h_3
      \end{pmatrix}
\end{aligned}
\end{split}\]</div>
<p><span class="math notranslate nohighlight">\(h_1\)</span>の計算を詳しく見ると、次のようになります。</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
h_1
   &amp;= c_{\mathrm{you}} w_{\mathrm{you},1}
      + c_{\mathrm{say}} w_{\mathrm{say},1}
      + c_{\mathrm{goodbye}} w_{\mathrm{goodbye},1}
      + c_{\mathrm{and}} w_{\mathrm{and},1}
      + c_{\mathrm{I}} w_{\mathrm{I},1}
      + c_{\mathrm{hello}} w_{\mathrm{hello},1}
      + c_{\mathrm{period}} w_{\mathrm{period},1}
\\
   &amp;= 1 w_{\mathrm{you},1}
      + 0 w_{\mathrm{say},1}
      + 0 w_{\mathrm{goodbye},1}
      + 0 w_{\mathrm{and},1}
      + 0 w_{\mathrm{I},1}
      + 0 w_{\mathrm{hello},1}
      + 0 w_{\mathrm{period},1}
\\
   &amp;= w_{\mathrm{you},1}
\end{aligned}
\end{split}\]</div>
<p>コンテキストと重みの対応する(同じ単語に関する)要素を掛けて、全ての単語で和をとります。しかしコンテキストは、<span class="math notranslate nohighlight">\(c_{you}\)</span>以外の要素が<span class="math notranslate nohighlight">\(0\)</span>なので、対応する重みの値の影響は消えていまします。また<span class="math notranslate nohighlight">\(c_{you}\)</span>は<span class="math notranslate nohighlight">\(1\)</span>なので、対応する重みの値<span class="math notranslate nohighlight">\(w_{\mathrm{you},1}\)</span>がそのまま中間層のニューロンに伝播します。</p>
<p>残りの2つの要素も同様に計算できるので、重み付き和</p>
<div class="math notranslate nohighlight">
\[
\mathbf{h}
    = \begin{pmatrix}
          w_{\mathrm{you},1} &amp; w_{\mathrm{you},2} &amp; w_{\mathrm{you},3}
      \end{pmatrix}
\]</div>
<p>は、単語「you」に関する重みの値となります。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="c1"># 適当にコンテキスト(one-hot表現)を指定</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;コンテキストの形状：</span><span class="si">{</span><span class="n">c</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># 重みをランダムに生成</span>
<span class="n">W</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;重み</span><span class="se">\n</span><span class="si">{</span><span class="n">W</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># 重み付き和を計算</span>
<span class="n">h</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">W</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;重み付き和</span><span class="se">\n</span><span class="si">{</span><span class="n">h</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;重み付き和の形状：</span><span class="si">{</span><span class="n">h</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>コンテキストの形状：(1, 7)
重み
[[ 1.71670313e+00 -8.63625037e-01 -5.35290530e-01]
 [-5.60747899e-01  1.50208489e+00 -1.67966102e+00]
 [ 1.81526839e+00 -1.17667527e+00  4.48485314e-01]
 [-8.65600176e-02  2.37075284e+00  6.47776688e-02]
 [-2.33527217e+00 -6.69067439e-01 -1.44158917e+00]
 [-9.21024532e-01 -9.52111113e-01  1.16383682e-03]
 [ 2.74139129e+00  1.92697421e-01  8.97070946e-02]]
重み付き和
[[ 1.71670313 -0.86362504 -0.53529053]]
重み付き和の形状：(1, 3)
</pre></div>
</div>
</div>
</div>
<p>コンテキストに複数な単語がある場合、入力層も複数になります。このとき、中間層にあるニューロンは、各入力層の全結合による変換後の値が平均されたものになります。</p>
<p>中間層のニューロンの数を入力層よりも減らすことによって、中間層には、単語を予測するために必要な情報が”コンパクト”に収められて、結果としては密なベクトル表現が得られます。このとき、この中間層の情報は、人間には理解できない「ブラックボックス」ような状態になります。この作業は、「エンコード」と言います。</p>
</section>
<section id="id6">
<h4>中間層から出力層(デコード)<a class="headerlink" href="#id6" title="Permalink to this headline">#</a></h4>
<p>中間層の情報から目的の結果を得る作業は、「デコード」と言います。ここでは、中間層のニューロンの値<span class="math notranslate nohighlight">\(\mathbf{h}\)</span>を各単語に対応した値になるように、つまり要素(行)数が単語の種類数となるように再度変換したものを、CBOWモデルの出力とします。</p>
<p>出力層の重みを</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{W}_{\mathrm{out}}
    = \begin{pmatrix}
          w_{1,\mathrm{you}} &amp; w_{1,\mathrm{say}} &amp; w_{1,\mathrm{goodbye}} &amp; w_{1,\mathrm{and}} &amp;
          w_{1,\mathrm{I}} &amp; w_{1,\mathrm{hello}} &amp; w_{1,\mathrm{period}} \\
          w_{2,\mathrm{you}} &amp; w_{2,\mathrm{say}} &amp; w_{2,\mathrm{goodbye}} &amp; w_{2,\mathrm{and}} &amp;
          w_{2,\mathrm{I}} &amp; w_{2,\mathrm{hello}} &amp; w_{2,\mathrm{period}} \\
          w_{3,\mathrm{you}} &amp; w_{3,\mathrm{say}} &amp; w_{3,\mathrm{goodbye}} &amp; w_{3,\mathrm{and}} &amp;
          w_{3,\mathrm{I}} &amp; w_{3,\mathrm{hello}} &amp; w_{3\mathrm{period}} \\
      \end{pmatrix}
\end{split}\]</div>
<p>とします。行数が中間層のニューロン数、列数が単語の種類数になります。</p>
<p>出力層も全結合層とすると、最終的な出力は</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbf{s}
   &amp;= \mathbf{h}
      \mathbf{W}_{\mathrm{out}}
\\
   &amp;= \begin{pmatrix}
          s_{\mathrm{you}} &amp; s_{\mathrm{say}} &amp; s_{\mathrm{goodbye}} &amp; s_{\mathrm{and}} &amp;
          s_{\mathrm{I}} &amp; s_{\mathrm{hello}} &amp; s_{\mathrm{period}}
      \end{pmatrix}
\end{aligned}
\end{split}\]</div>
<p>例えば、「you」に関する要素の計算は、</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
s_{\mathrm{you}}
   &amp;= \frac{1}{2} (w_{\mathrm{you},1} + w_{\mathrm{goodbye},1}) w_{1,\mathrm{you}}
      + \frac{1}{2} (w_{\mathrm{you},2} + w_{\mathrm{goodbye},2}) w_{2,\mathrm{you}}
      + \frac{1}{2} (w_{\mathrm{you},3} + w_{\mathrm{goodbye},3}) w_{3,\mathrm{you}}
\\
   &amp;= \frac{1}{2}
      \sum_{i=1}^3
          (w_{\mathrm{you},i} + w_{\mathrm{goodbye},i}) w_{i,\mathrm{you}}
\end{aligned}\end{split}\]</div>
<p>コンテキストに対応する入力層の重みの平均と「you」に関する出力の重みの積になります。</p>
<p>他の要素(単語)についても同様に計算できるので、最終的な出力は</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbf{s}
   &amp;= \begin{pmatrix}
          s_{\mathrm{you}} &amp; s_{\mathrm{say}} &amp; s_{\mathrm{goodbye}} &amp; s_{\mathrm{and}} &amp;
          s_{\mathrm{I}} &amp; s_{\mathrm{hello}} &amp; s_{\mathrm{period}}
      \end{pmatrix}
\\
   &amp;= \begin{pmatrix}
          \frac{1}{2} \sum_{i=1}^3 (w_{\mathrm{you},i} + w_{\mathrm{goodbye},i}) w_{i,\mathrm{you}} &amp;
          \frac{1}{2} \sum_{i=1}^3 (w_{\mathrm{you},i} + w_{\mathrm{goodbye},i}) w_{i,\mathrm{say}} &amp;
          \cdots &amp;
          \frac{1}{2} \sum_{i=1}^3 (w_{\mathrm{you},i} + w_{\mathrm{goodbye},i}) w_{i,\mathrm{hello}} &amp;
          \frac{1}{2} \sum_{i=1}^3 (w_{\mathrm{you},i} + w_{\mathrm{goodbye},i}) w_{i,\mathrm{period}}
      \end{pmatrix}
\end{aligned}
\end{split}\]</div>
<p>となります。</p>
<p>ここで、出力層のニューロンは各単語に対応し、各単語の「スコア」と言います。</p>
<p>「スコア」の値が高ければ高いほど、それに対応する単語の出現確率も高くなり、ターゲットの単語であるとして採用します。そのため、スコアを求める処理を推論処理と言います。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="c1"># Define the context data</span>
<span class="n">c0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="c1"># you</span>
<span class="n">c1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="c1"># goodbye</span>

<span class="c1"># Initialize weights randomly</span>
<span class="n">W_in</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>  <span class="c1"># Input layer weights</span>
<span class="n">W_out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="c1"># Output layer weights</span>

<span class="c1"># Define the layers using PyTorch&#39;s functional API</span>
<span class="k">def</span> <span class="nf">in_layer</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">W</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">W</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">out_layer</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">W</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">W</span><span class="p">)</span>

<span class="c1"># Forward pass through the input layers</span>
<span class="n">h0</span> <span class="o">=</span> <span class="n">in_layer</span><span class="p">(</span><span class="n">c0</span><span class="p">,</span> <span class="n">W_in</span><span class="p">)</span> <span class="c1"># you</span>
<span class="n">h1</span> <span class="o">=</span> <span class="n">in_layer</span><span class="p">(</span><span class="n">c1</span><span class="p">,</span> <span class="n">W_in</span><span class="p">)</span> <span class="c1"># goodbye</span>
<span class="n">h</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">h0</span> <span class="o">+</span> <span class="n">h1</span><span class="p">)</span>

<span class="c1"># Forward pass through the output layer (scores)</span>
<span class="n">s</span> <span class="o">=</span> <span class="n">out_layer</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">W_out</span><span class="p">)</span>

<span class="c1"># Print the outputs</span>
<span class="n">h0</span><span class="p">,</span> <span class="n">h1</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">decimals</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(tensor([[ 0.1224, -0.2435, -2.1643]]),
 tensor([[-1.4105, -1.4011, -0.0061]]),
 tensor([[-0.6440, -0.8223, -1.0852]]),
 tensor([[ 1.9580, -2.4850, -1.3920,  0.3000, -0.5260,  2.7190,  0.8220]]))
</pre></div>
</div>
</div>
</div>
<div class="sd-tab-set docutils">
<input checked="checked" id="7d12061f-ea87-41bb-99d6-3a93c94dbad8" name="3e4b7fa6-aca7-4e31-8fa6-32186adba709" type="radio">
</input><label class="sd-tab-label" for="7d12061f-ea87-41bb-99d6-3a93c94dbad8">
課題</label><div class="sd-tab-content docutils">
<p>正解は「say」として、Softmax関数によってスコア<code class="docutils literal notranslate"><span class="pre">s</span></code>を確率として扱えるように変換し、そして、正規化した値と教師ラベルを用いて損失を求めなさい。</p>
</div>
<input id="38ed478b-363f-40f9-b079-4c7730edb13c" name="3e4b7fa6-aca7-4e31-8fa6-32186adba709" type="radio">
</input><label class="sd-tab-label" for="38ed478b-363f-40f9-b079-4c7730edb13c">
ヒント</label><div class="sd-tab-content docutils">
<p>正解は「say」の場合、教師ラベルは<code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1,</span> <span class="pre">0,</span> <span class="pre">0,</span> <span class="pre">0,</span> <span class="pre">0,</span> <span class="pre">0]</span></code>になります。</p>
</div>
</div>
</section>
<section id="id7">
<h4>word2vecの重みと分散表現<a class="headerlink" href="#id7" title="Permalink to this headline">#</a></h4>
<p>与えられたコンテキストに対して単語を予測するときに、「良い重み」のネットワークがあれば、「確率」を表すニューロンにおいて、正解に対応するニューロンが高くなっていることが期待できます。そして、大規模コーパスを使って得られる単語の分散表現は、単語の意味や文法のルールにおいて、人間の直感と合致するケースが多く見られます。</p>
<p>word2vecモデルの学習で行うことが、正しい予測ができるように重みを調整することです。つまり、「コンテキストから出現単語」を予測するという偽タスクをニューラルネットで解いてきましたが、目的はニューラルネットの重みを求めることになります。</p>
<p>もっと具体的に言えば、word2vecで使用されるネットワークには二つの重みがあります。それは、入力層の重み<span class="math notranslate nohighlight">\(\mathbf{W_{in}}\)</span>と、出力層の重み<span class="math notranslate nohighlight">\(\mathbf{W_{out}}\)</span>です。それでは、どちらの重みを使えば良いでしょうか？</p>
<ol class="simple">
<li><p>入力側の重みを利用する</p></li>
<li><p>出力側の重みを利用する</p></li>
<li><p>二つの重みの両方を利用する</p></li>
</ol>
<p>Word2Vecモデルに関しては、多くの研究や応用例で、入力層の重みを単語のベクトル表現として使用さており、良好なパフォーマンスを示しています。</p>
</section>
</section>
</section>
<section id="id8">
<h2>Word2Vecモデルの実装<a class="headerlink" href="#id8" title="Permalink to this headline">#</a></h2>
<section id="id9">
<h3>学習データの準備<a class="headerlink" href="#id9" title="Permalink to this headline">#</a></h3>
<section id="id10">
<h4>コンテキストとターゲット<a class="headerlink" href="#id10" title="Permalink to this headline">#</a></h4>
<p>Word2Vecモデルためのニューラルネットワークでは、「コンテキスト」を入力した時に、「ターゲット」が出現する確率を高くになるように学習を行います。</p>
<p>そのため、コーパスから「コンテキスト」と「ターゲット」が対応するデータを作成する必要があります。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># 前処理関数の実装</span>
<span class="k">def</span> <span class="nf">preprocess</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="c1"># 前処理</span>
    <span class="n">text</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="c1"># 小文字に変換</span>
    <span class="n">text</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;.&#39;</span><span class="p">,</span> <span class="s1">&#39; .&#39;</span><span class="p">)</span> <span class="c1"># ピリオドの前にスペースを挿入</span>
    <span class="n">words</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39; &#39;</span><span class="p">)</span> <span class="c1"># 単語ごとに分割</span>
    
    <span class="c1"># ディクショナリを初期化</span>
    <span class="n">word_to_id</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">id_to_word</span> <span class="o">=</span> <span class="p">{}</span>
    
    <span class="c1"># 未収録の単語をディクショナリに格納</span>
    <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">words</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">word</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">word_to_id</span><span class="p">:</span> <span class="c1"># 未収録の単語のとき</span>
            <span class="c1"># 次の単語のidを取得</span>
            <span class="n">new_id</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">word_to_id</span><span class="p">)</span>
            
            <span class="c1"># 単語をキーとして単語IDを格納</span>
            <span class="n">word_to_id</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_id</span>
            
            <span class="c1"># 単語IDをキーとして単語を格納</span>
            <span class="n">id_to_word</span><span class="p">[</span><span class="n">new_id</span><span class="p">]</span> <span class="o">=</span> <span class="n">word</span>
    
    <span class="c1"># 単語IDリストを作成</span>
    <span class="n">corpus</span> <span class="o">=</span> <span class="p">[</span><span class="n">word_to_id</span><span class="p">[</span><span class="n">w</span><span class="p">]</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">words</span><span class="p">]</span>
    
    <span class="k">return</span> <span class="n">corpus</span><span class="p">,</span> <span class="n">word_to_id</span><span class="p">,</span> <span class="n">id_to_word</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># テキストを設定</span>
<span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;You say goodbye and I say hello.&#39;</span>

<span class="c1"># 前処理</span>
<span class="n">corpus</span><span class="p">,</span> <span class="n">word_to_id</span><span class="p">,</span> <span class="n">id_to_word</span> <span class="o">=</span> <span class="n">preprocess</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">word_to_id</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">id_to_word</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;you&#39;: 0, &#39;say&#39;: 1, &#39;goodbye&#39;: 2, &#39;and&#39;: 3, &#39;i&#39;: 4, &#39;hello&#39;: 5, &#39;.&#39;: 6}
{0: &#39;you&#39;, 1: &#39;say&#39;, 2: &#39;goodbye&#39;, 3: &#39;and&#39;, 4: &#39;i&#39;, 5: &#39;hello&#39;, 6: &#39;.&#39;}
[0, 1, 2, 3, 4, 1, 5, 6]
</pre></div>
</div>
</div>
</div>
<p>テキストの単語を単語IDに変換した<code class="docutils literal notranslate"><span class="pre">corpus</span></code>からターゲットを抽出します。</p>
<p>ターゲットはコンテキストの中央の単語なので、<code class="docutils literal notranslate"><span class="pre">corpus</span></code>の始めと終わりのウインドウサイズ分の単語は含めません。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># ウインドウサイズを指定</span>
<span class="n">window_size</span> <span class="o">=</span> <span class="mi">1</span>

<span class="c1"># ターゲットを抽出</span>
<span class="n">target</span> <span class="o">=</span> <span class="n">corpus</span><span class="p">[</span><span class="n">window_size</span><span class="p">:</span><span class="o">-</span><span class="n">window_size</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">target</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[1, 2, 3, 4, 1, 5]
</pre></div>
</div>
</div>
</div>
<p>ターゲットの単語に対して、for文で前後ウィンドウサイズの範囲の単語を順番に抽出し<code class="docutils literal notranslate"><span class="pre">cs</span></code>に格納します。</p>
<p>つまりウィンドウサイズを<span class="math notranslate nohighlight">\(1\)</span>とすると、<code class="docutils literal notranslate"><span class="pre">corpus</span></code>におけるターゲットのインデックス<code class="docutils literal notranslate"><span class="pre">idx</span></code>に対して、1つ前(<code class="docutils literal notranslate"><span class="pre">idx</span> <span class="pre">-</span> <span class="pre">window_size</span></code>)から1つ後(<code class="docutils literal notranslate"><span class="pre">idx</span> <span class="pre">+</span> <span class="pre">window_size</span></code>)までの範囲の単語を順番に<code class="docutils literal notranslate"><span class="pre">cs</span></code>格納します。ただしターゲット自体の単語はコンテキストに含めません。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># コンテキストを初期化(受け皿を作成)</span>
<span class="n">contexts</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># 1つ目のターゲットのインデックス</span>
<span class="n">idx</span> <span class="o">=</span> <span class="n">window_size</span>

<span class="c1"># 1つ目のターゲットのコンテキストを初期化(受け皿を作成)</span>
<span class="n">cs</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># 1つ目のターゲットのコンテキストを1単語ずつ格納</span>
<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="o">-</span><span class="n">window_size</span><span class="p">,</span> <span class="n">window_size</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
    
    <span class="c1"># tがターゲットのインデックスのとき処理しない</span>
    <span class="k">if</span> <span class="n">t</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">continue</span>
    
    <span class="c1"># コンテキストを格納</span>
    <span class="n">cs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">corpus</span><span class="p">[</span><span class="n">idx</span> <span class="o">+</span> <span class="n">t</span><span class="p">])</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">cs</span><span class="p">)</span>

<span class="c1"># 1つ目のターゲットのコンテキストを格納</span>
<span class="n">contexts</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">cs</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">contexts</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[0]
[0, 2]
[[0, 2]]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># コンテキストとターゲットの作成関数の実装</span>
<span class="k">def</span> <span class="nf">create_contexts_target</span><span class="p">(</span><span class="n">corpus</span><span class="p">,</span> <span class="n">window_size</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    
    <span class="c1"># ターゲットを抽出</span>
    <span class="n">target</span> <span class="o">=</span> <span class="n">corpus</span><span class="p">[</span><span class="n">window_size</span><span class="p">:</span><span class="o">-</span><span class="n">window_size</span><span class="p">]</span>
    
    <span class="c1"># コンテキストを初期化</span>
    <span class="n">contexts</span> <span class="o">=</span> <span class="p">[]</span>
    
    <span class="c1"># ターゲットごとにコンテキストを格納</span>
    <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">window_size</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span> <span class="o">-</span> <span class="n">window_size</span><span class="p">):</span>
        
        <span class="c1"># 現在のターゲットのコンテキストを初期化</span>
        <span class="n">cs</span> <span class="o">=</span> <span class="p">[]</span>
        
        <span class="c1"># 現在のターゲットのコンテキストを1単語ずつ格納</span>
        <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="o">-</span><span class="n">window_size</span><span class="p">,</span> <span class="n">window_size</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
            
            <span class="c1"># 0番目の要素はターゲットそのものなので処理を省略</span>
            <span class="k">if</span> <span class="n">t</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">continue</span>
            
            <span class="c1"># コンテキストを格納</span>
            <span class="n">cs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">corpus</span><span class="p">[</span><span class="n">idx</span> <span class="o">+</span> <span class="n">t</span><span class="p">])</span>
            
        <span class="c1"># 現在のターゲットのコンテキストのセットを格納</span>
        <span class="n">contexts</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">cs</span><span class="p">)</span>
    
    <span class="c1"># NumPy配列に変換</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">contexts</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">target</span><span class="p">)</span> 
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># コンテキストとターゲットを作成</span>
<span class="n">contexts</span><span class="p">,</span> <span class="n">targets</span> <span class="o">=</span> <span class="n">create_contexts_target</span><span class="p">(</span><span class="n">corpus</span><span class="p">,</span> <span class="n">window_size</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">contexts</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">targets</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[0 2]
 [1 3]
 [2 4]
 [3 1]
 [4 5]
 [1 6]]
[1 2 3 4 1 5]
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="pytorchcbow">
<h3>PytorchでCBOWモデルの実装<a class="headerlink" href="#pytorchcbow" title="Permalink to this headline">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span><span class="p">,</span> <span class="n">Dataset</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s1">&#39;cpu&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<section id="embedding">
<h4>Embeddingレイヤ<a class="headerlink" href="#embedding" title="Permalink to this headline">#</a></h4>
<p>先ほど、理解しやすいone-hot表現でコンテキストを変換する方法を説明しましたが、大規模なコーパスで学習する際、one-hot表現の次元数も大きくになって、非効率な学習の原因になります。</p>
<p>ただ、one-hot表現による計算は、単に行列の特定の行を抜き出すことだけですから、同じ機能を持つレイヤで入れ替えることは可能です。このような、重みパラメータから「単語IDに該当する行(ベクトル)」を抜き出すためのレイヤは「Embeddingレイヤ」と言います。</p>
<p>PyTorchで提供されるモジュール<code class="docutils literal notranslate"><span class="pre">nn.Embedding</span></code>を使うと、簡単にEmbeddingレイヤを実装することができます。</p>
<p>例えば、語彙に6つの単語があり、各埋め込みベクトルの次元数を3に設定した場合、nn.Embeddingの定義は以下のようになります。</p>
<p>そして、</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">embedding_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>もしインデックス2のトークンの埋め込みを取得したい場合、次のようにします：</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>
<span class="n">embedding</span> <span class="o">=</span> <span class="n">embedding_layer</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
<span class="n">embedding</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[[-0.3032,  1.2471,  0.5277],
         [-0.0986, -0.0490, -1.0046]]], grad_fn=&lt;EmbeddingBackward0&gt;)
</pre></div>
</div>
</div>
</div>
<p>埋め込みベクトルの和を取って、入力層から中間層までにエンコードの機能を実装できます。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">out</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">embedding</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">out</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[-0.4017,  1.1981, -0.4769]], grad_fn=&lt;SumBackward1&gt;)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">linear1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">F</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">linear1</span><span class="p">(</span><span class="n">out</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[-1.8239, -1.8256, -1.6864, -2.0974, -1.3789, -2.1398]],
       grad_fn=&lt;LogSoftmaxBackward0&gt;)
</pre></div>
</div>
</div>
</div>
</section>
<section id="id11">
<h4>ミニバッチ化データセットの作成<a class="headerlink" href="#id11" title="Permalink to this headline">#</a></h4>
<p>Word2Vecも含めて、深層学習によって学習を行う際には、ミニバッチ化して学習させることが一般的です。</p>
<p>pytorchで提供されている<code class="docutils literal notranslate"><span class="pre">DataSet</span></code>と<code class="docutils literal notranslate"><span class="pre">DataLoader</span></code>という機能を用いてミニバッチ化を簡単に実現できます。</p>
<section id="dataset">
<h5>DataSet<a class="headerlink" href="#dataset" title="Permalink to this headline">#</a></h5>
<p>DataSetは，元々のデータを全て持っていて、ある番号を指定されると、その番号の入出力のペアをただ一つ返します。クラスを使って実装します。</p>
<p>DataSetを実装する際には、クラスのメンバ関数として<code class="docutils literal notranslate"><span class="pre">__len__()</span></code>と<code class="docutils literal notranslate"><span class="pre">__getitem__()</span></code>を必ず作ります．</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">__len__()</span></code>は、<code class="docutils literal notranslate"><span class="pre">len()</span></code>を使ったときに呼ばれる関数です。</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">__getitem__()</span></code>は、<code class="docutils literal notranslate"><span class="pre">array[i]</span></code>のようにインデックスを使って要素を参照するときに呼ばれる関数です。</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">CBOWDataset</span><span class="p">(</span><span class="n">Dataset</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">contexts</span><span class="p">,</span> <span class="n">targets</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">contexts</span> <span class="o">=</span> <span class="n">contexts</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">targets</span> <span class="o">=</span> <span class="n">targets</span>
    
    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">targets</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">contexts</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">targets</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Convert contexts and targets to tensors</span>
<span class="n">contexts_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">contexts</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">targets_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">targets</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># Create the dataset</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">CBOWDataset</span><span class="p">(</span><span class="n">contexts_tensor</span><span class="p">,</span> <span class="n">targets_tensor</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;全データ数:&#39;</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;4番目のデータ:&#39;</span><span class="p">,</span><span class="n">dataset</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span> 
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;4~5番目のデータ:&#39;</span><span class="p">,</span><span class="n">dataset</span><span class="p">[</span><span class="mi">3</span><span class="p">:</span><span class="mi">5</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>全データ数: 6
4番目のデータ: (tensor([3, 1]), tensor(4))
4~5番目のデータ: (tensor([[3, 1],
        [4, 5]]), tensor([4, 1]))
</pre></div>
</div>
</div>
</div>
</section>
<section id="dataloader">
<h5>DataLoader<a class="headerlink" href="#dataloader" title="Permalink to this headline">#</a></h5>
<p><code class="docutils literal notranslate"><span class="pre">torch.utils.data</span></code>モジュールには、データのシャッフとミニバッチの整形に役立つ<code class="docutils literal notranslate"><span class="pre">DataLoader</span></code>というクラスが用意されます。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create the DataLoader</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">2</span>  <span class="c1"># You can adjust the batch size</span>
<span class="n">data_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">data_loader</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[tensor([[1, 6],
        [2, 4]]), tensor([5, 3])]
[tensor([[4, 5],
        [0, 2]]), tensor([1, 1])]
[tensor([[1, 3],
        [3, 1]]), tensor([2, 4])]
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="cbow">
<h4>CBOWモデルの構築<a class="headerlink" href="#cbow" title="Permalink to this headline">#</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span><span class="p">,</span> <span class="n">Dataset</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">self.embeddings</span> <span class="pre">=</span> <span class="pre">nn.Embedding(vocab_size,</span> <span class="pre">embedding_size)</span></code>: 語彙の各単語に対して<code class="docutils literal notranslate"><span class="pre">embedding_size</span></code>次元のベクトルを割り当てる埋め込み層を作成します。</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">self.linear1</span> <span class="pre">=</span> <span class="pre">nn.Linear(embedding_size,</span> <span class="pre">vocab_size)</span></code>: 埋め込みベクトルを受け取り、語彙のサイズに対応する出力を生成します。</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">embeds</span> <span class="pre">=</span> <span class="pre">self.embeddings(inputs)</span></code>:入力された単語のインデックスに基づいて、埋め込み層から対応するベクトルを取得します。</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">SimpleCBOW</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">embedding_size</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SimpleCBOW</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embeddings</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embedding_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embedding_size</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="n">embeds</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embeddings</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">embeds</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear1</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">log_probs</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">log_probs</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># パラメータの設定</span>
<span class="n">embedding_size</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">vocab_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">word_to_id</span><span class="p">)</span>

<span class="c1"># モデルのインスタンス化</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">SimpleCBOW</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embedding_size</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">loss_function</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>


<span class="c1"># Training loop with batch processing</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
    <span class="n">total_loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">context_batch</span><span class="p">,</span> <span class="n">target_batch</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">data_loader</span><span class="p">):</span>
        <span class="c1"># Zero out the gradients from the last step</span>
        <span class="n">model</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="c1"># Forward pass through the model</span>
        <span class="n">log_probs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">context_batch</span><span class="p">)</span>
        <span class="c1"># Compute the loss</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_function</span><span class="p">(</span><span class="n">log_probs</span><span class="p">,</span> <span class="n">target_batch</span><span class="p">)</span>
        <span class="c1"># Backward pass to compute gradients</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="c1"># Update the model parameters</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="c1"># Accumulate the loss</span>
        <span class="n">total_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
    <span class="c1"># Log the total loss for the epoch</span>
    <span class="k">if</span> <span class="n">epoch</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s1">, Total loss: </span><span class="si">{</span><span class="n">total_loss</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 0, Total loss: 6.11926794052124
Epoch 10, Total loss: 4.030880689620972
Epoch 20, Total loss: 2.9468559622764587
Epoch 30, Total loss: 2.3361576795578003
Epoch 40, Total loss: 1.9521117806434631
Epoch 50, Total loss: 1.6938353776931763
Epoch 60, Total loss: 1.515379935503006
Epoch 70, Total loss: 1.3846017718315125
Epoch 80, Total loss: 1.2865969985723495
Epoch 90, Total loss: 1.2101848423480988
</pre></div>
</div>
</div>
</div>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p><code class="docutils literal notranslate"><span class="pre">nn.CrossEntropyLoss</span></code>ターゲットラベルをクラスのインデックスとして受け取り、内部で必要な変換を行いますので、ターゲットをワンホットエンコーディングに変換する必要はありません。</p>
</aside>
<p>モデルの入力層の重みが単語分散表現であり、<span class="math notranslate nohighlight">\(単語 \times  埋め込み次元数\)</span>の形の行列になります。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">embeddings</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([7, 10])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">word_embeddings</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">embeddings</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span>

<span class="c1"># 各単語とそれに対応する分散表現を表示</span>
<span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">word_to_id</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">vector</span> <span class="o">=</span> <span class="n">word_embeddings</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Word: </span><span class="si">{</span><span class="n">word</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Vector: </span><span class="si">{</span><span class="n">vector</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Word: you
Vector: [ 0.43640447  2.3994656  -0.66580415 -1.234674    0.9123322   0.6803118
  1.0000849  -1.5062922  -1.2563758  -1.0379189 ]

Word: say
Vector: [-0.5300385   2.7432108   1.0345757  -0.07852454  0.10993158  0.10987139
 -1.8046503   1.3043185   0.3196185  -0.01576389]

Word: goodbye
Vector: [-1.3162869   0.74525213  1.0486401   1.687265    1.0929755  -0.41140622
  0.03988009  0.6244155   0.46562314 -1.2834216 ]

Word: and
Vector: [ 1.7745872   0.21953514  0.2982137   0.39705753  1.0635732  -1.9176912
  0.7215232  -0.12871008 -1.112822    0.41692844]

Word: i
Vector: [ 0.5610625  -0.3060593  -1.854972    0.5215753  -1.4160546   0.30283335
  0.69122404 -0.04279682  1.5830791   0.41434464]

Word: hello
Vector: [ 1.1180049   0.45558804  0.10969823 -1.3822559   0.27533448  0.89500386
 -0.50628495 -1.0254172  -0.06035456 -1.0706526 ]

Word: .
Vector: [-0.7480286  -0.7040065  -0.76120883  1.2911949  -1.2853703   1.6336079
 -0.31998158  0.6653414   3.1988146   1.4568119 ]
</pre></div>
</div>
</div>
</div>
<div class="sd-tab-set docutils">
<input checked="checked" id="e59a1d66-92c6-45ee-814b-08ec9c99c446" name="1aa1fb95-694d-4278-8071-226cf140976f" type="radio">
</input><label class="sd-tab-label" for="e59a1d66-92c6-45ee-814b-08ec9c99c446">
課題</label><div class="sd-tab-content docutils">
<p>与えられたテキストを用いて、単語分散表現を学習しなさい。</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">window_size</span></code>を2に設定します</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">batch_size</span></code>を10に設定します</p></li>
</ul>
</div>
<input id="6f4ae699-a06d-4eb0-bf0f-70b5c816d974" name="1aa1fb95-694d-4278-8071-226cf140976f" type="radio">
</input><label class="sd-tab-label" for="6f4ae699-a06d-4eb0-bf0f-70b5c816d974">
テキスト</label><div class="sd-tab-content docutils">
<p>““When forty winters shall besiege thy brow,
And dig deep trenches in thy beauty’s field,
Thy youth’s proud livery so gazed on now,
Will be a totter’d weed of small worth held:
Then being asked, where all thy beauty lies,
Where all the treasure of thy lusty days;
To say, within thine own deep sunken eyes,
Were an all-eating shame, and thriftless praise.
How much more praise deserv’d thy beauty’s use,
If thou couldst answer ‘This fair child of mine
Shall sum my count, and make my old excuse,’
Proving his beauty by succession thine!
This were to be new made when thou art old,
And see thy blood warm when thou feel’st it cold.””</p>
</div>
</div>
</section>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./notebook"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="word2vec_1.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">単語分散表現</p>
        </div>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By 呂　沢宇<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>