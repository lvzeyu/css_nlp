
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>word2vec &#8212; 計算社会科学のための自然言語処理</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="prev" title="単語分散表現" href="word2vec_1.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/tohoku-university-logo-vector.svg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">計算社会科学のための自然言語処理</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    計算社会科学と自然言語処理
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  イントロダクション
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="introduction.html">
   ガイダンス
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  基礎知識
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="nlp_basis.html">
   自然言語処理の基礎
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ml_basis.html">
   機械学習の基本概念
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  ニューラルネットワーク
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="NN.html">
   ニューラルネットワーク
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="backpropagation.html">
   誤差逆伝播法
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  PyTorch
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="pytorch.html">
   Pytorch
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  単語分散表現
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="word2vec_1.html">
   単語分散表現
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   word2vec
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://mybinder.org/v2/gh/lvzeyu/css_nlp/master?urlpath=lab/tree/notebook/word2vec_2.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../_static/images/logo_binder.svg">
  </span>
<span class="headerbtn__text-container">Binder</span>
</a>

      </li>
      
      <li>
        <a href="https://colab.research.google.com/github/lvzeyu/css_nlp/blob/master/notebook/word2vec_2.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Colab"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../_static/images/logo_colab.png">
  </span>
<span class="headerbtn__text-container">Colab</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/lvzeyu/css_nlp/tree/master"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/lvzeyu/css_nlp/tree/master/issues/new?title=Issue%20on%20page%20%2Fnotebook/word2vec_2.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../_sources/notebook/word2vec_2.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id3">
   推論ベース手法とニューラルネットワーク
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id4">
     推論ベース手法の設計
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#one-hot">
     one-hot表現
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#cbow-continuous-bag-of-words">
     CBOW（continuous bag-of-words）モデル
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id5">
       入力層から中間層(エンコード)
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id6">
       中間層から出力層(デコード)
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#cbow">
     CBOWモデルの学習
    </a>
   </li>
  </ul>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>word2vec</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id3">
   推論ベース手法とニューラルネットワーク
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id4">
     推論ベース手法の設計
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#one-hot">
     one-hot表現
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#cbow-continuous-bag-of-words">
     CBOW（continuous bag-of-words）モデル
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id5">
       入力層から中間層(エンコード)
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id6">
       中間層から出力層(デコード)
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#cbow">
     CBOWモデルの学習
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="word2vec">
<h1>word2vec<a class="headerlink" href="#word2vec" title="Permalink to this headline">#</a></h1>
<p>前章では、「カウントベースの手法」によって単語分散表現を得ました。具体的には、単語の共起行列を作り、その行列に対してSVDを適用することで、密なベクトくー 単語分散表現ーを獲得したのです。</p>
<p>しかし、カウントベースの手法にはいくつかの問題点があります。</p>
<ul class="simple">
<li><p>大規模なコーパスを扱う場合、巨大な共起行列に対してSVDを計算することが難しい。</p></li>
<li><p>コーパスの全体から一回の学習で単語分散表現を獲得していますので、新しい単語が追加される場合、再度最初から学習を行う必要があり、単語分散表現更新の効率が低い。</p></li>
</ul>
<p>「カウントベースの手法」に代わる強力な手法として「推論ベース」の手法が挙げられます。特に、Mikolov et al. <span id="id1">[<a class="reference internal" href="introduction.html#id20" title="Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. Linguistic regularities in continuous space word representations. In Lucy Vanderwende, Hal Daumé III, and Katrin Kirchhoff, editors, Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 746–751. Atlanta, Georgia, June 2013. Association for Computational Linguistics. URL: https://aclanthology.org/N13-1090.">Mikolov <em>et al.</em>, 2013</a>]</span> <span id="id2">[<a class="reference internal" href="introduction.html#id21" title="Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed representations of words and phrases and their compositionality. In C.J. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems, volume 26. Curran Associates, Inc., 2013. URL: https://proceedings.neurips.cc/paper_files/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf.">Mikolov <em>et al.</em>, 2013</a>]</span>　によって提案されたword2vecの有用性が多くの自然言語処理タスクにおいて示されてきたのです。</p>
<p>本章では、word2vecの仕組みについて説明し、それを実装することで理解を深めます。</p>
<section id="id3">
<h2>推論ベース手法とニューラルネットワーク<a class="headerlink" href="#id3" title="Permalink to this headline">#</a></h2>
<p>推論ベースの手法は、ミニバッチで学習する形で、ニューラルネットワークを用いて、重みを繰り返し更新することで単語分散表現を獲得します。</p>
<p><img alt="" src="../_images/inference.png" /></p>
<section id="id4">
<h3>推論ベース手法の設計<a class="headerlink" href="#id4" title="Permalink to this headline">#</a></h3>
<p>推論ベース手法では、<code class="docutils literal notranslate"><span class="pre">you</span> <span class="pre">【？】</span> <span class="pre">goodbye</span> <span class="pre">and</span> <span class="pre">I</span> <span class="pre">say</span> <span class="pre">hello</span> <span class="pre">.</span></code>のような、周囲の単語が与えられたときに、<code class="docutils literal notranslate"><span class="pre">【？】</span></code>にどのような単語が出現するのかを推測する推論問題を繰り返し解くことで、単語の出現バターンを学習します。</p>
<p>つまり、コンテキスト情報を入力として受け取り、各単語の出現する確率を出力する「モデル」を作成することは目標になります。ここで、正しい推測ができるように、コーパスを使って、ニューラルネットワークモデルの学習を行います。そして、その学習の結果として、単語の分散表現を得られます。</p>
<p><img alt="" src="../_images/inference2.png" /></p>
</section>
<section id="one-hot">
<h3>one-hot表現<a class="headerlink" href="#one-hot" title="Permalink to this headline">#</a></h3>
<p>ニューラルネットワークで単語を処理するには、それを「固定長のベクトル」に変換する必要があります。</p>
<p>そのための方法の一つは、単語をone-hot表現へと変換することです。one-hot表現とは、ベクトルの要素の中で一つだけが<span class="math notranslate nohighlight">\(1\)</span>で、残りは全て<span class="math notranslate nohighlight">\(0\)</span>であるようなベクトルと言います。</p>
<p>単語をone-hot表現に変換するには、語彙数分の要素を持つベクトルを用意して、単語IDの該当する箇所を<span class="math notranslate nohighlight">\(1\)</span>に、残りは全て<span class="math notranslate nohighlight">\(0\)</span>に設定します。</p>
<p><img alt="" src="../_images/one-hot.png" /></p>
</section>
<section id="cbow-continuous-bag-of-words">
<h3>CBOW（continuous bag-of-words）モデル<a class="headerlink" href="#cbow-continuous-bag-of-words" title="Permalink to this headline">#</a></h3>
<p>CBOWモデルは、コンテキストからターゲットを推測することを目的としたニューラルネットワークです。このCBOWモデルで、できるだけ正確な推測ができるように訓練することで、単語の分散表現を取得することができます。</p>
<p>ここで、例として、コンテキスト<code class="docutils literal notranslate"><span class="pre">[&quot;you&quot;,&quot;goodbye&quot;]</span></code>からターゲット<code class="docutils literal notranslate"><span class="pre">&quot;say&quot;</span></code>を予測するタスクを考えます。</p>
<p><img alt="" src="../_images/cbow.png" /></p>
<section id="id5">
<h4>入力層から中間層(エンコード)<a class="headerlink" href="#id5" title="Permalink to this headline">#</a></h4>
<p>one-hotエンコーディングで、単語を固定長のベクトルに変換するすることができます。</p>
<p>単語をベクトルで表すことができれば、そのベクトルはニューラルネットワークを構成する「レイヤ」によって処理することができるようになりました。</p>
<p>コンテキストを<span class="math notranslate nohighlight">\(\mathbf{c}\)</span>、重みを<span class="math notranslate nohighlight">\(\mathbf{W}\)</span>とし、それぞれ次の形状とします。</p>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p>イメージしやすいように、ここでは添字を対応する単語で表すことにします。ただし「.(ピリオド)」については「priod」とします。</p>
</aside>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{c}
    = \begin{pmatrix}
          c_{\mathrm{you}} &amp; c_{\mathrm{say}} &amp; c_{\mathrm{goodbye}} &amp; c_{\mathrm{and}} &amp; c_{\mathrm{I}} &amp; c_{\mathrm{hello}} &amp; c_{\mathrm{period}}
      \end{pmatrix}
,\ 
\mathbf{W}
    = \begin{pmatrix}
          w_{\mathrm{you},1} &amp; w_{\mathrm{you},2} &amp; w_{\mathrm{you},3} \\
          w_{\mathrm{say},1} &amp; w_{\mathrm{say},2} &amp; w_{\mathrm{say},3} \\
          w_{\mathrm{goodbye},1} &amp; w_{\mathrm{goodbye},2} &amp; w_{\mathrm{goodbye},3} \\
          w_{\mathrm{and},1} &amp; w_{\mathrm{and},2} &amp; w_{\mathrm{and},3} \\
          w_{\mathrm{I},1} &amp; w_{\mathrm{I},2} &amp; w_{\mathrm{I},3} \\
          w_{\mathrm{hello},1} &amp; w_{\mathrm{hello},2} &amp; w_{\mathrm{hello},3} \\
          w_{\mathrm{period},1} &amp; w_{\mathrm{period},2} &amp; w_{\mathrm{period},3} \\
      \end{pmatrix}
\end{split}\]</div>
<p>コンテキストの要素数(列数)と重みの行数が、単語の種類数に対応します。</p>
<p>コンテキスト(単語)はone-hot表現として扱うため、例えば「you」の場合は</p>
<div class="math notranslate nohighlight">
\[
\mathbf{c}_{\mathrm{you}}
    = \begin{pmatrix}
          1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0
      \end{pmatrix}
\]</div>
<p>とすることで、単語「you」を表現できます。</p>
<p>重み付き和<span class="math notranslate nohighlight">\(\mathbf{h}\)</span>は、行列の積で求められます。</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbf{h}
   &amp;= \mathbf{c}_{\mathrm{you}}
      \mathbf{W}
\\
   &amp;= \begin{pmatrix}
          h_1 &amp; h_2 &amp; h_3
      \end{pmatrix}
\end{aligned}
\end{split}\]</div>
<p><span class="math notranslate nohighlight">\(h_1\)</span>の計算を詳しく見ると、次のようになります。</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
h_1
   &amp;= c_{\mathrm{you}} w_{\mathrm{you},1}
      + c_{\mathrm{say}} w_{\mathrm{say},1}
      + c_{\mathrm{goodbye}} w_{\mathrm{goodbye},1}
      + c_{\mathrm{and}} w_{\mathrm{and},1}
      + c_{\mathrm{I}} w_{\mathrm{I},1}
      + c_{\mathrm{hello}} w_{\mathrm{hello},1}
      + c_{\mathrm{period}} w_{\mathrm{period},1}
\\
   &amp;= 1 w_{\mathrm{you},1}
      + 0 w_{\mathrm{say},1}
      + 0 w_{\mathrm{goodbye},1}
      + 0 w_{\mathrm{and},1}
      + 0 w_{\mathrm{I},1}
      + 0 w_{\mathrm{hello},1}
      + 0 w_{\mathrm{period},1}
\\
   &amp;= w_{\mathrm{you},1}
\end{aligned}
\end{split}\]</div>
<p>コンテキストと重みの対応する(同じ単語に関する)要素を掛けて、全ての単語で和をとります。しかしコンテキストは、<span class="math notranslate nohighlight">\(c_{you}\)</span>以外の要素が<span class="math notranslate nohighlight">\(0\)</span>なので、対応する重みの値の影響は消えていまします。また<span class="math notranslate nohighlight">\(c_{you}\)</span>は<span class="math notranslate nohighlight">\(1\)</span>なので、対応する重みの値<span class="math notranslate nohighlight">\(w_{\mathrm{you},1}\)</span>がそのまま中間層のニューロンに伝播します。</p>
<p>残りの2つの要素も同様に計算できるので、重み付き和</p>
<div class="math notranslate nohighlight">
\[
\mathbf{h}
    = \begin{pmatrix}
          w_{\mathrm{you},1} &amp; w_{\mathrm{you},2} &amp; w_{\mathrm{you},3}
      \end{pmatrix}
\]</div>
<p>は、単語「you」に関する重みの値となります。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="c1"># 適当にコンテキスト(one-hot表現)を指定</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;コンテキストの形状：</span><span class="si">{</span><span class="n">c</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># 重みをランダムに生成</span>
<span class="n">W</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;重み</span><span class="se">\n</span><span class="si">{</span><span class="n">W</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># 重み付き和を計算</span>
<span class="n">h</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">W</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;重み付き和</span><span class="se">\n</span><span class="si">{</span><span class="n">h</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;重み付き和の形状：</span><span class="si">{</span><span class="n">h</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>コンテキストの形状：(1, 7)
重み
[[-0.11689672 -0.87299645 -0.77468774]
 [-1.85511156 -0.11544881  1.60131885]
 [ 1.91680957 -0.8995188   0.08133384]
 [ 1.01660097  1.09108248 -0.10453889]
 [-0.07942408  1.03069214  0.82228615]
 [-0.14563418  0.33842248 -0.58624678]
 [-0.7433799  -0.30346645 -0.73233679]]
重み付き和
[[-0.11689672 -0.87299645 -0.77468774]]
重み付き和の形状：(1, 3)
</pre></div>
</div>
</div>
</div>
<p>コンテキストに複数な単語がある場合、入力層も複数になります。このとき、中間層にあるニューロンは、各入力層の全結合による変換後の値が平均されたものになります。</p>
<p>中間層のニューロンの数を入力層よりも減らすことによって、中間層には、単語を予測するために必要な情報が”コンパクト”に収められて、結果としては密なベクトル表現が得られます。このとき、この中間層の情報は、人間には理解できない「ブラックボックス」ような状態になります。この作業は、「エンコード」と言います。</p>
</section>
<section id="id6">
<h4>中間層から出力層(デコード)<a class="headerlink" href="#id6" title="Permalink to this headline">#</a></h4>
<p>中間層の情報から目的の結果を得る作業は、「デコード」と言います。ここでは、中間層のニューロンの値<span class="math notranslate nohighlight">\(\mathbf{h}\)</span>を各単語に対応した値になるように、つまり要素(行)数が単語の種類数となるように再度変換したものを、CBOWモデルの出力とします。</p>
<p>出力層の重みを</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{W}_{\mathrm{out}}
    = \begin{pmatrix}
          w_{1,\mathrm{you}} &amp; w_{1,\mathrm{say}} &amp; w_{1,\mathrm{goodbye}} &amp; w_{1,\mathrm{and}} &amp;
          w_{1,\mathrm{I}} &amp; w_{1,\mathrm{hello}} &amp; w_{1,\mathrm{period}} \\
          w_{2,\mathrm{you}} &amp; w_{2,\mathrm{say}} &amp; w_{2,\mathrm{goodbye}} &amp; w_{2,\mathrm{and}} &amp;
          w_{2,\mathrm{I}} &amp; w_{2,\mathrm{hello}} &amp; w_{2,\mathrm{period}} \\
          w_{3,\mathrm{you}} &amp; w_{3,\mathrm{say}} &amp; w_{3,\mathrm{goodbye}} &amp; w_{3,\mathrm{and}} &amp;
          w_{3,\mathrm{I}} &amp; w_{3,\mathrm{hello}} &amp; w_{3\mathrm{period}} \\
      \end{pmatrix}
\end{split}\]</div>
<p>とします。行数が中間層のニューロン数、列数が単語の種類数になります。</p>
<p>出力層も全結合層とすると、最終的な出力は</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbf{s}
   &amp;= \mathbf{h}
      \mathbf{W}_{\mathrm{out}}
\\
   &amp;= \begin{pmatrix}
          s_{\mathrm{you}} &amp; s_{\mathrm{say}} &amp; s_{\mathrm{goodbye}} &amp; s_{\mathrm{and}} &amp;
          s_{\mathrm{I}} &amp; s_{\mathrm{hello}} &amp; s_{\mathrm{period}}
      \end{pmatrix}
\end{aligned}
\end{split}\]</div>
<p>例えば、「you」に関する要素の計算は、</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
s_{\mathrm{you}}
   &amp;= \frac{1}{2} (w_{\mathrm{you},1} + w_{\mathrm{goodbye},1}) w_{1,\mathrm{you}}
      + \frac{1}{2} (w_{\mathrm{you},2} + w_{\mathrm{goodbye},2}) w_{2,\mathrm{you}}
      + \frac{1}{2} (w_{\mathrm{you},3} + w_{\mathrm{goodbye},3}) w_{3,\mathrm{you}}
\\
   &amp;= \frac{1}{2}
      \sum_{i=1}^3
          (w_{\mathrm{you},i} + w_{\mathrm{goodbye},i}) w_{i,\mathrm{you}}
\end{aligned}\end{split}\]</div>
<p>コンテキストに対応する入力層の重みの平均と「you」に関する出力の重みの積になります。</p>
<p>他の要素(単語)についても同様に計算できるので、最終的な出力は</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbf{s}
   &amp;= \begin{pmatrix}
          s_{\mathrm{you}} &amp; s_{\mathrm{say}} &amp; s_{\mathrm{goodbye}} &amp; s_{\mathrm{and}} &amp;
          s_{\mathrm{I}} &amp; s_{\mathrm{hello}} &amp; s_{\mathrm{period}}
      \end{pmatrix}
\\
   &amp;= \begin{pmatrix}
          \frac{1}{2} \sum_{i=1}^3 (w_{\mathrm{you},i} + w_{\mathrm{goodbye},i}) w_{i,\mathrm{you}} &amp;
          \frac{1}{2} \sum_{i=1}^3 (w_{\mathrm{you},i} + w_{\mathrm{goodbye},i}) w_{i,\mathrm{say}} &amp;
          \cdots &amp;
          \frac{1}{2} \sum_{i=1}^3 (w_{\mathrm{you},i} + w_{\mathrm{goodbye},i}) w_{i,\mathrm{hello}} &amp;
          \frac{1}{2} \sum_{i=1}^3 (w_{\mathrm{you},i} + w_{\mathrm{goodbye},i}) w_{i,\mathrm{period}}
      \end{pmatrix}
\end{aligned}
\end{split}\]</div>
<p>となります。</p>
<p>ここで、出力層のニューロンは各単語に対応し、各単語の「スコア」と言います。</p>
<p>「スコア」の値が高ければ高いほど、それに対応する単語の出現確率も高くなり、ターゲットの単語であるとして採用します。そのため、スコアを求める処理を推論処理と言います。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="c1"># Define the context data</span>
<span class="n">c0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="c1"># you</span>
<span class="n">c1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="c1"># goodbye</span>

<span class="c1"># Initialize weights randomly</span>
<span class="n">W_in</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>  <span class="c1"># Input layer weights</span>
<span class="n">W_out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="c1"># Output layer weights</span>

<span class="c1"># Define the layers using PyTorch&#39;s functional API</span>
<span class="k">def</span> <span class="nf">in_layer</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">W</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">W</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">out_layer</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">W</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">W</span><span class="p">)</span>

<span class="c1"># Forward pass through the input layers</span>
<span class="n">h0</span> <span class="o">=</span> <span class="n">in_layer</span><span class="p">(</span><span class="n">c0</span><span class="p">,</span> <span class="n">W_in</span><span class="p">)</span> <span class="c1"># you</span>
<span class="n">h1</span> <span class="o">=</span> <span class="n">in_layer</span><span class="p">(</span><span class="n">c1</span><span class="p">,</span> <span class="n">W_in</span><span class="p">)</span> <span class="c1"># goodbye</span>
<span class="n">h</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">h0</span> <span class="o">+</span> <span class="n">h1</span><span class="p">)</span>

<span class="c1"># Forward pass through the output layer (scores)</span>
<span class="n">s</span> <span class="o">=</span> <span class="n">out_layer</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">W_out</span><span class="p">)</span>

<span class="c1"># Print the outputs</span>
<span class="n">h0</span><span class="p">,</span> <span class="n">h1</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">decimals</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(tensor([[-1.3467, -1.5459, -0.9450]]),
 tensor([[-1.1224,  0.4517, -1.0530]]),
 tensor([[-1.2346, -0.5471, -0.9990]]),
 tensor([[-2.4770,  1.5870,  2.2100,  0.6500,  0.8110, -1.7260, -0.7230]]))
</pre></div>
</div>
</div>
</div>
<div class="sd-tab-set docutils">
<input checked="checked" id="622bbb0f-6f3f-4a72-b8b9-430fe33e649f" name="93e0a589-6ac0-4ef0-b4ca-a69d28f787a8" type="radio">
</input><label class="sd-tab-label" for="622bbb0f-6f3f-4a72-b8b9-430fe33e649f">
課題</label><div class="sd-tab-content docutils">
<p>正解は「you」として、Softmax関数によってスコア<code class="docutils literal notranslate"><span class="pre">s</span></code>を確率として扱えるように変換し、そして、正規化した値と教師ラベルを用いて損失を求めなさい。</p>
</div>
<input id="e6064bbc-8e3a-492a-ab90-4be98cf1368f" name="93e0a589-6ac0-4ef0-b4ca-a69d28f787a8" type="radio">
</input><label class="sd-tab-label" for="e6064bbc-8e3a-492a-ab90-4be98cf1368f">
ヒント</label><div class="sd-tab-content docutils">
<p>正解は「you」の場合、教師ラベルは<code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1,</span> <span class="pre">0,</span> <span class="pre">0,</span> <span class="pre">0,</span> <span class="pre">0,</span> <span class="pre">0]</span></code>になります。</p>
</div>
</div>
</section>
</section>
<section id="cbow">
<h3>CBOWモデルの学習<a class="headerlink" href="#cbow" title="Permalink to this headline">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[0.0044, 0.2584, 0.4818, 0.1013, 0.1190, 0.0094, 0.0256]])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
<span class="n">loss</span><span class="p">(</span><span class="n">s</span><span class="p">,</span><span class="n">t</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor(1.3532)
</pre></div>
</div>
</div>
</div>
<p>t = np.array([0, 1, 0, 0, 0, 0, 0, 0, 0, 0])</p>
<p>loss()</p>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./notebook"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="word2vec_1.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">単語分散表現</p>
        </div>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By 呂　沢宇<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>