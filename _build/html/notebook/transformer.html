
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Transformerアーキテクチャ &#8212; 計算社会科学のための自然言語処理</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="BERT" href="BERT.html" />
    <link rel="prev" title="Self-Attention" href="self-attention.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/tohoku-university-logo-vector.svg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">計算社会科学のための自然言語処理</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    計算社会科学と自然言語処理
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  イントロダクション
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="introduction.html">
   ガイダンス
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  基礎知識
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="nlp_basis.html">
   自然言語処理の基礎
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ml_basis.html">
   機械学習の基本概念
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  ニューラルネットワーク
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="NN.html">
   ニューラルネットワーク
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="backpropagation.html">
   誤差逆伝播法
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  PyTorch
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="pytorch.html">
   Pytorch
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  単語分散表現
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="word2vec_1.html">
   単語分散表現
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="word2vec_2_embedding.html">
   word2vec
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="word2vec_gensim.html">
   GensimによるWord2Vecの学習と使用
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="word2vec_sentiment.html">
   Word2Vecを用いるセンチメント分析
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="word2vec_application.html">
   Word2Vecが人文・社会科学研究における応用
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  RNN
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="rnn.html">
   RNNの基礎
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lstm.html">
   LSTM
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="pytorch_lstm.html">
   LSTMの実装
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lstm_classification.html">
   LSTMによる文書分類
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="seq2seq.html">
   Seq2seq
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Transformer
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="attention.html">
   Attention
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="self-attention.html">
   Self-Attention
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Transformerアーキテクチャ
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="BERT.html">
   BERT
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="GPT.html">
   GPT
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/lvzeyu/css_nlp/tree/master"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/lvzeyu/css_nlp/tree/master/issues/new?title=Issue%20on%20page%20%2Fnotebook/transformer.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../_sources/notebook/transformer.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id1">
   Transformerの構成要素
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#multi-head-attention">
     Multi-Head Attention
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#positional-encoding">
     位置埋め込み(Positional Encoding)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#add-norm">
     Add&amp;Norm
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#feed-forward">
     Feed-forward層
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id2">
   エンコーダ・デコーダ
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id3">
     エンコーダ
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id4">
     デコーダ
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#masked-multi-head-attention">
       Masked Multi-Head Attention
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#cross-attention">
       交差注意機構(Cross Attention)
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id5">
   まとめ
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Transformerアーキテクチャ</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id1">
   Transformerの構成要素
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#multi-head-attention">
     Multi-Head Attention
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#positional-encoding">
     位置埋め込み(Positional Encoding)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#add-norm">
     Add&amp;Norm
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#feed-forward">
     Feed-forward層
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id2">
   エンコーダ・デコーダ
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id3">
     エンコーダ
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id4">
     デコーダ
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#masked-multi-head-attention">
       Masked Multi-Head Attention
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#cross-attention">
       交差注意機構(Cross Attention)
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id5">
   まとめ
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="transformer">
<h1>Transformerアーキテクチャ<a class="headerlink" href="#transformer" title="Permalink to this headline">#</a></h1>
<p>OpenAIのGPTなど、現在主流となっている言語モデルには、基本的にTransformerというアーキテクチャが使われています。</p>
<p>Transformerは、自然言語処理の分野で大きな進歩をもたらした重要な技術です。このアーキテクチャは、特に大量のテキストデータからパターンを学習し、文脈に基づいた情報を処理するのに非常に効果的です。</p>
<p>(オリジナルの)Transformerはエンコーダ・デコーダーアーキテクチャをベースにしています。</p>
<ul class="simple">
<li><p>エンコーダ：入力されたトークン列を、埋め込みベクトル(隠れ状態)に変換します。</p></li>
<li><p>デコーダ：エンコーダの隠れ状態を利用して、トークンの出力系列を生成します。</p></li>
</ul>
<p><img alt="" src="../_images/transformer.png" /></p>
<section id="id1">
<h2>Transformerの構成要素<a class="headerlink" href="#id1" title="Permalink to this headline">#</a></h2>
<section id="multi-head-attention">
<h3>Multi-Head Attention<a class="headerlink" href="#multi-head-attention" title="Permalink to this headline">#</a></h3>
<p>Attentionは、各トークンに対して固定的な埋め込みを使う代わりに、系列全体を使って各埋め込みの加重平均を計算しています。</p>
<p>つまり、トークン埋め込みの系列<span class="math notranslate nohighlight">\(x_1,...,x_n\)</span>が与えるとき、Self-Attentionは新しい埋め込みの系列<span class="math notranslate nohighlight">\(x'_1,...x'_n\)</span>を生成します。ここで、<span class="math notranslate nohighlight">\(x'_i\)</span>はすべでの<span class="math notranslate nohighlight">\(x_j\)</span>の線形結合になります。</p>
<div class="math notranslate nohighlight">
\[
x'_i=\sum_{j=1}^n w_{ji}x_j
\]</div>
<p>係数<span class="math notranslate nohighlight">\(w_{ji}\)</span>はAteention weightと呼ばれます。各要素をクエリ（Query）、キー（Key）、バリュー（Value）として表現し、これらを用いて他の要素との関連を計算することでAteention weightを求めます。</p>
<p>Attention機構の表現力をさらに高めるために、Attention機構を同時に複数適用するのはMulti-Head Attentionになります。</p>
</section>
<section id="positional-encoding">
<h3>位置埋め込み(Positional Encoding)<a class="headerlink" href="#positional-encoding" title="Permalink to this headline">#</a></h3>
<p>同じ単語でも、文中の位置によって意味が変わることがあります。位置埋め込み(Positional Encoding)は名前通り、入力トークンの順序や位置情報をモデルに提供する役割を果たします。位置埋め込みにより、モデルは各単語の文中での相対的な位置を理解し、より正確な文脈解析が可能になります。</p>
<p>RNNようなモデルは、、入力データを順序付けて処理しますので、入力トークンの順序情報が自然に考慮されます。しかし、Transformerは入力を並列に処理し、基本的には単語の順序情報を無視します。そのため、文の意味を正確に理解するためには、単語の位置情報が別途必要となります。</p>
<p>位置埋め込みを実現する方法はいくつかあります。</p>
<ul class="simple">
<li><p>絶対位置表現:事前に定義された関数を用いて、各位置に対してユニークな埋め込みを生成します。オリジナルのTransformerモデルは、変調されたサインとコサイン信号から静的なパターンを使用します。</p></li>
<li><p>相対位置表現:Transformerベースの後続モデル（例えば、BERTやGPTなど）では、位置埋め込みがモデルのトレーニングプロセスの一部として学習され、より文脈に適応した表現を提供します。</p></li>
</ul>
<p>このように、埋め込み層は各トークンに対して1つの位置埋め込みを作成します。</p>
<p><img alt="" src="../_images/transformer_positional_encoding_example.png" /></p>
<p>位置埋め込みを可視化してみると、トークンの位置ごとに異なる位置符号のベクトルが割り当てられていることがわかります。しかも、隣接する符号の類似度が高いことも確認できます。</p>
<p>こうした位置符号の特徴は、近いトークン同士の方が遠いトークン同士よりも意味的・文法的関連度が高くなりやすい言語の特性を学習するにも役にたつと考えられます。</p>
<p><img alt="" src="../_images/transformer_positional_encoding_large_example.png" /></p>
</section>
<section id="add-norm">
<h3>Add&amp;Norm<a class="headerlink" href="#add-norm" title="Permalink to this headline">#</a></h3>
<p>Add&amp;Normは、Self-AttentionやFeed-Forwardネットワーク出力に対して適用されます。具体的には、「残差結合（Residual Connection）」と「レイヤー正規化（Layer Normalization）」の組み合わせから成り立っています。</p>
<ul class="simple">
<li><p>残差結合: ある層の出力にその層の入力を加算し、最終的な出力にします。</p></li>
<li><p>レイヤー正規化: 過剰に大きい値によって学習が不安定になることを防ぐために、残差接続の結果を正規化します。</p></li>
</ul>
<p><img alt="" src="../_images/transformer_resideual_layer_norm_2.png" /></p>
</section>
<section id="feed-forward">
<h3>Feed-forward層<a class="headerlink" href="#feed-forward" title="Permalink to this headline">#</a></h3>
<p>線形層と中間の非線形活性化関数で構成されています。ただ、注意してほしいのは、ここでは一連の埋め込み全体を一つのベクトルとして処理するのではなく、各埋め込みを独立に処理するように工夫しています。</p>
<p>このため、この層はposition-wise feed forward netwrokと呼ばれることもあります。</p>
<div class="math notranslate nohighlight">
\[
z_i=W_2f(W_1 u_i+b_1)+b_2
\]</div>
<p>Feed-forward層は、文脈に関連する情報をその大規模なパラメータの中に記憶しており、入力された文脈に対して関連する情報を付加する役割を果たしてると考えられています。</p>
</section>
</section>
<section id="id2">
<h2>エンコーダ・デコーダ<a class="headerlink" href="#id2" title="Permalink to this headline">#</a></h2>
<section id="id3">
<h3>エンコーダ<a class="headerlink" href="#id3" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>入力テキストをトークン化し、トークン埋め込むに変換します。特に、トークンの位置に関する情報を含む位置埋め込みも生成します。</p></li>
<li><p>複数のエンコード層を積み重ねることでエンコーダを構成します。各エンコード層の出力埋め込みと入力とは同じサイズはずです。エンコード層の主な役割は、<strong>入力埋め込みを「更新」して、系列中の何らかの文脈情報をエンコード表現を生成することになります。</strong></p>
<ul>
<li><p>Multi-Head Attention</p></li>
<li><p>各入力埋め込みに適用される全結合の順伝播層</p></li>
</ul>
</li>
</ul>
</section>
<section id="id4">
<h3>デコーダ<a class="headerlink" href="#id4" title="Permalink to this headline">#</a></h3>
<p>エンコーダの出力はデコーダに送られ、デコーダは系列中に最も可能性の高いトークンとして次のトークンを予測します。</p>
<p>一般的には、デコーダは、<code class="docutils literal notranslate"><span class="pre">&lt;BOS&gt;</span></code>のようなテキストの先頭であることを表すトークンを入力として受け取り、文章の生成を開始し、最大長に達するまでか、系列の終了を表す<code class="docutils literal notranslate"><span class="pre">&lt;EOS&gt;</span></code>のような特別トークンに到達するまで続けられます(shifted right)。</p>
<p>エンコーダも複数のエンコード層より構成されています。</p>
<section id="masked-multi-head-attention">
<h4>Masked Multi-Head Attention<a class="headerlink" href="#masked-multi-head-attention" title="Permalink to this headline">#</a></h4>
<p>デコーダ側にMulti-Head Attentionには、マスク処理(masking)が行われます。</p>
<p>エンコーダ・デコーダでは、エンコーダに入力されるトークン列<span class="math notranslate nohighlight">\(u_1,u_2,...,u_M\)</span>から<span class="math notranslate nohighlight">\(w_1,w_2,...w_i\)</span>を順に予測されるように学習します。ここで、<span class="math notranslate nohighlight">\(w_i\)</span>までを予測した状態のとき、学習は<span class="math notranslate nohighlight">\(u_1,u_2,...,u_M\)</span>と<span class="math notranslate nohighlight">\(w_1,w_2,...w_i\)</span>からトークン<span class="math notranslate nohighlight">\(w_{i+1}\)</span>を予測できるようにモデルを更新していくことで行われます。</p>
<p>しかし、Self-attentionはトークン全体から情報を取得するため、モデルは<span class="math notranslate nohighlight">\(w_{i+1}\)</span>を予測際は<span class="math notranslate nohighlight">\(w_{i+1},...w_N\)</span>の情報も利用できることになります。</p>
<p>マスク処理は、注意機構において位置<span class="math notranslate nohighlight">\(i\)</span>のトークンについて処理する際<span class="math notranslate nohighlight">\(i+1\)</span>以降のトークンのAttnetionスコアを<span class="math notranslate nohighlight">\(-\infty\)</span>に設定することで、各時点で生成するトークンが、過去の出力と現在予測されているトークンだけに基づいていることを保証します。</p>
</section>
<section id="cross-attention">
<h4>交差注意機構(Cross Attention)<a class="headerlink" href="#cross-attention" title="Permalink to this headline">#</a></h4>
<p>デコーダには、交差注意機構が使われています。具体的には、注意機構において、queryはデコーダの埋め込み列、keyとvalueにはエンコーダの出力埋め込み列が使われます。</p>
<p>このようにして、デコーダが次のトークンを生成する際に、入力シーケンス全体を考慮できるようにします。</p>
</section>
</section>
</section>
<section id="id5">
<h2>まとめ<a class="headerlink" href="#id5" title="Permalink to this headline">#</a></h2>
<p>Transformerは、主にエンコーダ、デコーダ、エンコーダ・デコーダーというアーキテクチャに大別されます。Transformerの成功は、Transformerベースの後続モデルの開発を引き起こしました。BERT、GPT、T5（Text-to-Text Transfer Transformer）などは代表的なバリエーションにであり、NLPの分野において画期的な進歩をもたらしました。</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./notebook"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="self-attention.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Self-Attention</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="BERT.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">BERT</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By 呂　沢宇<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>