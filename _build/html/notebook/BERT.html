
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>BERT &#8212; 計算社会科学のための自然言語処理</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=bd9e20870c6007c4c509" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=bd9e20870c6007c4c509" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=bd9e20870c6007c4c509" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.1/css/all.min.css?digest=bd9e20870c6007c4c509" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.1/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.1/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.1/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=384b581d" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css?v=0a3b3ea7" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=bd9e20870c6007c4c509" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=bd9e20870c6007c4c509" />
  <script src="../_static/vendor/fontawesome/6.5.1/js/all.min.js?digest=bd9e20870c6007c4c509"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=888ff710"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=efea14e4"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=36754332"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'notebook/BERT';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="BERTによるセンチメント分析" href="bert_sentiment.html" />
    <link rel="prev" title="Transformerアーキテクチャ" href="transformer.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a id="pst-skip-link" class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <header>
  
    <div class="bd-header navbar navbar-expand-lg bd-navbar">
    </div>
  
  </header>

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/tohoku-university-logo-vector.svg" class="logo__image only-light" alt="計算社会科学のための自然言語処理 - Home"/>
    <script>document.write(`<img src="../_static/tohoku-university-logo-vector.svg" class="logo__image only-dark" alt="計算社会科学のための自然言語処理 - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    計算社会科学と自然言語処理
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">イントロダクション</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="introduction.html">ガイダンス</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">基礎知識</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="nlp_basis2.html">自然言語処理の基礎</a></li>
<li class="toctree-l1"><a class="reference internal" href="ml_basis2.html">機械学習の基本概念</a></li>
<li class="toctree-l1"><a class="reference internal" href="math_basis2.html">数学基礎</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">ニューラルネットワーク</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="NN.html">ニューラルネットワーク</a></li>
<li class="toctree-l1"><a class="reference internal" href="backpropagation.html">誤差逆伝播法</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">PyTorch</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="pytorch.html">Pytorch</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">単語分散表現</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="word2vec_1.html">単語分散表現</a></li>
<li class="toctree-l1"><a class="reference internal" href="word2vec_2_embedding.html">word2vec</a></li>
<li class="toctree-l1"><a class="reference internal" href="word2vec_gensim.html">GensimによるWord2Vecの学習と使用</a></li>
<li class="toctree-l1"><a class="reference internal" href="word2vec_sentiment.html">Word2Vecを用いるセンチメント分析</a></li>
<li class="toctree-l1"><a class="reference internal" href="word2vec_application.html">Word2Vecが人文・社会科学研究における応用</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">RNN</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="rnn.html">RNNの基礎</a></li>
<li class="toctree-l1"><a class="reference internal" href="lstm.html">LSTM</a></li>
<li class="toctree-l1"><a class="reference internal" href="pytorch_lstm.html">LSTMの実装</a></li>
<li class="toctree-l1"><a class="reference internal" href="lstm_classification.html">LSTMによる文書分類</a></li>
<li class="toctree-l1"><a class="reference internal" href="seq2seq.html">Seq2seq</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Transformer</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="attention.html">Attention</a></li>
<li class="toctree-l1"><a class="reference internal" href="self-attention.html">Self-Attention</a></li>
<li class="toctree-l1"><a class="reference internal" href="transformer.html">Transformerアーキテクチャ</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">BERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="bert_sentiment.html">BERTによるセンチメント分析</a></li>
<li class="toctree-l1"><a class="reference internal" href="bert_topic.html">BERTopic</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">大規模言語モデル</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="GPT.html">GPT</a></li>
<li class="toctree-l1"><a class="reference internal" href="llm.html">大規模言語モデル</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/tree/master/blob/master/notebook/BERT.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onColab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/lvzeyu/css_nlp/tree/master" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/lvzeyu/css_nlp/tree/master/issues/new?title=Issue%20on%20page%20%2Fnotebook/BERT.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/notebook/BERT.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>BERT</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">BERTの事前学習</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">入力表現</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">マスク言語モデリング</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">次文予測</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#fine-tuning">ファインチューング(Fine-Tuning)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#huggingface-transformer">Huggingface transformerを使う</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#pipline">pipline</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#word2vecbert">まとめ : word2vecからBERTまで</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="bert">
<h1>BERT<a class="headerlink" href="#bert" title="Link to this heading">#</a></h1>
<p>2018年Googleが発表したBERT（Bidirectional Encoder Representations from Transformers）は、エンコーダ構成のTransformersを採用し、先行するトークン列と後続するトークン列の双方向性から文脈を捉えます。</p>
<p>BERTは、Wikipediaと7,000冊の書籍を合わせた大規模なコーパスを使って事前学習されたモデルを、下流タスクのデータセットでファインチューングすることで、様々なタスクの性能を大幅に改善できることが示されました。</p>
<ul class="simple">
<li><p>事前学習: 大規模なコーパスを用いて、特定なタスクを学習することで、広範な言語データからパターンを学習し、汎用的な言語理解の能力を身につける。</p></li>
<li><p>ファインチューング：、特定のタスクや領域に特化した小さなデータセットを用いて、事前学習したモデルを微調整します。この微調整により、モデルは特定のタスクや領域に適応し、高い精度を達成することが可能です。</p></li>
</ul>
<p><img alt="" src="../_images/transfer_learning.png" /></p>
<section id="id1">
<h2>BERTの事前学習<a class="headerlink" href="#id1" title="Link to this heading">#</a></h2>
<section id="id2">
<h3>入力表現<a class="headerlink" href="#id2" title="Link to this heading">#</a></h3>
<p><img alt="" src="../_images/bert_input.png" /></p>
<p>BERTで入力を作成する際、入力の開始を表す<code class="docutils literal notranslate"><span class="pre">[CLS]</span></code>トークンと、入力の区切りを表す<code class="docutils literal notranslate"><span class="pre">[SEP]</span></code>トークンという二つの特殊トークンが使われます。</p>
<p>またよく使われる特殊トークンとして、マスクタスクための<code class="docutils literal notranslate"><span class="pre">[MASK]</span></code>トークン、vocabularyに含まれていないことを示す<code class="docutils literal notranslate"><span class="pre">[UNK]</span></code>トークンがあります。</p>
<p>入力トークン埋め込みと位置埋め込み以外、それぞれのテキストの範囲を区別しやくするためにsegment embeddingという埋め込みが導入されています。</p>
<p>まとめると、BERTの入力埋め込み<span class="math notranslate nohighlight">\(x_i\)</span>は、トークン埋め込み、位置埋め込み、segment埋め込みより加算されます。</p>
</section>
<section id="id3">
<h3>マスク言語モデリング<a class="headerlink" href="#id3" title="Link to this heading">#</a></h3>
<p>マスク言語モデリングは、トークンの穴埋め問題を解けるタスクです。具体的には、ランダムにトークン列中のトークンを隠して、その周辺の単語からマスクされた単語を予測することが求められます。</p>
<p>ここで、先行するトークンと後続トークンの双方の情報が使われていますので、全体の文脈を捉える学習を実現しています。</p>
<p><img alt="" src="../_images/BERT-language-modeling-masked-lm.png" /></p>
</section>
<section id="id4">
<h3>次文予測<a class="headerlink" href="#id4" title="Link to this heading">#</a></h3>
<p>次文予測タスクでは、2つの文が与えられ、一方が他方の直後に来るかどうかを判定することが求められます。</p>
<p><img alt="" src="../_images/bert-next-sentence-prediction.png" /></p>
</section>
</section>
<section id="fine-tuning">
<h2>ファインチューング(Fine-Tuning)<a class="headerlink" href="#fine-tuning" title="Link to this heading">#</a></h2>
<p>様々なタスクにおいて、事前学習モデルをもとにしてそれぞれのタスクに特化したモデルを作るためのステップはファインチューングです。</p>
<p>事前学習タスクから下流タスクに切り替える時、モデルの最後のレイヤーをタスクに適したものに置き換える必要があります。この最後の層はヘッドと呼ばれ、タスクに固有な部分です。</p>
<p>残りの部分はボディと呼ばれ、タスクに依存しない事前学習された部分であり(トークン埋め込み層やTransformer層が含まれ)、一般的な言語の理解を行うための基本的な情報を含んでいます。</p>
<p>例えば、テキスト分類の場合、追加層としては、BERTの最後の隠れ層からの出力に冒頭のSpecial token[CLS]を全結合層（Dense Layer）に経由し、各カテゴリーに属する確率を出力します。それは、[CLS]トークンは入力テキスト全体の文脈情報を集約すると考えるためです。</p>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p>テキスト分類タスクにおいて、[CLS]トークンと全結合層（Dense Layer）を使用するのが一般的ですが、他の追加層の設計を用いることももちろん可能です。例えば、複数の全結合層を積み重ねるや、畳み込みニューラルネットワーク（CNN）を適用するなどの方法も提案されています。</p>
</aside>
<p><img alt="" src="../_images/bert_based_model.png" /></p>
<section id="huggingface-transformer">
<h3>Huggingface transformerを使う<a class="headerlink" href="#huggingface-transformer" title="Link to this heading">#</a></h3>
<p>転移学習は事前学習済みモデルを新しいタスクに再利用するといった強みがあります。そのため、事前学習済みのモデルを素早く共有、ロードすることは重要です。</p>
<p><a class="reference external" href="https://huggingface.co/">Hugging Face Hub</a>は、モデル、データセットとデモを備えたプラットフォームです。</p>
<p><img alt="" src="../_images/hf-ecosystem.png" /></p>
<p><a class="reference external" href="https://huggingface.co/docs/transformers/index">Huggingface transformer</a>は、自然言語処理を中心に最先端のTransformerベースのモデルを効率に利用するためのオープンソースライブラリです。</p>
<ul class="simple">
<li><p>多数の事前学習済みモデル: ライブラリは、BERT、GPT-2、RoBERTa、T5、DistilBERTなど、さまざまな有名なNLPモデルの事前学習済みバージョンを提供しています。</p></li>
<li><p>モデルの利用の簡易化: 事前学習済みのモデルを簡単にダウンロードし、特定のタスクにファインチューニングするための高レベルのAPIを提供しています。</p></li>
<li><p>Tokenizers: ほとんどのモデルには、テキストデータをモデルが扱える形式に変換するためのトークナイザが付属しています。これはテキストの前処理を簡単に行うためのツールです。</p></li>
<li><p>Model Hubの統合: Hugging FaceのModel Hubと直接統合されており、コミュニティによって共有されている数千もの事前学習済みモデルに簡単にアクセスできます。</p></li>
</ul>
<section id="pipline">
<h4>pipline<a class="headerlink" href="#pipline" title="Link to this heading">#</a></h4>
<p><a class="reference external" href="https://huggingface.co/docs/transformers/main_classes/pipelines"><code class="docutils literal notranslate"><span class="pre">pipeline</span></code></a>というクラスで、特定のタスクを実行するために事前学習されたモデルとトークンナイザーを統合し、簡単に使用することができます。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">pipeline</span>
<span class="c1">#!pip install unidic-lite</span>
<span class="c1">#!pip install fugashi </span>
<span class="n">fill_mask</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span>
    <span class="s2">&quot;fill-mask&quot;</span><span class="p">,</span>
    <span class="n">model</span><span class="o">=</span><span class="s2">&quot;cl-tohoku/bert-base-japanese-v3&quot;</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/Users/ryozawau/anaconda3/envs/jupyterbook/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html
  from .autonotebook import tqdm as notebook_tqdm
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/Users/ryozawau/anaconda3/envs/jupyterbook/lib/python3.12/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/Users/ryozawau/anaconda3/envs/jupyterbook/lib/python3.12/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/Users/ryozawau/anaconda3/envs/jupyterbook/lib/python3.12/site-packages/transformers/modeling_utils.py:519: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don&#39;t have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  return torch.load(checkpoint_file, map_location=map_location)
</pre></div>
</div>
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">ModuleNotFoundError</span><span class="g g-Whitespace">                       </span>Traceback (most recent call last)
<span class="nn">File ~/anaconda3/envs/jupyterbook/lib/python3.12/site-packages/transformers/models/bert_japanese/tokenization_bert_japanese.py:457,</span> in <span class="ni">MecabTokenizer.__init__</span><span class="nt">(self, do_lower_case, never_split, normalize_text, mecab_dic, mecab_option)</span>
<span class="g g-Whitespace">    </span><span class="mi">456</span> <span class="k">try</span><span class="p">:</span>
<span class="ne">--&gt; </span><span class="mi">457</span>     <span class="kn">import</span> <span class="nn">fugashi</span>
<span class="g g-Whitespace">    </span><span class="mi">458</span> <span class="k">except</span> <span class="ne">ModuleNotFoundError</span> <span class="k">as</span> <span class="n">error</span><span class="p">:</span>

<span class="ne">ModuleNotFoundError</span>: No module named &#39;fugashi&#39;

<span class="n">During</span> <span class="n">handling</span> <span class="n">of</span> <span class="n">the</span> <span class="n">above</span> <span class="n">exception</span><span class="p">,</span> <span class="n">another</span> <span class="n">exception</span> <span class="n">occurred</span><span class="p">:</span>

<span class="ne">ModuleNotFoundError</span><span class="g g-Whitespace">                       </span>Traceback (most recent call last)
<span class="n">Cell</span> <span class="n">In</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">line</span> <span class="mi">5</span>
<span class="g g-Whitespace">      </span><span class="mi">2</span> <span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">pipeline</span>
<span class="g g-Whitespace">      </span><span class="mi">3</span> <span class="c1">#!pip install unidic-lite</span>
<span class="g g-Whitespace">      </span><span class="mi">4</span> <span class="c1">#!pip install fugashi </span>
<span class="ne">----&gt; </span><span class="mi">5</span> <span class="n">fill_mask</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span>
<span class="g g-Whitespace">      </span><span class="mi">6</span>     <span class="s2">&quot;fill-mask&quot;</span><span class="p">,</span>
<span class="g g-Whitespace">      </span><span class="mi">7</span>     <span class="n">model</span><span class="o">=</span><span class="s2">&quot;cl-tohoku/bert-base-japanese-v3&quot;</span>
<span class="g g-Whitespace">      </span><span class="mi">8</span> <span class="p">)</span>

<span class="nn">File ~/anaconda3/envs/jupyterbook/lib/python3.12/site-packages/transformers/pipelines/__init__.py:967,</span> in <span class="ni">pipeline</span><span class="nt">(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)</span>
<span class="g g-Whitespace">    </span><span class="mi">964</span>             <span class="n">tokenizer_kwargs</span> <span class="o">=</span> <span class="n">model_kwargs</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="g g-Whitespace">    </span><span class="mi">965</span>             <span class="n">tokenizer_kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;torch_dtype&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
<span class="ne">--&gt; </span><span class="mi">967</span>         <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">968</span>             <span class="n">tokenizer_identifier</span><span class="p">,</span> <span class="n">use_fast</span><span class="o">=</span><span class="n">use_fast</span><span class="p">,</span> <span class="n">_from_pipeline</span><span class="o">=</span><span class="n">task</span><span class="p">,</span> <span class="o">**</span><span class="n">hub_kwargs</span><span class="p">,</span> <span class="o">**</span><span class="n">tokenizer_kwargs</span>
<span class="g g-Whitespace">    </span><span class="mi">969</span>         <span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">971</span> <span class="k">if</span> <span class="n">load_image_processor</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">972</span>     <span class="c1"># Try to infer image processor from model or config name (if provided as str)</span>
<span class="g g-Whitespace">    </span><span class="mi">973</span>     <span class="k">if</span> <span class="n">image_processor</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>

<span class="nn">File ~/anaconda3/envs/jupyterbook/lib/python3.12/site-packages/transformers/models/auto/tokenization_auto.py:787,</span> in <span class="ni">AutoTokenizer.from_pretrained</span><span class="nt">(cls, pretrained_model_name_or_path, *inputs, **kwargs)</span>
<span class="g g-Whitespace">    </span><span class="mi">783</span>     <span class="k">if</span> <span class="n">tokenizer_class</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">784</span>         <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">785</span>             <span class="sa">f</span><span class="s2">&quot;Tokenizer class </span><span class="si">{</span><span class="n">tokenizer_class_candidate</span><span class="si">}</span><span class="s2"> does not exist or is not currently imported.&quot;</span>
<span class="g g-Whitespace">    </span><span class="mi">786</span>         <span class="p">)</span>
<span class="ne">--&gt; </span><span class="mi">787</span>     <span class="k">return</span> <span class="n">tokenizer_class</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">pretrained_model_name_or_path</span><span class="p">,</span> <span class="o">*</span><span class="n">inputs</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">789</span> <span class="c1"># Otherwise we have to be creative.</span>
<span class="g g-Whitespace">    </span><span class="mi">790</span> <span class="c1"># if model is an encoder decoder, the encoder tokenizer class is used by default</span>
<span class="g g-Whitespace">    </span><span class="mi">791</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">EncoderDecoderConfig</span><span class="p">):</span>

<span class="nn">File ~/anaconda3/envs/jupyterbook/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2028,</span> in <span class="ni">PreTrainedTokenizerBase.from_pretrained</span><span class="nt">(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, *init_inputs, **kwargs)</span>
<span class="g g-Whitespace">   </span><span class="mi">2025</span>     <span class="k">else</span><span class="p">:</span>
<span class="g g-Whitespace">   </span><span class="mi">2026</span>         <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;loading file </span><span class="si">{</span><span class="n">file_path</span><span class="si">}</span><span class="s2"> from cache at </span><span class="si">{</span><span class="n">resolved_vocab_files</span><span class="p">[</span><span class="n">file_id</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="ne">-&gt; </span><span class="mi">2028</span> <span class="k">return</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_from_pretrained</span><span class="p">(</span>
<span class="g g-Whitespace">   </span><span class="mi">2029</span>     <span class="n">resolved_vocab_files</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">2030</span>     <span class="n">pretrained_model_name_or_path</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">2031</span>     <span class="n">init_configuration</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">2032</span>     <span class="o">*</span><span class="n">init_inputs</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">2033</span>     <span class="n">token</span><span class="o">=</span><span class="n">token</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">2034</span>     <span class="n">cache_dir</span><span class="o">=</span><span class="n">cache_dir</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">2035</span>     <span class="n">local_files_only</span><span class="o">=</span><span class="n">local_files_only</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">2036</span>     <span class="n">_commit_hash</span><span class="o">=</span><span class="n">commit_hash</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">2037</span>     <span class="n">_is_local</span><span class="o">=</span><span class="n">is_local</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">2038</span>     <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">2039</span> <span class="p">)</span>

<span class="nn">File ~/anaconda3/envs/jupyterbook/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2260,</span> in <span class="ni">PreTrainedTokenizerBase._from_pretrained</span><span class="nt">(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, *init_inputs, **kwargs)</span>
<span class="g g-Whitespace">   </span><span class="mi">2258</span> <span class="c1"># Instantiate the tokenizer.</span>
<span class="g g-Whitespace">   </span><span class="mi">2259</span> <span class="k">try</span><span class="p">:</span>
<span class="ne">-&gt; </span><span class="mi">2260</span>     <span class="n">tokenizer</span> <span class="o">=</span> <span class="bp">cls</span><span class="p">(</span><span class="o">*</span><span class="n">init_inputs</span><span class="p">,</span> <span class="o">**</span><span class="n">init_kwargs</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">2261</span> <span class="k">except</span> <span class="ne">OSError</span><span class="p">:</span>
<span class="g g-Whitespace">   </span><span class="mi">2262</span>     <span class="k">raise</span> <span class="ne">OSError</span><span class="p">(</span>
<span class="g g-Whitespace">   </span><span class="mi">2263</span>         <span class="s2">&quot;Unable to load vocabulary from file. &quot;</span>
<span class="g g-Whitespace">   </span><span class="mi">2264</span>         <span class="s2">&quot;Please check that the provided vocabulary is accessible and not corrupted.&quot;</span>
<span class="g g-Whitespace">   </span><span class="mi">2265</span>     <span class="p">)</span>

<span class="nn">File ~/anaconda3/envs/jupyterbook/lib/python3.12/site-packages/transformers/models/bert_japanese/tokenization_bert_japanese.py:192,</span> in <span class="ni">BertJapaneseTokenizer.__init__</span><span class="nt">(self, vocab_file, spm_file, do_lower_case, do_word_tokenize, do_subword_tokenize, word_tokenizer_type, subword_tokenizer_type, never_split, unk_token, sep_token, pad_token, cls_token, mask_token, mecab_kwargs, sudachi_kwargs, jumanpp_kwargs, **kwargs)</span>
<span class="g g-Whitespace">    </span><span class="mi">188</span>     <span class="bp">self</span><span class="o">.</span><span class="n">word_tokenizer</span> <span class="o">=</span> <span class="n">BasicTokenizer</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">189</span>         <span class="n">do_lower_case</span><span class="o">=</span><span class="n">do_lower_case</span><span class="p">,</span> <span class="n">never_split</span><span class="o">=</span><span class="n">never_split</span><span class="p">,</span> <span class="n">tokenize_chinese_chars</span><span class="o">=</span><span class="kc">False</span>
<span class="g g-Whitespace">    </span><span class="mi">190</span>     <span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">191</span> <span class="k">elif</span> <span class="n">word_tokenizer_type</span> <span class="o">==</span> <span class="s2">&quot;mecab&quot;</span><span class="p">:</span>
<span class="ne">--&gt; </span><span class="mi">192</span>     <span class="bp">self</span><span class="o">.</span><span class="n">word_tokenizer</span> <span class="o">=</span> <span class="n">MecabTokenizer</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">193</span>         <span class="n">do_lower_case</span><span class="o">=</span><span class="n">do_lower_case</span><span class="p">,</span> <span class="n">never_split</span><span class="o">=</span><span class="n">never_split</span><span class="p">,</span> <span class="o">**</span><span class="p">(</span><span class="n">mecab_kwargs</span> <span class="ow">or</span> <span class="p">{})</span>
<span class="g g-Whitespace">    </span><span class="mi">194</span>     <span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">195</span> <span class="k">elif</span> <span class="n">word_tokenizer_type</span> <span class="o">==</span> <span class="s2">&quot;sudachi&quot;</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">196</span>     <span class="bp">self</span><span class="o">.</span><span class="n">word_tokenizer</span> <span class="o">=</span> <span class="n">SudachiTokenizer</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">197</span>         <span class="n">do_lower_case</span><span class="o">=</span><span class="n">do_lower_case</span><span class="p">,</span> <span class="n">never_split</span><span class="o">=</span><span class="n">never_split</span><span class="p">,</span> <span class="o">**</span><span class="p">(</span><span class="n">sudachi_kwargs</span> <span class="ow">or</span> <span class="p">{})</span>
<span class="g g-Whitespace">    </span><span class="mi">198</span>     <span class="p">)</span>

<span class="nn">File ~/anaconda3/envs/jupyterbook/lib/python3.12/site-packages/transformers/models/bert_japanese/tokenization_bert_japanese.py:459,</span> in <span class="ni">MecabTokenizer.__init__</span><span class="nt">(self, do_lower_case, never_split, normalize_text, mecab_dic, mecab_option)</span>
<span class="g g-Whitespace">    </span><span class="mi">457</span>     <span class="kn">import</span> <span class="nn">fugashi</span>
<span class="g g-Whitespace">    </span><span class="mi">458</span> <span class="k">except</span> <span class="ne">ModuleNotFoundError</span> <span class="k">as</span> <span class="n">error</span><span class="p">:</span>
<span class="ne">--&gt; </span><span class="mi">459</span>     <span class="k">raise</span> <span class="n">error</span><span class="o">.</span><span class="vm">__class__</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">460</span>         <span class="s2">&quot;You need to install fugashi to use MecabTokenizer. &quot;</span>
<span class="g g-Whitespace">    </span><span class="mi">461</span>         <span class="s2">&quot;See https://pypi.org/project/fugashi/ for installation.&quot;</span>
<span class="g g-Whitespace">    </span><span class="mi">462</span>     <span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">464</span> <span class="n">mecab_option</span> <span class="o">=</span> <span class="n">mecab_option</span> <span class="ow">or</span> <span class="s2">&quot;&quot;</span>
<span class="g g-Whitespace">    </span><span class="mi">466</span> <span class="k">if</span> <span class="n">mecab_dic</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>

<span class="ne">ModuleNotFoundError</span>: You need to install fugashi to use MecabTokenizer. See https://pypi.org/project/fugashi/ for installation.
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">masked_text</span> <span class="o">=</span> <span class="s2">&quot;東北大学は[MASK]市に位置しています。&quot;</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">fill_mask</span><span class="p">(</span><span class="n">masked_text</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">display</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">output</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>score</th>
      <th>token</th>
      <th>token_str</th>
      <th>sequence</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.600762</td>
      <td>14424</td>
      <td>仙台</td>
      <td>東北 大学 は 仙台 市 に 位置 し て い ます 。</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.048195</td>
      <td>19197</td>
      <td>盛岡</td>
      <td>東北 大学 は 盛岡 市 に 位置 し て い ます 。</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.037116</td>
      <td>15135</td>
      <td>青森</td>
      <td>東北 大学 は 青森 市 に 位置 し て い ます 。</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.026598</td>
      <td>15394</td>
      <td>山形</td>
      <td>東北 大学 は 山形 市 に 位置 し て い ます 。</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.025400</td>
      <td>14062</td>
      <td>福島</td>
      <td>東北 大学 は 福島 市 に 位置 し て い ます 。</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">summarizer</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="s2">&quot;summarization&quot;</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="s2">&quot;facebook/bart-large-cnn&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "e942b723ad934e818c4e5ab3cb9e4a5f", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "a6cb46820c5d416080cc896214a81f5d", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "1fe625ff9ed54636b847eb508284955b", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "4742612e1a9645f696829e5899a08777", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "7d07a5b8e24f43908149bdd9ccbc0d38", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "fe8c1121b264407fba67f2ba4689aa23", "version_major": 2, "version_minor": 0}</script></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ARTICLE</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot; New York (CNN)When Liana Barrientos was 23 years old, she got married in Westchester County, New York.</span>
<span class="s2">A year later, she got married again in Westchester County, but to a different man and without divorcing her first husband.</span>
<span class="s2">Only 18 days after that marriage, she got hitched yet again. Then, Barrientos declared &quot;I do&quot; five more times, sometimes only within two weeks of each other.</span>
<span class="s2">In 2010, she married once more, this time in the Bronx. In an application for a marriage license, she stated it was her &quot;first and only&quot; marriage.</span>
<span class="s2">Barrientos, now 39, is facing two criminal counts of &quot;offering a false instrument for filing in the first degree,&quot; referring to her false statements on the</span>
<span class="s2">2010 marriage license application, according to court documents.</span>
<span class="s2">Prosecutors said the marriages were part of an immigration scam.</span>
<span class="s2">On Friday, she pleaded not guilty at State Supreme Court in the Bronx, according to her attorney, Christopher Wright, who declined to comment further.</span>
<span class="s2">After leaving court, Barrientos was arrested and charged with theft of service and criminal trespass for allegedly sneaking into the New York subway through an emergency exit, said Detective</span>
<span class="s2">Annette Markowski, a police spokeswoman. In total, Barrientos has been married 10 times, with nine of her marriages occurring between 1999 and 2002.</span>
<span class="s2">All occurred either in Westchester County, Long Island, New Jersey or the Bronx. She is believed to still be married to four men, and at one time, she was married to eight men at once, prosecutors say.</span>
<span class="s2">Prosecutors said the immigration scam involved some of her husbands, who filed for permanent residence status shortly after the marriages.</span>
<span class="s2">Any divorces happened only after such filings were approved. It was unclear whether any of the men will be prosecuted.</span>
<span class="s2">The case was referred to the Bronx District Attorney</span><span class="se">\&#39;</span><span class="s2">s Office by Immigration and Customs Enforcement and the Department of Homeland Security</span><span class="se">\&#39;</span><span class="s2">s</span>
<span class="s2">Investigation Division. Seven of the men are from so-called &quot;red-flagged&quot; countries, including Egypt, Turkey, Georgia, Pakistan and Mali.</span>
<span class="s2">Her eighth husband, Rashid Rajput, was deported in 2006 to his native Pakistan after an investigation by the Joint Terrorism Task Force.</span>
<span class="s2">If convicted, Barrientos faces up to four years in prison.  Her next court appearance is scheduled for May 18.</span>
<span class="s2">&quot;&quot;&quot;</span>
<span class="nb">print</span><span class="p">(</span><span class="n">summarizer</span><span class="p">(</span><span class="n">ARTICLE</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">130</span><span class="p">,</span> <span class="n">min_length</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[{&#39;summary_text&#39;: &#39;Liana Barrientos, 39, is charged with two counts of &quot;offering a false instrument for filing in the first degree&quot; In total, she has been married 10 times, with nine of her marriages occurring between 1999 and 2002. She is believed to still be married to four men.&#39;}]
</pre></div>
</div>
</div>
</div>
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-0" name="sd-tab-set-0" type="radio">
</input><label class="sd-tab-label" for="sd-tab-item-0">
課題 1</label><div class="sd-tab-content docutils">
<p>Huggingface Hubで日本語のセンチメント分類ためのモデルを探し、piplineでセンチメント分類器を実装しなさい。
以下のテキストに対する分類結果を確認しよう。</p>
<ul class="simple">
<li><p>この製品は全く役に立ちませんでした</p></li>
<li><p>今日はいい天気ですね</p></li>
<li><p>世界経済も、米国が12月に続き３月にも追加利上げを実施するなど、先進国を中心に回復の動きとなりました</p></li>
<li><p>一度は訪れてみたいけど、たぶんもう行かない</p></li>
<li><p>あの政治家に向けられているのは、政策への批判じゃなくて誹謗中傷だろ</p></li>
</ul>
</div>
</div>
</section>
</section>
</section>
<section id="word2vecbert">
<h2>まとめ : word2vecからBERTまで<a class="headerlink" href="#word2vecbert" title="Link to this heading">#</a></h2>
<p>人間が使う自然言語をコンピュータに処理させるため、言語を数値形式で表現するモデリングのプロセスが必要とされ、どのようなモデルを採用するかによって分析の方向性は異なっています。</p>
<ul class="simple">
<li><p>最も基本的なモデリングアプローチとして、文書を単語の集合とそれぞれの単語の頻度情報に変換するバグオブワーズ(bag of words)があげられます。この手法は、文書の基本的な内容を捉えるのに有効であるが、<u>単語の順番や意味のニュアンスなどの情報はすべて捨象されています。</u></p></li>
<li><p>より複雑な言語の特性を捉えるために、大量のコーパスを使った学習により、言語の文法や意味構造など多くの情報を埋め込んだ高度なモデルが期待されています。word2vecをはじめとする単語分散表現モデルは、単語を「意味」情報を表現したベクトルにマッピングすることができます。</p>
<ul>
<li><p>word2vecに単語分散表現の学習では、「単語の意味は、その単語の周囲の単語（文脈）によって決まる」という分布仮説に基づく手法が用いられます。この仮説にしたがうモデルでは、ある単語がどのような文脈で生じやすいかということをある程度考慮し、単語間の関係をベクトルで表現することができます。</p></li>
<li><p>ただ、word2vecではいくつの欠点があります。特に、word2vecではあくまで<span class="math notranslate nohighlight">\(1\)</span>単語<span class="math notranslate nohighlight">\(1\)</span>ベクトルでしたが、実際のケースでは、文脈によって単語の意味が変わることがありますので、<u>文脈に依存する分散表現が望ましいです。</u></p></li>
</ul>
</li>
<li><p>テキストの「文脈」を表現するための言語モデルが開発されました。</p>
<ul>
<li><p>RNNとLSTMではテキストデータの時系列的な性質を捉え、<u>文中の単語の順序や時間的な関連性をモデルが学習できるようになります</u>。ただ、</p>
<ul>
<li><p>長距離の依存関係を効果的に対処できでいない</p></li>
<li><p>計算コストが高い</p></li>
</ul>
</li>
<li><p>Self-Attenttionでは、<u>すべての単語間の関係を並列に計算することで、長距離の依存関係を効果的に捉えます。</u></p></li>
</ul>
</li>
<li><p>Transformerをベースにしたモデルでは、単語が出現する具体的な文脈に基づいてその単語の埋め込みを生成することができます。</p></li>
</ul>
<p><img alt="" src="../_images/bert_embedding.png" /></p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./notebook"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="transformer.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Transformerアーキテクチャ</p>
      </div>
    </a>
    <a class="right-next"
       href="bert_sentiment.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">BERTによるセンチメント分析</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">BERTの事前学習</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">入力表現</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">マスク言語モデリング</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">次文予測</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#fine-tuning">ファインチューング(Fine-Tuning)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#huggingface-transformer">Huggingface transformerを使う</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#pipline">pipline</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#word2vecbert">まとめ : word2vecからBERTまで</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By 呂　沢宇
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=bd9e20870c6007c4c509"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=bd9e20870c6007c4c509"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>