{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorboard'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader, Dataset\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensorboard\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SummaryWriter\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./Data/dokujo-tsushin.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m,encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     14\u001b[0m     corpus \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/envs/jupyterbook/lib/python3.12/site-packages/torch/utils/tensorboard/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorboard\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_vendor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpackaging\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Version\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(tensorboard, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__version__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m Version(\n\u001b[1;32m      5\u001b[0m     tensorboard\u001b[38;5;241m.\u001b[39m__version__\n\u001b[1;32m      6\u001b[0m ) \u001b[38;5;241m<\u001b[39m Version(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1.15\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorboard'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pickle\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "with open(\"./Data/dokujo-tsushin.txt\", mode=\"r\",encoding=\"utf-8\") as f:\n",
    "    corpus = []\n",
    "    for line in f:\n",
    "        cleaned_line = line.replace('\\u3000', '').replace('\\n', '')\n",
    "        if cleaned_line!=\"\":\n",
    "            corpus.append(cleaned_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# コンテキストとターゲットの作成関数の実装\n",
    "def create_contexts_target(corpus, window_size=1):\n",
    "    \n",
    "    # ターゲットを抽出\n",
    "    target = corpus[window_size:-window_size]\n",
    "    \n",
    "    # コンテキストを初期化\n",
    "    contexts = []\n",
    "    \n",
    "    # ターゲットごとにコンテキストを格納\n",
    "    for idx in range(window_size, len(corpus) - window_size):\n",
    "        \n",
    "        # 現在のターゲットのコンテキストを初期化\n",
    "        cs = []\n",
    "        \n",
    "        # 現在のターゲットのコンテキストを1単語ずつ格納\n",
    "        for t in range(-window_size, window_size + 1):\n",
    "            \n",
    "            # 0番目の要素はターゲットそのものなので処理を省略\n",
    "            if t == 0:\n",
    "                continue\n",
    "            \n",
    "            # コンテキストを格納\n",
    "            cs.append(corpus[idx + t])\n",
    "            \n",
    "        # 現在のターゲットのコンテキストのセットを格納\n",
    "        contexts.append(cs)\n",
    "    \n",
    "    # NumPy配列に変換\n",
    "    return np.array(contexts), np.array(target) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MeCab\n",
    "from tqdm.notebook import tqdm\n",
    "def tokenize_with_mecab(sentences):\n",
    "    # Initialize MeCab with the specified dictionary\n",
    "    corpus = []\n",
    "    for sentence in sentences:\n",
    "        sentence = re.sub(\"http://news.livedoor.com/article/detail/[0-9]{7}/\",\"\", sentence) # 注2）\n",
    "        sentence = re.sub(\"[0-9]{4}-[0-9]{2}-[0-9]{2}T[0-9]{2}:[0-9]{2}:[0-9]{2}\\+[0-9]{4}\",\"\", sentence) # 注3）\n",
    "        sentence = re.sub(\"[「」]\",\"\", sentence)\n",
    "        # Parse the sentence\n",
    "        node = mecab.parseToNode(sentence)\n",
    "        # Iterate over all nodes\n",
    "        while node:\n",
    "            # Extract the surface form of the word\n",
    "            word = node.surface\n",
    "            # Skip empty words and add to the corpus\n",
    "            if word:\n",
    "                corpus.append(word)\n",
    "            node = node.next\n",
    "    return corpus\n",
    "\n",
    "\n",
    "# Initialize the MeCab tokenizer\n",
    "#mecab = MeCab.Tagger()\n",
    "mecab = MeCab.Tagger()\n",
    "corpus = tokenize_with_mecab(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_frequency(tokens, min_freq=5):\n",
    "    # Count word frequencies\n",
    "    frequency = Counter(tokens)\n",
    "    # Filter tokens by frequency\n",
    "    tokens = [token for token in tokens if frequency[token] >= min_freq]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_id = {}\n",
    "id_to_word = {}\n",
    "\n",
    "for word in corpus:\n",
    "    if word not in word_to_id:\n",
    "        new_id = len(word_to_id)\n",
    "        word_to_id[word] = new_id\n",
    "        id_to_word[new_id] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# リストに変換\n",
    "corpus = [word_to_id[word] for word in corpus]\n",
    "\n",
    "# NumPy配列に変換\n",
    "corpus = np.array(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25427"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   0,    1,    3,    4],\n",
      "        [   1,    2,    4,    5],\n",
      "        [   2,    3,    5,    6],\n",
      "        ...,\n",
      "        [  93,  832, 1225, 1226],\n",
      "        [ 832,  506, 1226, 1227],\n",
      "        [ 506, 1225, 1227,   96]], device='cuda:0')\n",
      "tensor([   2,    3,    4,  ...,  506, 1225, 1226], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# コンテキストとターゲットを作成\n",
    "contexts, targets = create_contexts_target(corpus, window_size=2)\n",
    "contexts = torch.tensor(contexts, dtype=torch.long).to(device)\n",
    "targets = torch.tensor(targets, dtype=torch.long).to(device)\n",
    "print(contexts)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_174364/1217301416.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  contexts_tensor = torch.tensor(contexts, dtype=torch.long).to(device)\n",
      "/tmp/ipykernel_174364/1217301416.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  targets_tensor = torch.tensor(targets, dtype=torch.long).to(device)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CBOWDataset(Dataset):\n",
    "    def __init__(self, contexts, targets):\n",
    "        self.contexts = contexts\n",
    "        self.targets = targets\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.contexts[idx], self.targets[idx]\n",
    "\n",
    "# Convert contexts and targets to tensors\n",
    "contexts_tensor = torch.tensor(contexts, dtype=torch.long).to(device)\n",
    "targets_tensor = torch.tensor(targets, dtype=torch.long).to(device)\n",
    "\n",
    "# Create the dataset\n",
    "dataset = CBOWDataset(contexts_tensor, targets_tensor)\n",
    "\n",
    "# Create the DataLoader\n",
    "batch_size = 256  # You can adjust the batch size\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size):\n",
    "        super(SimpleCBOW, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.linear1 = nn.Linear(embedding_size, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Embed the input words. \n",
    "        # Inputs should have the shape [batch_size, context_size]\n",
    "        embeds = self.embeddings(inputs)  # Resulting shape [batch_size, context_size, embedding_size]\n",
    "        \n",
    "        # Sum the embeddings for each context word to get a single embedding vector per batch sample.\n",
    "        # The resulting shape should be [batch_size, embedding_size]\n",
    "        out = torch.sum(embeds, dim=1)\n",
    "        \n",
    "        # Pass the summed embeddings through the linear layer\n",
    "        # The output shape will be [batch_size, vocab_size]\n",
    "        out = self.linear1(out)\n",
    "        \n",
    "        # Apply log softmax to get log probabilities over the vocabulary for each sample in the batch\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Total loss: 31397.376534461975\n",
      "Epoch 1, Total loss: 28187.27686357498\n",
      "Epoch 2, Total loss: 26342.247032165527\n",
      "Epoch 3, Total loss: 25164.694566488266\n",
      "Epoch 4, Total loss: 24353.24726819992\n",
      "Epoch 5, Total loss: 23723.983380794525\n",
      "Epoch 6, Total loss: 23238.247279167175\n",
      "Epoch 7, Total loss: 22841.27783536911\n",
      "Epoch 8, Total loss: 22517.91306066513\n",
      "Epoch 9, Total loss: 22247.803350925446\n",
      "Epoch 10, Total loss: 21995.72559070587\n",
      "Epoch 11, Total loss: 21773.0639834404\n",
      "Epoch 12, Total loss: 21581.493795871735\n",
      "Epoch 13, Total loss: 21411.580601215363\n",
      "Epoch 14, Total loss: 21256.410546302795\n",
      "Epoch 15, Total loss: 21103.692857265472\n",
      "Epoch 16, Total loss: 20973.788657188416\n",
      "Epoch 17, Total loss: 20847.244908571243\n",
      "Epoch 18, Total loss: 20738.265578985214\n",
      "Epoch 19, Total loss: 20643.255791187286\n",
      "Epoch 20, Total loss: 20544.659994602203\n",
      "Epoch 21, Total loss: 20456.14571905136\n",
      "Epoch 22, Total loss: 20366.695989608765\n",
      "Epoch 23, Total loss: 20292.23054742813\n",
      "Epoch 24, Total loss: 20217.244695663452\n",
      "Epoch 25, Total loss: 20146.588396072388\n",
      "Epoch 26, Total loss: 20079.78717470169\n",
      "Epoch 27, Total loss: 20019.27250981331\n",
      "Epoch 28, Total loss: 19961.13470363617\n",
      "Epoch 29, Total loss: 19910.1701130867\n",
      "Epoch 30, Total loss: 19855.205790519714\n",
      "Epoch 31, Total loss: 19802.713310718536\n",
      "Epoch 32, Total loss: 19756.16063785553\n",
      "Epoch 33, Total loss: 19710.295114517212\n",
      "Epoch 34, Total loss: 19664.314708709717\n",
      "Epoch 35, Total loss: 19621.991496801376\n",
      "Epoch 36, Total loss: 19582.196169376373\n",
      "Epoch 37, Total loss: 19545.746068954468\n",
      "Epoch 38, Total loss: 19506.374752998352\n",
      "Epoch 39, Total loss: 19469.252543449402\n",
      "Epoch 40, Total loss: 19433.223071575165\n",
      "Epoch 41, Total loss: 19401.942276000977\n"
     ]
    }
   ],
   "source": [
    "# パラメータの設定\n",
    "embedding_size = 10\n",
    "learning_rate = 0.01\n",
    "epochs = 500\n",
    "vocab_size = len(word_to_id)\n",
    "\n",
    "# モデルのインスタンス化\n",
    "model = SimpleCBOW(vocab_size, embedding_size).to(device)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "writer = SummaryWriter('runs/cbow_experiment_2')\n",
    "\n",
    "# Training loop with batch processing\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for i, (context_batch, target_batch) in enumerate(data_loader):\n",
    "        # Zero out the gradients from the last step\n",
    "        model.zero_grad()\n",
    "        # Forward pass through the model\n",
    "        log_probs = model(context_batch)\n",
    "        # Compute the loss\n",
    "        loss = loss_function(log_probs, target_batch)\n",
    "        # Backward pass to compute gradients\n",
    "        loss.backward()\n",
    "        # Update the model parameters\n",
    "        optimizer.step()\n",
    "        # Accumulate the loss\n",
    "        total_loss += loss.item()\n",
    "        writer.add_scalar('Training loss', loss.item(), epoch * len(data_loader) + i)\n",
    "    # Log the total loss for the epoch\n",
    "    writer.add_scalar('Total Training loss', total_loss, epoch)\n",
    "    print(f'Epoch {epoch}, Total loss: {total_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings = model.embeddings.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2567, -0.6427, -0.5169,  ..., -0.7604, -0.0346,  0.0217],\n",
       "        [ 0.8107, -0.9065, -1.1830,  ..., -0.4636, -0.0320,  0.9819],\n",
       "        [ 1.7672, -0.7820, -0.0137,  ...,  2.3656, -0.4327,  0.0266],\n",
       "        ...,\n",
       "        [ 1.5716,  0.7011,  1.5195,  ...,  0.6961,  1.1100,  0.1157],\n",
       "        [ 0.5649,  0.5198, -0.5296,  ...,  0.2395,  0.1166,  0.7280],\n",
       "        [-2.4054, -0.4683, -1.3342,  ...,  0.9051, -1.2672, -0.4244]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [id_to_word[i] for i in range(len(id_to_word))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Initialize the writer\n",
    "writer = SummaryWriter('runs/cbow_embeddings')\n",
    "\n",
    "# Add embedding to the writer\n",
    "writer.add_embedding(word_embeddings, metadata=words)\n",
    "\n",
    "# Close the writer\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}