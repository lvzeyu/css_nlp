{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pickle\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "with open(\"./Data/dokujo-tsushin.txt\", mode=\"r\",encoding=\"utf-8\") as f:\n",
    "    corpus = []\n",
    "    for line in f:\n",
    "        cleaned_line = line.replace('\\u3000', '').replace('\\n', '')\n",
    "        if cleaned_line!=\"\":\n",
    "            corpus.append(cleaned_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# コンテキストとターゲットの作成関数の実装\n",
    "def create_contexts_target(corpus, window_size=1):\n",
    "    \n",
    "    # ターゲットを抽出\n",
    "    target = corpus[window_size:-window_size]\n",
    "    \n",
    "    # コンテキストを初期化\n",
    "    contexts = []\n",
    "    \n",
    "    # ターゲットごとにコンテキストを格納\n",
    "    for idx in range(window_size, len(corpus) - window_size):\n",
    "        \n",
    "        # 現在のターゲットのコンテキストを初期化\n",
    "        cs = []\n",
    "        \n",
    "        # 現在のターゲットのコンテキストを1単語ずつ格納\n",
    "        for t in range(-window_size, window_size + 1):\n",
    "            \n",
    "            # 0番目の要素はターゲットそのものなので処理を省略\n",
    "            if t == 0:\n",
    "                continue\n",
    "            \n",
    "            # コンテキストを格納\n",
    "            cs.append(corpus[idx + t])\n",
    "            \n",
    "        # 現在のターゲットのコンテキストのセットを格納\n",
    "        contexts.append(cs)\n",
    "    \n",
    "    # NumPy配列に変換\n",
    "    return np.array(contexts), np.array(target) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MeCab\n",
    "from tqdm.notebook import tqdm\n",
    "def tokenize_with_mecab(sentences):\n",
    "    # Initialize MeCab with the specified dictionary\n",
    "    corpus = []\n",
    "    for sentence in sentences:\n",
    "        sentence = re.sub(\"http://news.livedoor.com/article/detail/[0-9]{7}/\",\"\", sentence) # 注2）\n",
    "        sentence = re.sub(\"[0-9]{4}-[0-9]{2}-[0-9]{2}T[0-9]{2}:[0-9]{2}:[0-9]{2}\\+[0-9]{4}\",\"\", sentence) # 注3）\n",
    "        sentence = re.sub(\"[「」]\",\"\", sentence)\n",
    "        # Parse the sentence\n",
    "        node = mecab.parseToNode(sentence)\n",
    "        # Iterate over all nodes\n",
    "        while node:\n",
    "            # Extract the surface form of the word\n",
    "            word = node.surface\n",
    "            # Skip empty words and add to the corpus\n",
    "            if word:\n",
    "                corpus.append(word)\n",
    "            node = node.next\n",
    "    return corpus\n",
    "\n",
    "\n",
    "# Initialize the MeCab tokenizer\n",
    "#mecab = MeCab.Tagger()\n",
    "mecab = MeCab.Tagger()\n",
    "corpus = tokenize_with_mecab(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_frequency(tokens, min_freq=5):\n",
    "    # Count word frequencies\n",
    "    frequency = Counter(tokens)\n",
    "    # Filter tokens by frequency\n",
    "    tokens = [token for token in tokens if frequency[token] >= min_freq]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_id = {}\n",
    "id_to_word = {}\n",
    "\n",
    "for word in corpus:\n",
    "    if word not in word_to_id:\n",
    "        new_id = len(word_to_id)\n",
    "        word_to_id[word] = new_id\n",
    "        id_to_word[new_id] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# リストに変換\n",
    "corpus = [word_to_id[word] for word in corpus]\n",
    "\n",
    "# NumPy配列に変換\n",
    "corpus = np.array(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25547"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   0,    1,    3,    4],\n",
      "        [   1,    2,    4,    5],\n",
      "        [   2,    3,    5,    6],\n",
      "        ...,\n",
      "        [  93,  830, 1228, 1229],\n",
      "        [ 830,  504, 1229, 1230],\n",
      "        [ 504, 1228, 1230,   96]])\n",
      "tensor([   2,    3,    4,  ...,  504, 1228, 1229])\n"
     ]
    }
   ],
   "source": [
    "# コンテキストとターゲットを作成\n",
    "contexts, targets = create_contexts_target(corpus, window_size=2)\n",
    "contexts = torch.tensor(contexts, dtype=torch.long).to(device)\n",
    "targets = torch.tensor(targets, dtype=torch.long).to(device)\n",
    "print(contexts)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wm/5xxpvjcj15g89khxd5jwn9200000gn/T/ipykernel_5786/1217301416.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  contexts_tensor = torch.tensor(contexts, dtype=torch.long).to(device)\n",
      "/var/folders/wm/5xxpvjcj15g89khxd5jwn9200000gn/T/ipykernel_5786/1217301416.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  targets_tensor = torch.tensor(targets, dtype=torch.long).to(device)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CBOWDataset(Dataset):\n",
    "    def __init__(self, contexts, targets):\n",
    "        self.contexts = contexts\n",
    "        self.targets = targets\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.contexts[idx], self.targets[idx]\n",
    "\n",
    "# Convert contexts and targets to tensors\n",
    "contexts_tensor = torch.tensor(contexts, dtype=torch.long).to(device)\n",
    "targets_tensor = torch.tensor(targets, dtype=torch.long).to(device)\n",
    "\n",
    "# Create the dataset\n",
    "dataset = CBOWDataset(contexts_tensor, targets_tensor)\n",
    "\n",
    "# Create the DataLoader\n",
    "batch_size = 256  # You can adjust the batch size\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size):\n",
    "        super(SimpleCBOW, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.linear1 = nn.Linear(embedding_size, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Embed the input words. \n",
    "        # Inputs should have the shape [batch_size, context_size]\n",
    "        embeds = self.embeddings(inputs)  # Resulting shape [batch_size, context_size, embedding_size]\n",
    "        \n",
    "        # Sum the embeddings for each context word to get a single embedding vector per batch sample.\n",
    "        # The resulting shape should be [batch_size, embedding_size]\n",
    "        out = torch.sum(embeds, dim=1)\n",
    "        \n",
    "        # Pass the summed embeddings through the linear layer\n",
    "        # The output shape will be [batch_size, vocab_size]\n",
    "        out = self.linear1(out)\n",
    "        \n",
    "        # Apply log softmax to get log probabilities over the vocabulary for each sample in the batch\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_function(log_probs, target_batch)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Backward pass to compute gradients\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Update the model parameters\u001b[39;00m\n\u001b[1;32m     27\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/jupyterbook/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/jupyterbook/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# パラメータの設定\n",
    "embedding_size = 10\n",
    "learning_rate = 0.01\n",
    "epochs = 500\n",
    "vocab_size = len(word_to_id)\n",
    "\n",
    "# モデルのインスタンス化\n",
    "model = SimpleCBOW(vocab_size, embedding_size).to(device)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "writer = SummaryWriter('runs/cbow_experiment_2')\n",
    "\n",
    "# Training loop with batch processing\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for i, (context_batch, target_batch) in enumerate(data_loader):\n",
    "        # Zero out the gradients from the last step\n",
    "        model.zero_grad()\n",
    "        # Forward pass through the model\n",
    "        log_probs = model(context_batch)\n",
    "        # Compute the loss\n",
    "        loss = loss_function(log_probs, target_batch)\n",
    "        # Backward pass to compute gradients\n",
    "        loss.backward()\n",
    "        # Update the model parameters\n",
    "        optimizer.step()\n",
    "        # Accumulate the loss\n",
    "        total_loss += loss.item()\n",
    "        writer.add_scalar('Training loss', loss.item(), epoch * len(data_loader) + i)\n",
    "    # Log the total loss for the epoch\n",
    "    writer.add_scalar('Total Training loss', total_loss, epoch)\n",
    "    print(f'Epoch {epoch}, Total loss: {total_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings = model.embeddings.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2567, -0.6427, -0.5169,  ..., -0.7604, -0.0346,  0.0217],\n",
       "        [ 0.8107, -0.9065, -1.1830,  ..., -0.4636, -0.0320,  0.9819],\n",
       "        [ 1.7672, -0.7820, -0.0137,  ...,  2.3656, -0.4327,  0.0266],\n",
       "        ...,\n",
       "        [ 1.5716,  0.7011,  1.5195,  ...,  0.6961,  1.1100,  0.1157],\n",
       "        [ 0.5649,  0.5198, -0.5296,  ...,  0.2395,  0.1166,  0.7280],\n",
       "        [-2.4054, -0.4683, -1.3342,  ...,  0.9051, -1.2672, -0.4244]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [id_to_word[i] for i in range(len(id_to_word))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Initialize the writer\n",
    "writer = SummaryWriter('runs/cbow_embeddings')\n",
    "\n",
    "# Add embedding to the writer\n",
    "writer.add_embedding(word_embeddings, metadata=words)\n",
    "\n",
    "# Close the writer\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}