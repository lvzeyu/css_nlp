{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前処理関数の実装\n",
    "def preprocess(text):\n",
    "    # 前処理\n",
    "    text = text.lower() # 小文字に変換\n",
    "    text = text.replace('.', ' .') # ピリオドの前にスペースを挿入\n",
    "    words = text.split(' ') # 単語ごとに分割\n",
    "    \n",
    "    # ディクショナリを初期化\n",
    "    word_to_id = {}\n",
    "    id_to_word = {}\n",
    "    \n",
    "    # 未収録の単語をディクショナリに格納\n",
    "    for word in words:\n",
    "        if word not in word_to_id: # 未収録の単語のとき\n",
    "            # 次の単語のidを取得\n",
    "            new_id = len(word_to_id)\n",
    "            \n",
    "            # 単語をキーとして単語IDを格納\n",
    "            word_to_id[word] = new_id\n",
    "            \n",
    "            # 単語IDをキーとして単語を格納\n",
    "            id_to_word[new_id] = word\n",
    "    \n",
    "    # 単語IDリストを作成\n",
    "    corpus = [word_to_id[w] for w in words]\n",
    "    \n",
    "    return corpus, word_to_id, id_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_contexts_target(corpus, window_size=1):\n",
    "    \n",
    "    # ターゲットを抽出\n",
    "    target = corpus[window_size:-window_size]\n",
    "    \n",
    "    # コンテキストを初期化\n",
    "    contexts = []\n",
    "    \n",
    "    # ターゲットごとにコンテキストを格納\n",
    "    for idx in range(window_size, len(corpus) - window_size):\n",
    "        \n",
    "        # 現在のターゲットのコンテキストを初期化\n",
    "        cs = []\n",
    "        \n",
    "        # 現在のターゲットのコンテキストを1単語ずつ格納\n",
    "        for t in range(-window_size, window_size + 1):\n",
    "            \n",
    "            # 0番目の要素はターゲットそのものなので処理を省略\n",
    "            if t == 0:\n",
    "                continue\n",
    "            \n",
    "            # コンテキストを格納\n",
    "            cs.append(corpus[idx + t])\n",
    "            \n",
    "        # 現在のターゲットのコンテキストのセットを格納\n",
    "        contexts.append(cs)\n",
    "    \n",
    "    # NumPy配列に変換\n",
    "    return np.array(contexts), np.array(target) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"When forty winters shall besiege thy brow,\n",
    "And dig deep trenches in thy beauty's field,\n",
    "Thy youth's proud livery so gazed on now,\n",
    "Will be a totter'd weed of small worth held:\n",
    "Then being asked, where all thy beauty lies,\n",
    "Where all the treasure of thy lusty days;\n",
    "To say, within thine own deep sunken eyes,\n",
    "Were an all-eating shame, and thriftless praise.\n",
    "How much more praise deserv'd thy beauty's use,\n",
    "If thou couldst answer 'This fair child of mine\n",
    "Shall sum my count, and make my old excuse,'\n",
    "Proving his beauty by succession thine!\n",
    "This were to be new made when thou art old,\n",
    "And see thy blood warm when thou feel'st it cold.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "contexts, targets = create_contexts_target(corpus, window_size=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOWDataset(Dataset):\n",
    "    def __init__(self, contexts, targets):\n",
    "        self.contexts = contexts\n",
    "        self.targets = targets\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.contexts[idx], self.targets[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size):\n",
    "        super(SimpleCBOW, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.linear1 = nn.Linear(embedding_size, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs)\n",
    "        out = torch.sum(embeds, dim=1)\n",
    "        out = self.linear1(out)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# Convert contexts and targets to tensors\n",
    "contexts_tensor = torch.tensor(contexts, dtype=torch.long).to(device)\n",
    "targets_tensor = torch.tensor(targets, dtype=torch.long).to(device)\n",
    "\n",
    "# Create the dataset\n",
    "dataset = CBOWDataset(contexts_tensor, targets_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DataLoader\n",
    "batch_size = 10  # You can adjust the batch size\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Total loss: 51.98939657211304\n",
      "Epoch 10, Total loss: 46.7331166267395\n",
      "Epoch 20, Total loss: 42.40438795089722\n",
      "Epoch 30, Total loss: 38.7656192779541\n",
      "Epoch 40, Total loss: 35.66978859901428\n",
      "Epoch 50, Total loss: 33.01869082450867\n",
      "Epoch 60, Total loss: 30.706968784332275\n",
      "Epoch 70, Total loss: 28.69171452522278\n",
      "Epoch 80, Total loss: 26.913305282592773\n",
      "Epoch 90, Total loss: 25.33737862110138\n"
     ]
    }
   ],
   "source": [
    "# パラメータの設定\n",
    "embedding_size = 10\n",
    "learning_rate = 0.01\n",
    "epochs = 100\n",
    "vocab_size = len(word_to_id)\n",
    "\n",
    "# モデルのインスタンス化\n",
    "model = SimpleCBOW(vocab_size, embedding_size).to(device)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "# Training loop with batch processing\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for i, (context_batch, target_batch) in enumerate(data_loader):\n",
    "        # Zero out the gradients from the last step\n",
    "        model.zero_grad()\n",
    "        # Forward pass through the model\n",
    "        log_probs = model(context_batch)\n",
    "        # Compute the loss\n",
    "        loss = loss_function(log_probs, target_batch)\n",
    "        # Backward pass to compute gradients\n",
    "        loss.backward()\n",
    "        # Update the model parameters\n",
    "        optimizer.step()\n",
    "        # Accumulate the loss\n",
    "        total_loss += loss.item()\n",
    "    # Log the total loss for the epoch\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Total loss: {total_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyterbook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}