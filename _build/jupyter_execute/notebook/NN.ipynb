{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ニューラルネットワーク\n",
    "\n",
    "ニューラルネットワークは、人間の脳に似た層状構造で相互接続されたノードやニューロンを使用するの計算モデルです。\n",
    "\n",
    "ニューラルネットワークは、画像認識、自然言語処理、音声認識など、さまざまな領域で広く利用されています。特に、大量のデータと計算能力が利用可能になった近年、ディープニューラルネットワーク(DNN)の研究や応用が急速に進展しています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ニューラルネットワークの構造\n",
    "\n",
    "### パーセプトロン\n",
    "\n",
    "パーセプトロンとは、複数の入力を受け取り、重み付けして、1つの信号を出力するアルゴリズムです。\n",
    "\n",
    "例えば,$x_1$と$x_2$の2つの入力を受け取り、yを出力するパーセプトロンを考えます。\n",
    "\n",
    "- $w_1$や$w_2$は各入力の「重み」を表すパラメータで、各入力の重要性をコントロールします。\n",
    "- $b$はバイアス\n",
    "\n",
    "![](./Figure/nn1.png)\n",
    "\n",
    "パーセプトロンの「○」で表されている部分は、ニューロンやノードと呼びます。\n",
    "\n",
    "\n",
    "\n",
    "### 活性化関数\n",
    "\n",
    "活性化関数とは、ニューロンにおける、入力のなんらかの合計から、出力を決定するための関数です。\n",
    "\n",
    "例えば、関数の入力(パーセプトロンだと重み付き和)が0以下のとき0を、0より大きいとき1を出力することが考えます。\n",
    "\n",
    "$$\n",
    "y   = \\begin{cases}\n",
    "          0 \\quad (w_1 x_1 + w_2 x_2 + b \\leq 0) \\\\\n",
    "          1 \\quad (w_1 x_1 + w_2 x_2 + b > 0)\n",
    "      \\end{cases}\n",
    "$$\n",
    "\n",
    "出力に関する計算数式を分解すると、\n",
    "\n",
    "$$y   = h(a)$$\n",
    "$$h(a)\n",
    "    = \\begin{cases}\n",
    "          0 \\quad (a \\leq 0) \\\\\n",
    "          1 \\quad (a > 0)\n",
    "      \\end{cases}\n",
    "$$\n",
    "で書けます。つまり、入力の重み付き和の結果が$a$というノードになり、そして活性化関数$h()$によって$y$という出力が計算されます。\n",
    "\n",
    "![](./Figure/nn2.png)\n",
    "\n",
    "活性化関数を使うことで表現の自由度を上げて、複数のパーセプトロンを適当につなげることで、入出力間が非線形な関係でも表現できるようになります。\n",
    "\n",
    "例えば、線形変換のみで下図右の白い丸で表される観測データから$x$と$y$の関係を近似した場合、点線のような直線が得られたとします。これでは、一部のデータについてはあまりよく当てはまっていないのが分かります。\n",
    "\n",
    "しかし、もし図右の実線のような曲線を表現することができれば、両者の関係をより適切に表現することができます。\n",
    "\n",
    "![](./Figure/transform_function2.gif)\n",
    "\n",
    "活性化関数にはいくつか種類があります。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "活性化関数にはいくつか種類があり、異なる特性や用途を持っています。\n",
    "\n",
    "![](./Figure/transform_function3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ニューラルネットワークの仕組み\n",
    "\n",
    "ニューラルネットワークの仕組みは下の図で表さます。左側から、最初の層を入力層 (input layer)、最後の層を出力層 (output layer)といいます。\n",
    "\n",
    "その間にある層は中間層 （intermediate layer) もしくは隠れ層 (hidden layer) といいます。中間層において、層の数を増やすことによって、ディープニューラルネットワークを実現することができます。\n",
    "\n",
    "ニューラルネットワークは、層から層へ、値を変換していきます。 そのため、ニューラルネットワークとはこの変換がいくつも連なってできる一つの大きな関数だと考えることができます。 従って、基本的には、入力を受け取って、何か出力を返すものです。 そして、どのようなデータを入力し、どのような出力を作りたいかによって、入力層と出力層のノード数が決定されます。\n",
    "\n",
    " ここで、層と層の間にあるノード間の結合は、一つ一つが重みを持っており、上のような全結合型ニューラルネットワークの場合は、それらの重みをまとめて、一つの行列で表現します。 \n",
    "\n",
    "![](./Figure/nn4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ニューラルネットワークの計算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "それでは、下図に示す$3$層ニューラルネットワークを例として、入力から出力への計算のについて解説を行います。\n",
    "\n",
    "![](./Figure/nn_a.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 記号の説明\n",
    "\n",
    "ニューラルネットワークの計算を説明するにあたって、導入される記号の定義から始めます。\n",
    "\n",
    "入力層の$x_1$と$x_2$ニューロンから、次層のニューロン$a_1^{(1)}$への信号伝達を見ていきます。\n",
    "\n",
    "- $w_{12}^{(1)}$ は前層の$2$番目のニューロン($x_2$)から次層の$1$番目のニューロン($a_1^{(1)}$)への重みであることを意味します。\n",
    "    - 右上$(1)$は第$1$層の重みということ意味します\n",
    "    - 右下$12$ような数字の並びは、次層のニューロン($1$)と前層のニューロンのインデックス番号($2$)から構成されます\n",
    "- $a_1^{(1)}$は第$1$層$1$番目のニューロンであることを意味します。\n",
    "    - 右上$(1)$は第$1$層のニューロンということ意味します\n",
    "    - 右下$1$は$1$番目のニューロンということ意味します\n",
    "![](./Figure/nn_b.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 各層における信号伝達"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まず、入力層から「第$1$層の$1$番目のニューロン」への信号伝達を見ていきます。ここでは。バイアス項も追加し、$a_1^{(1)}$を以下の数式で計算します。\n",
    "\n",
    "![](./Figure/nn_c.png)\n",
    "\n",
    "$$\n",
    " a_1^{(1)}= w_{11}^{(1)}x_{1} + w_{12}^{(1)}x_{2} + b_1^{(1)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同じ形で、第$1$層におけるすべでのニューロンの計算式を書けます。\n",
    "$$\n",
    "\\begin{split}\\begin{cases}\n",
    "    a_1^{(1)} = w_{11}^{(1)}x_{1} + w_{12}^{(1)}x_{1}x_{2} + b_1^{(1)} \\\\\n",
    "    a_2^{(1)} = w_{21}^{(1)}x_{1} + w_{22}^{(1)}x_{1}x_{2} + b_2^{(1)} \\\\\n",
    "    a_3^{(1)} = w_{31}^{(1)}x_{1} + w_{32}^{(1)}x_{1}x_{2} + b_3^{(1)}\n",
    "\\end{cases}\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "行列で第$1$層におけるニューロンの計算式をまとめて表すことができます。\n",
    "- 入力 $\\mathbf{X}=\\begin{pmatrix} x_1 & x_2 \\end{pmatrix}$\n",
    "- バイアス $\\mathbf{B} = \\begin{pmatrix} b_{1}^{(1)} & b_{2}^{(1)} & b_{3}^{(1)} \\end{pmatrix}$\n",
    "- 重み $$\\begin{split} \\mathbf{W} = \\begin{pmatrix}\n",
    "    w_{11}^{(1)} & w_{21}^{(1)} & w_{31}^{(1)} \\\\\n",
    "    w_{12}^{(1)} & w_{22}^{(1)} & w_{32}^{(1)}\n",
    "\\end{pmatrix}\\end{split}$$\n",
    "\n",
    "- 入力・バイアスと重みの総和: $\\mathbf{A} = \\begin{pmatrix}\n",
    "    a_1^{(1)} & a_2^{(1)} & a_3^{(1)}\n",
    "\\end{pmatrix}$\n",
    "$$\n",
    "\\mathbf{A}^{(1)}\n",
    "     = \\mathbf{X} \\mathbf{W}^{(1)} + \\mathbf{B}^{(1)}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "さらに、活性化関数を導入します。入力・バイアスと重みの総和を$a$で表し、活性化関数$h()$による変換された結果を$z$で表すことにします。\n",
    "![](./Figure/nn_d.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数値を見ながら計算の流れを確認\n",
    "\n",
    "それでは、```NumPy```の多次元配列を使って、入力 $x_1$,$x_2$,$x_3$から出力が計算される過程を確認してみましょう。入力、重み、バイアスは適当な値を設定します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "入力の形状: (2,)\n",
      "重みの形状: (2, 3)\n",
      "バイアスの形状: (3,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "X = np.array([1.0, 0.5])\n",
    "W1 = np.array([[0.1, 0.3, 0.5],[0.2, 0.4, 0.6]])\n",
    "B1 = np.array([0.1, 0.2, 0.3])\n",
    "print(r\"入力の形状: {}\".format(X.shape))\n",
    "print(r\"重みの形状: {}\".format(W1.shape))\n",
    "print(r\"バイアスの形状: {}\".format(B1.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一層隠れ層で重み付きとバイアスの総和を計算し、活性化関数で変換された結果を返します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "A1 = np.dot(X, W1) + B1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z1 = sigmoid(A1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "続いて、同じ形で第1層から第2層目への信号伝達を行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2 = np.array([[0.1, 0.4],[0.2, 0.5],[0.3, 0.6]])\n",
    "B2 = np.array([0.1, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "A2 = np.dot(Z1, W2) + B2\n",
    "Z2 = sigmoid(A2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最後に、第2層から出力層への信号を行います。出力層の活性化関数は、恒等関数を用います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "W3 = np.array([[0.1, 0.3],[0.2, 0.4]])\n",
    "B3 = np.array([0.1, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "A3 = np.dot(Z2, W3) + B3 # Y = A3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{note}\n",
    "ニューラルネットワークは、分類問題と回帰問題の両方に用いることができます。ただし、分類問題と回帰問題のどちらに用いるかで、出力層の活性化関数を変更する必要があります。\n",
    "\n",
    "回帰問題では恒等関数を使います。\n",
    "\n",
    "分類問題の場合は、クラス数と同じだけのノードを出力層に用意しておき、各ノードがあるクラスに入力が属する確率を表すようにします。 このため、全出力ノードの値の合計が 1 になるよう正規化します。 これには、要素ごとに適用される活性化関数ではなく、層ごとに活性値を計算する別の関数を用いる必要があります。 そのような目的に使用される代表的な関数には、ソフトマックス関数があります。\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}