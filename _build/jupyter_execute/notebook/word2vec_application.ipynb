{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vecが人文・社会科学研究における応用 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Geometry of Culture: Analyzing the Meanings of Class through Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 問題関心\n",
    "\n",
    "- 「階層」という概念は多次元な側面より構成されています\n",
    "    - Affluence\n",
    "    - Occupation\n",
    "    - Symbolic Manifestations(social honor and prestige)\n",
    "    - Gender\n",
    "\n",
    "- 「階層」という概念は時間とともに変化しています"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 方法\n",
    "\n",
    "![](./Figure/project.jpeg)\n",
    "\n",
    "#### Cultural Dimensionsの測定\n",
    "\n",
    "Cultural Dimensionsの測定は、単語分散表現を用いて類推問題を解決する能力を活用しています。\n",
    "\n",
    "文化的な概念を反映する単語のペア間のベクトル計算でCultural Dimensionsを測定することが可能です($A$)。\n",
    "\n",
    "- $\\vec{male}-\\vec{female}$は$Gender$という概念反映しています。同じロジックで、$\\vec{king}-\\vec{queen}$のような単語のペアも$Gender$という概念を反映できると考えられます。\n",
    "- $\\vec{rich}-\\vec{poor}$、$\\vec{affluence}-\\vec{poverty}$のような単語のペアは$Affluence$という概念を反映できると考えられます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdownloader\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mgensim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mword2vec-google-news-300\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/jupyterbook/lib/python3.10/site-packages/gensim/downloader.py:503\u001b[0m, in \u001b[0;36mload\u001b[0;34m(name, return_path)\u001b[0m\n\u001b[1;32m    501\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;241m0\u001b[39m, BASE_DIR)\n\u001b[1;32m    502\u001b[0m module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28m__import__\u001b[39m(name)\n\u001b[0;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/gensim-data/word2vec-google-news-300/__init__.py:8\u001b[0m, in \u001b[0;36mload_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_data\u001b[39m():\n\u001b[1;32m      7\u001b[0m     path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(base_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mword2vec-google-news-300\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mword2vec-google-news-300.gz\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mKeyedVectors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_word2vec_format\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbinary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m/opt/anaconda3/envs/jupyterbook/lib/python3.10/site-packages/gensim/models/keyedvectors.py:1719\u001b[0m, in \u001b[0;36mKeyedVectors.load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header)\u001b[0m\n\u001b[1;32m   1672\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m   1673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_word2vec_format\u001b[39m(\n\u001b[1;32m   1674\u001b[0m         \u001b[38;5;28mcls\u001b[39m, fname, fvocab\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf8\u001b[39m\u001b[38;5;124m'\u001b[39m, unicode_errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   1675\u001b[0m         limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, datatype\u001b[38;5;241m=\u001b[39mREAL, no_header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1676\u001b[0m     ):\n\u001b[1;32m   1677\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load KeyedVectors from a file produced by the original C word2vec-tool format.\u001b[39;00m\n\u001b[1;32m   1678\u001b[0m \n\u001b[1;32m   1679\u001b[0m \u001b[38;5;124;03m    Warnings\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1717\u001b[0m \n\u001b[1;32m   1718\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1719\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load_word2vec_format\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1720\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbinary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbinary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43municode_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43municode_errors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1721\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatatype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatatype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mno_header\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mno_header\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1722\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/jupyterbook/lib/python3.10/site-packages/gensim/models/keyedvectors.py:2065\u001b[0m, in \u001b[0;36m_load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header, binary_chunk_size)\u001b[0m\n\u001b[1;32m   2062\u001b[0m kv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(vector_size, vocab_size, dtype\u001b[38;5;241m=\u001b[39mdatatype)\n\u001b[1;32m   2064\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[0;32m-> 2065\u001b[0m     \u001b[43m_word2vec_read_binary\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2066\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcounts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvector_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatatype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43municode_errors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbinary_chunk_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\n\u001b[1;32m   2067\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2068\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2069\u001b[0m     _word2vec_read_text(fin, kv, counts, vocab_size, vector_size, datatype, unicode_errors, encoding)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/jupyterbook/lib/python3.10/site-packages/gensim/models/keyedvectors.py:1958\u001b[0m, in \u001b[0;36m_word2vec_read_binary\u001b[0;34m(fin, kv, counts, vocab_size, vector_size, datatype, unicode_errors, binary_chunk_size, encoding)\u001b[0m\n\u001b[1;32m   1955\u001b[0m tot_processed_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   1957\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m tot_processed_words \u001b[38;5;241m<\u001b[39m vocab_size:\n\u001b[0;32m-> 1958\u001b[0m     new_chunk \u001b[38;5;241m=\u001b[39m \u001b[43mfin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbinary_chunk_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1959\u001b[0m     chunk \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m new_chunk\n\u001b[1;32m   1960\u001b[0m     processed_words, chunk \u001b[38;5;241m=\u001b[39m _add_bytes_to_kv(\n\u001b[1;32m   1961\u001b[0m         kv, counts, chunk, vocab_size, vector_size, datatype, unicode_errors, encoding)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/jupyterbook/lib/python3.10/gzip.py:301\u001b[0m, in \u001b[0;36mGzipFile.read\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01merrno\u001b[39;00m\n\u001b[1;32m    300\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(errno\u001b[38;5;241m.\u001b[39mEBADF, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread() on write-only GzipFile object\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 301\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_buffer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/jupyterbook/lib/python3.10/_compression.py:68\u001b[0m, in \u001b[0;36mDecompressReader.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreadinto\u001b[39m(\u001b[38;5;28mself\u001b[39m, b):\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mmemoryview\u001b[39m(b) \u001b[38;5;28;01mas\u001b[39;00m view, view\u001b[38;5;241m.\u001b[39mcast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m byte_view:\n\u001b[0;32m---> 68\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbyte_view\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m         byte_view[:\u001b[38;5;28mlen\u001b[39m(data)] \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/jupyterbook/lib/python3.10/gzip.py:484\u001b[0m, in \u001b[0;36m_GzipReader.read\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    480\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new_member \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    481\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decompressor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decomp_factory(\n\u001b[1;32m    482\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decomp_args)\n\u001b[0;32m--> 484\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new_member:\n\u001b[1;32m    485\u001b[0m     \u001b[38;5;66;03m# If the _new_member flag is set, we have to\u001b[39;00m\n\u001b[1;32m    486\u001b[0m     \u001b[38;5;66;03m# jump to the next member, if there is one.\u001b[39;00m\n\u001b[1;32m    487\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_read()\n\u001b[1;32m    488\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_gzip_header():\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gensim.downloader\n",
    "model = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "rich_list=[\"rich\",\"richer\",\"affluence\",\"luxury\"]\n",
    "poor_list=[\"poor\",\"poorer\",\"poverty\",\"cheap\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "affluence_vec=[]\n",
    "for i,j in zip(rich_list,poor_list):\n",
    "    affluence_vec.append(model[i]-model[j])\n",
    "affluence_vec=np.array(affluence_vec)\n",
    "affluence_vec=np.mean(affluence_vec,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````{tab-set}\n",
    "```{tab-item} 質問1\n",
    "なぜ複数の単語ペアで計算する必要がありますか？\n",
    "```\n",
    "\n",
    "```{tab-item} 質問2\n",
    "単語ペアでベクトル減法を行なって、結果の平均を取る方法以外、Dimensionsを測定する方法がありますか？\n",
    "```\n",
    "\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cultural Dimensionsで概念の「理解」\n",
    "\n",
    "ある単語を「Cultural Dimensions」でどのように解釈されるかを、その単語のベクトルと文化的次元ベクトルとの間の角度を計算することで求めるのです。($B$)\n",
    "\n",
    "この角度が小さいほど、その単語はその文化的次元に強く関連していると言えます。この方法により、単語が持つ文化的な意味合いやニュアンスを数値的に分析することが可能になります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "$$cos(\\theta))=\\frac{D \\cdot V}{|D||V|} $$\n",
    "$$\\theta = \\arccos(cos(\\theta))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_consine(vector, dimension):\n",
    "    \"\"\"\n",
    "    Calculate the angle between the vector and the given dimension\n",
    "    \"\"\"\n",
    "    v_dot_d = np.dot(vector, dimension)\n",
    "    v_d = np.linalg.norm(vector) * np.linalg.norm(dimension)\n",
    "    return v_dot_d / v_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_consine(model[\"tennis\"],affluence_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.10311404]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cosine_similarity(model[\"tennis\"].reshape(1,-1),affluence_vec.reshape(1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angle(vector, dimension,degree=False):\n",
    "    \"\"\"\n",
    "    Calculate the angle between the vector and the given dimension\n",
    "    \"\"\"\n",
    "    c = get_consine(vector, dimension)\n",
    "    if degree:\n",
    "        return np.degrees(np.arccos(np.clip(c, -1, 1)))\n",
    "    else:\n",
    "        return np.arccos(np.clip(c, -1, 1)) #return radian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sports=[\"tennis\",\"soccer\",\"basketball\",\"boxing\",\"golf\",\"swimming\",\"volleyball\",\"camping\",\"weightlifting\",\"hiking\",\"hockey\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tennis 84.08148088988168\n",
      "soccer 86.44827814928067\n",
      "basketball 87.4947624687713\n",
      "boxing 96.19771983578752\n",
      "golf 81.23038078574373\n",
      "swimming 87.66950350249786\n",
      "volleyball 84.87990835557244\n",
      "camping 92.64046862065912\n",
      "weightlifting 92.84652198285549\n",
      "hiking 89.06679342925825\n",
      "hockey 88.2316913132707\n"
     ]
    }
   ],
   "source": [
    "for sport in sports:\n",
    "    print(sport,get_angle(model[sport],affluence_vec,degree=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````{tab-set}\n",
    "```{tab-item} 質問3\n",
    "この結果をどのように解釈すべきですか？\n",
    "```\n",
    "```{tab-item} 課題\n",
    "- 性別に関するDimensionを作成しなさい\n",
    "- 性別Dimensionで運動の位置付けを確認し、その結果を解釈しなさい\n",
    "```\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class意味の推移\n",
    "\n",
    "階層という概念は多次元な側面より構成されて、さらにその構成は時間とともに変化している。\n",
    "\n",
    "- Cultural Dimensionsを構築することで、「階層」の各構成の「意味」を定量的に測定する\n",
    "- 「Affluence」が他の要素とどのように関係していることは、階層の意味構成を説明している"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vector(word_pair):\n",
    "    vec=[]\n",
    "    for i in word_pair:\n",
    "        vec.append(model[i[0]]-model[i[1]])\n",
    "    vec=np.array(vec)\n",
    "    vec=np.mean(vec,axis=0)\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "education_pair=[(\"educated\",\"uneducated\"),(\"learned\",\"unlearned\"),(\"taught\",\"untaught\"),\n",
    "                (\"schooled\",\"unschooled\"),(\"trained\",\"untrained\"),(\"lettered\",\"unlettered\"),\n",
    "                (\"tutored\",\"untutored\"),(\"literate\",\"illiterate\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "education_vec=create_vector(education_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_pair=[(\"man\",\"woman\"),(\"men\",\"women\"),(\"he\",\"she\"),(\"him\",\"her\"),\n",
    "             (\"his\",\"her\"),(\"boy\",\"girl\"),(\"male\",\"female\"),(\"masculine\",\"feminine\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_vec=create_vector(gender_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.04156308]], dtype=float32)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(gender_vec.reshape(1,-1),affluence_vec.reshape(1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.20604998]], dtype=float32)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(education_vec.reshape(1,-1),affluence_vec.reshape(1,-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````{tab-set}\n",
    "```{tab-item} 質問4\n",
    "この結果をどのように解釈すべきですか？\n",
    "```\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 言語モデルは学習コーパスに含まれている「バイアス」をそのまま反映しています。例えば、言語モデルでは、エンジニア、トラック運転手は男性、モデル、看護師は女性というような、職業と性別の関係についての典型的なステレオタイプを学習していることがわかります。\n",
    "\n",
    "- 同様に、異なる時期のコーパスには、特定な時期の考え方や認識に関する情報が含められますので、そのコーパスで学習した言語モデルも特定な時期の考え方や認識を反映できると考えられます。\n",
    "\n",
    "![](./Figure/sim_dynamics.jpeg)\n",
    "![](./Figure/OLS_dynamic.jpeg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````{tab-set}\n",
    "```{tab-item} 質問4\n",
    "この図の結果をどのように解釈すべきですか？\n",
    "```\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vecを応用する研究\n",
    "\n",
    "### Semantic projection recovers rich human knowledge of multiple object features from word embeddings {cite}`Grand2022`\n",
    "\n",
    "- 単語分散表現は表出する意味的特徴が人間の評価を近似することができることを体系的に説明しました。\n",
    "\n",
    "![](./Figure/projection_example.jpeg)\n",
    "\n",
    "![](./Figure/projection_example2.jpeg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word embeddings quantify 100 years of gender and ethnic stereotypes {cite}`Garg2018`\n",
    "\n",
    "- 単語分散表現が、性別ステレオタイプと民族マイノリティに対する態度の変化を定量化するのにどのように役立つかを示しています。\n",
    "- 性別と民族に関する単語で、形容詞や職業などの中立的な単語(neutral words )と比較する手法\n",
    "\n",
    "    - 性別(男性、女性)と職業に関連する単語リストをまとめます\n",
    "    - 女性を代表する単語（例：she, female）と職業の単語（例：teacher, lawyer）との間の平均埋め込み距離を計算します\n",
    "    - 男性を代表する単語と同じ職業の単語との平均埋め込み距離も計算します\n",
    "    - 女性の平均距離から男性の平均距離を引く結果は「性別バイアス」と考えます。$\\to$ 値がマイナスの場合、該当する職業は男性とより密接に関連付けていることを意味しています\n",
    "\n",
    "$$\n",
    "\\text{relative norm distance} = \\sum_{v_m \\in M} \\left( \\|v_m - v_1\\|_2 - \\|v_m - v_2\\|_2 \\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./Figure/gender_bias.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````{tab-set}\n",
    "```{tab-item} 課題\n",
    "-「engineer」、「nurse」、「housekeeper」の$Gender Bias$を計算しなさい\n",
    "- Norm distanceはNumpyを使って実装できます\n",
    "```\n",
    "\n",
    "```{tab-item} 質問5\n",
    "$Ethnic Bias$はどのように計算すべきのかを考えなさい。Ethnicのグループでは$2$以上であることを注意してください。\n",
    "```\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 同じ手法を使用した研究 {cite}`Jones2020`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参考文献\n",
    "\n",
    "```{bibliography}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyterbook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}