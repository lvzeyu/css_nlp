<!DOCTYPE html><html lang="en" class="" style="scroll-padding:60px"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width,initial-scale=1"/><title>BERT - 計算社会科学と自然言語処理</title><meta property="og:title" content="BERT - 計算社会科学と自然言語処理"/><meta name="generator" content="mystmd"/><meta name="keywords" content=""/><meta name="image" content="/build/7e2db436150c38a00650f96925aa5581.svg"/><meta property="og:image" content="/build/7e2db436150c38a00650f96925aa5581.svg"/><link rel="stylesheet" href="/build/_assets/app-AIT5GAEP.css"/><link rel="stylesheet" href="/build/_assets/thebe-core-VKVHG5VY.css"/><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/jupyter-matplotlib@0.11.3/css/mpl_widget.css"/><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.css"/><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css" integrity="sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ" crossorigin="anonymous"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-85RFPTYEE3"></script><script>window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'G-85RFPTYEE3');</script><link rel="icon" href="/favicon.ico"/><link rel="stylesheet" href="/myst-theme.css"/><script>
  const savedTheme = localStorage.getItem("myst:theme");
  const theme = window.matchMedia("(prefers-color-scheme: light)").matches ? 'light' : 'dark';
  const classes = document.documentElement.classList;
  const hasAnyTheme = classes.contains('light') || classes.contains('dark');
  if (!hasAnyTheme) classes.add(savedTheme ?? theme);
</script></head><body class="m-0 transition-colors duration-500 bg-white dark:bg-stone-900"><div class="myst-skip-to-article fixed top-1 left-1 h-[0px] w-[0px] focus-within:z-40 focus-within:h-auto focus-within:w-auto bg-white overflow-hidden focus-within:p-2 focus-within:ring-1" aria-label="skip to content options"><a href="#skip-to-frontmatter" class="myst-skip-to-link block px-2 py-1 text-black underline">Skip to article frontmatter</a><a href="#skip-to-article" class="myst-skip-to-link block px-2 py-1 text-black underline">Skip to article content</a></div><dialog id="myst-no-css" style="position:fixed;left:0px;top:0px;width:100vw;height:100vh;font-size:4rem;padding:1rem;color:black;background:white"><strong>Site not loading correctly?</strong><p>This may be due to an incorrect <code>BASE_URL</code> configuration. See<!-- --> <a href="https://mystmd.org/guide/deployment#deploy-base-url">the MyST Documentation</a> <!-- -->for reference.</p><script>
    (() => {
            // Test for has-styling variable set by the MyST stylesheet
            const node = document.currentScript.parentNode;
            const hasCSS = window.getComputedStyle(node).getPropertyValue("--has-styling");
            if (hasCSS === ""){
                    node.showModal();
            }

    })()
</script></dialog><div class="myst-top-nav bg-white/80 backdrop-blur dark:bg-stone-900/80 shadow dark:shadow-stone-700 p-3 md:px-8 sticky w-screen top-0 z-30 h-[60px]"><nav class="myst-top-nav-bar flex items-center justify-between flex-nowrap max-w-[1440px] mx-auto"><div class="flex flex-row xl:min-w-[19.5rem] mr-2 sm:mr-7 justify-start items-center shrink-0"><div class="block xl:hidden"><button class="myst-top-nav-menu-button flex items-center justify-center border-stone-400 text-stone-800 hover:text-stone-900 dark:text-stone-200 hover:dark:text-stone-100 w-10 h-10"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" data-slot="icon" width="1.5rem" height="1.5rem"><path fill-rule="evenodd" d="M3 6.75A.75.75 0 0 1 3.75 6h16.5a.75.75 0 0 1 0 1.5H3.75A.75.75 0 0 1 3 6.75ZM3 12a.75.75 0 0 1 .75-.75h16.5a.75.75 0 0 1 0 1.5H3.75A.75.75 0 0 1 3 12Zm0 5.25a.75.75 0 0 1 .75-.75h16.5a.75.75 0 0 1 0 1.5H3.75a.75.75 0 0 1-.75-.75Z" clip-rule="evenodd"></path></svg><span class="sr-only">Open Menu</span></button></div><a class="myst-home-link flex items-center ml-3 dark:text-white w-fit md:ml-5 xl:ml-7" href="/"><div class="myst-home-link-logo mr-3 flex items-center dark:bg-white dark:rounded px-1"><img src="/build/tohoku-university-lo-68dede7d9dcb965aff25e8429e755296.svg" class="h-9" height="2.25rem"/></div><span class="text-md sm:text-xl tracking-tight sm:mr-5 sr-only">Made with MyST</span></a></div><div class="flex items-center flex-grow w-auto"><div class="flex-grow hidden text-md lg:block"></div><div class="flex-grow block"></div><button type="button" aria-haspopup="dialog" aria-expanded="false" aria-controls="radix-:R75cp:" data-state="closed" class="myst-search-bar flex items-center h-10 aspect-square sm:w-64 text-left text-gray-600 border border-gray-300 dark:border-gray-600 rounded-lg bg-gray-50 dark:bg-gray-700 myst-search-bar-disabled hover:ring-blue-500 dark:hover:ring-blue-500 hover:border-blue-500 dark:hover:border-blue-500"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" data-slot="icon" class="p-2.5 h-10 w-10 aspect-square"><path fill-rule="evenodd" d="M10.5 3.75a6.75 6.75 0 1 0 0 13.5 6.75 6.75 0 0 0 0-13.5ZM2.25 10.5a8.25 8.25 0 1 1 14.59 5.28l4.69 4.69a.75.75 0 1 1-1.06 1.06l-4.69-4.69A8.25 8.25 0 0 1 2.25 10.5Z" clip-rule="evenodd"></path></svg><span class="myst-search-text-placeholder hidden sm:block grow">Search</span><div aria-hidden="true" class="myst-search-shortcut items-center hidden mx-1 font-mono text-sm text-gray-600 sm:flex gap-x-1"><kbd class="px-2 py-1 border border-gray-300 dark:border-gray-600 rounded-md shadow-[0px_2px_0px_0px_rgba(0,0,0,0.08)] dark:shadow-none hide-mac">CTRL</kbd><kbd class="px-2 py-1 border border-gray-300 dark:border-gray-600 rounded-md shadow-[0px_2px_0px_0px_rgba(0,0,0,0.08)] dark:shadow-none show-mac">⌘</kbd><kbd class="px-2 py-1 border border-gray-300 dark:border-gray-600 rounded-md shadow-[0px_2px_0px_0px_rgba(0,0,0,0.08)] dark:shadow-none ">K</kbd><script>
;(() => {
const script = document.currentScript;
const root = script.parentElement;

const isMac = /mac/i.test(
      window.navigator.userAgentData?.platform ?? window.navigator.userAgent,
    );
root.querySelectorAll(".hide-mac").forEach(node => {node.classList.add(isMac ? "hidden" : "block")});
root.querySelectorAll(".show-mac").forEach(node => {node.classList.add(!isMac ? "hidden" : "block")});
})()</script></div></button><button class="myst-theme-button theme rounded-full aspect-square border border-stone-700 dark:border-white hover:bg-neutral-100 border-solid overflow-hidden text-stone-700 dark:text-white hover:text-stone-500 dark:hover:text-neutral-800 w-10 h-10 mx-3" title="Toggle theme between light and dark mode" aria-label="Toggle theme between light and dark mode"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" data-slot="icon" class="myst-theme-moon-icon h-full w-full p-0.5 hidden dark:block"><path fill-rule="evenodd" d="M9.528 1.718a.75.75 0 0 1 .162.819A8.97 8.97 0 0 0 9 6a9 9 0 0 0 9 9 8.97 8.97 0 0 0 3.463-.69.75.75 0 0 1 .981.98 10.503 10.503 0 0 1-9.694 6.46c-5.799 0-10.5-4.7-10.5-10.5 0-4.368 2.667-8.112 6.46-9.694a.75.75 0 0 1 .818.162Z" clip-rule="evenodd"></path></svg><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="myst-theme-sun-icon h-full w-full p-0.5 dark:hidden"><path stroke-linecap="round" stroke-linejoin="round" d="M12 3v2.25m6.364.386-1.591 1.591M21 12h-2.25m-.386 6.364-1.591-1.591M12 18.75V21m-4.773-4.227-1.591 1.591M5.25 12H3m4.227-4.773L5.636 5.636M15.75 12a3.75 3.75 0 1 1-7.5 0 3.75 3.75 0 0 1 7.5 0Z"></path></svg></button><div class="block sm:hidden"></div><div class="hidden sm:block"></div></div></nav></div><div class="myst-primary-sidebar fixed xl:article-grid grid-gap xl:w-screen xl:pointer-events-none overflow-auto max-xl:min-w-[300px] hidden z-10" style="top:60px"><div class="myst-primary-sidebar-pointer pointer-events-auto xl:col-margin-left flex-col overflow-hidden hidden xl:flex"><div class="myst-primary-sidebar-nav flex-grow py-6 overflow-y-auto primary-scrollbar"><nav aria-label="Navigation" class="myst-primary-sidebar-topnav overflow-y-hidden transition-opacity ml-3 xl:ml-0 mr-3 max-w-[350px] lg:hidden"><div class="w-full px-1 dark:text-white font-medium"></div></nav><div class="my-3 border-b-2 lg:hidden"></div><nav aria-label="Table of Contents" class="myst-primary-sidebar-toc flex-grow overflow-y-hidden transition-opacity ml-3 xl:ml-0 mr-3 max-w-[350px]"><div class="myst-toc w-full px-1 dark:text-white"><a title="計算社会科学と自然言語処理" class="block break-words focus:outline outline-blue-200 outline-2 rounded myst-toc-item p-2 my-1 rounded-lg hover:bg-slate-300/30 font-bold" href="/">計算社会科学と自然言語処理</a><div data-state="closed" class="w-full"><div class="myst-toc-item flex flex-row w-full gap-2 px-2 my-1 text-left rounded-lg outline-none hover:bg-slate-300/30"><div title="イントロダクション" class="block break-words rounded py-2 grow cursor-pointer">イントロダクション</div><button class="self-center flex-none rounded-md group hover:bg-slate-300/30 focus:outline outline-blue-200 outline-2" aria-label="Open Folder" type="button" aria-controls="radix-:Rmpsp:" aria-expanded="false" data-state="closed"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" data-slot="icon" class="transition-transform duration-300 group-data-[state=open]:rotate-90 text-text-slate-700 dark:text-slate-100" height="1.5rem" width="1.5rem"><path fill-rule="evenodd" d="M16.28 11.47a.75.75 0 0 1 0 1.06l-7.5 7.5a.75.75 0 0 1-1.06-1.06L14.69 12 7.72 5.03a.75.75 0 0 1 1.06-1.06l7.5 7.5Z" clip-rule="evenodd"></path></svg></button></div><div data-state="closed" id="radix-:Rmpsp:" hidden="" class="pl-3 pr-[2px] collapsible-content"></div></div><div data-state="closed" class="w-full"><div class="myst-toc-item flex flex-row w-full gap-2 px-2 my-1 text-left rounded-lg outline-none hover:bg-slate-300/30"><div title="基礎知識" class="block break-words rounded py-2 grow cursor-pointer">基礎知識</div><button class="self-center flex-none rounded-md group hover:bg-slate-300/30 focus:outline outline-blue-200 outline-2" aria-label="Open Folder" type="button" aria-controls="radix-:Rupsp:" aria-expanded="false" data-state="closed"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" data-slot="icon" class="transition-transform duration-300 group-data-[state=open]:rotate-90 text-text-slate-700 dark:text-slate-100" height="1.5rem" width="1.5rem"><path fill-rule="evenodd" d="M16.28 11.47a.75.75 0 0 1 0 1.06l-7.5 7.5a.75.75 0 0 1-1.06-1.06L14.69 12 7.72 5.03a.75.75 0 0 1 1.06-1.06l7.5 7.5Z" clip-rule="evenodd"></path></svg></button></div><div data-state="closed" id="radix-:Rupsp:" hidden="" class="pl-3 pr-[2px] collapsible-content"></div></div><div data-state="closed" class="w-full"><div class="myst-toc-item flex flex-row w-full gap-2 px-2 my-1 text-left rounded-lg outline-none hover:bg-slate-300/30"><div title="ニューラルネットワーク" class="block break-words rounded py-2 grow cursor-pointer">ニューラルネットワーク</div><button class="self-center flex-none rounded-md group hover:bg-slate-300/30 focus:outline outline-blue-200 outline-2" aria-label="Open Folder" type="button" aria-controls="radix-:R16psp:" aria-expanded="false" data-state="closed"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" data-slot="icon" class="transition-transform duration-300 group-data-[state=open]:rotate-90 text-text-slate-700 dark:text-slate-100" height="1.5rem" width="1.5rem"><path fill-rule="evenodd" d="M16.28 11.47a.75.75 0 0 1 0 1.06l-7.5 7.5a.75.75 0 0 1-1.06-1.06L14.69 12 7.72 5.03a.75.75 0 0 1 1.06-1.06l7.5 7.5Z" clip-rule="evenodd"></path></svg></button></div><div data-state="closed" id="radix-:R16psp:" hidden="" class="pl-3 pr-[2px] collapsible-content"></div></div><div data-state="closed" class="w-full"><div class="myst-toc-item flex flex-row w-full gap-2 px-2 my-1 text-left rounded-lg outline-none hover:bg-slate-300/30"><div title="PyTorch" class="block break-words rounded py-2 grow cursor-pointer">PyTorch</div><button class="self-center flex-none rounded-md group hover:bg-slate-300/30 focus:outline outline-blue-200 outline-2" aria-label="Open Folder" type="button" aria-controls="radix-:R1epsp:" aria-expanded="false" data-state="closed"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" data-slot="icon" class="transition-transform duration-300 group-data-[state=open]:rotate-90 text-text-slate-700 dark:text-slate-100" height="1.5rem" width="1.5rem"><path fill-rule="evenodd" d="M16.28 11.47a.75.75 0 0 1 0 1.06l-7.5 7.5a.75.75 0 0 1-1.06-1.06L14.69 12 7.72 5.03a.75.75 0 0 1 1.06-1.06l7.5 7.5Z" clip-rule="evenodd"></path></svg></button></div><div data-state="closed" id="radix-:R1epsp:" hidden="" class="pl-3 pr-[2px] collapsible-content"></div></div><div data-state="closed" class="w-full"><div class="myst-toc-item flex flex-row w-full gap-2 px-2 my-1 text-left rounded-lg outline-none hover:bg-slate-300/30"><div title="単語分散表現" class="block break-words rounded py-2 grow cursor-pointer">単語分散表現</div><button class="self-center flex-none rounded-md group hover:bg-slate-300/30 focus:outline outline-blue-200 outline-2" aria-label="Open Folder" type="button" aria-controls="radix-:R1mpsp:" aria-expanded="false" data-state="closed"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" data-slot="icon" class="transition-transform duration-300 group-data-[state=open]:rotate-90 text-text-slate-700 dark:text-slate-100" height="1.5rem" width="1.5rem"><path fill-rule="evenodd" d="M16.28 11.47a.75.75 0 0 1 0 1.06l-7.5 7.5a.75.75 0 0 1-1.06-1.06L14.69 12 7.72 5.03a.75.75 0 0 1 1.06-1.06l7.5 7.5Z" clip-rule="evenodd"></path></svg></button></div><div data-state="closed" id="radix-:R1mpsp:" hidden="" class="pl-3 pr-[2px] collapsible-content"></div></div><div data-state="closed" class="w-full"><div class="myst-toc-item flex flex-row w-full gap-2 px-2 my-1 text-left rounded-lg outline-none hover:bg-slate-300/30"><div title="RNN" class="block break-words rounded py-2 grow cursor-pointer">RNN</div><button class="self-center flex-none rounded-md group hover:bg-slate-300/30 focus:outline outline-blue-200 outline-2" aria-label="Open Folder" type="button" aria-controls="radix-:R1upsp:" aria-expanded="false" data-state="closed"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" data-slot="icon" class="transition-transform duration-300 group-data-[state=open]:rotate-90 text-text-slate-700 dark:text-slate-100" height="1.5rem" width="1.5rem"><path fill-rule="evenodd" d="M16.28 11.47a.75.75 0 0 1 0 1.06l-7.5 7.5a.75.75 0 0 1-1.06-1.06L14.69 12 7.72 5.03a.75.75 0 0 1 1.06-1.06l7.5 7.5Z" clip-rule="evenodd"></path></svg></button></div><div data-state="closed" id="radix-:R1upsp:" hidden="" class="pl-3 pr-[2px] collapsible-content"></div></div><div data-state="open" class="w-full"><div class="myst-toc-item flex flex-row w-full gap-2 px-2 my-1 text-left rounded-lg outline-none hover:bg-slate-300/30"><div title="Transformer" class="block break-words rounded py-2 grow font-semibold text-blue-800 dark:text-blue-200 cursor-pointer">Transformer</div><button class="self-center flex-none rounded-md group hover:bg-slate-300/30 focus:outline outline-blue-200 outline-2" aria-label="Open Folder" type="button" aria-controls="radix-:R26psp:" aria-expanded="true" data-state="open"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" data-slot="icon" class="transition-transform duration-300 group-data-[state=open]:rotate-90 text-text-slate-700 dark:text-slate-100" height="1.5rem" width="1.5rem"><path fill-rule="evenodd" d="M16.28 11.47a.75.75 0 0 1 0 1.06l-7.5 7.5a.75.75 0 0 1-1.06-1.06L14.69 12 7.72 5.03a.75.75 0 0 1 1.06-1.06l7.5 7.5Z" clip-rule="evenodd"></path></svg></button></div><div data-state="open" id="radix-:R26psp:" class="pl-3 pr-[2px] collapsible-content"><a title="Attention" class="block break-words focus:outline outline-blue-200 outline-2 rounded myst-toc-item p-2 my-1 rounded-lg hover:bg-slate-300/30" href="/attention">Attention</a><a title="Self-Attention" class="block break-words focus:outline outline-blue-200 outline-2 rounded myst-toc-item p-2 my-1 rounded-lg hover:bg-slate-300/30" href="/self-attention">Self-Attention</a><a title="Transformerアーキテクチャ" class="block break-words focus:outline outline-blue-200 outline-2 rounded myst-toc-item p-2 my-1 rounded-lg hover:bg-slate-300/30" href="/transformer">Transformerアーキテクチャ</a><a title="BERT" aria-current="page" class="block break-words focus:outline outline-blue-200 outline-2 rounded myst-toc-item p-2 my-1 rounded-lg myst-toc-item-exact bg-blue-300/30 active" href="/bert">BERT</a><a title="BERTによるセンチメント分析" class="block break-words focus:outline outline-blue-200 outline-2 rounded myst-toc-item p-2 my-1 rounded-lg hover:bg-slate-300/30" href="/bert-sentiment">BERTによるセンチメント分析</a><a title="BERTopic" class="block break-words focus:outline outline-blue-200 outline-2 rounded myst-toc-item p-2 my-1 rounded-lg hover:bg-slate-300/30" href="/bert-topic">BERTopic</a></div></div><div data-state="closed" class="w-full"><div class="myst-toc-item flex flex-row w-full gap-2 px-2 my-1 text-left rounded-lg outline-none hover:bg-slate-300/30"><div title="大規模言語モデル" class="block break-words rounded py-2 grow cursor-pointer">大規模言語モデル</div><button class="self-center flex-none rounded-md group hover:bg-slate-300/30 focus:outline outline-blue-200 outline-2" aria-label="Open Folder" type="button" aria-controls="radix-:R2epsp:" aria-expanded="false" data-state="closed"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" data-slot="icon" class="transition-transform duration-300 group-data-[state=open]:rotate-90 text-text-slate-700 dark:text-slate-100" height="1.5rem" width="1.5rem"><path fill-rule="evenodd" d="M16.28 11.47a.75.75 0 0 1 0 1.06l-7.5 7.5a.75.75 0 0 1-1.06-1.06L14.69 12 7.72 5.03a.75.75 0 0 1 1.06-1.06l7.5 7.5Z" clip-rule="evenodd"></path></svg></button></div><div data-state="closed" id="radix-:R2epsp:" hidden="" class="pl-3 pr-[2px] collapsible-content"></div></div></div></nav></div><div class="myst-primary-sidebar-footer flex-none py-6 transition-all duration-700 translate-y-6 opacity-0"><a class="myst-made-with-myst flex mx-auto text-gray-700 w-fit hover:text-blue-700 dark:text-gray-200 dark:hover:text-blue-400" href="https://mystmd.org/made-with-myst" target="_blank" rel="noreferrer"><svg style="width:24px;height:24px" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 100 100" stroke="none"><g id="icon"><path fill="currentColor" d="M23.8,54.8v-3.6l4.7-0.8V17.5l-4.7-0.8V13H36l13.4,31.7h0.2l13-31.7h12.6v3.6l-4.7,0.8v32.9l4.7,0.8v3.6h-15
          v-3.6l4.9-0.8V20.8H65L51.4,53.3h-3.8l-14-32.5h-0.1l0.2,17.4v12.1l5,0.8v3.6H23.8z"></path><path fill="#F37726" d="M47,86.9c0-5.9-3.4-8.8-10.1-8.8h-8.4c-5.2,0-9.4-1.3-12.5-3.8c-3.1-2.5-5.4-6.2-6.8-11l4.8-1.6
          c1.8,5.6,6.4,8.6,13.8,8.8h9.2c6.4,0,10.8,2.5,13.1,7.5c2.3-5,6.7-7.5,13.1-7.5h8.4c7.8,0,12.7-2.9,14.6-8.7l4.8,1.6
          c-1.4,4.9-3.6,8.6-6.8,11.1c-3.1,2.5-7.3,3.7-12.4,3.8H63c-6.7,0-10,2.9-10,8.8"></path></g></svg><span class="self-center ml-2 text-sm">Made with MyST</span></a></div></div></div><main class="article-grid grid-gap"><article class="article-grid subgrid-gap col-screen article content"><div class="hidden"></div><div id="skip-to-frontmatter" aria-label="article frontmatter" class="myst-fm-block mb-8 pt-9"><div class="myst-fm-block-header flex items-center mb-5 h-6 text-sm font-light"><div class="flex-grow"></div><div class="myst-fm-block-badges"><a href="https://github.com/lvzeyu/css_nlp" title="GitHub Repository: lvzeyu/css_nlp" target="_blank" rel="noopener noreferrer" class="myst-fm-github-link text-inherit hover:text-inherit"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" width="1.25rem" height="1.25rem" class="myst-fm-github-icon inline-block mr-1 opacity-60 hover:opacity-100"><path d="M12 2.5c-5.4 0-9.8 4.4-9.8 9.7 0 4.3 2.8 8 6.7 9.2.5.1.7-.2.7-.5v-1.8c-2.4.5-3.1-.6-3.3-1.1-.1-.3-.6-1.1-1-1.4-.3-.2-.8-.6 0-.6s1.3.7 1.5 1c.9 1.5 2.3 1.1 2.8.8.1-.6.3-1.1.6-1.3-2.2-.2-4.4-1.1-4.4-4.8 0-1.1.4-1.9 1-2.6-.1-.2-.4-1.2.1-2.6 0 0 .8-.3 2.7 1 .8-.2 1.6-.3 2.4-.3.8 0 1.7.1 2.4.3 1.9-1.3 2.7-1 2.7-1 .5 1.3.2 2.3.1 2.6.6.7 1 1.5 1 2.6 0 3.7-2.3 4.6-4.4 4.8.4.3.7.9.7 1.8V21c0 .3.2.6.7.5 3.9-1.3 6.6-4.9 6.6-9.2 0-5.4-4.4-9.8-9.8-9.8z"></path></svg></a></div><a href="https://github.com/lvzeyu/css_nlp/edit/master/notebook/BERT.ipynb" title="Edit This Page" target="_blank" rel="noopener noreferrer" class="myst-fm-edit-link text-inherit hover:text-inherit"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="1.25rem" height="1.25rem" class="myst-fm-edit-icon inline-block mr-1 opacity-60 hover:opacity-100"><path stroke-linecap="round" stroke-linejoin="round" d="m16.862 4.487 1.687-1.688a1.875 1.875 0 1 1 2.652 2.652L10.582 16.07a4.5 4.5 0 0 1-1.897 1.13L6 18l.8-2.685a4.5 4.5 0 0 1 1.13-1.897l8.932-8.931Zm0 0L19.5 7.125M18 14v4.75A2.25 2.25 0 0 1 15.75 21H5.25A2.25 2.25 0 0 1 3 18.75V8.25A2.25 2.25 0 0 1 5.25 6H10"></path></svg></a><div class="myst-fm-downloads-dropdown relative flex inline-block mx-1 grow-0" data-headlessui-state=""><button class="myst-fm-downloads-button relative ml-2 -mr-1" id="headlessui-menu-button-:Rs8ucp:" type="button" aria-haspopup="menu" aria-expanded="false" data-headlessui-state=""><span class="sr-only">Downloads</span><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="1.25rem" height="1.25rem" class="myst-fm-downloads-icon"><title>Download</title><path stroke-linecap="round" stroke-linejoin="round" d="M3 16.5v2.25A2.25 2.25 0 0 0 5.25 21h13.5A2.25 2.25 0 0 0 21 18.75V16.5M16.5 12 12 16.5m0 0L7.5 12m4.5 4.5V3"></path></svg></button></div></div><h1 class="myst-fm-block-title mb-0">BERT</h1></div><div class="block my-10 lg:sticky lg:z-10 lg:h-0 lg:pt-0 lg:my-0 lg:ml-10 lg:col-margin-right" style="top:60px"><nav></nav></div><div id="skip-to-article"></div><div id="RaBjv67VHB" class="myst-jp-nb-block relative group/block"><p><a target="_blank" rel="noreferrer" href="https://colab.research.google.com/github/lvzeyu/css_nlp/blob/master/notebook/BERT.ipynb" class=""><img id="YGTQuCUUOW" style="margin-left:auto;margin-right:auto" src="/build/7e2db436150c38a00650f96925aa5581.svg" alt="Open In Colab" data-canonical-url="https://colab.research.google.com/assets/colab-badge.svg" class=""/></a></p><p>2018年Googleが発表したBERT（Bidirectional Encoder Representations from Transformers）は、エンコーダ構成のTransformersを採用し、先行するトークン列と後続するトークン列の双方向性から文脈を捉えます。</p><p>BERTは、Wikipediaと7,000冊の書籍を合わせた大規模なコーパスを使って事前学習されたモデルを、下流タスクのデータセットでファインチューングすることで、様々なタスクの性能を大幅に改善できることが示されました。</p><ul><li><p>事前学習: 大規模なコーパスを用いて、特定なタスクを学習することで、広範な言語データからパターンを学習し、汎用的な言語理解の能力を身につける。</p></li><li><p>ファインチューング：、特定のタスクや領域に特化した小さなデータセットを用いて、事前学習したモデルを微調整します。この微調整により、モデルは特定のタスクや領域に適応し、高い精度を達成することが可能です。</p></li></ul><img id="QK4WzDdKST" style="margin-left:auto;margin-right:auto" src="/build/transfer_learning-fab84a1b72c9948cd22f51ce047d6966.png" data-canonical-url="./Figure/transfer_learning.png" class=""/></div><div id="Z8lfQKYQ8D" class="myst-jp-nb-block relative group/block"><h2 id="contextualized-embedding" class="relative group"><span class="mr-3 select-none">1</span><span class="heading-text">Contextualized Embedding</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#contextualized-embedding" title="Link to this Section" aria-label="Link to this Section">¶</a></h2></div><div id="JmSMabGSnX" class="myst-jp-nb-block relative group/block"><img id="lnZGzZsC6B" style="margin-left:auto;margin-right:auto" src="/build/bert_embedding-0526bca2865a2e6067113d4812fad34e.png" data-canonical-url="./Figure/bert_embedding.png" class=""/></div><div id="uRlA0hb4M2" class="myst-jp-nb-block relative group/block"><h2 id="bert" class="relative group"><span class="mr-3 select-none">2</span><span class="heading-text">BERTの事前学習</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#bert" title="Link to this Section" aria-label="Link to this Section">¶</a></h2><h3 id="bert-1" class="relative group"><span class="mr-3 select-none">2.1</span><span class="heading-text">BERTの構造</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#bert-1" title="Link to this Section" aria-label="Link to this Section">¶</a></h3><img id="zuTsXlTAZv" style="margin-left:auto;margin-right:auto" src="/build/bert-base-bert-large-fed58a17a2474952bb33515d15373330.png" data-canonical-url="./Figure/bert-base-bert-large-encoders.png" class=""/><p>BERTは、Transformerのエンコーダ部分をベースにしたモデルになります。</p><p>複数のエンコーダ層を積み重ねることで、より複雑で長い文脈を処理できるようになり、幅広い自然言語処理タスクで高い性能を発揮しています。</p><img id="y6izDxqFpT" style="margin-left:auto;margin-right:auto" src="/build/bert-contexualized-e-75a833157513561a5b6e30d86f7758db.png" data-canonical-url="./Figure/bert-contexualized-embeddings.png" class=""/></div><div id="rHfFzszMBk" class="myst-jp-nb-block relative group/block"><h3 id="id" class="relative group"><span class="mr-3 select-none">2.2</span><span class="heading-text">入力表現</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#id" title="Link to this Section" aria-label="Link to this Section">¶</a></h3><img id="hnMVzDmmz9" style="margin-left:auto;margin-right:auto" src="/build/bert_input-733c4b03d8c0337848f70f7bacfa588e.png" data-canonical-url="./Figure/bert_input.png" class=""/><p>BERTで入力を作成する際、入力の開始を表す<code>[CLS]</code>トークンと、入力の区切りを表す<code>[SEP]</code>トークンという二つの特殊トークンが使われます。</p><p>またよく使われる特殊トークンとして、マスクタスクための<code>[MASK]</code>トークン、vocabularyに含まれていないことを示す<code>[UNK]</code>トークンがあります。</p><p>入力トークン埋め込みと位置埋め込み以外、それぞれのテキストの範囲を区別しやくするためにsegment embeddingという埋め込みが導入されています。</p><p>まとめると、BERTの入力埋め込み<span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">x_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span>は、トークン埋め込み、位置埋め込み、segment埋め込みより加算されます。</p></div><div id="XgoGmV3jyR" class="myst-jp-nb-block relative group/block"><h3 id="bert-2" class="relative group"><span class="mr-3 select-none">2.3</span><span class="heading-text">BERTの出力</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#bert-2" title="Link to this Section" aria-label="Link to this Section">¶</a></h3><img id="bsat1o6yU5" style="margin-left:auto;margin-right:auto" src="/build/bert-output-vector-4f36ad946b4a43590a83e7bc07c7123c.png" data-canonical-url="./Figure/bert-output-vector.png" class=""/><p>BERTの出力は、入力文の各トークンに対応する文脈化された埋め込みベクトル（contextual embeddings）です。これらの埋め込みは、モデルが入力文全体の文脈を考慮しながら、各トークンの意味を表現したものです。</p></div><div id="kQMFCbN5qB" class="myst-jp-nb-block relative group/block"><aside class="myst-admonition myst-admonition-note my-5 shadow-md dark:shadow-2xl dark:shadow-neutral-900 bg-gray-50/10 dark:bg-stone-800 overflow-hidden myst-admonition-default rounded border-l-4 border-blue-500"><div class="myst-admonition-header m-0 font-medium py-1 flex min-w-0 text-lg text-blue-600 bg-blue-50 dark:bg-slate-900"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="2rem" height="2rem" class="myst-admonition-header-icon inline-block pl-2 mr-2 self-center flex-none text-blue-600"><path stroke-linecap="round" stroke-linejoin="round" d="m11.25 11.25.041-.02a.75.75 0 0 1 1.063.852l-.708 2.836a.75.75 0 0 0 1.063.853l.041-.021M21 12a9 9 0 1 1-18 0 9 9 0 0 1 18 0Zm-9-3.75h.008v.008H12V8.25Z"></path></svg><div class="myst-admonition-header-text text-neutral-900 dark:text-white grow self-center overflow-hidden break-words">Note</div></div><div class="myst-admonition-body px-4 py-1"><p>BERTの出力の中で、入力文の最初に挿入される特殊トークン <code>[CLS]</code> に対応する埋め込みは、文全体の情報を要約するように設計されています。</p></div></aside></div><div id="fYeusgCFAT" class="myst-jp-nb-block relative group/block"><h3 id="id-1" class="relative group"><span class="mr-3 select-none">2.4</span><span class="heading-text">マスク言語モデリング</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#id-1" title="Link to this Section" aria-label="Link to this Section">¶</a></h3><p>マスク言語モデリングは、トークンの穴埋め問題を解けるタスクです。具体的には、ランダムにトークン列中のトークンを隠して、その周辺の単語からマスクされた単語を予測することが求められます。</p><p>ここで、先行するトークンと後続トークンの双方の情報が使われていますので、全体の文脈を捉える学習を実現しています。</p><img id="qnsfImXGkY" style="margin-left:auto;margin-right:auto" src="/build/BERT-language-modeli-0d1934130a1e36c3307c7ef266c7b29c.png" data-canonical-url="./Figure/BERT-language-modeling-masked-lm.png" class=""/></div><div id="OEtHV6WhaD" class="myst-jp-nb-block relative group/block"><h3 id="id-2" class="relative group"><span class="mr-3 select-none">2.5</span><span class="heading-text">次文予測</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#id-2" title="Link to this Section" aria-label="Link to this Section">¶</a></h3><p>次文予測タスクでは、2つの文が与えられ、一方が他方の直後に来るかどうかを判定することが求められます。</p><img id="NBqi2wVkPe" style="margin-left:auto;margin-right:auto" src="/build/bert-next-sentence-p-32a370f50b97ee2353f708c030f0d6ec.png" data-canonical-url="./Figure/bert-next-sentence-prediction.png" class=""/></div><div id="O5yO7MqWnj" class="myst-jp-nb-block relative group/block"><h2 id="id-fine-tuning" class="relative group"><span class="mr-3 select-none">3</span><span class="heading-text">ファインチューング(Fine-Tuning)</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#id-fine-tuning" title="Link to this Section" aria-label="Link to this Section">¶</a></h2><p>様々なタスクにおいて、事前学習モデルをもとにしてそれぞれのタスクに特化したモデルを作るためのステップはファインチューングです。</p><p>事前学習タスクから下流タスクに切り替える時、モデルの最後のレイヤーをタスクに適したものに置き換える必要があります。この最後の層はヘッドと呼ばれ、タスクに固有な部分です。</p><p>残りの部分はボディと呼ばれ、タスクに依存しない事前学習された部分であり(トークン埋め込み層やTransformer層が含まれ)、一般的な言語の理解を行うための基本的な情報を含んでいます。</p><p>例えば、テキスト分類の場合、追加層としては、BERTの最後の隠れ層からの出力に冒頭のSpecial token<code>[CLS]</code>を全結合層（Dense Layer）に経由し、各カテゴリーに属する確率を出力します。それは、[CLS]トークンは入力テキスト全体の文脈情報を集約すると考えるためです。</p><aside class="myst-admonition myst-admonition-note my-5 shadow-md dark:shadow-2xl dark:shadow-neutral-900 bg-gray-50/10 dark:bg-stone-800 overflow-hidden myst-admonition-default rounded border-l-4 border-blue-500"><div class="myst-admonition-header m-0 font-medium py-1 flex min-w-0 text-lg text-blue-600 bg-blue-50 dark:bg-slate-900"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="2rem" height="2rem" class="myst-admonition-header-icon inline-block pl-2 mr-2 self-center flex-none text-blue-600"><path stroke-linecap="round" stroke-linejoin="round" d="m11.25 11.25.041-.02a.75.75 0 0 1 1.063.852l-.708 2.836a.75.75 0 0 0 1.063.853l.041-.021M21 12a9 9 0 1 1-18 0 9 9 0 0 1 18 0Zm-9-3.75h.008v.008H12V8.25Z"></path></svg><div class="myst-admonition-header-text text-neutral-900 dark:text-white grow self-center overflow-hidden break-words">Note</div></div><div class="myst-admonition-body px-4 py-1"><p>テキスト分類タスクにおいて、[CLS]トークンと全結合層（Dense Layer）を使用するのが一般的ですが、他の追加層の設計を用いることももちろん可能です。</p></div></aside><img id="EUoIHBKmLq" style="margin-left:auto;margin-right:auto" src="/build/bert_based_model-28e7006a9de208e9f074c5e5df95b023.png" data-canonical-url="./Figure/bert_based_model.png" class=""/></div><div id="QjKUltRGiC" class="myst-jp-nb-block relative group/block"><h3 id="huggingface-transformer" class="relative group"><span class="mr-3 select-none">3.1</span><span class="heading-text">Huggingface transformerを使う</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#huggingface-transformer" title="Link to this Section" aria-label="Link to this Section">¶</a></h3><p>転移学習は事前学習済みモデルを新しいタスクに再利用するといった強みがあります。そのため、事前学習済みのモデルを素早く共有、ロードすることは重要です。</p><p><a target="_blank" rel="noreferrer" href="https://huggingface.co/" class="">Hugging Face Hub</a>は、モデル、データセットとデモを備えたプラットフォームです。</p><img id="OHM48KNmHN" style="margin-left:auto;margin-right:auto" src="/build/hf-ecosystem-6680c5be90b07a733baabb4de6aaa31e.png" data-canonical-url="./Figure/hf-ecosystem.png" class=""/><p><a target="_blank" rel="noreferrer" href="https://huggingface.co/docs/transformers/index" class="">Huggingface transformer</a>は、自然言語処理を中心に最先端のTransformerベースのモデルを効率に利用するためのオープンソースライブラリです。</p><ul><li><p>多数の事前学習済みモデル: ライブラリは、BERT、GPT-2、RoBERTa、T5、DistilBERTなど、さまざまな有名なNLPモデルの事前学習済みバージョンを提供しています。</p></li><li><p>モデルの利用の簡易化: 事前学習済みのモデルを簡単にダウンロードし、特定のタスクにファインチューニングするための高レベルのAPIを提供しています。</p></li><li><p>Tokenizers: ほとんどのモデルには、テキストデータをモデルが扱える形式に変換するためのトークナイザが付属しています。これはテキストの前処理を簡単に行うためのツールです。</p></li><li><p>Model Hubの統合: Hugging FaceのModel Hubと直接統合されており、コミュニティによって共有されている数千もの事前学習済みモデルに簡単にアクセスできます。</p></li></ul><h4 id="pipline" class="relative group"><span class="mr-3 select-none">3.1.1</span><span class="heading-text">pipline</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#pipline" title="Link to this Section" aria-label="Link to this Section">¶</a></h4><p><a target="_blank" rel="noreferrer" href="https://huggingface.co/docs/transformers/main_classes/pipelines" class=""><code>pipeline</code></a>というクラスで、特定のタスクを実行するために事前学習されたモデルとトークンナイザーを統合し、簡単に使用することができます。</p></div><div id="YryQvJUN0I" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">#!pip install fugashi
#!pip install unidic-lite</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="fWr-Aif6bfypx4t3-e_Gv" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left"></div></div><div id="zBCwWqSxfC" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">import pandas as pd
from transformers import pipeline

fill_mask = pipeline(
    &quot;fill-mask&quot;,
    model=&quot;cl-tohoku/bert-base-japanese-v3&quot;
)</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="lh1E9gN7A12-5ytJ8hzCF" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left"></div></div><div id="p0HQaO3JZh" class="myst-jp-nb-block relative group/block"></div><div id="NY1TmRdu60" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">masked_text = &quot;東北大学は[MASK]市に位置しています。&quot;
output = fill_mask(masked_text)</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="trfRCXUrRNYzY8gsD_DIx" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left"></div></div><div id="S8bLcW1o0Q" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">display(pd.DataFrame(output))</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="94tXpr_os2-d0a0uJxing" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div><div class="p-2.5">Loading...</div></div></div></div><div id="uNArEhDC6G" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">summarizer = pipeline(&quot;summarization&quot;, model=&quot;facebook/bart-large-cnn&quot;)</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="IYvjefO1SKvCofQuNvtcM" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left"></div></div><div id="poZTxpPVm4" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">ARTICLE = &quot;&quot;&quot; New York (CNN)When Liana Barrientos was 23 years old, she got married in Westchester County, New York.
A year later, she got married again in Westchester County, but to a different man and without divorcing her first husband.
Only 18 days after that marriage, she got hitched yet again. Then, Barrientos declared &quot;I do&quot; five more times, sometimes only within two weeks of each other.
In 2010, she married once more, this time in the Bronx. In an application for a marriage license, she stated it was her &quot;first and only&quot; marriage.
Barrientos, now 39, is facing two criminal counts of &quot;offering a false instrument for filing in the first degree,&quot; referring to her false statements on the
2010 marriage license application, according to court documents.
Prosecutors said the marriages were part of an immigration scam.
On Friday, she pleaded not guilty at State Supreme Court in the Bronx, according to her attorney, Christopher Wright, who declined to comment further.
After leaving court, Barrientos was arrested and charged with theft of service and criminal trespass for allegedly sneaking into the New York subway through an emergency exit, said Detective
Annette Markowski, a police spokeswoman. In total, Barrientos has been married 10 times, with nine of her marriages occurring between 1999 and 2002.
All occurred either in Westchester County, Long Island, New Jersey or the Bronx. She is believed to still be married to four men, and at one time, she was married to eight men at once, prosecutors say.
Prosecutors said the immigration scam involved some of her husbands, who filed for permanent residence status shortly after the marriages.
Any divorces happened only after such filings were approved. It was unclear whether any of the men will be prosecuted.
The case was referred to the Bronx District Attorney\&#x27;s Office by Immigration and Customs Enforcement and the Department of Homeland Security\&#x27;s
Investigation Division. Seven of the men are from so-called &quot;red-flagged&quot; countries, including Egypt, Turkey, Georgia, Pakistan and Mali.
Her eighth husband, Rashid Rajput, was deported in 2006 to his native Pakistan after an investigation by the Joint Terrorism Task Force.
If convicted, Barrientos faces up to four years in prison.  Her next court appearance is scheduled for May 18.
&quot;&quot;&quot;
print(summarizer(ARTICLE, max_length=130, min_length=30, do_sample=False))</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="5n1-JmuIVW42YuQhJwxWH" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div data-name="safe-output-stream"><div><pre class="myst-jp-stream-output text-sm font-thin font-system"><code><span>[{&#x27;summary_text&#x27;: &#x27;Liana Barrientos, 39, is charged with two counts of &quot;offering a false instrument for filing in the first degree&quot; In total, she has been married 10 times, with nine of her marriages occurring between 1999 and 2002. She is believed to still be married to four men.&#x27;}]
</span></code></pre></div></div></div></div><div id="ZjUyWoFym8" class="myst-jp-nb-block relative group/block"><div class="myst-tab-set my-5"><div class="myst-tab-set-row flex flex-row flex-wrap overflow-x-auto border-b border-b-gray-100"><div class="myst-tab-item-header flex-none px-3 py-1 font-semibold cursor-pointer myst-tab-item-header-active text-blue-600 border-b-2 border-b-blue-600 dark:border-b-white dark:text-white">課題 1</div></div><div class="myst-tab-item-body flex shadow"><div class="myst-tab-item-text w-full px-6"><div class=""><p>Huggingface Hubで日本語のセンチメント分類ためのモデルを探し、piplineでセンチメント分類器を実装しなさい。
以下のテキストに対する分類結果を確認しよう。</p><ul><li><p><code>この製品は全く役に立ちませんでした</code></p></li><li><p><code>今日はいい天気ですね</code></p></li><li><p><code>世界経済も、米国が12月に続き３月にも追加利上げを実施するなど、先進国を中心に回復の動きとなりました</code></p></li></ul></div></div></div></div></div><div id="G8GHiAgzY7" class="myst-jp-nb-block relative group/block"><h2 id="id-word2vec-bert" class="relative group"><span class="mr-3 select-none">4</span><span class="heading-text">まとめ : word2vecからBERTまで</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#id-word2vec-bert" title="Link to this Section" aria-label="Link to this Section">¶</a></h2><p>人間が使う自然言語をコンピュータに処理させるため、言語を数値形式で表現するモデリングのプロセスが必要とされ、どのようなモデルを採用するかによって分析の方向性は異なっています。</p><ul><li><p>最も基本的なモデリングアプローチとして、文書を単語の集合とそれぞれの単語の頻度情報に変換するバグオブワーズ(bag of words)があげられます。この手法は、文書の基本的な内容を捉えるのに有効であるが、<span style="text-decoration:underline">単語の順番や意味のニュアンスなどの情報はすべて捨象されています。</span></p></li><li><p>より複雑な言語の特性を捉えるために、大量のコーパスを使った学習により、言語の文法や意味構造など多くの情報を埋め込んだ高度なモデルが期待されています。word2vecをはじめとする単語分散表現モデルは、単語を「意味」情報を表現したベクトルにマッピングすることができます。</p><ul><li><p>word2vecに単語分散表現の学習では、「単語の意味は、その単語の周囲の単語（文脈）によって決まる」という分布仮説に基づく手法が用いられます。この仮説にしたがうモデルでは、ある単語がどのような文脈で生じやすいかということをある程度考慮し、単語間の関係をベクトルで表現することができます。</p></li><li><p>ただ、word2vecではいくつの欠点があります。特に、word2vecではあくまで<!-- -->1<!-- -->単語<!-- -->1<!-- -->ベクトルでしたが、実際のケースでは、文脈によって単語の意味が変わることがありますので、<span style="text-decoration:underline">文脈に依存する分散表現が望ましいです。</span></p></li></ul></li><li><p>テキストの「文脈」を表現するための言語モデルが開発されました。</p><ul><li><p>RNNとLSTMではテキストデータの時系列的な性質を捉え、<span style="text-decoration:underline">文中の単語の順序や時間的な関連性をモデルが学習できるようになります</span>。ただ、</p><ul><li><p>長距離の依存関係を効果的に対処できでいない</p></li><li><p>計算コストが高い</p></li></ul></li><li><p>Self-Attenttionでは、<span style="text-decoration:underline">すべての単語間の関係を並列に計算することで、長距離の依存関係を効果的に捉えます。</span></p></li></ul></li><li><p>Transformerをベースにしたモデルでは、単語が出現する具体的な文脈に基づいてその単語の埋め込みを生成することができます。</p></li></ul></div><div class="myst-backmatter-parts"></div><div class="myst-footer-links flex pt-10 mb-10 space-x-4"><a class="myst-footer-link flex-1 block p-4 font-normal text-gray-600 no-underline border border-gray-200 rounded shadow-sm group hover:border-blue-600 dark:hover:border-blue-400 hover:text-blue-600 dark:hover:text-blue-400 dark:text-gray-100 dark:border-gray-500 hover:shadow-lg dark:shadow-neutral-700 myst-footer-link-prev" href="/transformer"><div class="flex h-full align-middle"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="1.5rem" height="1.5rem" class="myst-footer-link-icon self-center transition-transform group-hover:-translate-x-1 shrink-0"><path stroke-linecap="round" stroke-linejoin="round" d="M10.5 19.5 3 12m0 0 7.5-7.5M3 12h18"></path></svg><div class="flex-grow text-right"><div class="myst-footer-link-group text-xs text-gray-500 dark:text-gray-400">Transformer</div>Transformerアーキテクチャ</div></div></a><a class="myst-footer-link flex-1 block p-4 font-normal text-gray-600 no-underline border border-gray-200 rounded shadow-sm group hover:border-blue-600 dark:hover:border-blue-400 hover:text-blue-600 dark:hover:text-blue-400 dark:text-gray-100 dark:border-gray-500 hover:shadow-lg dark:shadow-neutral-700 myst-footer-link-next" href="/bert-sentiment"><div class="flex h-full align-middle"><div class="flex-grow"><div class="myst-footer-link-group text-xs text-gray-500 dark:text-gray-400">Transformer</div>BERTによるセンチメント分析</div><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="1.5rem" height="1.5rem" class="myst-footer-link-icon self-center transition-transform group-hover:translate-x-1 shrink-0"><path stroke-linecap="round" stroke-linejoin="round" d="M13.5 4.5 21 12m0 0-7.5 7.5M21 12H3"></path></svg></div></a></div></article></main><script>((a,l)=>{if(!window.history.state||!window.history.state.key){let u=Math.random().toString(32).slice(2);window.history.replaceState({key:u},"")}try{let d=JSON.parse(sessionStorage.getItem(a)||"{}")[l||window.history.state.key];typeof d=="number"&&window.scrollTo(0,d)}catch(u){console.error(u),sessionStorage.removeItem(a)}})("positions", null)</script><link rel="modulepreload" href="/build/entry.client-PCJPW7TK.js"/><link rel="modulepreload" href="/build/_shared/chunk-AQ2CODAG.js"/><link rel="modulepreload" href="/build/_shared/chunk-JJXTQVMA.js"/><link rel="modulepreload" href="/build/_shared/chunk-OZE3FFNP.js"/><link rel="modulepreload" href="/build/_shared/chunk-OYMW4E3D.js"/><link rel="modulepreload" href="/build/_shared/chunk-C4DFGG5C.js"/><link rel="modulepreload" href="/build/_shared/chunk-J7TUH54J.js"/><link rel="modulepreload" href="/build/_shared/chunk-FZ2S7OYD.js"/><link rel="modulepreload" href="/build/_shared/chunk-JEM6JXYA.js"/><link rel="modulepreload" href="/build/_shared/chunk-34XIY2DH.js"/><link rel="modulepreload" href="/build/_shared/chunk-KQM5FBHR.js"/><link rel="modulepreload" href="/build/_shared/chunk-OCWQY3HK.js"/><link rel="modulepreload" href="/build/_shared/chunk-7HNKBP4B.js"/><link rel="modulepreload" href="/build/_shared/chunk-CUKUDK3R.js"/><link rel="modulepreload" href="/build/_shared/chunk-3EBOCCHJ.js"/><link rel="modulepreload" href="/build/_shared/chunk-O4VQNZ62.js"/><link rel="modulepreload" href="/build/_shared/chunk-4OEDG4JQ.js"/><link rel="modulepreload" href="/build/_shared/chunk-GUCIBHGO.js"/><link rel="modulepreload" href="/build/root-CXYA7X5D.js"/><link rel="modulepreload" href="/build/_shared/chunk-DATP5P2X.js"/><link rel="modulepreload" href="/build/routes/$-JRBPULBO.js"/><script>window.__remixContext = {"url":"/bert","state":{"loaderData":{"root":{"config":{"version":3,"myst":"1.7.0","options":{"favicon":"/build/tohoku-university-lo-68dede7d9dcb965aff25e8429e755296.svg","logo":"/build/tohoku-university-lo-68dede7d9dcb965aff25e8429e755296.svg","analytics_google":"G-85RFPTYEE3"},"nav":[],"actions":[],"projects":[{"numbering":{"heading_1":{"template":"enabled","enabled":true},"heading_2":{"template":"enabled","enabled":true},"heading_3":{"template":"enabled","enabled":true},"heading_4":{"template":"enabled","enabled":true},"heading_5":{"template":"enabled","enabled":true},"heading_6":{"template":"enabled","enabled":true}},"title":"計算社会科学と自然言語処理","github":"https://github.com/lvzeyu/css_nlp","id":"356ed62a-cf04-49ae-b7bc-d9f79f2ef8bb","exports":[],"bibliography":[],"index":"intro","pages":[{"title":"イントロダクション","level":1},{"slug":"introduction","title":"ガイダンス","description":"このページで、行動科学演習(LB63310)と計算人文社会学研究演習Ⅱ (LM23309)の授業資料を公開しています。","date":"","thumbnail":"/build/NLP_history-80702ee1d37040856a480174f5bcbbfc.png","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"title":"基礎知識","level":1},{"slug":"nlp-basis2","title":"自然言語処理の基礎","description":"","date":"","thumbnail":"/build/7e2db436150c38a00650f96925aa5581.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"ml-basis2","title":"機械学習の基本概念","description":"","date":"","thumbnail":"/build/supervised-learning-b5be7f5e73313e60f2c06d545c226553.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"math-basis2","title":"数学基礎","description":"","date":"","thumbnail":"/build/7e2db436150c38a00650f96925aa5581.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"title":"ニューラルネットワーク","level":1},{"slug":"nn2","title":"ニューラルネットワーク","description":"","date":"","thumbnail":"/build/7e2db436150c38a00650f96925aa5581.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"backpropagation","title":"誤差逆伝播法","description":"","date":"","thumbnail":"/build/7e2db436150c38a00650f96925aa5581.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"sgd","title":"学習に関するテクニック","description":"","date":"","thumbnail":"/build/7e2db436150c38a00650f96925aa5581.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"title":"PyTorch","level":1},{"slug":"pytorch2","title":"Pytorch","description":"","date":"","thumbnail":"/build/7e2db436150c38a00650f96925aa5581.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"title":"単語分散表現","level":1},{"slug":"word2vec-1","title":"単語分散表現","description":"","date":"","thumbnail":"/build/7e2db436150c38a00650f96925aa5581.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"word2vec-2-embedding","title":"word2vec","description":"","date":"","thumbnail":"/build/7e2db436150c38a00650f96925aa5581.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"word2vec-gensim","title":"GensimによるWord2Vecの学習と使用","description":"","date":"","thumbnail":"/build/7e2db436150c38a00650f96925aa5581.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"word2vec-application","title":"Word2Vecが人文・社会科学研究における応用","description":"","date":"","thumbnail":"/build/7e2db436150c38a00650f96925aa5581.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"title":"RNN","level":1},{"slug":"rnn","title":"RNNの基礎","description":"","date":"","thumbnail":"/build/rnn-4dbf33ec81abf2baa6d5bbe97bb61683.png","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"lstm","title":"LSTM","description":"","date":"","thumbnail":"/build/lstm1-4ea8525c2f72b4f2ff8298a9f7b9c0be.png","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"pytorch-lstm","title":"LSTMの実装","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"lstm-classification","title":"LSTMによる文書分類","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"seq2seq","title":"Seq2seq","description":"","date":"","thumbnail":"/build/7e2db436150c38a00650f96925aa5581.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"title":"Transformer","level":1},{"slug":"attention","title":"Attention","description":"","date":"","thumbnail":"/build/attention-eed7f8fafc28c8cb6bd5276bbd04714b.png","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"self-attention","title":"Self-Attention","description":"","date":"","thumbnail":"/build/7e2db436150c38a00650f96925aa5581.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"transformer","title":"Transformerアーキテクチャ","description":"","date":"","thumbnail":"/build/transformer-493148401fbb971a745b29ab5b44e10f.png","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"bert","title":"BERT","description":"","date":"","thumbnail":"/build/7e2db436150c38a00650f96925aa5581.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"bert-sentiment","title":"BERTによるセンチメント分析","description":"","date":"","thumbnail":"/build/7e2db436150c38a00650f96925aa5581.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"bert-topic","title":"BERTopic","description":"","date":"","thumbnail":"/build/7e2db436150c38a00650f96925aa5581.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"title":"大規模言語モデル","level":1},{"slug":"gpt","title":"GPT","description":"","date":"","thumbnail":"/build/gpt_history-c08ab6ce749d3aed300b023e318cb96c.png","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"llm","title":"大規模言語モデルの基本","description":"","date":"","thumbnail":"/build/LLMs_parameter-ec157a4028abc601fb429210b7f4b07c.png","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"langchain-basic","title":"LLMSの応用(1)：Langchainの基礎","description":"","date":"","thumbnail":"/build/7e2db436150c38a00650f96925aa5581.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2}]}]},"CONTENT_CDN_PORT":"3100","MODE":"static"},"routes/$":{"config":{"version":3,"myst":"1.7.0","options":{"favicon":"/build/tohoku-university-lo-68dede7d9dcb965aff25e8429e755296.svg","logo":"/build/tohoku-university-lo-68dede7d9dcb965aff25e8429e755296.svg","analytics_google":"G-85RFPTYEE3"},"nav":[],"actions":[],"projects":[{"numbering":{"heading_1":{"template":"enabled","enabled":true},"heading_2":{"template":"enabled","enabled":true},"heading_3":{"template":"enabled","enabled":true},"heading_4":{"template":"enabled","enabled":true},"heading_5":{"template":"enabled","enabled":true},"heading_6":{"template":"enabled","enabled":true}},"title":"計算社会科学と自然言語処理","github":"https://github.com/lvzeyu/css_nlp","id":"356ed62a-cf04-49ae-b7bc-d9f79f2ef8bb","exports":[],"bibliography":[],"index":"intro","pages":[{"title":"イントロダクション","level":1},{"slug":"introduction","title":"ガイダンス","description":"このページで、行動科学演習(LB63310)と計算人文社会学研究演習Ⅱ (LM23309)の授業資料を公開しています。","date":"","thumbnail":"/build/NLP_history-80702ee1d37040856a480174f5bcbbfc.png","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"title":"基礎知識","level":1},{"slug":"nlp-basis2","title":"自然言語処理の基礎","description":"","date":"","thumbnail":"/build/7e2db436150c38a00650f96925aa5581.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"ml-basis2","title":"機械学習の基本概念","description":"","date":"","thumbnail":"/build/supervised-learning-b5be7f5e73313e60f2c06d545c226553.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"math-basis2","title":"数学基礎","description":"","date":"","thumbnail":"/build/7e2db436150c38a00650f96925aa5581.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"title":"ニューラルネットワーク","level":1},{"slug":"nn2","title":"ニューラルネットワーク","description":"","date":"","thumbnail":"/build/7e2db436150c38a00650f96925aa5581.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"backpropagation","title":"誤差逆伝播法","description":"","date":"","thumbnail":"/build/7e2db436150c38a00650f96925aa5581.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"sgd","title":"学習に関するテクニック","description":"","date":"","thumbnail":"/build/7e2db436150c38a00650f96925aa5581.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"title":"PyTorch","level":1},{"slug":"pytorch2","title":"Pytorch","description":"","date":"","thumbnail":"/build/7e2db436150c38a00650f96925aa5581.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"title":"単語分散表現","level":1},{"slug":"word2vec-1","title":"単語分散表現","description":"","date":"","thumbnail":"/build/7e2db436150c38a00650f96925aa5581.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"word2vec-2-embedding","title":"word2vec","description":"","date":"","thumbnail":"/build/7e2db436150c38a00650f96925aa5581.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"word2vec-gensim","title":"GensimによるWord2Vecの学習と使用","description":"","date":"","thumbnail":"/build/7e2db436150c38a00650f96925aa5581.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"word2vec-application","title":"Word2Vecが人文・社会科学研究における応用","description":"","date":"","thumbnail":"/build/7e2db436150c38a00650f96925aa5581.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"title":"RNN","level":1},{"slug":"rnn","title":"RNNの基礎","description":"","date":"","thumbnail":"/build/rnn-4dbf33ec81abf2baa6d5bbe97bb61683.png","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"lstm","title":"LSTM","description":"","date":"","thumbnail":"/build/lstm1-4ea8525c2f72b4f2ff8298a9f7b9c0be.png","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"pytorch-lstm","title":"LSTMの実装","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"lstm-classification","title":"LSTMによる文書分類","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"seq2seq","title":"Seq2seq","description":"","date":"","thumbnail":"/build/7e2db436150c38a00650f96925aa5581.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"title":"Transformer","level":1},{"slug":"attention","title":"Attention","description":"","date":"","thumbnail":"/build/attention-eed7f8fafc28c8cb6bd5276bbd04714b.png","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"self-attention","title":"Self-Attention","description":"","date":"","thumbnail":"/build/7e2db436150c38a00650f96925aa5581.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"transformer","title":"Transformerアーキテクチャ","description":"","date":"","thumbnail":"/build/transformer-493148401fbb971a745b29ab5b44e10f.png","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"bert","title":"BERT","description":"","date":"","thumbnail":"/build/7e2db436150c38a00650f96925aa5581.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"bert-sentiment","title":"BERTによるセンチメント分析","description":"","date":"","thumbnail":"/build/7e2db436150c38a00650f96925aa5581.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"bert-topic","title":"BERTopic","description":"","date":"","thumbnail":"/build/7e2db436150c38a00650f96925aa5581.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"title":"大規模言語モデル","level":1},{"slug":"gpt","title":"GPT","description":"","date":"","thumbnail":"/build/gpt_history-c08ab6ce749d3aed300b023e318cb96c.png","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"llm","title":"大規模言語モデルの基本","description":"","date":"","thumbnail":"/build/LLMs_parameter-ec157a4028abc601fb429210b7f4b07c.png","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"langchain-basic","title":"LLMSの応用(1)：Langchainの基礎","description":"","date":"","thumbnail":"/build/7e2db436150c38a00650f96925aa5581.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2}]}]},"page":{"version":3,"kind":"Notebook","sha256":"efec5eb0e3637438a430ccde5018beba55ab5bb58e2f2ab0d2dfc2f5754f1a7d","slug":"bert","location":"/notebook/BERT.ipynb","dependencies":[],"frontmatter":{"title":"BERT","content_includes_title":false,"kernelspec":{"name":"python3","display_name":"jupyterbook","language":"python"},"github":"https://github.com/lvzeyu/css_nlp","numbering":{"heading_1":{"enabled":true,"template":"enabled"},"heading_2":{"enabled":true,"template":"enabled"},"heading_3":{"enabled":true,"template":"enabled"},"heading_4":{"enabled":true,"template":"enabled"},"heading_5":{"enabled":true,"template":"enabled"},"heading_6":{"enabled":true,"template":"enabled"},"title":{"offset":1}},"source_url":"https://github.com/lvzeyu/css_nlp/blob/master/notebook/BERT.ipynb","edit_url":"https://github.com/lvzeyu/css_nlp/edit/master/notebook/BERT.ipynb","thumbnail":"/build/7e2db436150c38a00650f96925aa5581.svg","exports":[{"format":"ipynb","filename":"BERT.ipynb","url":"/build/BERT-7832dbc82174d9ce269840739f8a0705.ipynb"}]},"widgets":{},"mdast":{"type":"root","children":[{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"link","url":"https://colab.research.google.com/github/lvzeyu/css_nlp/blob/master/notebook/BERT.ipynb","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"image","url":"/build/7e2db436150c38a00650f96925aa5581.svg","alt":"Open In Colab","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"YGTQuCUUOW","urlSource":"https://colab.research.google.com/assets/colab-badge.svg"}],"urlSource":"https://colab.research.google.com/github/lvzeyu/css_nlp/blob/master/notebook/BERT.ipynb","key":"GIrL39yt7M"}],"key":"XowxZ5EcBD"},{"type":"paragraph","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"text","value":"2018年Googleが発表したBERT（Bidirectional Encoder Representations from Transformers）は、エンコーダ構成のTransformersを採用し、先行するトークン列と後続するトークン列の双方向性から文脈を捉えます。","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"CXnAVtxCIn"}],"key":"JKTOgE5eUt"},{"type":"paragraph","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"text","value":"BERTは、Wikipediaと7,000冊の書籍を合わせた大規模なコーパスを使って事前学習されたモデルを、下流タスクのデータセットでファインチューングすることで、様々なタスクの性能を大幅に改善できることが示されました。","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"aNAQptRTjV"}],"key":"WzExyfWycb"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":11,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"事前学習: 大規模なコーパスを用いて、特定なタスクを学習することで、広範な言語データからパターンを学習し、汎用的な言語理解の能力を身につける。","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"m9wjbKwIMr"}],"key":"W2C8JH1E8b"}],"key":"xjze9uPHi8"},{"type":"listItem","spread":true,"position":{"start":{"line":12,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"ファインチューング：、特定のタスクや領域に特化した小さなデータセットを用いて、事前学習したモデルを微調整します。この微調整により、モデルは特定のタスクや領域に適応し、高い精度を達成することが可能です。","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"xJMiU5IIAv"}],"key":"zIR6N1yMmP"}],"key":"nK6haGnMSI"}],"key":"F08gqM6Nyp"},{"type":"image","url":"/build/transfer_learning-fab84a1b72c9948cd22f51ce047d6966.png","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"key":"QK4WzDdKST","urlSource":"./Figure/transfer_learning.png"}],"key":"RaBjv67VHB"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Contextualized Embedding","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"bUa9cggCRO"}],"identifier":"contextualized-embedding","label":"Contextualized Embedding","html_id":"contextualized-embedding","implicit":true,"enumerator":"1","key":"JKqXpLYinG"}],"key":"Z8lfQKYQ8D"},{"type":"block","kind":"notebook-content","children":[{"type":"image","url":"/build/bert_embedding-0526bca2865a2e6067113d4812fad34e.png","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"lnZGzZsC6B","urlSource":"./Figure/bert_embedding.png"}],"key":"JmSMabGSnX"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"BERTの事前学習","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ljjVQyOsqe"}],"identifier":"bert","label":"BERTの事前学習","html_id":"bert","implicit":true,"enumerator":"2","key":"TvTkVfWA0e"},{"type":"heading","depth":3,"position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"BERTの構造","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"rupmiLKS5f"}],"identifier":"bert","label":"BERTの構造","html_id":"bert-1","implicit":true,"enumerator":"2.1","key":"bFgRXQNs1Y"},{"type":"image","url":"/build/bert-base-bert-large-fed58a17a2474952bb33515d15373330.png","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"zuTsXlTAZv","urlSource":"./Figure/bert-base-bert-large-encoders.png"},{"type":"paragraph","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"text","value":"BERTは、Transformerのエンコーダ部分をベースにしたモデルになります。","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"yW7On0khhZ"}],"key":"ehP6jUDDWI"},{"type":"paragraph","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"text","value":"複数のエンコーダ層を積み重ねることで、より複雑で長い文脈を処理できるようになり、幅広い自然言語処理タスクで高い性能を発揮しています。","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"ZhidR7yYNj"}],"key":"WLlEkIIu6D"},{"type":"image","url":"/build/bert-contexualized-e-75a833157513561a5b6e30d86f7758db.png","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"y6izDxqFpT","urlSource":"./Figure/bert-contexualized-embeddings.png"}],"key":"uRlA0hb4M2"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"入力表現","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ZmCefvdwz6"}],"identifier":"id","label":"入力表現","html_id":"id","implicit":true,"enumerator":"2.2","key":"CYrIlcMq6D"},{"type":"image","url":"/build/bert_input-733c4b03d8c0337848f70f7bacfa588e.png","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"hnMVzDmmz9","urlSource":"./Figure/bert_input.png"},{"type":"paragraph","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"BERTで入力を作成する際、入力の開始を表す","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"z4BnYTi3Bq"},{"type":"inlineCode","value":"[CLS]","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"Wy9JZVQOS6"},{"type":"text","value":"トークンと、入力の区切りを表す","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"sDFTtRqMbr"},{"type":"inlineCode","value":"[SEP]","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"wo1uxba9tD"},{"type":"text","value":"トークンという二つの特殊トークンが使われます。","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"suZFp16SPe"}],"key":"W1jCtW0aq4"},{"type":"paragraph","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"text","value":"またよく使われる特殊トークンとして、マスクタスクための","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"KJfrKNluHJ"},{"type":"inlineCode","value":"[MASK]","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"vincCup3zF"},{"type":"text","value":"トークン、vocabularyに含まれていないことを示す","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"g2D84cUiiQ"},{"type":"inlineCode","value":"[UNK]","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"r9itbGeqer"},{"type":"text","value":"トークンがあります。","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"WBTVCnYSnz"}],"key":"x4LWbs8UEV"},{"type":"paragraph","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"text","value":"入力トークン埋め込みと位置埋め込み以外、それぞれのテキストの範囲を区別しやくするためにsegment embeddingという埋め込みが導入されています。","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"pZoZpNVIjN"}],"key":"orO4SjBXAF"},{"type":"paragraph","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"text","value":"まとめると、BERTの入力埋め込み","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"AhJoscssQv"},{"type":"inlineMath","value":"x_i","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"html":"\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmsub\u003e\u003cmi\u003ex\u003c/mi\u003e\u003cmi\u003ei\u003c/mi\u003e\u003c/msub\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003ex_i\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.5806em;vertical-align:-0.15em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e\u003cspan class=\"mord mathnormal\"\u003ex\u003c/span\u003e\u003cspan class=\"msupsub\"\u003e\u003cspan class=\"vlist-t vlist-t2\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.3117em;\"\u003e\u003cspan style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:2.7em;\"\u003e\u003c/span\u003e\u003cspan class=\"sizing reset-size6 size3 mtight\"\u003e\u003cspan class=\"mord mathnormal mtight\"\u003ei\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-s\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.15em;\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e","key":"kMfvCNaz5k"},{"type":"text","value":"は、トークン埋め込み、位置埋め込み、segment埋め込みより加算されます。","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"jIX8Z000MG"}],"key":"E8kycJsykl"}],"key":"rHfFzszMBk"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"BERTの出力","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"LVLyhtvWjK"}],"identifier":"bert","label":"BERTの出力","html_id":"bert-2","implicit":true,"enumerator":"2.3","key":"TTRbxHKuVg"},{"type":"image","url":"/build/bert-output-vector-4f36ad946b4a43590a83e7bc07c7123c.png","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"bsat1o6yU5","urlSource":"./Figure/bert-output-vector.png"},{"type":"paragraph","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"BERTの出力は、入力文の各トークンに対応する文脈化された埋め込みベクトル（contextual embeddings）です。これらの埋め込みは、モデルが入力文全体の文脈を考慮しながら、各トークンの意味を表現したものです。","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"oPIiNZKbyt"}],"key":"TMLKeUWhz8"}],"key":"XgoGmV3jyR"},{"type":"block","kind":"notebook-content","children":[{"type":"admonition","kind":"note","children":[{"type":"admonitionTitle","children":[{"type":"text","value":"Note","key":"ZnCAbbKWKK"}],"key":"hzudxYsiOm"},{"type":"paragraph","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"text","value":"BERTの出力の中で、入力文の最初に挿入される特殊トークン ","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"LoChl72Lo8"},{"type":"inlineCode","value":"[CLS]","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"srzxCJYZh6"},{"type":"text","value":" に対応する埋め込みは、文全体の情報を要約するように設計されています。","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"gSCJ6kbNzm"}],"key":"xJLJtmrDOX"}],"key":"aqDXDNGLHI"}],"key":"kQMFCbN5qB"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"マスク言語モデリング","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"tjpQjqGonr"}],"identifier":"id","label":"マスク言語モデリング","html_id":"id-1","implicit":true,"enumerator":"2.4","key":"sAF9QKoebH"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"マスク言語モデリングは、トークンの穴埋め問題を解けるタスクです。具体的には、ランダムにトークン列中のトークンを隠して、その周辺の単語からマスクされた単語を予測することが求められます。","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"GxCw8Gr6SF"}],"key":"RcxrfEQkBa"},{"type":"paragraph","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"ここで、先行するトークンと後続トークンの双方の情報が使われていますので、全体の文脈を捉える学習を実現しています。","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"paGetwnvyz"}],"key":"XIyRP7qKZ3"},{"type":"image","url":"/build/BERT-language-modeli-0d1934130a1e36c3307c7ef266c7b29c.png","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"qnsfImXGkY","urlSource":"./Figure/BERT-language-modeling-masked-lm.png"}],"key":"fYeusgCFAT"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"次文予測","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"o8mOzIbkP6"}],"identifier":"id","label":"次文予測","html_id":"id-2","implicit":true,"enumerator":"2.5","key":"zVdoJsMLGX"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"次文予測タスクでは、2つの文が与えられ、一方が他方の直後に来るかどうかを判定することが求められます。","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"sXBkgsM9yq"}],"key":"nmQb2artKj"},{"type":"image","url":"/build/bert-next-sentence-p-32a370f50b97ee2353f708c030f0d6ec.png","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"NBqi2wVkPe","urlSource":"./Figure/bert-next-sentence-prediction.png"}],"key":"OEtHV6WhaD"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"ファインチューング(Fine-Tuning)","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Jy4RhV25NL"}],"identifier":"id-fine-tuning","label":"ファインチューング(Fine-Tuning)","html_id":"id-fine-tuning","implicit":true,"enumerator":"3","key":"hmJDKNhByS"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"様々なタスクにおいて、事前学習モデルをもとにしてそれぞれのタスクに特化したモデルを作るためのステップはファインチューングです。","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"gNnkVbSi1h"}],"key":"b3qQnccYMr"},{"type":"paragraph","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"事前学習タスクから下流タスクに切り替える時、モデルの最後のレイヤーをタスクに適したものに置き換える必要があります。この最後の層はヘッドと呼ばれ、タスクに固有な部分です。","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"Fs8KTQ4uEG"}],"key":"Kr8jFL2xsm"},{"type":"paragraph","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"text","value":"残りの部分はボディと呼ばれ、タスクに依存しない事前学習された部分であり(トークン埋め込み層やTransformer層が含まれ)、一般的な言語の理解を行うための基本的な情報を含んでいます。","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"oZ89IVjwbW"}],"key":"R1rPaYFxn1"},{"type":"paragraph","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"text","value":"例えば、テキスト分類の場合、追加層としては、BERTの最後の隠れ層からの出力に冒頭のSpecial token","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"k1vCL0hfnw"},{"type":"inlineCode","value":"[CLS]","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"dYnnqFxwBG"},{"type":"text","value":"を全結合層（Dense Layer）に経由し、各カテゴリーに属する確率を出力します。それは、[CLS]トークンは入力テキスト全体の文脈情報を集約すると考えるためです。","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"qTVzBlm5eX"}],"key":"UFJdYxChTm"},{"type":"admonition","kind":"note","children":[{"type":"admonitionTitle","children":[{"type":"text","value":"Note","key":"driXpM5eDw"}],"key":"b25z8YW9b5"},{"type":"paragraph","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"children":[{"type":"text","value":"テキスト分類タスクにおいて、[CLS]トークンと全結合層（Dense Layer）を使用するのが一般的ですが、他の追加層の設計を用いることももちろん可能です。","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"BJE9uywo25"}],"key":"mlvwVAXTCS"}],"key":"f91Wh7Nj5A"},{"type":"image","url":"/build/bert_based_model-28e7006a9de208e9f074c5e5df95b023.png","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"key":"EUoIHBKmLq","urlSource":"./Figure/bert_based_model.png"}],"key":"O5yO7MqWnj"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Huggingface transformerを使う","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"bG8bWTdOl0"}],"identifier":"huggingface-transformer","label":"Huggingface transformerを使う","html_id":"huggingface-transformer","implicit":true,"enumerator":"3.1","key":"zhXJYMzTtj"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"転移学習は事前学習済みモデルを新しいタスクに再利用するといった強みがあります。そのため、事前学習済みのモデルを素早く共有、ロードすることは重要です。","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"xeVdghDJYc"}],"key":"pTjief1y6j"},{"type":"paragraph","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"link","url":"https://huggingface.co/","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"Hugging Face Hub","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"gtdLRRvF5t"}],"urlSource":"https://huggingface.co/","key":"AOe2wywlnk"},{"type":"text","value":"は、モデル、データセットとデモを備えたプラットフォームです。","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"DyMiqMoyk0"}],"key":"HbZT3qWtY8"},{"type":"image","url":"/build/hf-ecosystem-6680c5be90b07a733baabb4de6aaa31e.png","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"OHM48KNmHN","urlSource":"./Figure/hf-ecosystem.png"},{"type":"paragraph","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"children":[{"type":"link","url":"https://huggingface.co/docs/transformers/index","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"children":[{"type":"text","value":"Huggingface transformer","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"w4tGJ3y8ct"}],"urlSource":"https://huggingface.co/docs/transformers/index","key":"iY1AYofZ10"},{"type":"text","value":"は、自然言語処理を中心に最先端のTransformerベースのモデルを効率に利用するためのオープンソースライブラリです。","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"dDoiaC7j63"}],"key":"RwfHMl7tWo"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":12,"column":1},"end":{"line":16,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"多数の事前学習済みモデル: ライブラリは、BERT、GPT-2、RoBERTa、T5、DistilBERTなど、さまざまな有名なNLPモデルの事前学習済みバージョンを提供しています。","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"MQD7noQB7x"}],"key":"t5AyaDWuET"}],"key":"Z4ApsuBeMU"},{"type":"listItem","spread":true,"position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"モデルの利用の簡易化: 事前学習済みのモデルを簡単にダウンロードし、特定のタスクにファインチューニングするための高レベルのAPIを提供しています。","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"qscVyAjQo0"}],"key":"G1MmiDIn5p"}],"key":"AOkVpLHBiK"},{"type":"listItem","spread":true,"position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Tokenizers: ほとんどのモデルには、テキストデータをモデルが扱える形式に変換するためのトークナイザが付属しています。これはテキストの前処理を簡単に行うためのツールです。","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"key":"WEEUoSupth"}],"key":"lbsRsNtGpf"}],"key":"W61p9mpE0X"},{"type":"listItem","spread":true,"position":{"start":{"line":15,"column":1},"end":{"line":16,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Model Hubの統合: Hugging FaceのModel Hubと直接統合されており、コミュニティによって共有されている数千もの事前学習済みモデルに簡単にアクセスできます。","position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"key":"jGQzlcTU9D"}],"key":"exlwEqnKP9"}],"key":"ZbUw1vzDzt"}],"key":"aF3IXGeUoC"},{"type":"heading","depth":4,"position":{"start":{"line":17,"column":1},"end":{"line":17,"column":1}},"children":[{"type":"text","value":"pipline","position":{"start":{"line":17,"column":1},"end":{"line":17,"column":1}},"key":"YKJJ8UAYGS"}],"identifier":"pipline","label":"pipline","html_id":"pipline","implicit":true,"enumerator":"3.1.1","key":"pmflDyawS2"},{"type":"paragraph","position":{"start":{"line":20,"column":1},"end":{"line":20,"column":1}},"children":[{"type":"link","url":"https://huggingface.co/docs/transformers/main_classes/pipelines","position":{"start":{"line":20,"column":1},"end":{"line":20,"column":1}},"children":[{"type":"inlineCode","value":"pipeline","position":{"start":{"line":20,"column":1},"end":{"line":20,"column":1}},"key":"ens05ujBWZ"}],"urlSource":"https://huggingface.co/docs/transformers/main_classes/pipelines","key":"iFleETDQgS"},{"type":"text","value":"というクラスで、特定のタスクを実行するために事前学習されたモデルとトークンナイザーを統合し、簡単に使用することができます。","position":{"start":{"line":20,"column":1},"end":{"line":20,"column":1}},"key":"MOjpr20TMn"}],"key":"Csl9MD77zG"}],"key":"QjKUltRGiC"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"#!pip install fugashi\n#!pip install unidic-lite","key":"YL0QjpRXo0"},{"type":"outputs","id":"fWr-Aif6bfypx4t3-e_Gv","children":[],"key":"ifzv4yo9h5"}],"key":"YryQvJUN0I"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"import pandas as pd\nfrom transformers import pipeline\n\nfill_mask = pipeline(\n    \"fill-mask\",\n    model=\"cl-tohoku/bert-base-japanese-v3\"\n)","key":"A4uiUy4Ivq"},{"type":"outputs","id":"lh1E9gN7A12-5ytJ8hzCF","children":[],"key":"HjiFcdZYNw"}],"key":"zBCwWqSxfC"},{"type":"block","kind":"notebook-content","children":[],"key":"p0HQaO3JZh"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"masked_text = \"東北大学は[MASK]市に位置しています。\"\noutput = fill_mask(masked_text)","key":"xOFj84ng9h"},{"type":"outputs","id":"trfRCXUrRNYzY8gsD_DIx","children":[],"key":"A4bCHqQJPv"}],"key":"NY1TmRdu60"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"display(pd.DataFrame(output))","key":"QsNhUH8Ci7"},{"type":"outputs","id":"94tXpr_os2-d0a0uJxing","children":[{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"text/html":{"content":"\u003cdiv\u003e\n\u003cstyle scoped\u003e\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n\u003c/style\u003e\n\u003ctable border=\"1\" class=\"dataframe\"\u003e\n  \u003cthead\u003e\n    \u003ctr style=\"text-align: right;\"\u003e\n      \u003cth\u003e\u003c/th\u003e\n      \u003cth\u003escore\u003c/th\u003e\n      \u003cth\u003etoken\u003c/th\u003e\n      \u003cth\u003etoken_str\u003c/th\u003e\n      \u003cth\u003esequence\u003c/th\u003e\n    \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n    \u003ctr\u003e\n      \u003cth\u003e0\u003c/th\u003e\n      \u003ctd\u003e0.600761\u003c/td\u003e\n      \u003ctd\u003e14424\u003c/td\u003e\n      \u003ctd\u003e仙台\u003c/td\u003e\n      \u003ctd\u003e東北 大学 は 仙台 市 に 位置 し て い ます 。\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e1\u003c/th\u003e\n      \u003ctd\u003e0.048195\u003c/td\u003e\n      \u003ctd\u003e19197\u003c/td\u003e\n      \u003ctd\u003e盛岡\u003c/td\u003e\n      \u003ctd\u003e東北 大学 は 盛岡 市 に 位置 し て い ます 。\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e2\u003c/th\u003e\n      \u003ctd\u003e0.037116\u003c/td\u003e\n      \u003ctd\u003e15135\u003c/td\u003e\n      \u003ctd\u003e青森\u003c/td\u003e\n      \u003ctd\u003e東北 大学 は 青森 市 に 位置 し て い ます 。\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e3\u003c/th\u003e\n      \u003ctd\u003e0.026598\u003c/td\u003e\n      \u003ctd\u003e15394\u003c/td\u003e\n      \u003ctd\u003e山形\u003c/td\u003e\n      \u003ctd\u003e東北 大学 は 山形 市 に 位置 し て い ます 。\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e4\u003c/th\u003e\n      \u003ctd\u003e0.025400\u003c/td\u003e\n      \u003ctd\u003e14062\u003c/td\u003e\n      \u003ctd\u003e福島\u003c/td\u003e\n      \u003ctd\u003e東北 大学 は 福島 市 に 位置 し て い ます 。\u003c/td\u003e\n    \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003c/div\u003e","content_type":"text/html"},"text/plain":{"content":"      score  token token_str                      sequence\n0  0.600761  14424        仙台  東北 大学 は 仙台 市 に 位置 し て い ます 。\n1  0.048195  19197        盛岡  東北 大学 は 盛岡 市 に 位置 し て い ます 。\n2  0.037116  15135        青森  東北 大学 は 青森 市 に 位置 し て い ます 。\n3  0.026598  15394        山形  東北 大学 は 山形 市 に 位置 し て い ます 。\n4  0.025400  14062        福島  東北 大学 は 福島 市 に 位置 し て い ます 。","content_type":"text/plain"}}},"children":[],"key":"RQEVGSxHXb"}],"key":"VgDObwOKKt"}],"key":"S8bLcW1o0Q"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")","key":"mAL0mhMOAk"},{"type":"outputs","id":"IYvjefO1SKvCofQuNvtcM","children":[],"key":"GfOOwjomYd"}],"key":"uNArEhDC6G"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"ARTICLE = \"\"\" New York (CNN)When Liana Barrientos was 23 years old, she got married in Westchester County, New York.\nA year later, she got married again in Westchester County, but to a different man and without divorcing her first husband.\nOnly 18 days after that marriage, she got hitched yet again. Then, Barrientos declared \"I do\" five more times, sometimes only within two weeks of each other.\nIn 2010, she married once more, this time in the Bronx. In an application for a marriage license, she stated it was her \"first and only\" marriage.\nBarrientos, now 39, is facing two criminal counts of \"offering a false instrument for filing in the first degree,\" referring to her false statements on the\n2010 marriage license application, according to court documents.\nProsecutors said the marriages were part of an immigration scam.\nOn Friday, she pleaded not guilty at State Supreme Court in the Bronx, according to her attorney, Christopher Wright, who declined to comment further.\nAfter leaving court, Barrientos was arrested and charged with theft of service and criminal trespass for allegedly sneaking into the New York subway through an emergency exit, said Detective\nAnnette Markowski, a police spokeswoman. In total, Barrientos has been married 10 times, with nine of her marriages occurring between 1999 and 2002.\nAll occurred either in Westchester County, Long Island, New Jersey or the Bronx. She is believed to still be married to four men, and at one time, she was married to eight men at once, prosecutors say.\nProsecutors said the immigration scam involved some of her husbands, who filed for permanent residence status shortly after the marriages.\nAny divorces happened only after such filings were approved. It was unclear whether any of the men will be prosecuted.\nThe case was referred to the Bronx District Attorney\\'s Office by Immigration and Customs Enforcement and the Department of Homeland Security\\'s\nInvestigation Division. Seven of the men are from so-called \"red-flagged\" countries, including Egypt, Turkey, Georgia, Pakistan and Mali.\nHer eighth husband, Rashid Rajput, was deported in 2006 to his native Pakistan after an investigation by the Joint Terrorism Task Force.\nIf convicted, Barrientos faces up to four years in prison.  Her next court appearance is scheduled for May 18.\n\"\"\"\nprint(summarizer(ARTICLE, max_length=130, min_length=30, do_sample=False))","key":"ul2EyHwbRj"},{"type":"outputs","id":"5n1-JmuIVW42YuQhJwxWH","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"[{'summary_text': 'Liana Barrientos, 39, is charged with two counts of \"offering a false instrument for filing in the first degree\" In total, she has been married 10 times, with nine of her marriages occurring between 1999 and 2002. She is believed to still be married to four men.'}]\n"},"children":[],"key":"g05S1Q82JZ"}],"key":"LRrvHJQSmW"}],"key":"poZTxpPVm4"},{"type":"block","kind":"notebook-content","children":[{"type":"tabSet","children":[{"type":"tabItem","title":"課題 1","children":[{"type":"paragraph","position":{"start":{"line":4,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"Huggingface Hubで日本語のセンチメント分類ためのモデルを探し、piplineでセンチメント分類器を実装しなさい。\n以下のテキストに対する分類結果を確認しよう。","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"reQ3BhjJP0"}],"key":"JvZ2NMFICN"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":7,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"paragraph","children":[{"type":"inlineCode","value":"この製品は全く役に立ちませんでした","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"uOXao2EC0o"}],"key":"WZpnKjDA1N"}],"key":"SC1K4cEfHZ"},{"type":"listItem","spread":true,"position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"paragraph","children":[{"type":"inlineCode","value":"今日はいい天気ですね","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"key":"V3UskbgFvs"}],"key":"JYbn9yFY8z"}],"key":"w7fNq3yD5M"},{"type":"listItem","spread":true,"position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"paragraph","children":[{"type":"inlineCode","value":"世界経済も、米国が12月に続き３月にも追加利上げを実施するなど、先進国を中心に回復の動きとなりました","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"Gs8NTyhVk8"}],"key":"lMLK3EviHQ"}],"key":"ShLJHGVHwO"}],"key":"m5mLnfIgGP"}],"key":"dx1r56M1Lk"}],"key":"RqNGtXK8vn"}],"key":"ZjUyWoFym8"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"まとめ : word2vecからBERTまで","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"v6TsOpm82g"}],"identifier":"id-word2vec-bert","label":"まとめ : word2vecからBERTまで","html_id":"id-word2vec-bert","implicit":true,"enumerator":"4","key":"cUYXht44Pp"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"人間が使う自然言語をコンピュータに処理させるため、言語を数値形式で表現するモデリングのプロセスが必要とされ、どのようなモデルを採用するかによって分析の方向性は異なっています。","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"k9XUtlNQwe"}],"key":"woQwWbQHeu"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":5,"column":1},"end":{"line":18,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":5,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"paragraph","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"最も基本的なモデリングアプローチとして、文書を単語の集合とそれぞれの単語の頻度情報に変換するバグオブワーズ(bag of words)があげられます。この手法は、文書の基本的な内容を捉えるのに有効であるが、","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"g89jRwEhAd"},{"type":"underline","children":[{"type":"text","value":"単語の順番や意味のニュアンスなどの情報はすべて捨象されています。","key":"vbdu6drU93"}],"key":"UDb4JMGse9"}],"key":"V16M8m1FkW"}],"key":"AZRxfVO7Yg"},{"type":"listItem","spread":true,"position":{"start":{"line":8,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"paragraph","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"text","value":"より複雑な言語の特性を捉えるために、大量のコーパスを使った学習により、言語の文法や意味構造など多くの情報を埋め込んだ高度なモデルが期待されています。word2vecをはじめとする単語分散表現モデルは、単語を「意味」情報を表現したベクトルにマッピングすることができます。","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"key":"MoHv0kzvYS"}],"key":"aMLQnkACDm"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":9,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"word2vecに単語分散表現の学習では、「単語の意味は、その単語の周囲の単語（文脈）によって決まる」という分布仮説に基づく手法が用いられます。この仮説にしたがうモデルでは、ある単語がどのような文脈で生じやすいかということをある程度考慮し、単語間の関係をベクトルで表現することができます。","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"jxcP0ghSxr"}],"key":"CxqxkHvQOV"}],"key":"vhl0LuaGz0"},{"type":"listItem","spread":true,"position":{"start":{"line":10,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"ただ、word2vecではいくつの欠点があります。特に、word2vecではあくまで","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"D69C1ngpP2"},{"type":"text","value":"1","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"RnCXZmr5zo"},{"type":"text","value":"単語","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"NoOrQCJk7d"},{"type":"text","value":"1","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"MUpVl6DpoP"},{"type":"text","value":"ベクトルでしたが、実際のケースでは、文脈によって単語の意味が変わることがありますので、","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"ctBQV08vLU"},{"type":"underline","children":[{"type":"text","value":"文脈に依存する分散表現が望ましいです。","key":"TZBdWz1HFs"}],"key":"n0Gaqa0sOi"}],"key":"sPfULzX9iN"}],"key":"CfpXmoHjmq"}],"key":"cmCFwXlFTH"}],"key":"A3MjiZ470g"},{"type":"listItem","spread":true,"position":{"start":{"line":12,"column":1},"end":{"line":17,"column":1}},"children":[{"type":"paragraph","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"children":[{"type":"text","value":"テキストの「文脈」を表現するための言語モデルが開発されました。","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"TWCAeGnKiO"}],"key":"RZwkdtuJQB"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":13,"column":1},"end":{"line":17,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":13,"column":1},"end":{"line":15,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"RNNとLSTMではテキストデータの時系列的な性質を捉え、","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"ZEhsbx2gJw"},{"type":"underline","children":[{"type":"text","value":"文中の単語の順序や時間的な関連性をモデルが学習できるようになります","key":"VSSYB53DNb"}],"key":"LOTJHu0mBQ"},{"type":"text","value":"。ただ、","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"y2InomhnVj"}],"key":"J0IJESgTJv"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":14,"column":1},"end":{"line":15,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"長距離の依存関係を効果的に対処できでいない","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"key":"dmj1vJWnNM"}],"key":"LBGbl6QjCx"}],"key":"R9dljkEcjM"},{"type":"listItem","spread":true,"position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"計算コストが高い","position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"key":"KAubtmFvlG"}],"key":"O3GEGoX3xZ"}],"key":"HwIkm7zPrt"}],"key":"rvAfdEpP6Q"}],"key":"ViWeV9yAdP"},{"type":"listItem","spread":true,"position":{"start":{"line":16,"column":1},"end":{"line":17,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Self-Attenttionでは、","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"key":"J5d4fRumOV"},{"type":"underline","children":[{"type":"text","value":"すべての単語間の関係を並列に計算することで、長距離の依存関係を効果的に捉えます。","key":"dTLK6qjGwe"}],"key":"KdPwfEIKpj"}],"key":"S2QePMR4I1"}],"key":"lgpb7Yogrb"}],"key":"ChksMMMcBf"}],"key":"o8SeWi7AYD"},{"type":"listItem","spread":true,"position":{"start":{"line":18,"column":1},"end":{"line":18,"column":1}},"children":[{"type":"paragraph","position":{"start":{"line":18,"column":1},"end":{"line":18,"column":1}},"children":[{"type":"text","value":"Transformerをベースにしたモデルでは、単語が出現する具体的な文脈に基づいてその単語の埋め込みを生成することができます。","position":{"start":{"line":18,"column":1},"end":{"line":18,"column":1}},"key":"pak7mM2x4C"}],"key":"wkKUzy4TnQ"}],"key":"qVMoMjfixZ"}],"key":"zi63MdoVuG"}],"key":"G8GHiAgzY7"}],"key":"ilJHC1OjAb"},"references":{"cite":{"order":[],"data":{}}},"footer":{"navigation":{"prev":{"title":"Transformerアーキテクチャ","url":"/transformer","group":"Transformer"},"next":{"title":"BERTによるセンチメント分析","url":"/bert-sentiment","group":"Transformer"}}},"domain":"http://localhost:3001"},"project":{"numbering":{"heading_1":{"template":"enabled","enabled":true},"heading_2":{"template":"enabled","enabled":true},"heading_3":{"template":"enabled","enabled":true},"heading_4":{"template":"enabled","enabled":true},"heading_5":{"template":"enabled","enabled":true},"heading_6":{"template":"enabled","enabled":true}},"title":"計算社会科学と自然言語処理","github":"https://github.com/lvzeyu/css_nlp","id":"356ed62a-cf04-49ae-b7bc-d9f79f2ef8bb","exports":[],"bibliography":[],"index":"intro","pages":[{"title":"イントロダクション","level":1},{"slug":"introduction","title":"ガイダンス","description":"このページで、行動科学演習(LB63310)と計算人文社会学研究演習Ⅱ (LM23309)の授業資料を公開しています。","date":"","thumbnail":"/build/NLP_history-80702ee1d37040856a480174f5bcbbfc.png","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"title":"基礎知識","level":1},{"slug":"nlp-basis2","title":"自然言語処理の基礎","description":"","date":"","thumbnail":"/build/7e2db436150c38a00650f96925aa5581.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"ml-basis2","title":"機械学習の基本概念","description":"","date":"","thumbnail":"/build/supervised-learning-b5be7f5e73313e60f2c06d545c226553.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"math-basis2","title":"数学基礎","description":"","date":"","thumbnail":"/build/7e2db436150c38a00650f96925aa5581.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"title":"ニューラルネットワーク","level":1},{"slug":"nn2","title":"ニューラルネットワーク","description":"","date":"","thumbnail":"/build/7e2db436150c38a00650f96925aa5581.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"backpropagation","title":"誤差逆伝播法","description":"","date":"","thumbnail":"/build/7e2db436150c38a00650f96925aa5581.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"sgd","title":"学習に関するテクニック","description":"","date":"","thumbnail":"/build/7e2db436150c38a00650f96925aa5581.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"title":"PyTorch","level":1},{"slug":"pytorch2","title":"Pytorch","description":"","date":"","thumbnail":"/build/7e2db436150c38a00650f96925aa5581.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"title":"単語分散表現","level":1},{"slug":"word2vec-1","title":"単語分散表現","description":"","date":"","thumbnail":"/build/7e2db436150c38a00650f96925aa5581.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"word2vec-2-embedding","title":"word2vec","description":"","date":"","thumbnail":"/build/7e2db436150c38a00650f96925aa5581.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"word2vec-gensim","title":"GensimによるWord2Vecの学習と使用","description":"","date":"","thumbnail":"/build/7e2db436150c38a00650f96925aa5581.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"word2vec-application","title":"Word2Vecが人文・社会科学研究における応用","description":"","date":"","thumbnail":"/build/7e2db436150c38a00650f96925aa5581.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"title":"RNN","level":1},{"slug":"rnn","title":"RNNの基礎","description":"","date":"","thumbnail":"/build/rnn-4dbf33ec81abf2baa6d5bbe97bb61683.png","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"lstm","title":"LSTM","description":"","date":"","thumbnail":"/build/lstm1-4ea8525c2f72b4f2ff8298a9f7b9c0be.png","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"pytorch-lstm","title":"LSTMの実装","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"lstm-classification","title":"LSTMによる文書分類","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"seq2seq","title":"Seq2seq","description":"","date":"","thumbnail":"/build/7e2db436150c38a00650f96925aa5581.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"title":"Transformer","level":1},{"slug":"attention","title":"Attention","description":"","date":"","thumbnail":"/build/attention-eed7f8fafc28c8cb6bd5276bbd04714b.png","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"self-attention","title":"Self-Attention","description":"","date":"","thumbnail":"/build/7e2db436150c38a00650f96925aa5581.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"transformer","title":"Transformerアーキテクチャ","description":"","date":"","thumbnail":"/build/transformer-493148401fbb971a745b29ab5b44e10f.png","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"bert","title":"BERT","description":"","date":"","thumbnail":"/build/7e2db436150c38a00650f96925aa5581.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"bert-sentiment","title":"BERTによるセンチメント分析","description":"","date":"","thumbnail":"/build/7e2db436150c38a00650f96925aa5581.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"bert-topic","title":"BERTopic","description":"","date":"","thumbnail":"/build/7e2db436150c38a00650f96925aa5581.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"title":"大規模言語モデル","level":1},{"slug":"gpt","title":"GPT","description":"","date":"","thumbnail":"/build/gpt_history-c08ab6ce749d3aed300b023e318cb96c.png","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"llm","title":"大規模言語モデルの基本","description":"","date":"","thumbnail":"/build/LLMs_parameter-ec157a4028abc601fb429210b7f4b07c.png","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"langchain-basic","title":"LLMSの応用(1)：Langchainの基礎","description":"","date":"","thumbnail":"/build/7e2db436150c38a00650f96925aa5581.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2}]}}},"actionData":null,"errors":null},"future":{"unstable_dev":false,"unstable_postcss":false,"unstable_tailwind":false,"v2_errorBoundary":true,"v2_headers":true,"v2_meta":true,"v2_normalizeFormMethod":true,"v2_routeConvention":true}};</script><script type="module" async="">import "/build/manifest-86A905A4.js";
import * as route0 from "/build/root-CXYA7X5D.js";
import * as route1 from "/build/routes/$-JRBPULBO.js";
window.__remixRouteModules = {"root":route0,"routes/$":route1};

import("/build/entry.client-PCJPW7TK.js");</script></body></html>