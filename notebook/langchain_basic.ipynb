{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLMSの応用(1)：Langchainの基礎\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](\n",
    "https://colab.research.google.com/github/lvzeyu/css_nlp/blob/master/notebook/langchain_basic.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[LangChain](https://www.langchain.com/)は大規模言語モデルを活用したアプリケーション開発を支えるフレームワークです。具体的にはプロンプト設計、RAGインデックス構築、会話メモリ、外部ツール連携、自律エージェントなどLLMアプリに必要な一連の機能を統合的に提供します。これにより、実験段階のプロトタイプから実運用を想定したシステムまで、開発を効率化しやすくなります。\n",
    "\n",
    "- ```llms```: 言語モデルを呼び出すためのラッパーを提供します\n",
    "- ```prompts```: プロンプトのテンプレートを作成する機能を提供します\n",
    "- ```chains```: ひとつのワークフロー内で LLM やプロンプトテンプレートを組み合わせて使用するための機能を提供します\n",
    "- ```agents```: エージェントを使用することで、課題の解決順序をも LLM を用いて決定し、実行させることができます\n",
    "- ```memory```: チェーンとエージェントに状態を持たせるための機能を提供します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain==0.1.12\n",
    "#!pip install langchain-community\n",
    "#!pip install langchain-core\n",
    "#!pip install langsmith\n",
    "#!pip install openai\n",
    "#!pip install python-dotenv\n",
    "#!pip install langchain-openai\n",
    "#!pip install -U --quiet langchain-google-genai pillow\n",
    "#!pip install google-generativeai\n",
    "#!pip install google-search-results numexpr wikipedia langchain-experimental\n",
    "#!pip install duckduckgo-search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChainでLLMを使う\n",
    "\n",
    "\n",
    "LLMは、ほとんどの生成AIアプリケーションを駆動する原動力である。LangChainは、あらゆるLLM APIプロバイダーとやり取りするためのシンプルなインターフェースを2種類提供している。\n",
    "\n",
    "- チャットモデルインターフェース: チャットモデルインターフェースを使用すると、ユーザーとモデルの間で双方向の対話を行える\n",
    "- LLMインターフェース: LLMインターフェースは、文字列プロンプトを入力として受け取り、それをモデルプロバイダーへ送信し、予測結果を出力として返すだけである。\n",
    "\n",
    "\n",
    "[OpenAI](https://openai.com/blog/openai-api)と[Google(Gemini)](https://deepmind.google/technologies/gemini/#introduction)が提供するAPIを通じて、多岐にわたるAIモデルへのアクセスを可能になります。\n",
    "\n",
    "APIを使うためには、まず「自分がサービスを利用できる証」となる「APIキー」を発行する必要があります。そして、OpenAIのAPIは、このAPIキーによって利用した使用量に応じて、課金される仕組みです。\n",
    "\n",
    "そのため、APIキーが外部に漏れると、他者によって不正に使用されて料金が発生してしまうため、他の人へ共有しないように注意しましょう。\n",
    "\n",
    "LangChainは、さまざまな LLM に汎用インターフェースを提供し、ユーザーがAPIを介してさまざまなモデルを操作できるようにします。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI API\n",
    "\n",
    "`OpenAI`クラスのインスタンスでGPT-3モデルを使用するための設定を行っています。\n",
    "\n",
    "- `model_name`という引数で、使用する[モデルの名前](https://platform.openai.com/docs/models)を指定します。\n",
    "\n",
    "\n",
    "- `temperature`という引数は、生成されるテキストのランダム性を制御します。`temperature`が高いほど（1に近いほど）、出力はよりランダムになります。逆に、`temperature`が低いほど（0に近いほど）、モデルの出力はより一貫性があり、予測可能になります。\n",
    "\n",
    "- `max_tokens`という引数は、生成するテキストの最大トークン数を指定します。この場合、生成されるテキストは最大で256トークンになります。トークンとは、テキストを分割した単位のことで、一般的には単語や句読点などが1トークンとなります。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import getpass\n",
    "from langchain_openai import OpenAI, ChatOpenAI     \n",
    "\n",
    "load_dotenv(dotenv_path='../.env')\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(\n",
    "    model=\"gpt-3.5-turbo-instruct\",\n",
    "    temperature=0.9,\n",
    "    max_tokens=512\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n東北大学は日本の東北地方に位置する国立大学で、高い教育水準と優れた研究成果で知られています。1886年に創立され、現在は仙台市を中心に6つのキャンパスで約16,000人の学生が学ぶ総合大学です。\\n\\n東北大学には11学部・16研究科があり、理学、工学、経済学、法学、医学、歯学、薬学、農学、教育学、生命科学、国際文化研究を学ぶことができます。また、東北地方で唯一の獣医学部を有しており、幅広い分野での専門教育を提供しています。\\n\\n東北大学はグローバルな教育を重視し、多様な国・地域からの留学生や外国人教員を受け入れています。学生は国際交流プログラムや海外研修プログラムに参加し、世界各国での学びや経験を積むことができます。\\n\\nまた、東北大学は幅広い研究分野で国内外から高い評価を受けており、特に科学技術分野での研究成果は世界的に高い水準を誇っています。さまざまな国内外の企業や研究機関との共同研究も積極的に行われており、卒業生の就職率も非常に高いです。\\n\\n東北大学は東日本大震災の被災地に位置し'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"東北大学を紹介してください：\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "チャットモデルインターフェースを使用すると、ユーザーとモデルの間で双方向の対話を行える。これが別のインターフェースとして用意されている理由は、OpenAIのような主要LLMプロバイダーが、送受信されるメッセージを「ユーザー」「アシスタント」「システム」というロールに分けて扱っているためである.\n",
    "\n",
    "ここでのロールとは、メッセージに含まれるコンテンツの種類を示す）。\n",
    "\n",
    "- システム（system）」ロール: モデルに与える指示を記述する。\n",
    "- ユーザー（user）ロール: ユーザーのクエリおよびユーザーが生成したコンテンツを表す。\n",
    "- アシスタント（assistant）ロール: モデルが生成したコンテンツを表す。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='東北大学は日本の宮城県仙台市に位置する国立大学であり、東北地方における最高峰の総合大学です。1907年に創立され、現在は学部・大学院を含む12学部、13大学院があります。\\n\\n東北大学は、幅広い学問分野をカバーする「研究型大学」として知られており、工学、理学、医学、人文社会科学など多岐にわたる分野で高い研究力を持っています。特に理工学部や医学部は国内外から高い評価を受けており、多くの優秀な研究者を輩出しています。\\n\\nまた、東北大学は学生支援体制も充実しており、学生寮や奨学金制度、留学プログラムなど、学生が安心して学び、研究に取り組む環境が整っています。\\n\\n東北大学は、地域社会や産業界との連携を重視し、地域に貢献する研究活動やイノベーション創出にも積極的に取り組んでいます。その他にも、学内外の様々な国際交流プログラムやイベントが充実しており、多様な価値観や文化と触れ合う機会も提供されています。\\n\\n総合的に見て、東北大学は、高い学術水準と研究力、豊富な学生支援体制、そして社会貢献活動において、日本を代表する大学の一つとして、国内外で高い評価を受けています。', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 531, 'prompt_tokens': 18, 'total_tokens': 549, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-D0HhMdLc0IszUBjoF02omJBbL8Gu7', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019bde41-a7af-7083-8f39-e5f174f6640c-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 18, 'output_tokens': 531, 'total_tokens': 549, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "model = ChatOpenAI()\n",
    "prompt = [HumanMessage(\"東北大学を紹介してください\")]\n",
    "\n",
    "model.invoke(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='東北大学は日本の宮城県にある有名な大学です!!!東北地方で最も歴史と伝統のある大学の1つです!!!研究分野も豊富で、学生のレベルも高いです!!!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 77, 'prompt_tokens': 37, 'total_tokens': 114, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-D0HluCvkJOuwacPEF7z9jj47ebzfZ', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019bde45-f417-7481-b5b5-03794e351c03-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 37, 'output_tokens': 77, 'total_tokens': 114, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI()         \n",
    "system_msg = SystemMessage(\n",
    "    '''You are a helpful assistant that responds to questions with three exclamation marks.'''\n",
    ")# あなたは優秀なアシスタントです。質問に対して3つの感嘆符を付けて回答してください。\n",
    "\n",
    "human_msg = HumanMessage(\"東北大学を紹介してください\")\n",
    "model.invoke([system_msg, human_msg])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gemini API\n",
    "\n",
    "#### Gemini APIの設定\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"GOOGLE_API_KEY\" not in os.environ:\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your GOOGLE API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lvzeyu/anaconda3/envs/jupyterbook/lib/python3.10/site-packages/google/api_core/_python_version_support.py:275: FutureWarning: You are using a Python version (3.10.19) which Google will stop supporting in new releases of google.api_core once it reaches its end of life (2026-10-04). Please upgrade to the latest Python version, or at least Python 3.11, to continue receiving updates for google.api_core past that date.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/tmp/ipykernel_152305/3039597036.py:1: FutureWarning: \n",
      "\n",
      "All support for the `google.generativeai` package has ended. It will no longer be receiving \n",
      "updates or bug fixes. Please switch to the `google.genai` package as soon as possible.\n",
      "See README for more details:\n",
      "\n",
      "https://github.com/google-gemini/deprecated-generative-ai-python/blob/main/README.md\n",
      "\n",
      "  import google.generativeai as genai\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Model(name='models/embedding-gecko-001',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Embedding Gecko',\n",
       "       description='Obtain a distributed representation of a text.',\n",
       "       input_token_limit=1024,\n",
       "       output_token_limit=1,\n",
       "       supported_generation_methods=['embedText', 'countTextTokens'],\n",
       "       temperature=None,\n",
       "       max_temperature=None,\n",
       "       top_p=None,\n",
       "       top_k=None),\n",
       " Model(name='models/gemini-2.5-flash',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini 2.5 Flash',\n",
       "       description=('Stable version of Gemini 2.5 Flash, our mid-size multimodal model that '\n",
       "                    'supports up to 1 million tokens, released in June of 2025.'),\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=65536,\n",
       "       supported_generation_methods=['generateContent',\n",
       "                                     'countTokens',\n",
       "                                     'createCachedContent',\n",
       "                                     'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-2.5-pro',\n",
       "       base_model_id='',\n",
       "       version='2.5',\n",
       "       display_name='Gemini 2.5 Pro',\n",
       "       description='Stable release (June 17th, 2025) of Gemini 2.5 Pro',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=65536,\n",
       "       supported_generation_methods=['generateContent',\n",
       "                                     'countTokens',\n",
       "                                     'createCachedContent',\n",
       "                                     'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-2.0-flash-exp',\n",
       "       base_model_id='',\n",
       "       version='2.0',\n",
       "       display_name='Gemini 2.0 Flash Experimental',\n",
       "       description='Gemini 2.0 Flash Experimental',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent', 'countTokens', 'bidiGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=40),\n",
       " Model(name='models/gemini-2.0-flash',\n",
       "       base_model_id='',\n",
       "       version='2.0',\n",
       "       display_name='Gemini 2.0 Flash',\n",
       "       description='Gemini 2.0 Flash',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent',\n",
       "                                     'countTokens',\n",
       "                                     'createCachedContent',\n",
       "                                     'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=40),\n",
       " Model(name='models/gemini-2.0-flash-001',\n",
       "       base_model_id='',\n",
       "       version='2.0',\n",
       "       display_name='Gemini 2.0 Flash 001',\n",
       "       description=('Stable version of Gemini 2.0 Flash, our fast and versatile multimodal model '\n",
       "                    'for scaling across diverse tasks, released in January of 2025.'),\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent',\n",
       "                                     'countTokens',\n",
       "                                     'createCachedContent',\n",
       "                                     'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=40),\n",
       " Model(name='models/gemini-2.0-flash-exp-image-generation',\n",
       "       base_model_id='',\n",
       "       version='2.0',\n",
       "       display_name='Gemini 2.0 Flash (Image Generation) Experimental',\n",
       "       description='Gemini 2.0 Flash (Image Generation) Experimental',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent', 'countTokens', 'bidiGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=40),\n",
       " Model(name='models/gemini-2.0-flash-lite-001',\n",
       "       base_model_id='',\n",
       "       version='2.0',\n",
       "       display_name='Gemini 2.0 Flash-Lite 001',\n",
       "       description='Stable version of Gemini 2.0 Flash-Lite',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent',\n",
       "                                     'countTokens',\n",
       "                                     'createCachedContent',\n",
       "                                     'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=40),\n",
       " Model(name='models/gemini-2.0-flash-lite',\n",
       "       base_model_id='',\n",
       "       version='2.0',\n",
       "       display_name='Gemini 2.0 Flash-Lite',\n",
       "       description='Gemini 2.0 Flash-Lite',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent',\n",
       "                                     'countTokens',\n",
       "                                     'createCachedContent',\n",
       "                                     'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=40),\n",
       " Model(name='models/gemini-2.0-flash-lite-preview-02-05',\n",
       "       base_model_id='',\n",
       "       version='preview-02-05',\n",
       "       display_name='Gemini 2.0 Flash-Lite Preview 02-05',\n",
       "       description='Preview release (February 5th, 2025) of Gemini 2.0 Flash-Lite',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent',\n",
       "                                     'countTokens',\n",
       "                                     'createCachedContent',\n",
       "                                     'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=40),\n",
       " Model(name='models/gemini-2.0-flash-lite-preview',\n",
       "       base_model_id='',\n",
       "       version='preview-02-05',\n",
       "       display_name='Gemini 2.0 Flash-Lite Preview',\n",
       "       description='Preview release (February 5th, 2025) of Gemini 2.0 Flash-Lite',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent',\n",
       "                                     'countTokens',\n",
       "                                     'createCachedContent',\n",
       "                                     'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=40),\n",
       " Model(name='models/gemini-exp-1206',\n",
       "       base_model_id='',\n",
       "       version='2.5-exp-03-25',\n",
       "       display_name='Gemini Experimental 1206',\n",
       "       description='Experimental release (March 25th, 2025) of Gemini 2.5 Pro',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=65536,\n",
       "       supported_generation_methods=['generateContent',\n",
       "                                     'countTokens',\n",
       "                                     'createCachedContent',\n",
       "                                     'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-2.5-flash-preview-tts',\n",
       "       base_model_id='',\n",
       "       version='gemini-2.5-flash-exp-tts-2025-05-19',\n",
       "       display_name='Gemini 2.5 Flash Preview TTS',\n",
       "       description='Gemini 2.5 Flash Preview TTS',\n",
       "       input_token_limit=8192,\n",
       "       output_token_limit=16384,\n",
       "       supported_generation_methods=['countTokens', 'generateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-2.5-pro-preview-tts',\n",
       "       base_model_id='',\n",
       "       version='gemini-2.5-pro-preview-tts-2025-05-19',\n",
       "       display_name='Gemini 2.5 Pro Preview TTS',\n",
       "       description='Gemini 2.5 Pro Preview TTS',\n",
       "       input_token_limit=8192,\n",
       "       output_token_limit=16384,\n",
       "       supported_generation_methods=['countTokens', 'generateContent', 'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemma-3-1b-it',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemma 3 1B',\n",
       "       description='',\n",
       "       input_token_limit=32768,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=None,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemma-3-4b-it',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemma 3 4B',\n",
       "       description='',\n",
       "       input_token_limit=32768,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=None,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemma-3-12b-it',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemma 3 12B',\n",
       "       description='',\n",
       "       input_token_limit=32768,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=None,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemma-3-27b-it',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemma 3 27B',\n",
       "       description='',\n",
       "       input_token_limit=131072,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=None,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemma-3n-e4b-it',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemma 3n E4B',\n",
       "       description='',\n",
       "       input_token_limit=8192,\n",
       "       output_token_limit=2048,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=None,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemma-3n-e2b-it',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemma 3n E2B',\n",
       "       description='',\n",
       "       input_token_limit=8192,\n",
       "       output_token_limit=2048,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=None,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-flash-latest',\n",
       "       base_model_id='',\n",
       "       version='Gemini Flash Latest',\n",
       "       display_name='Gemini Flash Latest',\n",
       "       description='Latest release of Gemini Flash',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=65536,\n",
       "       supported_generation_methods=['generateContent',\n",
       "                                     'countTokens',\n",
       "                                     'createCachedContent',\n",
       "                                     'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-flash-lite-latest',\n",
       "       base_model_id='',\n",
       "       version='Gemini Flash-Lite Latest',\n",
       "       display_name='Gemini Flash-Lite Latest',\n",
       "       description='Latest release of Gemini Flash-Lite',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=65536,\n",
       "       supported_generation_methods=['generateContent',\n",
       "                                     'countTokens',\n",
       "                                     'createCachedContent',\n",
       "                                     'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-pro-latest',\n",
       "       base_model_id='',\n",
       "       version='Gemini Pro Latest',\n",
       "       display_name='Gemini Pro Latest',\n",
       "       description='Latest release of Gemini Pro',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=65536,\n",
       "       supported_generation_methods=['generateContent',\n",
       "                                     'countTokens',\n",
       "                                     'createCachedContent',\n",
       "                                     'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-2.5-flash-lite',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini 2.5 Flash-Lite',\n",
       "       description='Stable version of Gemini 2.5 Flash-Lite, released in July of 2025',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=65536,\n",
       "       supported_generation_methods=['generateContent',\n",
       "                                     'countTokens',\n",
       "                                     'createCachedContent',\n",
       "                                     'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-2.5-flash-image',\n",
       "       base_model_id='',\n",
       "       version='2.0',\n",
       "       display_name='Nano Banana',\n",
       "       description='Gemini 2.5 Flash Preview Image',\n",
       "       input_token_limit=32768,\n",
       "       output_token_limit=32768,\n",
       "       supported_generation_methods=['generateContent', 'countTokens', 'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=1.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-2.5-flash-preview-09-2025',\n",
       "       base_model_id='',\n",
       "       version='Gemini 2.5 Flash Preview 09-2025',\n",
       "       display_name='Gemini 2.5 Flash Preview Sep 2025',\n",
       "       description='Gemini 2.5 Flash Preview Sep 2025',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=65536,\n",
       "       supported_generation_methods=['generateContent',\n",
       "                                     'countTokens',\n",
       "                                     'createCachedContent',\n",
       "                                     'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-2.5-flash-lite-preview-09-2025',\n",
       "       base_model_id='',\n",
       "       version='2.5-preview-09-25',\n",
       "       display_name='Gemini 2.5 Flash-Lite Preview Sep 2025',\n",
       "       description='Preview release (Septempber 25th, 2025) of Gemini 2.5 Flash-Lite',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=65536,\n",
       "       supported_generation_methods=['generateContent',\n",
       "                                     'countTokens',\n",
       "                                     'createCachedContent',\n",
       "                                     'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-3-pro-preview',\n",
       "       base_model_id='',\n",
       "       version='3-pro-preview-11-2025',\n",
       "       display_name='Gemini 3 Pro Preview',\n",
       "       description='Gemini 3 Pro Preview',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=65536,\n",
       "       supported_generation_methods=['generateContent',\n",
       "                                     'countTokens',\n",
       "                                     'createCachedContent',\n",
       "                                     'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-3-flash-preview',\n",
       "       base_model_id='',\n",
       "       version='3-flash-preview-12-2025',\n",
       "       display_name='Gemini 3 Flash Preview',\n",
       "       description='Gemini 3 Flash Preview',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=65536,\n",
       "       supported_generation_methods=['generateContent',\n",
       "                                     'countTokens',\n",
       "                                     'createCachedContent',\n",
       "                                     'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-3-pro-image-preview',\n",
       "       base_model_id='',\n",
       "       version='3.0',\n",
       "       display_name='Nano Banana Pro',\n",
       "       description='Gemini 3 Pro Image Preview',\n",
       "       input_token_limit=131072,\n",
       "       output_token_limit=32768,\n",
       "       supported_generation_methods=['generateContent', 'countTokens', 'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=1.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/nano-banana-pro-preview',\n",
       "       base_model_id='',\n",
       "       version='3.0',\n",
       "       display_name='Nano Banana Pro',\n",
       "       description='Gemini 3 Pro Image Preview',\n",
       "       input_token_limit=131072,\n",
       "       output_token_limit=32768,\n",
       "       supported_generation_methods=['generateContent', 'countTokens', 'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=1.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-robotics-er-1.5-preview',\n",
       "       base_model_id='',\n",
       "       version='1.5-preview',\n",
       "       display_name='Gemini Robotics-ER 1.5 Preview',\n",
       "       description='Gemini Robotics-ER 1.5 Preview',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=65536,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-2.5-computer-use-preview-10-2025',\n",
       "       base_model_id='',\n",
       "       version='Gemini 2.5 Computer Use Preview 10-2025',\n",
       "       display_name='Gemini 2.5 Computer Use Preview 10-2025',\n",
       "       description='Gemini 2.5 Computer Use Preview 10-2025',\n",
       "       input_token_limit=131072,\n",
       "       output_token_limit=65536,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/deep-research-pro-preview-12-2025',\n",
       "       base_model_id='',\n",
       "       version='deepthink-exp-05-20',\n",
       "       display_name='Deep Research Pro Preview (Dec-12-2025)',\n",
       "       description='Preview release (December 12th, 2025) of Deep Research Pro',\n",
       "       input_token_limit=131072,\n",
       "       output_token_limit=65536,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/embedding-001',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Embedding 001',\n",
       "       description='Obtain a distributed representation of a text.',\n",
       "       input_token_limit=2048,\n",
       "       output_token_limit=1,\n",
       "       supported_generation_methods=['embedContent'],\n",
       "       temperature=None,\n",
       "       max_temperature=None,\n",
       "       top_p=None,\n",
       "       top_k=None),\n",
       " Model(name='models/text-embedding-004',\n",
       "       base_model_id='',\n",
       "       version='004',\n",
       "       display_name='Text Embedding 004',\n",
       "       description='Obtain a distributed representation of a text.',\n",
       "       input_token_limit=2048,\n",
       "       output_token_limit=1,\n",
       "       supported_generation_methods=['embedContent'],\n",
       "       temperature=None,\n",
       "       max_temperature=None,\n",
       "       top_p=None,\n",
       "       top_k=None),\n",
       " Model(name='models/gemini-embedding-exp-03-07',\n",
       "       base_model_id='',\n",
       "       version='exp-03-07',\n",
       "       display_name='Gemini Embedding Experimental 03-07',\n",
       "       description='Obtain a distributed representation of a text.',\n",
       "       input_token_limit=8192,\n",
       "       output_token_limit=1,\n",
       "       supported_generation_methods=['embedContent', 'countTextTokens', 'countTokens'],\n",
       "       temperature=None,\n",
       "       max_temperature=None,\n",
       "       top_p=None,\n",
       "       top_k=None),\n",
       " Model(name='models/gemini-embedding-exp',\n",
       "       base_model_id='',\n",
       "       version='exp-03-07',\n",
       "       display_name='Gemini Embedding Experimental',\n",
       "       description='Obtain a distributed representation of a text.',\n",
       "       input_token_limit=8192,\n",
       "       output_token_limit=1,\n",
       "       supported_generation_methods=['embedContent', 'countTextTokens', 'countTokens'],\n",
       "       temperature=None,\n",
       "       max_temperature=None,\n",
       "       top_p=None,\n",
       "       top_k=None),\n",
       " Model(name='models/gemini-embedding-001',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini Embedding 001',\n",
       "       description='Obtain a distributed representation of a text.',\n",
       "       input_token_limit=2048,\n",
       "       output_token_limit=1,\n",
       "       supported_generation_methods=['embedContent', 'countTextTokens', 'countTokens', 'asyncBatchEmbedContent'],\n",
       "       temperature=None,\n",
       "       max_temperature=None,\n",
       "       top_p=None,\n",
       "       top_k=None),\n",
       " Model(name='models/aqa',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Model that performs Attributed Question Answering.',\n",
       "       description=('Model trained to return answers to questions that are grounded in provided '\n",
       "                    'sources, along with estimating answerable probability.'),\n",
       "       input_token_limit=7168,\n",
       "       output_token_limit=1024,\n",
       "       supported_generation_methods=['generateAnswer'],\n",
       "       temperature=0.2,\n",
       "       max_temperature=None,\n",
       "       top_p=1.0,\n",
       "       top_k=40),\n",
       " Model(name='models/imagen-4.0-generate-preview-06-06',\n",
       "       base_model_id='',\n",
       "       version='01',\n",
       "       display_name='Imagen 4 (Preview)',\n",
       "       description='Vertex served Imagen 4.0 model',\n",
       "       input_token_limit=480,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['predict'],\n",
       "       temperature=None,\n",
       "       max_temperature=None,\n",
       "       top_p=None,\n",
       "       top_k=None),\n",
       " Model(name='models/imagen-4.0-ultra-generate-preview-06-06',\n",
       "       base_model_id='',\n",
       "       version='01',\n",
       "       display_name='Imagen 4 Ultra (Preview)',\n",
       "       description='Vertex served Imagen 4.0 ultra model',\n",
       "       input_token_limit=480,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['predict'],\n",
       "       temperature=None,\n",
       "       max_temperature=None,\n",
       "       top_p=None,\n",
       "       top_k=None),\n",
       " Model(name='models/imagen-4.0-generate-001',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Imagen 4',\n",
       "       description='Vertex served Imagen 4.0 model',\n",
       "       input_token_limit=480,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['predict'],\n",
       "       temperature=None,\n",
       "       max_temperature=None,\n",
       "       top_p=None,\n",
       "       top_k=None),\n",
       " Model(name='models/imagen-4.0-ultra-generate-001',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Imagen 4 Ultra',\n",
       "       description='Vertex served Imagen 4.0 ultra model',\n",
       "       input_token_limit=480,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['predict'],\n",
       "       temperature=None,\n",
       "       max_temperature=None,\n",
       "       top_p=None,\n",
       "       top_k=None),\n",
       " Model(name='models/imagen-4.0-fast-generate-001',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Imagen 4 Fast',\n",
       "       description='Vertex served Imagen 4.0 Fast model',\n",
       "       input_token_limit=480,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['predict'],\n",
       "       temperature=None,\n",
       "       max_temperature=None,\n",
       "       top_p=None,\n",
       "       top_k=None),\n",
       " Model(name='models/veo-2.0-generate-001',\n",
       "       base_model_id='',\n",
       "       version='2.0',\n",
       "       display_name='Veo 2',\n",
       "       description=('Vertex served Veo 2 model. Access to this model requires billing to be '\n",
       "                    'enabled on the associated Google Cloud Platform account. Please visit '\n",
       "                    'https://console.cloud.google.com/billing to enable it.'),\n",
       "       input_token_limit=480,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['predictLongRunning'],\n",
       "       temperature=None,\n",
       "       max_temperature=None,\n",
       "       top_p=None,\n",
       "       top_k=None),\n",
       " Model(name='models/veo-3.0-generate-001',\n",
       "       base_model_id='',\n",
       "       version='3.0',\n",
       "       display_name='Veo 3',\n",
       "       description='Veo 3',\n",
       "       input_token_limit=480,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['predictLongRunning'],\n",
       "       temperature=None,\n",
       "       max_temperature=None,\n",
       "       top_p=None,\n",
       "       top_k=None),\n",
       " Model(name='models/veo-3.0-fast-generate-001',\n",
       "       base_model_id='',\n",
       "       version='3.0',\n",
       "       display_name='Veo 3 fast',\n",
       "       description='Veo 3 fast',\n",
       "       input_token_limit=480,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['predictLongRunning'],\n",
       "       temperature=None,\n",
       "       max_temperature=None,\n",
       "       top_p=None,\n",
       "       top_k=None),\n",
       " Model(name='models/veo-3.1-generate-preview',\n",
       "       base_model_id='',\n",
       "       version='3.1',\n",
       "       display_name='Veo 3.1',\n",
       "       description='Veo 3.1',\n",
       "       input_token_limit=480,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['predictLongRunning'],\n",
       "       temperature=None,\n",
       "       max_temperature=None,\n",
       "       top_p=None,\n",
       "       top_k=None),\n",
       " Model(name='models/veo-3.1-fast-generate-preview',\n",
       "       base_model_id='',\n",
       "       version='3.1',\n",
       "       display_name='Veo 3.1 fast',\n",
       "       description='Veo 3.1 fast',\n",
       "       input_token_limit=480,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['predictLongRunning'],\n",
       "       temperature=None,\n",
       "       max_temperature=None,\n",
       "       top_p=None,\n",
       "       top_k=None),\n",
       " Model(name='models/gemini-2.5-flash-native-audio-latest',\n",
       "       base_model_id='',\n",
       "       version='Gemini 2.5 Flash Native Audio Latest',\n",
       "       display_name='Gemini 2.5 Flash Native Audio Latest',\n",
       "       description='Latest release of Gemini 2.5 Flash Native Audio',\n",
       "       input_token_limit=131072,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['countTokens', 'bidiGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-2.5-flash-native-audio-preview-09-2025',\n",
       "       base_model_id='',\n",
       "       version='gemini-2.5-flash-preview-native-audio-dialog-2025-05-19',\n",
       "       display_name='Gemini 2.5 Flash Native Audio Preview 09-2025',\n",
       "       description='Gemini 2.5 Flash Native Audio Preview 09-2025',\n",
       "       input_token_limit=131072,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['countTokens', 'bidiGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-2.5-flash-native-audio-preview-12-2025',\n",
       "       base_model_id='',\n",
       "       version='12-2025',\n",
       "       display_name='Gemini 2.5 Flash Native Audio Preview 12-2025',\n",
       "       description='Gemini 2.5 Flash Native Audio Preview 12-2025',\n",
       "       input_token_limit=131072,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['countTokens', 'bidiGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "models = [m for m in genai.list_models()]\n",
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "llm = genai.GenerativeModel('models/gemini-2.5-flash')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm.generate_content(\"東北大学を紹介してください：\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "東北大学は、日本を代表する国立大学の一つであり、特に旧帝国大学の一角として、国内外から非常に高い評価を受けています。宮城県仙台市に本部を置き、その歴史、理念、研究力、そして地域との結びつきにおいて、多くの魅力を持っています。\n",
       "\n",
       "以下に東北大学の主な特徴を紹介します。\n",
       "\n",
       "---\n",
       "\n",
       "### 東北大学の主な特徴\n",
       "\n",
       "1.  **歴史と伝統：旧帝国大学の一角**\n",
       "    *   1907年に東北帝国大学として創立され、東京、京都に次ぐ3番目の帝国大学として設立されました。日本の高等教育、特に科学技術分野の発展に大きく貢献してきました。\n",
       "    *   「旧帝大」としての揺るぎない地位は、その教育・研究水準の高さを示す証でもあります。\n",
       "\n",
       "2.  **三つの基本理念：門戸開放、実学尊重、研究第一主義**\n",
       "    *   **門戸開放 (Open-Door Policy)：** 創立当初から、出身地や家柄、性別にとらわれず、意欲と能力のある学生に広く門戸を開くという革新的な理念を掲げました。実際、日本で初めて女子学生と外国人留学生を受け入れた大学の一つとしても知られています。\n",
       "    *   **実学尊重 (Emphasis on Practical Learning)：** 理論だけでなく、それが社会や産業にどう貢献できるかという「実学」を重んじています。基礎研究から応用研究まで、幅広く実践的な学問を追求しています。\n",
       "    *   **研究第一主義 (Research First Principle)：** 教育とともに研究を大学の重要な使命と位置づけ、世界をリードする独創的な研究を推進しています。この理念のもと、多くの世界的な発見や革新的な技術が生まれています。\n",
       "\n",
       "3.  **学術分野の多様性と研究力の高さ**\n",
       "    *   理学、工学、医学、薬学、農学、文学、法学、経済学、教育学など、幅広い分野にわたる学部・研究科を有し、総合大学としての強みを持っています。\n",
       "    *   特に、**材料科学、物理学、半導体関連技術、ロボット工学、医学・生命科学、災害科学**などの分野では世界的に高い研究水準を誇り、多くの研究機関（金属材料研究所、電気通信研究所、多元物質科学研究所など）が設置されています。\n",
       "    *   スパコン「青葉」など、最先端の研究設備も充実しています。\n",
       "\n",
       "4.  **国際性とグローバルな展開**\n",
       "    *   「門戸開放」の理念は、国際性にもつながっています。世界各国の大学や研究機関との連携を強化し、多くの外国人留学生を受け入れています。\n",
       "    *   国際共同研究や国際会議の開催にも積極的で、グローバルリーダーの育成にも力を入れています。\n",
       "\n",
       "5.  **「学都仙台」の中心的存在**\n",
       "    *   仙台市は「学都」として知られており、その中心に東北大学があります。地域社会との連携も深く、産学連携プロジェクト、市民向けの公開講座、地域貢献活動などを通じて、地域の活性化に貢献しています。\n",
       "    *   豊かな自然と都市機能が調和した仙台の環境は、学生にとって恵まれた学習・生活環境を提供しています。\n",
       "\n",
       "6.  **美しいキャンパスと充実した学生生活**\n",
       "    *   広大なキャンパスには、歴史的建造物と最新の研究施設が共存しています。特に青葉山キャンパスは自然豊かで、学びに集中できる環境です。\n",
       "    *   学園祭「東北大学祭」をはじめとする学生イベント、多様なサークル活動が盛んで、充実した学生生活を送ることができます。\n",
       "\n",
       "---\n",
       "\n",
       "東北大学は、その伝統に裏打ちされた高い研究力と教育力、そして常に未来を見据える革新的な精神で、国内外から優秀な人材が集まる魅力的な学府です。研究者を目指す方、社会をリードする人材になりたい方にとって、非常に魅力的な選択肢となるでしょう。"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLMプロンプトを再利用する: Prompt Templates\n",
    "\n",
    "プロンプトがモデルの出力に大きく影響することを示した。プロンプトはモデルに文脈を理解させ、クエリに対する適切な回答を生成させる助けとなります。\n",
    "\n",
    "プロンプトは一見単なる文字列であるが、どのような内容を含めるべきか、またユーザー入力に応じてどのように変化させるべきかを設計するのは容易ではない。\n",
    "\n",
    "プロンプトテンプレートは、プロンプトを作成する再現可能な方法を指します。これには、エンドユーザーから一連のパラメーターを受け取り、プロンプトを生成するテキスト文字列(テンプレート)が含まれます。\n",
    "\n",
    "テンプレートは動的パラメータを挿入する位置の定義することで、静的かつ具体的なプロンプトを生成するレシピとして利用できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"language\",\"text\"],\n",
    "    template=\"次の日本語のテキストを{language}に翻訳してください：{text}\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "次の日本語のテキストを英語に翻訳してください：東北大学は日本の東北地方にある大学です。\n"
     ]
    }
   ],
   "source": [
    "print(prompt.format(language=\"英語\", text=\"東北大学は日本の東北地方にある大学です。\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## チェーン\n",
    "\n",
    "### チェーンの基本\n",
    "\n",
    "LLM は単独でも十分に強力に機能します。 しかし、 LLM 同士を組合わせたり、ある機能に特化した他のモジュールとともに利用することで、より複雑なアプリケーションを構築することができます。 LangChain では、このような他の機能と連結するための汎用的なインターフェースとして、チェーンを提供しています。 チェーンを用いることで、LLM の利用を含む \"一連の処理\" を一つのまとまりとして扱うことができます。\n",
    "\n",
    "つまり、平易な言葉でいえば、チェーンは「複数の処理の連なり」です。 この処理の連鎖の部品となるチェーンの構成要素のことを リンク と呼びます。 リンクの一例は、LLM の呼び出しなどの基本的な処理です。さらには、その他のチェーン全体をリンクとして含むチェーンも作成できます。\n",
    "\n",
    "チェーンの代表例は、LLM とプロンプトテンプレートを組合わせて使用するための```LLMChain```です。 このチェーンを用いると、\n",
    "\n",
    "- ユーザーの入力を受け取り\n",
    "- それをPromptTemplateでフォーマットし\n",
    "- フォーマットされたレスポンスを LLM に渡す\n",
    "\n",
    "という一連の操作を一つのまとまりとして実行できます。 \n",
    "\n",
    "基本的な使い方としては、LLM や プロンプトテンプレートなどの基本要素を組み合わせて使用することが考えられます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"models/gemini-2.5-flash\", google_api_key=os.getenv(\"GOOGLE_API_KEY\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Tohoku University is a university in the Tohoku region of Japan.', additional_kwargs={}, response_metadata={'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'model_provider': 'google_genai'}, id='lc_run--019bde36-c7e5-7563-9c0b-43972be9772f-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 21, 'output_tokens': 41, 'total_tokens': 62, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 27}})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | llm\n",
    "chain.invoke(\n",
    "    {\"language\":\"英語\", \"text\":\"東北大学は日本の東北地方にある大学です。\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='東北大学是位于日本东北地方的一所大学。', additional_kwargs={}, response_metadata={'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'model_provider': 'google_genai'}, id='lc_run--019bde36-d7b9-7cb3-a333-3702e5edb5c2-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 22, 'output_tokens': 593, 'total_tokens': 615, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 582}})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"language\":\"中国語\", \"text\":\"東北大学は日本の東北地方にある大学です。\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 実装例:Few Shot Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate, FewShotPromptTemplate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\"word\": \"楽しい\", \"antonym\": \"悲しい\"},\n",
    "    {\"word\": \"高い\", \"antonym\": \"低い\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_formatter_template = \"\"\"\n",
    "Word: {word}\n",
    "Antonym: {antonym}\\n\n",
    "\"\"\"\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"word\", \"antonym\"],\n",
    "    template=example_formatter_template,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- `examples`: モデルに示す例を指定します。\n",
    "- `example_prompt`: 例をどのように提示するかを指定します。\n",
    "- `prefix`: プロンプトの前置詞を指定します。一般的には、モデルにタスクを説明するためのものです。\n",
    "- `suffix`: プロンプトの後置詞を指定します。一般的には、モデルに入力と出力の形式を示すためのものです。\n",
    "- `input_variables`: 入力の変数名を指定します。\n",
    "- `example_separator`: 例を区切るための文字列を指定します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_prompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"Give the antonym of every input\",\n",
    "    suffix=\"Word: {input}\\nAntonym:\",\n",
    "    input_variables=[\"input\"],\n",
    "    example_separator=\"\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Give the antonym of every input\n",
      "\n",
      "Word: 楽しい\n",
      "Antonym: 悲しい\n",
      "\n",
      "\n",
      "\n",
      "Word: 高い\n",
      "Antonym: 低い\n",
      "\n",
      "\n",
      "Word: 大きい\n",
      "Antonym:\n"
     ]
    }
   ],
   "source": [
    "print(few_shot_prompt.format(input=\"大きい\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGoogleGenerativeAI(model=\"models/gemini-2.5-flash\", google_api_key=os.getenv(\"GOOGLE_API_KEY\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "小さい\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "chain = few_shot_prompt | llm | StrOutputParser()\n",
    "\n",
    "var = few_shot_prompt.input_variables[0] \n",
    "print(chain.invoke({var: \"大きい\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 課題\n",
    "\n",
    "Few Shot Learningでセンチメント分析を実装しなさい。\n",
    "\n",
    "- データフレームから一部のテキストとラベル($n=5$)を抽出し、`examples`を作成します\n",
    "- Few Shot Learningためのpromptを作成します\n",
    "- chainを作成し、テストデータから任意のテキストに対するセンチメント予測結果を確認します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install datasets\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_sample = dataset[\"train\"].to_pandas().sample(5, random_state = 123)\n",
    "df_test_sample = dataset[\"test\"].to_pandas().sample(5, random_state = 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "label",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "e2253b25-f8f8-49d4-9b8e-181c6a1b57aa",
       "rows": [
        [
         "20000",
         "After reading some quite negative views for this movie, I was not sure whether I should fork out some money to rent it. However, it was a pleasant surprise. I haven't seen the original movie, but if its better than this, I'd be in heaven.<br /><br />Tom Cruise gives a strong performance as the seemingly unstable David, convincing me that he is more than a smile on legs (for only the third time in his career- the other examples were Magnolia and Born on the Fourth of July). Penelope Cruz is slightly lightweight but fills the demands for her role, as does Diaz. The only disappointment is the slightly bland Kurt Russell. In the movie, however, it is not the acting that really impresses- its the filmmaking.<br /><br />Cameron Crowe excels in the director's role, providing himself with a welcome change of pace from his usual schtick. The increasing insanity of the movie is perfectly executed by Crowe (the brief sequence where Cruise walks through an empty Time Square is incredibly effective). The soundtrack (a distinguishing feature of a Crowe movie) is also sublime.<br /><br />You will be shocked and challenged as a viewer. The plot does seem a little contrived but the issues explored behind it are endlessly discussable. The movie isn't perfect, but its a welcome change of pace for Cruise and Crowe and for those raised on a diet of Hollywood gloss, should be a revelation.",
         "1"
        ],
        [
         "5515",
         "Really no reason to examine this much further because of a few very glaring and bias misleading statements.<br /><br />A perfect example is when the filmmaker claims \"Saul\" or Paul of Tarus (the writer of The Book of Hebrews He asserts) has no idea Jesus is or was a human being, this assertion is either purposely false as he accuses others of presenting, or he is ignorant of what \"The Bible\" says.<br /><br />first we can examine his misleading claim about Hebrews 8.4; which he shows a quote \"If Jesus was on earth, he would not be a priest\", hence right here He sets up the ignorant and unlearned viewer to accept his false premise.. why? He does what most so called Bible believing people he accuses of doing, the same.. That is TAKING things out of context.<br /><br />verse one of Hebrews 8 is; 1..\"Now of the things which we have spoken this is the sum: We have such an high priest, who is set on the right hand of the throne of the Majesty in the heavens\" The context above is CLEARLY speaking of a Jesus who was on earth and ASCENDED into heaven after his alleged resurrection.<br /><br />It has nothing to do with how the filmmaker wants the viewer to take his out of context scripture. Here he offers a foundation, that \"Paul was not aware of a HUMAN Jesus, but only one in \"heaven\"<br /><br />follow?<br /><br />lets see if the filmmaker is being honest; Hebrews 7; 14. \"For it is evident that our Lord sprang out of Judah; of which tribe Moses spake nothing concerning priesthood.\"<br /><br />heh, didn't the filmmaker just quote from the writer of Hebrews trying to show the writer of that book has no knowledge of a \"Human Jesus\"? it's likely anyways Paul didn't write Hebrews, but I will not go into that here, but The film maker asserts Paul did, and that is the premise of the point given here.<br /><br />It is not like this film maker does not make decent points in certain areas, he does, but he is engaging in the same blind bias of the religion he is bashing on. Once he engages in these tactics, in my strong opinion, he loses credibility as the religion he picks out, and the film is no longer a documentary, but a personal opinion, and a bias of the film maker, nothing more, nothing less.",
         "0"
        ],
        [
         "966",
         "\"Happy Go Lovely\" has only two things going for it. And those two things are Vera-Ellen's legs. This is a British (Excelsior Films) version of an M-G-M musical complete with second tier stars. I would imagine that Vera-Ellen took this role thinking that it might finally propel her to the status of a major musical star. But, I'm sorry to say, Ms. Ellen's chance did not pay off.<br /><br />Opening with a horrible Scottish number and stumbling thru awful dialog to the next dull tune, this movie seems very heavy handed and sloppy. The predictable mistaken identity plot is very thin, and with the exception of David Niven, Cesar Romero (who is way over the top in his role of a Producer) and Bobby Howes (who is totally wasted in a nothing role) the rest of the cast is totally forgettable.<br /><br />The choreography is boring, but Ms. Ellen gives it her all. She was never as famous as most of the other musical stars(and she shouldn't be since she couldn't sing and even had a \"dancing stand in\" in several of her pictures\". But when she did dance, it was just entrancing.<br /><br />It's too bad that this film that could have made her a star did not give her the tools she needed to shine.<br /><br />4 out of 10",
         "0"
        ],
        [
         "22726",
         "This movie is the first of Miikes triad society trilogy, and the trilogy kicks of to a great start. The movies in the trilogy are only connected thematically, and these themes are actually apparent in all his films, if you look close enough. Shinjuku Triad Society is about a cop trying to prevent his kid brother from getting too involved with a rather extreme gang of outsiders, struggling their way to the top of Tokyos yakuza. The kid brother is a lawyer, and the triad gang is becoming increasingly in need of one, as the movie progresses. The movie takes place in a very harsh environment, and is therefore pretty violent and tough. Miike has done worse, but since this is a serious movie it hits you very hard. As usual there is also a lot of perverted sex, mostly homosexual in this one. The movie is in many ways a typical gangster movie, but with a great drive and true grittiness. If you've only seen Miikes far-out movies (Ichi the killer, Fudoh etc.) this is worth checking out since it is sort of a compromise between his aggressive over-the-top style displayed in those movies and his more serious side, as seen in the other films of the trilogy. And as always with Miike, there are at least two scenes in this that you'll NEVER forget (see it and figure out which ones for yourself).<br /><br />8/10",
         "1"
        ],
        [
         "2690",
         "Normally I would never rent a movie like this, because you know it's going to be bad just by looking at the box. I rented seven movies at the same time, including Nightmare on Elm Street 5, 6 and Wes Craven's New Nightmare. Unfortunately, when I got home I found out the videostore-guy gave me the wrong tape. In the box of Wes Craven's New Nightmare I found this lame movie.<br /><br />This movie is incredibly boring, the acting is bad and the plot doesn't make any sense. It's hard to write a good review, because I have no idea what the movie was really about. At the end of the movie you have more questions then answers.<br /><br />On 'Max Power's Scale of 1 to 10' I rate this movie: 1<br /><br />PS I would like to correct Corinthian's review (right below mine). He says Robert Englund is ripping off lingerie, riding horses naked, etc. The guy that did those things was Mahmoud, played by Juliano Mer, not by Robert Englund.",
         "0"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20000</th>\n",
       "      <td>After reading some quite negative views for th...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5515</th>\n",
       "      <td>Really no reason to examine this much further ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>966</th>\n",
       "      <td>\"Happy Go Lovely\" has only two things going fo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22726</th>\n",
       "      <td>This movie is the first of Miikes triad societ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2690</th>\n",
       "      <td>Normally I would never rent a movie like this,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label\n",
       "20000  After reading some quite negative views for th...      1\n",
       "5515   Really no reason to examine this much further ...      0\n",
       "966    \"Happy Go Lovely\" has only two things going fo...      0\n",
       "22726  This movie is the first of Miikes triad societ...      1\n",
       "2690   Normally I would never rent a movie like this,...      0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_sample"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyterbook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
