{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT(Generative Pretrained Transformer)はTransformerベースの言語モデルです。ChatGPT などの生成系 AI アプリケーションの基礎となっている人工知能 (AI) の重要な新技術です。GPT モデルにより、アプリケーションは人間のようにテキストやコンテンツ (画像、音楽など) を作成したり、会話形式で質問に答えたりすることができます。さまざまな業界の組織が、Q&A ボット、テキスト要約、コンテンツ生成、検索に GPT モデルと生成系 AI を使用しています。\n",
    "\n",
    "GPTはOpenAIによって定期的に新しいバージョンが公開されていますが、ここではGPT-2について解説します。\n",
    "\n",
    "![](./Figure/gpt_history.png)\n",
    "\n",
    "![](./Figure/gpt2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 入力表現"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPTの入力は、入力トークン列に対応するトークン埋め込み$e_{w_i}$と位置埋め込む$p_i$を加算した埋め込み列です。\n",
    "\n",
    "$$\n",
    "x_i=e_{w_i}+p_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 事前学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPTの事前学習タスクは、入力されたトークン列の次のトークンを予測することです。ここで、GPTはデコーダ構成のTransformerを用います。\n",
    "\n",
    "![](./Figure/gpt_input_representation.png)\n",
    "\n",
    "```{margin}\n",
    "GPTはオリジナルのTransformerにいくつかの改装を行いました。\n",
    "```\n",
    "\n",
    "学習に用いるトークン列$w_1,w_2,...,w_N$におけるのトークン$w_i$を予測することを考えます。GPTでは、予測確率を使った負の対数尤度を損失関数として事前学習を行います。\n",
    "\n",
    "$$\n",
    "\\zeta(\\theta)=- \\sum_i log P(w_i|w_{i-K},....w_{i-1},\\theta)\n",
    "$$\n",
    "\n",
    "ここで、$\\theta$はモデルに含まれるすべてのパラメータを表します。\n",
    "\n",
    "学習時にはMasked Self-Attention機構が導入され、入力トークン列の各位置において次のトークンを予測して学習が行われます。\n",
    "\n",
    "\n",
    "![](./Figure/gpt_decoder.png)\n",
    "\n",
    "学習時にはMasked Self-Attention機構が導入され、入力トークン列の各位置において次のトークンを予測して学習が行われます。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ファインチューング\n",
    "\n",
    "GPTの事前学習済みモデルに、下流タスクに合わせて変換するためのヘッドを追加し、下流タスクのデータセットを用いてモデル全体を調整します。\n",
    "\n",
    "GPTは下流タスクを解く際、特殊トークンを用いて入力テキストを拡張します。\n",
    "\n",
    "- テキスト分類のタスクにおいては、文書の最初に```<s>```、最後に```<e>```が追加されます。\n",
    "- 自然言語推論のタスクにおいては、テキストの境界に```$```が挿入されます。\n",
    "\n",
    "![](./Figure/gpt-2-autoregression-2.gif)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Huggingface transformerを使う\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "#!pip install sentencepiece\n",
    "#!pip install protobuf\n",
    "generator = pipeline(\"text-generation\", model=\"abeja/gpt2-large-japanese\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'generated_text': '東北大学は7月3日、今年7月に福島県福島市で行われる「第1回日本MRS医学会大会(仮称)」の開催準備に関する協定書に、同大会への運営協力に関する覚書を交わしたと発表した。 参加予定者の名簿は7月1日〜7月3日頃に東北大学多元物質科学研究所で閲覧できる予定。また、参加申請を希望する場合は、7月4日までに、学会事務局から学会事務局宛にE-mailで'}\n",
      "{'generated_text': '東北大学は3月10日に東京・港区の明治記念館で定例会見を行い、2020年度末までに工学部の新規設置を正式に断念することを明らかにした。 東京・港区の明治記念館で定例会見を行い、2020年度末までに工学部の新規設置を正式に断念することを明らかにした。 東北大学は2020年に設立50周年を迎えることから、2023年までに工学部を2つに再編する方針を固めた。 1つ目は「建築デザイン学部」だ。この学部は、建築や都市'}\n",
      "{'generated_text': '東北大学は、文部科学省の「職業実践専門課程(職業実践力育成プログラム)」に認定されており、「ビジネススクール」とも言われています。このビジネススクールは、各企業・団体等と連携を取りながら、職業上の知識や技術のみならず、ビジネスに対する考え方や経営センスを修得して、職業教育を行う教育プログラムです。 東北経済産業局では、本学を希望する学生の皆様の学びをより深めていただくことを目的に、「東北エリア職業実践専門'}\n"
     ]
    }
   ],
   "source": [
    "generated = generator(\n",
    "    \"東北大学は\",\n",
    "    max_length=100,\n",
    "    do_sample=True,\n",
    "    num_return_sequences=3,\n",
    "    top_p=0.95,\n",
    "    top_k=50,\n",
    "    pad_token_id=3\n",
    ")\n",
    "print(*generated, sep=\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
