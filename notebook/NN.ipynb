{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ニューラルネットワーク\n",
    "\n",
    "ニューラルネットワークは、人間の脳に似た層状構造で相互接続されたノードやニューロンを使用するの計算モデルです。\n",
    "\n",
    "ニューラルネットワークは、画像認識、自然言語処理、音声認識など、さまざまな領域で広く利用されています。特に、大量のデータと計算能力が利用可能になった近年、ディープニューラルネットワーク(DNN)の研究や応用が急速に進展しています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ニューラルネットワークの構造\n",
    "\n",
    "### パーセプトロン\n",
    "\n",
    "パーセプトロンとは、複数の入力を受け取り、重み付けして、1つの信号を出力するアルゴリズムです。\n",
    "\n",
    "例えば,$x_1$と$x_2$の2つの入力を受け取り、yを出力するパーセプトロンを考えます。\n",
    "\n",
    "- $w_1$や$w_2$は各入力の「重み」を表すパラメータで、各入力の重要性をコントロールします。\n",
    "- $b$はバイアス\n",
    "\n",
    "![](./Figure/nn1.png)\n",
    "\n",
    "パーセプトロンの「○」で表されている部分は、ニューロンやノードと呼びます。\n",
    "\n",
    "\n",
    "\n",
    "### 活性化関数\n",
    "\n",
    "活性化関数とは、ニューロンにおける、入力のなんらかの合計から、出力を決定するための関数です。\n",
    "\n",
    "例えば、関数の入力(パーセプトロンだと重み付き和)が0以下のとき0を、0より大きいとき1を出力することが考えます。\n",
    "\n",
    "$$\n",
    "y   = \\begin{cases}\n",
    "          0 \\quad (w_1 x_1 + w_2 x_2 + b \\leq 0) \\\\\n",
    "          1 \\quad (w_1 x_1 + w_2 x_2 + b > 0)\n",
    "      \\end{cases}\n",
    "$$\n",
    "\n",
    "出力に関する計算数式を分解すると、\n",
    "\n",
    "$$y   = h(a)$$\n",
    "$$h(a)\n",
    "    = \\begin{cases}\n",
    "          0 \\quad (a \\leq 0) \\\\\n",
    "          1 \\quad (a > 0)\n",
    "      \\end{cases}\n",
    "$$\n",
    "で書けます。つまり、入力の重み付き和の結果が$a$というノードになり、そして活性化関数$h()$によって$y$という出力が計算されます。\n",
    "\n",
    "![](./Figure/nn2.png)\n",
    "\n",
    "活性化関数を使うことで表現の自由度を上げて、複数のパーセプトロンを適当につなげることで、入出力間が非線形な関係でも表現できるようになります。\n",
    "\n",
    "例えば、線形変換のみで下図右の白い丸で表される観測データから$x$と$y$の関係を近似した場合、点線のような直線が得られたとします。これでは、一部のデータについてはあまりよく当てはまっていないのが分かります。\n",
    "\n",
    "しかし、もし図右の実線のような曲線を表現することができれば、両者の関係をより適切に表現することができます。\n",
    "\n",
    "![](./Figure/transform_function2.gif)\n",
    "\n",
    "活性化関数にはいくつか種類があります。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "活性化関数にはいくつか種類があり、異なる特性や用途を持っています。\n",
    "\n",
    "![](./Figure/transform_function3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ニューラルネットワークの仕組み\n",
    "\n",
    "ニューラルネットワークの仕組みは下の図で表さます。左側から、最初の層を入力層 (input layer)、最後の層を出力層 (output layer)といいます。\n",
    "\n",
    "その間にある層は中間層 （intermediate layer) もしくは隠れ層 (hidden layer) といいます。中間層において、層の数を増やすことによって、ディープニューラルネットワークを実現することができます。\n",
    "\n",
    "ニューラルネットワークは、層から層へ、値を変換していきます。 そのため、ニューラルネットワークとはこの変換がいくつも連なってできる一つの大きな関数だと考えることができます。 従って、基本的には、入力を受け取って、何か出力を返すものです。 そして、どのようなデータを入力し、どのような出力を作りたいかによって、入力層と出力層のノード数が決定されます。\n",
    "\n",
    " ここで、層と層の間にあるノード間の結合は、一つ一つが重みを持っており、上のような全結合型ニューラルネットワークの場合は、それらの重みをまとめて、一つの行列で表現します。 \n",
    "\n",
    "![](./Figure/nn4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ニューラルネットワークの計算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "それでは、下図に示す$3$層ニューラルネットワークを例として、入力から出力への計算のについて解説を行います。\n",
    "\n",
    "![](./Figure/nn_a.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 記号の説明\n",
    "\n",
    "ニューラルネットワークの計算を説明するにあたって、導入される記号の定義から始めます。\n",
    "\n",
    "入力層の$x_1$と$x_2$ニューロンから、次層のニューロン$a_1^{(1)}$への信号伝達を見ていきます。\n",
    "\n",
    "- $w_{12}^{(1)}$ は前層の$2$番目のニューロン($x_2$)から次層の$1$番目のニューロン($a_1^{(1)}$)への重みであることを意味します。\n",
    "    - 右上$(1)$は第$1$層の重みということ意味します\n",
    "    - 右下$12$ような数字の並びは、次層のニューロン($1$)と前層のニューロンのインデックス番号($2$)から構成されます\n",
    "- $a_1^{(1)}$は第$1$層$1$番目のニューロンであることを意味します。\n",
    "    - 右上$(1)$は第$1$層のニューロンということ意味します\n",
    "    - 右下$1$は$1$番目のニューロンということ意味します\n",
    "![](./Figure/nn_b.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 各層における信号伝達"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まず、入力層から「第$1$層の$1$番目のニューロン」への信号伝達を見ていきます。ここでは。バイアス項も追加し、$a_1^{(1)}$を以下の数式で計算します。\n",
    "\n",
    "![](./Figure/nn_c.png)\n",
    "\n",
    "$$\n",
    " a_1^{(1)}= w_{11}^{(1)}x_{1} + w_{12}^{(1)}x_{2} + b_1^{(1)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同じ形で、第$1$層におけるすべでのニューロンの計算式を書けます。\n",
    "$$\n",
    "\\begin{split}\\begin{cases}\n",
    "    a_1^{(1)} = w_{11}^{(1)}x_{1} + w_{12}^{(1)}x_{1}x_{2} + b_1^{(1)} \\\\\n",
    "    a_2^{(1)} = w_{21}^{(1)}x_{1} + w_{22}^{(1)}x_{1}x_{2} + b_2^{(1)} \\\\\n",
    "    a_3^{(1)} = w_{31}^{(1)}x_{1} + w_{32}^{(1)}x_{1}x_{2} + b_3^{(1)}\n",
    "\\end{cases}\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "行列で第$1$層におけるニューロンの計算式をまとめて表すことができます。\n",
    "- 入力 $\\mathbf{X}=\\begin{pmatrix} x_1 & x_2 \\end{pmatrix}$\n",
    "- バイアス $\\mathbf{B} = \\begin{pmatrix} b_{1}^{(1)} & b_{2}^{(1)} & b_{3}^{(1)} \\end{pmatrix}$\n",
    "- 重み $$\\begin{split} \\mathbf{W} = \\begin{pmatrix}\n",
    "    w_{11}^{(1)} & w_{21}^{(1)} & w_{31}^{(1)} \\\\\n",
    "    w_{12}^{(1)} & w_{22}^{(1)} & w_{32}^{(1)}\n",
    "\\end{pmatrix}\\end{split}$$\n",
    "\n",
    "- 入力・バイアスと重みの総和: $\\mathbf{A} = \\begin{pmatrix}\n",
    "    a_1^{(1)} & a_2^{(1)} & a_3^{(1)}\n",
    "\\end{pmatrix}$\n",
    "$$\n",
    "\\mathbf{A}^{(1)}\n",
    "     = \\mathbf{X} \\mathbf{W}^{(1)} + \\mathbf{B}^{(1)}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "さらに、活性化関数を導入します。入力・バイアスと重みの総和を$a$で表し、活性化関数$h()$による変換された結果を$z$で表すことにします。\n",
    "![](./Figure/nn_d.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数値を見ながら計算の流れを確認\n",
    "\n",
    "それでは、```NumPy```の多次元配列を使って、入力 $x_1$,$x_2$,$x_3$から出力が計算される過程を確認してみましょう。入力、重み、バイアスは適当な値を設定します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "入力の形状: (2,)\n",
      "重みの形状: (2, 3)\n",
      "バイアスの形状: (3,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "X = np.array([1.0, 0.5])\n",
    "W1 = np.array([[0.1, 0.3, 0.5],[0.2, 0.4, 0.6]])\n",
    "B1 = np.array([0.1, 0.2, 0.3])\n",
    "print(r\"入力の形状: {}\".format(X.shape))\n",
    "print(r\"重みの形状: {}\".format(W1.shape))\n",
    "print(r\"バイアスの形状: {}\".format(B1.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一層隠れ層で重み付きとバイアスの総和を計算し、活性化関数で変換された結果を返します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "A1 = np.dot(X, W1) + B1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z1 = sigmoid(A1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "続いて、同じ形で第1層から第2層目への信号伝達を行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2 = np.array([[0.1, 0.4],[0.2, 0.5],[0.3, 0.6]])\n",
    "B2 = np.array([0.1, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "A2 = np.dot(Z1, W2) + B2\n",
    "Z2 = sigmoid(A2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最後に、第2層から出力層への信号を行います。出力層の活性化関数は、恒等関数を用います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "W3 = np.array([[0.1, 0.3],[0.2, 0.4]])\n",
    "B3 = np.array([0.1, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "A3 = np.dot(Z2, W3) + B3 # Y = A3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ニューラルネットワークの学習\n",
    "\n",
    "重回帰分析では、最小二乗法などの推定方法で行列計算や微分方程式を用いて解を導出することができます。つまり、実際の数値を使うことなく変数のまま、解（最適なパラメータ）を求めることができました。このように、変数のままで解を求めることを解析的に解くと言い、その答えのことを解析解 (analytical solution) と呼びます。\n",
    "\n",
    "しかし、ニューラルネットワークで表現されるような複雑な関数の場合、パラメータの数は数億二及ぶこともありますので、最適解を解析的に解くことはほとんどの場合困難です。そのため、別の方法を考える必要があります。具体的には、解析的に解く方法に対し、計算機を使って繰り返し数値計算を行って解を求めることを数値的に解くといい、求まった解は数値解 (numerical solution) と呼ばれます。\n",
    "\n",
    "ニューラルネットワークでは、基本的に数値的な手法によって最適なパラメータを求めます。\n",
    "\n",
    "### 損失関数\n",
    "\n",
    "損失関数（Loss function）とは、「正解値」と、モデルによる出力された「予測値」とのズレの大きさ（これを「Loss：損失」と呼ぶ）を計算するための関数です。損失関数の値は、学習アルゴリズムがモデルのパラメータを調整する際の指標となります。\n",
    "\n",
    "#### 平均二乗誤差\n",
    "\n",
    "平均二乗誤差 (mean squared error) は、回帰問題を解きたい場合によく用いられる目的関数です。 重回帰分析の解説中に紹介した二乗和誤差と似ていますが、各データ点における誤差の総和をとるだけでなく、それをデータ数で割って、誤差の平均値を計算している点が異なります。\n",
    "\n",
    "$$\n",
    "L = \\frac{1}{N} \\sum_{n=1}^N (t_n - y_n)^2\n",
    "$$\n",
    "\n",
    "ここで、$N$はサンプルサイズ、$y_n$は$n$個目のデータに対するニューラルネットワークの出力値、$t_n$は$n$個目のデータに対する望ましい正解の値です。\n",
    "\n",
    "#### 交差エントロピー\n",
    "\n",
    "交差エントロピー (cross entropy) は、分類問題を解きたい際によく用いられる目的関数です。 \n",
    "\n",
    "例として、$K$クラスの分類問題を考えてみましょう。 ある入力$x$が与えられたとき、ニューラルネットワークの出力層に$K$個のノードがあり、それぞれがこの入力が$k$番目のクラスに属する確率\n",
    "\n",
    "$$\n",
    "y_k = p(y=k|x)\n",
    "$$\n",
    "\n",
    "を表しているとします。 これは、入力$x$が与えられたという条件のもとで、予測クラスを意味する$y$が$k$であるような確率、を表す条件付き確率です。\n",
    "\n",
    "ここで、$x$が所属するクラスの正解が、\n",
    "\n",
    "$$\n",
    "{\\bf t} = \\begin{bmatrix} t_1 & t_2 & \\dots & t_K \\end{bmatrix}^{\\rm T}\n",
    "$$\n",
    "\n",
    "というベクトルで与えられているとします。 ただし、このベクトルは$t_k (k=1,2,...,K)$ のいずれか一つだけが$1$であり、それ以外は$0$であるようなベクトルであるとします。\n",
    "\n",
    "そして、この一つだけ値が$1$となっている要素は、その要素のインデックスに対応したクラスが正解であることを意味します。\n",
    "\n",
    "以上を用いて、交差エントロピーは以下のように定義されます。\n",
    "\n",
    "$$\n",
    "- \\sum_{k=1}^{K}t_{k}\\log y_{k}\n",
    "$$\n",
    "\n",
    "これは、$t_k$が $k=1,2,...,K$ のうち正解クラスである一つの$k$の値でだけ$1$となるので、正解クラスであるような$k$での$\\log y_{k}$を取り出して$−1$を掛けているのと同じです。 また、$N$個すべてのサンプルを考慮すると、交差エントロピーは、\n",
    "\n",
    "$$\n",
    "L = - \\sum_{n=1}^{N} \\sum_{k=1}^{K}t_{n, k}\\log y_{n, k}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 損失関数の最適化：勾配法\n",
    "\n",
    "下の図は，パラメータ$w$を変化させた際の損失関数$L$の値を表しています。損失関数の値を最小にするようなパラメータの値を求めることで、ニューラルネットワークを訓練します。ただ、実際のニューラルネットワークの目的関数は、多次元で、かつもっと複雑な形をしていることがほとんどです。 そこで、勾配を利用して関数の最小値を探す勾配法がよく用いられます。\n",
    "\n",
    "\n",
    "![](./Figure/loss_fucnction.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "勾配は、各地点における関数の傾きであり、関数の値が最も急速に変化する方向と大きさを示します。\n",
    "\n",
    "\n",
    "今は$L$の値を小さくしたいわけです。勾配の反対方向に進むことで関数の値を最も減らせることができますので、勾配の情報を手がかりに、できるだけ小さな値となる関数の場所を探します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今は$L$の値を小さくしたいわけです。勾配の反対方向に進むことで関数の値を最も減らせることができますので、勾配の情報を手がかりに、できるだけ小さな値となる関数の場所を探します。。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{note}\n",
    "ニューラルネットワークは、分類問題と回帰問題の両方に用いることができます。ただし、分類問題と回帰問題のどちらに用いるかで、出力層の活性化関数を変更する必要があります。\n",
    "\n",
    "回帰問題では恒等関数を使います。\n",
    "\n",
    "分類問題の場合は、クラス数と同じだけのノードを出力層に用意しておき、各ノードがあるクラスに入力が属する確率を表すようにします。 このため、全出力ノードの値の合計が$1$になるよう正規化します。 これには、要素ごとに適用される活性化関数ではなく、層ごとに活性値を計算する別の関数を用いる必要があります。 そのような目的に使用される代表的な関数には、ソフトマックス関数があります。\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
