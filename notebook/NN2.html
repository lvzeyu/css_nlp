
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>ニューラルネットワーク &#8212; 計算社会科学のための自然言語処理</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=bd9e20870c6007c4c509" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=bd9e20870c6007c4c509" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=bd9e20870c6007c4c509" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.1/css/all.min.css?digest=bd9e20870c6007c4c509" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.1/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.1/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.1/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=384b581d" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css?v=0a3b3ea7" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=bd9e20870c6007c4c509" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=bd9e20870c6007c4c509" />
  <script src="../_static/vendor/fontawesome/6.5.1/js/all.min.js?digest=bd9e20870c6007c4c509"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=888ff710"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=efea14e4"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=36754332"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'notebook/NN2';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="誤差逆伝播法" href="backpropagation.html" />
    <link rel="prev" title="数学基礎" href="math_basis2.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a id="pst-skip-link" class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <header>
  
    <div class="bd-header navbar navbar-expand-lg bd-navbar">
    </div>
  
  </header>

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/tohoku-university-logo-vector.svg" class="logo__image only-light" alt="計算社会科学のための自然言語処理 - Home"/>
    <script>document.write(`<img src="../_static/tohoku-university-logo-vector.svg" class="logo__image only-dark" alt="計算社会科学のための自然言語処理 - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    計算社会科学と自然言語処理
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">イントロダクション</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="introduction.html">ガイダンス</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">基礎知識</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="nlp_basis2.html">自然言語処理の基礎</a></li>
<li class="toctree-l1"><a class="reference internal" href="ml_basis2.html">機械学習の基本概念</a></li>
<li class="toctree-l1"><a class="reference internal" href="math_basis2.html">数学基礎</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">ニューラルネットワーク</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="NN.html">ニューラルネットワーク</a></li>
<li class="toctree-l1"><a class="reference internal" href="backpropagation.html">誤差逆伝播法</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">PyTorch</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="pytorch.html">Pytorch</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">単語分散表現</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="word2vec_1.html">単語分散表現</a></li>
<li class="toctree-l1"><a class="reference internal" href="word2vec_2_embedding.html">word2vec</a></li>
<li class="toctree-l1"><a class="reference internal" href="word2vec_gensim.html">GensimによるWord2Vecの学習と使用</a></li>
<li class="toctree-l1"><a class="reference internal" href="word2vec_sentiment.html">Word2Vecを用いるセンチメント分析</a></li>
<li class="toctree-l1"><a class="reference internal" href="word2vec_application.html">Word2Vecが人文・社会科学研究における応用</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">RNN</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="rnn.html">RNNの基礎</a></li>
<li class="toctree-l1"><a class="reference internal" href="lstm.html">LSTM</a></li>
<li class="toctree-l1"><a class="reference internal" href="pytorch_lstm.html">LSTMの実装</a></li>
<li class="toctree-l1"><a class="reference internal" href="lstm_classification.html">LSTMによる文書分類</a></li>
<li class="toctree-l1"><a class="reference internal" href="seq2seq.html">Seq2seq</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Transformer</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="attention.html">Attention</a></li>
<li class="toctree-l1"><a class="reference internal" href="self-attention.html">Self-Attention</a></li>
<li class="toctree-l1"><a class="reference internal" href="transformer.html">Transformerアーキテクチャ</a></li>
<li class="toctree-l1"><a class="reference internal" href="BERT.html">BERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="bert_sentiment.html">BERTによるセンチメント分析</a></li>
<li class="toctree-l1"><a class="reference internal" href="bert_topic.html">BERTopic</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">大規模言語モデル</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="GPT.html">GPT</a></li>
<li class="toctree-l1"><a class="reference internal" href="llm.html">大規模言語モデル</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/lvzeyu/css_nlp/blob/master/notebook/NN2.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onColab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/lvzeyu/css_nlp" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://colab.research.google.com/github/lvzeyu/css_nlp/issues/new?title=Issue%20on%20page%20%2Fnotebook/NN2.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/notebook/NN2.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>ニューラルネットワーク</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">ニューラルネットワークの構造</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">パーセプトロン</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">活性化関数</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">ニューラルネットワークの仕組み</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">ニューラルネットワークの計算</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">記号の説明</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">各層における信号伝達</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id9">数値を見ながら計算の流れを確認</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id10">出力層の設計</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id11">ソフトマックス関数</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id12">ニューラルネットワークの学習</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id13">損失関数</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id14">平均二乗誤差</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id15">交差エントロピー</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id16">損失関数の最適化</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id17">勾配法</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id18">勾配下降法の実装</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#f-x-x-2"><span class="math notranslate nohighlight">\(f(x)=x^2\)</span>に対する最適化</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#example-f-x-0-x-1-x-0-2-x-1-2">Example: <span class="math notranslate nohighlight">\(f(x_0,x_1)=x_0^2+x_1^2\)</span>に対する最適化</a></li>
</ul>
</li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id19">ニューラルネットワークに対する勾配</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id20">2層ニューラルネットワークの実装</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id21">数式の確認</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id22">入力層</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id23">隠れ層</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id24">出力層</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id25">損失の計算</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id26">勾配の計算</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id27">パラメータの更新</a></li>
</ul>
</li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="id1">
<h1>ニューラルネットワーク<a class="headerlink" href="#id1" title="Link to this heading">#</a></h1>
<p>ニューラルネットワークは、人間の脳に似た層状構造で相互接続されたノードやニューロンを使用するの計算モデルです。</p>
<p>ニューラルネットワークは、画像認識、自然言語処理、音声認識など、さまざまな領域で広く利用されています。特に、大量のデータと計算能力が利用可能になった近年、ディープニューラルネットワーク(DNN)の研究や応用が急速に進展しています。</p>
<p><img alt="" src="../_images/NN_history.png" /></p>
<section id="id2">
<h2>ニューラルネットワークの構造<a class="headerlink" href="#id2" title="Link to this heading">#</a></h2>
<section id="id3">
<h3>パーセプトロン<a class="headerlink" href="#id3" title="Link to this heading">#</a></h3>
<p>パーセプトロンとは、複数の入力を受け取り、重み付けして、1つの信号を出力するアルゴリズムです。</p>
<p>例えば,<span class="math notranslate nohighlight">\(x_1\)</span>と<span class="math notranslate nohighlight">\(x_2\)</span>の2つの入力を受け取り、yを出力するパーセプトロンを考えます。</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(w_1\)</span>や<span class="math notranslate nohighlight">\(w_2\)</span>は各入力の「重み」を表すパラメータで、各入力の重要性をコントロールします。</p></li>
<li><p><span class="math notranslate nohighlight">\(b\)</span>はバイアス</p></li>
</ul>
<figure class="align-center" id="id28">
<a class="reference internal image-reference" href="../_images/nn1.png"><img alt="../_images/nn1.png" src="../_images/nn1.png" style="width: 385.0px; height: 357.5px;" /></a>
<figcaption>
<p><span class="caption-text">パーセプトロン</span><a class="headerlink" href="#id28" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>パーセプトロンの「○」で表されている部分は、ニューロンやノードと呼びます。</p>
</section>
<section id="id4">
<h3>活性化関数<a class="headerlink" href="#id4" title="Link to this heading">#</a></h3>
<p>活性化関数はニューラルネットワークの各層において、入力データに対して非線形性を導入するために使用される関数です。</p>
<p>例えば、関数の入力(パーセプトロンだと重み付き和)が0以下のとき0を、0より大きいとき1を出力することが考えます。</p>
<div class="math notranslate nohighlight">
\[\begin{split}
y   = \begin{cases}
          0 \quad (w_1 x_1 + w_2 x_2 + b \leq 0) \\
          1 \quad (w_1 x_1 + w_2 x_2 + b &gt; 0)
      \end{cases}
\end{split}\]</div>
<p>出力に関する計算数式を分解すると、</p>
<div class="math notranslate nohighlight">
\[y   = h(a)\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}h(a)
    = \begin{cases}
          0 \quad (a \leq 0) \\
          1 \quad (a &gt; 0)
      \end{cases}
\end{split}\]</div>
<p>で書けます。つまり、入力の重み付き和の結果が<span class="math notranslate nohighlight">\(a\)</span>というノードになり、そして活性化関数<span class="math notranslate nohighlight">\(h()\)</span>によって<span class="math notranslate nohighlight">\(y\)</span>という出力が計算されます。</p>
<figure class="align-center" id="id29">
<a class="reference internal image-reference" href="../_images/nn2.png"><img alt="../_images/nn2.png" src="../_images/nn2.png" style="width: 575.0px; height: 357.5px;" /></a>
<figcaption>
<p><span class="caption-text">活性化関数があるパーセプトロン</span><a class="headerlink" href="#id29" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>活性化関数の主な役割は、入力の加重和に非線形な変換を加えることで、ニューラルネットワークが複雑なパターンを学習できるようにすることです。</p>
<p>例えば、線形変換のみで下図右の丸で表される観測データから<span class="math notranslate nohighlight">\(x\)</span>と<span class="math notranslate nohighlight">\(y\)</span>の関係を近似した場合、点線のような直線が得られたとします。これでは、一部のデータについてはあまりよく当てはまっていないのが分かります。</p>
<p>しかし、もし図右の実線のような曲線を表現することができれば、両者の関係をより適切に表現することができます。</p>
<p><img alt="" src="../_images/transform_function2.gif" /></p>
<p>活性化関数にはいくつか種類があり、異なる特性や用途を持っています。</p>
<ul class="simple">
<li><p>シグモイド関数</p>
<ul>
<li><p>任意の値を<span class="math notranslate nohighlight">\(0\)</span>から<span class="math notranslate nohighlight">\(1\)</span>に変換します</p></li>
</ul>
</li>
<li><p>ReLU</p>
<ul>
<li><p>負の入力は0として、0もしくは正の入力はそのまま出力</p></li>
</ul>
</li>
</ul>
<figure class="align-center" id="id30">
<img alt="../_images/transform_function3.png" src="../_images/transform_function3.png" />
<figcaption>
<p><span class="caption-text">活性化関数の種類</span><a class="headerlink" href="#id30" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="id5">
<h3>ニューラルネットワークの仕組み<a class="headerlink" href="#id5" title="Link to this heading">#</a></h3>
<p>ニューラルネットワークの仕組みは下の図で表さます。左側から、最初の層を入力層 (input layer)、最後の層を出力層 (output layer)といいます。</p>
<p>その間にある層は中間層 （intermediate layer) もしくは隠れ層 (hidden layer) といいます。中間層において、層の数を増やすことによって、ディープニューラルネットワークを実現することができます。</p>
<p>ニューラルネットワークは、層から層へ、値を変換していきます。 そのため、ニューラルネットワークとはこの変換がいくつも連なってできる一つの大きな関数だと考えることができます。 従って、基本的には、入力を受け取って、何か出力を返すものです。 そして、どのようなデータを入力し、どのような出力を作りたいかによって、入力層と出力層のノード数が決定されます。</p>
<p><img alt="" src="../_images/nn4.png" /></p>
</section>
</section>
<section id="id6">
<h2>ニューラルネットワークの計算<a class="headerlink" href="#id6" title="Link to this heading">#</a></h2>
<p>それでは、下図に示す<span class="math notranslate nohighlight">\(3\)</span>層ニューラルネットワークを例として、入力から出力への計算のについて解説を行います。</p>
<p><img alt="" src="../_images/nn_a.png" /></p>
<section id="id7">
<h3>記号の説明<a class="headerlink" href="#id7" title="Link to this heading">#</a></h3>
<p>ニューラルネットワークの計算を説明するにあたって、導入される記号の定義から始めます。</p>
<p>入力層の<span class="math notranslate nohighlight">\(x_1\)</span>と<span class="math notranslate nohighlight">\(x_2\)</span>ニューロンから、次層のニューロン<span class="math notranslate nohighlight">\(a_1^{(1)}\)</span>への信号伝達を見ていきます。</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(w_{12}^{(1)}\)</span> は前層の<span class="math notranslate nohighlight">\(2\)</span>番目のニューロン(<span class="math notranslate nohighlight">\(x_2\)</span>)から次層の<span class="math notranslate nohighlight">\(1\)</span>番目のニューロン(<span class="math notranslate nohighlight">\(a_1^{(1)}\)</span>)への重みであることを意味します。</p>
<ul>
<li><p>右上<span class="math notranslate nohighlight">\((1)\)</span>は第<span class="math notranslate nohighlight">\(1\)</span>層の重みということ意味します</p></li>
<li><p>右下<span class="math notranslate nohighlight">\(12\)</span>ような数字の並びは、次層のニューロン(<span class="math notranslate nohighlight">\(1\)</span>)と前層のニューロンのインデックス番号(<span class="math notranslate nohighlight">\(2\)</span>)から構成されます</p></li>
</ul>
</li>
<li><p><span class="math notranslate nohighlight">\(a_1^{(1)}\)</span>は第<span class="math notranslate nohighlight">\(1\)</span>層<span class="math notranslate nohighlight">\(1\)</span>番目のニューロンであることを意味します。</p>
<ul>
<li><p>右上<span class="math notranslate nohighlight">\((1)\)</span>は第<span class="math notranslate nohighlight">\(1\)</span>層のニューロンということ意味します</p></li>
<li><p>右下<span class="math notranslate nohighlight">\(1\)</span>は<span class="math notranslate nohighlight">\(1\)</span>番目のニューロンということ意味します
<img alt="" src="../_images/nn_b.png" /></p></li>
</ul>
</li>
</ul>
</section>
<section id="id8">
<h3>各層における信号伝達<a class="headerlink" href="#id8" title="Link to this heading">#</a></h3>
<p>まず、入力層から「第<span class="math notranslate nohighlight">\(1\)</span>層の<span class="math notranslate nohighlight">\(1\)</span>番目のニューロン」への信号伝達を見ていきます。ここでは。バイアス項も追加し、<span class="math notranslate nohighlight">\(a_1^{(1)}\)</span>を以下の数式で計算します。</p>
<p><img alt="" src="../_images/nn_c.png" /></p>
<div class="math notranslate nohighlight">
\[
 a_1^{(1)}= w_{11}^{(1)}x_{1} + w_{12}^{(1)}x_{2} + b_1^{(1)}
\]</div>
<p>同じ形で、第<span class="math notranslate nohighlight">\(1\)</span>層におけるすべでのニューロンの計算式を書けます。</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{split}\begin{cases}
    a_1^{(1)} = w_{11}^{(1)}x_{1} + w_{12}^{(1)}x_{1}x_{2} + b_1^{(1)} \\
    a_2^{(1)} = w_{21}^{(1)}x_{1} + w_{22}^{(1)}x_{1}x_{2} + b_2^{(1)} \\
    a_3^{(1)} = w_{31}^{(1)}x_{1} + w_{32}^{(1)}x_{1}x_{2} + b_3^{(1)}
\end{cases}\end{split}
\end{split}\]</div>
<p>行列で第<span class="math notranslate nohighlight">\(1\)</span>層におけるニューロンの計算式をまとめて表すことができます。</p>
<ul class="simple">
<li><p>入力 <span class="math notranslate nohighlight">\(\mathbf{X}=\begin{pmatrix} x_1 &amp; x_2 \end{pmatrix}\)</span></p></li>
<li><p>バイアス <span class="math notranslate nohighlight">\(\mathbf{B} = \begin{pmatrix} b_{1}^{(1)} &amp; b_{2}^{(1)} &amp; b_{3}^{(1)} \end{pmatrix}\)</span></p></li>
<li><p>重み</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{split} \mathbf{W} = \begin{pmatrix}
    w_{11}^{(1)} &amp; w_{21}^{(1)} &amp; w_{31}^{(1)} \\
    w_{12}^{(1)} &amp; w_{22}^{(1)} &amp; w_{32}^{(1)}
\end{pmatrix}\end{split}
\end{split}\]</div>
<ul class="simple">
<li><p>入力・バイアスと重みの総和: <span class="math notranslate nohighlight">\(\mathbf{A} = \begin{pmatrix}
  a_1^{(1)} &amp; a_2^{(1)} &amp; a_3^{(1)}
\end{pmatrix}\)</span></p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\mathbf{A}^{(1)}
     = \mathbf{X} \mathbf{W}^{(1)} + \mathbf{B}^{(1)}
\]</div>
<p>さらに、活性化関数を導入します。入力・バイアスと重みの総和を<span class="math notranslate nohighlight">\(a\)</span>で表し、活性化関数<span class="math notranslate nohighlight">\(h()\)</span>による変換された結果を<span class="math notranslate nohighlight">\(z\)</span>で表すことにします。
<img alt="" src="../_images/nn_d.png" /></p>
</section>
<section id="id9">
<h3>数値を見ながら計算の流れを確認<a class="headerlink" href="#id9" title="Link to this heading">#</a></h3>
<p>それでは、<code class="docutils literal notranslate"><span class="pre">NumPy</span></code>の多次元配列を使って、入力 <span class="math notranslate nohighlight">\(x_1\)</span>,<span class="math notranslate nohighlight">\(x_2\)</span>,<span class="math notranslate nohighlight">\(x_3\)</span>から出力が計算される過程を確認してみましょう。入力、重み、バイアスは適当な値を設定します。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">])</span>
<span class="n">W1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">]])</span>
<span class="n">B1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;入力の形状: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;重みの形状: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">W1</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;バイアスの形状: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">B1</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>入力の形状: (2,)
重みの形状: (2, 3)
バイアスの形状: (3,)
</pre></div>
</div>
</div>
</div>
<p>第一層隠れ層で重み付きとバイアスの総和を計算し、活性化関数で変換された結果を返します。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">A1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">W1</span><span class="p">)</span> <span class="o">+</span> <span class="n">B1</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Z1</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">A1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>続いて、同じ形で第1層から第2層目への信号伝達を行います。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">W2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">],[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],[</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">]])</span>
<span class="n">B2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">A2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Z1</span><span class="p">,</span> <span class="n">W2</span><span class="p">)</span> <span class="o">+</span> <span class="n">B2</span>
<span class="n">Z2</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">A2</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>最後に、第2層から出力層への信号を行います。出力層の活性化関数は、恒等関数を用います。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">W3</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">],[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">]])</span>
<span class="n">B3</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">A3</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Z2</span><span class="p">,</span> <span class="n">W3</span><span class="p">)</span> <span class="o">+</span> <span class="n">B3</span> <span class="c1"># Y = A3</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="id10">
<h3>出力層の設計<a class="headerlink" href="#id10" title="Link to this heading">#</a></h3>
<p>ニューラルネットワークは、分類問題と回帰問題の両方に用いることができます。ただし、分類問題と回帰問題のどちらに用いるかで、出力層の活性化関数を変更する必要があります。</p>
<ul class="simple">
<li><p>回帰問題</p>
<ul>
<li><p>モデルの出力は連続的な実数値でなければなりません。そのため、出力層に活性化関数を使用しないか、出力の範囲を制限しない活性化関数を使用します。</p></li>
</ul>
</li>
<li><p>分類問題</p>
<ul>
<li><p>分類問題では、モデルの出力を特定のクラスに分類するため、出力は確率として解釈できる必要があります。一般的には クラス数と同じだけのノードを出力層に用意しておき、各ノードがあるクラスに入力が属する確率を表すようにします。 このため、全出力ノードの値の合計が<span class="math notranslate nohighlight">\(1\)</span>になるよう正規化します。 これには、要素ごとに適用される活性化関数ではなく、層ごとに活性値を計算する別の関数を用いる必要があります。 そのような目的に使用される代表的な関数には、ソフトマックス関数があります。</p></li>
</ul>
</li>
</ul>
<section id="id11">
<h4>ソフトマックス関数<a class="headerlink" href="#id11" title="Link to this heading">#</a></h4>
<p>ソフトマックス関数は複数値からなるベクトルを入力し、それを正規化したベクトルを出力します。ソフトマックス関数は、次の式で定義されます。</p>
<div class="math notranslate nohighlight">
\[
y_k = \frac{\exp(a_k)}{\sum_{k'=0}^{K-1} \exp(a_{k'})}
\]</div>
<p><span class="math notranslate nohighlight">\(K\)</span>個の要素<span class="math notranslate nohighlight">\(\mathbf{a} = (a_0, a_1, \cdots, a_{K-1})\)</span>を入力して、<span class="math notranslate nohighlight">\(0 \leq y_k \leq 1\)</span>、<span class="math notranslate nohighlight">\(\sum_{k=0}^{K-1} y_k = 1\)</span>となる<span class="math notranslate nohighlight">\(\mathbf{y} = (y_0, y_1, \cdots, y_{K-1})\)</span>を出力します。つまり、ソフトマックス関数を適用することで、各成分は区間 <span class="math notranslate nohighlight">\((0, 1)\)</span> に収まり、全ての成分の和が <span class="math notranslate nohighlight">\(1\)</span> になるため、「確率」として解釈できるようになります。</p>
<p>実装の際、指数関数の計算のため容易に大きな値になり、計算結果は<code class="docutils literal notranslate"><span class="pre">inf</span></code>が返ってきますので、数値が不安定になってしまう「オーバーフロー」問題を対応するため。入力の最大値を引くことで、正しく計算するようにする方法が採用されています。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="k">def</span> <span class="nf">softmax</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">exp_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">c</span><span class="p">)</span>
    <span class="n">sum_exp_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">exp_x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">exp_x</span> <span class="o">/</span> <span class="n">sum_exp_x</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;活性化関数に適用する前に: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">A3</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;最終出力: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">softmax</span><span class="p">(</span><span class="n">A3</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>活性化関数に適用する前に: [0.31682708 0.69627909]
最終出力: [0.40625907 0.59374093]
</pre></div>
</div>
</div>
</div>
</section>
</section>
</section>
<section id="id12">
<h2>ニューラルネットワークの学習<a class="headerlink" href="#id12" title="Link to this heading">#</a></h2>
<p>重回帰分析では、最小二乗法などの推定方法で行列計算や微分方程式を用いて解を導出することができます。つまり、実際の数値を使うことなく変数のまま、解（最適なパラメータ）を求めることができました。このように、変数のままで解を求めることを解析的に解くと言い、その答えのことを解析解 (analytical solution) と呼びます。</p>
<p>しかし、ニューラルネットワークで表現されるような複雑な関数の場合、パラメータの数は数億二及ぶこともありますので、最適解を解析的に解くことはほとんどの場合困難です。そのため、別の方法を考える必要があります。具体的には、解析的に解く方法に対し、計算機を使って繰り返し数値計算を行って解を求めることを数値的に解くといい、求まった解は<strong>数値解</strong> (numerical solution) と呼ばれます。</p>
<p>ニューラルネットワークでは、基本的に数値的な手法によって最適なパラメータを求めます。</p>
<section id="id13">
<h3>損失関数<a class="headerlink" href="#id13" title="Link to this heading">#</a></h3>
<p>損失関数（Loss function）とは、「正解値」と、モデルによる出力された「予測値」とのズレの大きさ（これを「Loss：損失」と呼ぶ）を計算するための関数です。損失関数の値は、学習アルゴリズムがモデルのパラメータを調整する際の指標となります。</p>
<section id="id14">
<h4>平均二乗誤差<a class="headerlink" href="#id14" title="Link to this heading">#</a></h4>
<p>平均二乗誤差 (mean squared error) は、回帰問題を解きたい場合によく用いられる目的関数です。 重回帰分析の解説中に紹介した二乗和誤差と似ていますが、各データ点における誤差の総和をとるだけでなく、それをデータ数で割って、誤差の平均値を計算している点が異なります。</p>
<div class="math notranslate nohighlight">
\[
L = \frac{1}{N} \sum_{n=1}^N (t_n - y_n)^2
\]</div>
<p>ここで、<span class="math notranslate nohighlight">\(N\)</span>はサンプルサイズ、<span class="math notranslate nohighlight">\(y_n\)</span>は<span class="math notranslate nohighlight">\(n\)</span>個目のデータに対するニューラルネットワークの出力値、<span class="math notranslate nohighlight">\(t_n\)</span>は<span class="math notranslate nohighlight">\(n\)</span>個目のデータに対する望ましい正解の値です。</p>
</section>
<section id="id15">
<h4>交差エントロピー<a class="headerlink" href="#id15" title="Link to this heading">#</a></h4>
<p>交差エントロピー (cross entropy) は、分類問題を解きたい際によく用いられる目的関数です。</p>
<p>例として、<span class="math notranslate nohighlight">\(K\)</span>クラスの分類問題を考えてみましょう。 ある入力<span class="math notranslate nohighlight">\(x\)</span>が与えられたとき、ニューラルネットワークの出力層に<span class="math notranslate nohighlight">\(K\)</span>個のノードがあり、それぞれがこの入力が<span class="math notranslate nohighlight">\(k\)</span>番目のクラスに属する確率</p>
<div class="math notranslate nohighlight">
\[
y_k = p(y=k|x)
\]</div>
<p>を表しているとします。 これは、入力<span class="math notranslate nohighlight">\(x\)</span>が与えられたという条件のもとで、予測クラスを意味する<span class="math notranslate nohighlight">\(y\)</span>が<span class="math notranslate nohighlight">\(k\)</span>であるような確率、を表す条件付き確率です。</p>
<p>ここで、<span class="math notranslate nohighlight">\(x\)</span>が所属するクラスの正解が、</p>
<div class="math notranslate nohighlight">
\[
{\bf t} = \begin{bmatrix} t_1 &amp; t_2 &amp; \dots &amp; t_K \end{bmatrix}^{\rm T}
\]</div>
<p>というベクトルで与えられているとします。 ただし、このベクトルは<span class="math notranslate nohighlight">\(t_k (k=1,2,...,K)\)</span> のいずれか一つだけが<span class="math notranslate nohighlight">\(1\)</span>であり、それ以外は<span class="math notranslate nohighlight">\(0\)</span>であるようなベクトルであるとします。</p>
<p>そして、この一つだけ値が<span class="math notranslate nohighlight">\(1\)</span>となっている要素は、その要素のインデックスに対応したクラスが正解であることを意味します。</p>
<p>以上を用いて、交差エントロピーは以下のように定義されます。</p>
<div class="math notranslate nohighlight">
\[
- \sum_{k=1}^{K}t_{k}\log y_{k}
\]</div>
<p>これは、<span class="math notranslate nohighlight">\(t_k\)</span>が <span class="math notranslate nohighlight">\(k=1,2,...,K\)</span> のうち正解クラスである一つの<span class="math notranslate nohighlight">\(k\)</span>の値でだけ<span class="math notranslate nohighlight">\(1\)</span>となるので、正解クラスであるような<span class="math notranslate nohighlight">\(k\)</span>での<span class="math notranslate nohighlight">\(\log y_{k}\)</span>を取り出して<span class="math notranslate nohighlight">\(−1\)</span>を掛けているのと同じです。 また、<span class="math notranslate nohighlight">\(N\)</span>個すべてのサンプルを考慮すると、交差エントロピーは、</p>
<figure class="margin align-default" id="id31">
<a class="reference internal image-reference" href="../_images/Average_Loss.png"><img alt="../_images/Average_Loss.png" src="../_images/Average_Loss.png" style="width: 199.0px; height: 342.0px;" /></a>
<figcaption>
<p><span class="caption-text">Average_Loss</span><a class="headerlink" href="#id31" title="Link to this image">#</a></p>
</figcaption>
</figure>
<div class="math notranslate nohighlight">
\[
L = - \sum_{n=1}^{N} \sum_{k=1}^{K}t_{n, k}\log y_{n, k}
\]</div>
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-0" name="sd-tab-set-0" type="radio">
</input><label class="sd-tab-label" for="sd-tab-item-0">
課題</label><div class="sd-tab-content docutils">
<p>3クラス分類問題を考えます。</p>
<p>予測は<span class="math notranslate nohighlight">\(y=(0.1,0.2,0.3)\)</span>、真のラベルは<span class="math notranslate nohighlight">\(t=(0,0,1)\)</span>の場合、交差エントロピーの計算式を書いてください。</p>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">t</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>損失関数は、すべての訓練データを対象として求める必要がありますが、場合によるすべてのデータを一気に計算するのは現実ではありません。そこで、データの中から一部を選びだし、つまりミニバッチごとに学習を行います。このような手法をミニバッチ学習と言います。</p>
</div>
</section>
</section>
<section id="id16">
<h3>損失関数の最適化<a class="headerlink" href="#id16" title="Link to this heading">#</a></h3>
<section id="id17">
<h4>勾配法<a class="headerlink" href="#id17" title="Link to this heading">#</a></h4>
<p>下の図は，パラメータ<span class="math notranslate nohighlight">\(w\)</span>を変化させた際の損失関数<span class="math notranslate nohighlight">\(L\)</span>の値を表しています。損失関数の値を最小にするようなパラメータの値を求めることで、ニューラルネットワークを訓練します。ただ、実際のニューラルネットワークの目的関数は、多次元で、かつもっと複雑な形をしていることがほとんどです。 そこで、勾配を利用して関数の最小値を探す勾配法がよく用いられます。</p>
<p><img alt="" src="../_images/loss_fucnction.png" /></p>
<p>勾配は、各地点における関数の傾きであり、関数の値が最も急速に変化する方向と大きさを示します。</p>
<p>今は<span class="math notranslate nohighlight">\(L\)</span>の値を小さくしたいわけです。勾配の反対方向に進むことで関数の値を最も減らせることができますので、勾配の情報を手がかりに、できるだけ小さな値となる関数の場所を探します。</p>
<p>損失を求めるまでの計算を1つの関数とみなして、重みの勾配<span class="math notranslate nohighlight">\(\frac{\partial L}{\partial \mathbf{W}}\)</span>ととバイアスの勾配<span class="math notranslate nohighlight">\(\frac{\partial L}{\partial \mathbf{b}}\)</span>を求めます。各要素は、それぞれパラメータの対応する要素の偏微分です。各パラメータの勾配<span class="math notranslate nohighlight">\(\frac{\partial L}{\partial \mathbf{W}}\)</span>、<span class="math notranslate nohighlight">\(\frac{\partial L}{\partial \mathbf{b}}\)</span>を用いて、勾配降下法によりパラメータ<span class="math notranslate nohighlight">\(\mathbf{W},\ \mathbf{b}\)</span>を更新します。</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbf{W}^{(\mathrm{new})}
   &amp;= \mathbf{W}
      - \eta \frac{\partial L}{\partial \mathbf{W}}
\\
\mathbf{b}^{(\mathrm{new})}
   &amp;= \mathbf{b}
      - \eta \frac{\partial L}{\partial \mathbf{b}}
\end{aligned}
\end{split}\]</div>
<p><span class="math notranslate nohighlight">\(\eta\)</span>は学習率と言います。<span class="math notranslate nohighlight">\(1\)</span>回の学習で、どれだけパラメータを更新するか、ということを決めます。</p>
</section>
<section id="id18">
<h4>勾配下降法の実装<a class="headerlink" href="#id18" title="Link to this heading">#</a></h4>
<section id="f-x-x-2">
<h5><span class="math notranslate nohighlight">\(f(x)=x^2\)</span>に対する最適化<a class="headerlink" href="#f-x-x-2" title="Link to this heading">#</a></h5>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># 関数とその勾配</span>
<span class="k">def</span> <span class="nf">function_f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span> <span class="o">**</span> <span class="mi">2</span>

<span class="k">def</span> <span class="nf">numerical_gradient</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">h</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">f</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">h</span><span class="p">)</span> <span class="o">-</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">h</span><span class="p">))</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">h</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">gradient_descent</span><span class="p">(</span><span class="n">initial_x</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">,</span> <span class="n">num_iterations</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">initial_x</span>
    <span class="n">x_history</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">]</span>
    
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_iterations</span><span class="p">):</span>
        <span class="n">grad</span> <span class="o">=</span> <span class="n">numerical_gradient</span><span class="p">(</span><span class="n">function_f</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">-</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">grad</span>
        <span class="n">x_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Iteration </span><span class="si">{}</span><span class="s2">: x = </span><span class="si">{}</span><span class="s2">, f(x) = </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">function_f</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
    
    <span class="k">return</span> <span class="n">x_history</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># パラメータ設定</span>
<span class="n">initial_x</span> <span class="o">=</span> <span class="mf">5.0</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">num_iterations</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">x_history</span> <span class="o">=</span> <span class="n">gradient_descent</span><span class="p">(</span><span class="n">initial_x</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">,</span> <span class="n">num_iterations</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Iteration 0: x = 4.000000000037858, f(x) = 16.00000000030286
Iteration 10: x = 0.42949672960284735, f(x) = 0.18446744073954138
Iteration 20: x = 0.04611686018453473, f(x) = 0.0021267647932799246
Iteration 30: x = 0.004951760157169053, f(x) = 2.4519928654126888e-05
Iteration 40: x = 0.0005316911983169281, f(x) = 2.82695530367691e-07
Iteration 50: x = 5.708990770855627e-05, f(x) = 3.259257562171473e-09
Iteration 60: x = 6.129982163497689e-06, f(x) = 3.7576681324799807e-11
Iteration 70: x = 6.582018229321472e-07, f(x) = 4.3322963971120166e-13
Iteration 80: x = 7.067388259152886e-08, f(x) = 4.994797680561205e-15
Iteration 90: x = 7.588550360298902e-09, f(x) = 5.75860965707926e-17
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># プロット</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="n">initial_x</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">initial_x</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="mi">400</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">function_f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="s1">&#39;-b&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;$f(x) = x^2$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_history</span><span class="p">,</span> <span class="p">[</span><span class="n">function_f</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">x_history</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Gradient Descent Steps&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Gradient Descent Visualization&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;$x$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;$f(x)$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/4da2fbb2ee71055b39a258aa520189a36f8b68e7de66287b19be043f6ec022ef.png" src="../_images/4da2fbb2ee71055b39a258aa520189a36f8b68e7de66287b19be043f6ec022ef.png" />
</div>
</div>
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-1" name="sd-tab-set-1" type="radio">
</input><label class="sd-tab-label" for="sd-tab-item-1">
課題</label><div class="sd-tab-content docutils">
<p>勾配法で<span class="math notranslate nohighlight">\(f(x)=2x^2-10x-80\)</span>の最小値を求めます。</p>
<ul class="simple">
<li><p>勾配降下法のアルゴリズムを実装する。</p></li>
<li><p>アルゴリズムを使って関数の最小値を求める。</p></li>
</ul>
</div>
</div>
</section>
<section id="example-f-x-0-x-1-x-0-2-x-1-2">
<h5>Example: <span class="math notranslate nohighlight">\(f(x_0,x_1)=x_0^2+x_1^2\)</span>に対する最適化<a class="headerlink" href="#example-f-x-0-x-1-x-0-2-x-1-2" title="Link to this heading">#</a></h5>
<p>今度は、変数が複数(2つ)あるの関数に対する最適化を実装してみます。複数の変数からなる関数の微分は偏微分といいます。</p>
<p>偏微分の場合、複数ある変数の中でターゲットとする変数を一つに絞り、他の変数はある値に固定します。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># 関数定義</span>
<span class="k">def</span> <span class="nf">function_f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span>
<span class="c1"># 勾配</span>
<span class="k">def</span> <span class="nf">numerical_gradient</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">h</span> <span class="o">=</span> <span class="mf">1e-4</span><span class="p">):</span>
    <span class="n">grad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="c1"># x の各成分について順番にループを回し、数値的に勾配を計算します。</span>
    <span class="n">it</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nditer</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">flags</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;multi_index&#39;</span><span class="p">],</span> <span class="n">op_flags</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;readwrite&#39;</span><span class="p">])</span>
    <span class="k">while</span> <span class="ow">not</span> <span class="n">it</span><span class="o">.</span><span class="n">finished</span><span class="p">:</span>
        <span class="n">idx</span> <span class="o">=</span> <span class="n">it</span><span class="o">.</span><span class="n">multi_index</span>
        <span class="n">tmp_val</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
        <span class="n">x</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">tmp_val</span> <span class="o">+</span> <span class="n">h</span>
        <span class="n">fxh1</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1"># f(x+h)</span>

        <span class="n">x</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">tmp_val</span> <span class="o">-</span> <span class="n">h</span> 
        <span class="n">fxh2</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1"># f(x-h)</span>

        <span class="n">grad</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">fxh1</span> <span class="o">-</span> <span class="n">fxh2</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">h</span><span class="p">)</span>

        <span class="n">x</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">tmp_val</span> 
        <span class="n">it</span><span class="o">.</span><span class="n">iternext</span><span class="p">()</span>   

    <span class="k">return</span> <span class="n">grad</span>

<span class="k">def</span> <span class="nf">gradient_descent</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">init_x</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">num_iterations</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">init_x</span>
    <span class="n">x_history</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="o">.</span><span class="n">copy</span><span class="p">()]</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_iterations</span><span class="p">):</span>
        <span class="n">grad</span> <span class="o">=</span> <span class="n">numerical_gradient</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">-=</span> <span class="n">lr</span> <span class="o">*</span> <span class="n">grad</span>
        <span class="n">x_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">copy</span><span class="p">())</span>

    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">x_history</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">numerical_gradient</span></code>ちう関数で、配列の各要素に対して数値微分を求めます。例えば、点<span class="math notranslate nohighlight">\((-3,4)\)</span>での勾配を求めてみます。</p>
<p>勾配が示す方向は、各場所において関数の値を最も減らす方向であり、その方向に晋ことで関数の値を最も減らせることができます。つまり、勾配の情報を手がかりに、進む方向を決めるべきでしょう。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># 勾配の計算</span>
<span class="n">numerical_gradient</span><span class="p">(</span><span class="n">function_f</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">]))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([-6.,  8.])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Parameters</span>
<span class="n">init_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">])</span>
<span class="n">lr</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">num_iterations</span> <span class="o">=</span> <span class="mi">20</span>

<span class="c1"># Run gradient descent</span>
<span class="n">x_history</span> <span class="o">=</span> <span class="n">gradient_descent</span><span class="p">(</span><span class="n">function_f</span><span class="p">,</span> <span class="n">init_x</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">num_iterations</span><span class="p">)</span>

<span class="c1"># Generate mesh data for visualization</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">4.5</span><span class="p">,</span> <span class="mf">4.5</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">4.5</span><span class="p">,</span> <span class="mf">4.5</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span>
<span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">Z</span> <span class="o">=</span> <span class="n">X</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">Y</span><span class="o">**</span><span class="mi">2</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="c1"># 1st subplot: 3D plot</span>
<span class="n">ax1</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">,</span> <span class="n">projection</span><span class="o">=</span><span class="s1">&#39;3d&#39;</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">plot_surface</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;viridis&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">zdir</span><span class="o">=</span><span class="s1">&#39;z&#39;</span><span class="p">,</span> <span class="n">offset</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;viridis&#39;</span><span class="p">,</span> <span class="n">linestyles</span><span class="o">=</span><span class="s1">&#39;dashed&#39;</span><span class="p">)</span>
<span class="n">Z_history</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">function_f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">x_history</span><span class="p">])</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_history</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">x_history</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">Z_history</span><span class="p">,</span> <span class="s1">&#39;o-&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_history</span><span class="p">)):</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">quiver</span><span class="p">(</span><span class="n">x_history</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">x_history</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">],</span> <span class="n">Z_history</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> 
               <span class="n">x_history</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">-</span><span class="n">x_history</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> 
               <span class="n">x_history</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="o">-</span><span class="n">x_history</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">],</span> 
               <span class="n">Z_history</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">-</span><span class="n">Z_history</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> 
               <span class="n">color</span><span class="o">=</span><span class="s2">&quot;blue&quot;</span><span class="p">,</span> <span class="n">arrow_length_ratio</span><span class="o">=</span><span class="mf">0.05</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;$X_0$&quot;</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;$X_1$&quot;</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_zlabel</span><span class="p">(</span><span class="s2">&quot;$f(X_0, X_1)$&quot;</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;3D Visualization&quot;</span><span class="p">)</span>

<span class="c1"># 2nd subplot: 2D contour plot</span>
<span class="n">ax2</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">)</span>
<span class="n">contour</span> <span class="o">=</span> <span class="n">ax2</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">levels</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">colors</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">linestyles</span><span class="o">=</span><span class="s1">&#39;dashed&#39;</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">clabel</span><span class="p">(</span><span class="n">contour</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_history</span><span class="p">)):</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">quiver</span><span class="p">(</span><span class="n">x_history</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">x_history</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">],</span> 
               <span class="n">x_history</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">-</span><span class="n">x_history</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> 
               <span class="n">x_history</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="o">-</span><span class="n">x_history</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">],</span> 
               <span class="n">angles</span><span class="o">=</span><span class="s2">&quot;xy&quot;</span><span class="p">,</span> <span class="n">scale_units</span><span class="o">=</span><span class="s2">&quot;xy&quot;</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;blue&quot;</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_history</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">x_history</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="s1">&#39;o-&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="o">-</span><span class="mf">4.5</span><span class="p">,</span> <span class="mf">4.5</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">4.5</span><span class="p">,</span> <span class="mf">4.5</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;$X_0$&quot;</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;$X_1$&quot;</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;2D Visualization&quot;</span><span class="p">)</span>

<span class="c1"># Display the figure</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/9e742d38ef331a93ed54ac82902f8422f0641c9cb7939aa54e7ace62a6596b26.png" src="../_images/9e742d38ef331a93ed54ac82902f8422f0641c9cb7939aa54e7ace62a6596b26.png" />
</div>
</div>
</section>
</section>
<section id="id19">
<h4>ニューラルネットワークに対する勾配<a class="headerlink" href="#id19" title="Link to this heading">#</a></h4>
<p>ニューラルネットワークにおいて、重みパラメータの勾配を求める計算を確認します。</p>
<p>ここで、形状が<span class="math notranslate nohighlight">\(2 \times 3\)</span>の重み<span class="math notranslate nohighlight">\(\mathbf{W}\)</span>を持つニューラルネットワークがあり、損失関数を<span class="math notranslate nohighlight">\(L\)</span>で表すことを考えましょう。この場合、勾配は<span class="math notranslate nohighlight">\(\frac{\partial L}{\partial \mathbf{W}}\)</span>で表すことができます。</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\begin{split}\mathbf{W}
    = \begin{pmatrix}
          w_{0,0} &amp; w_{0,1} &amp; w_{0,2} \\
          w_{1,0} &amp; w_{1,1} &amp; w_{1,2}
      \end{pmatrix},\end{split}\\\begin{split}\frac{\partial L}{\partial \mathbf{W}}
    = \begin{pmatrix}
          \frac{\partial L}{\partial w_{0,0}} &amp; \frac{\partial L}{\partial w_{0,1}} &amp; \frac{\partial L}{\partial w_{0,2}} \\
          \frac{\partial L}{\partial w_{1,0}} &amp; \frac{\partial L}{\partial w_{1,1}} &amp; \frac{\partial L}{\partial w_{0,2}}
      \end{pmatrix}
\end{split}\end{aligned}\end{align} \]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># (仮の)入力データを作成</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># (仮の)教師データを作成</span>
<span class="n">t</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[0.6 0.9]
[0 0 1]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="c1"># 乱数のシードを固定</span>
<span class="k">class</span> <span class="nc">simplenet</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="c1"># 重みを初期化する関数を定義</span>
    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="p">)</span> <span class="c1"># 重み付き和を計算</span>
    <span class="k">def</span> <span class="nf">loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
        <span class="n">z</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">z</span><span class="p">)</span> <span class="c1"># ソフトマックス関数による正規化</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">cross_entropy_error</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span> <span class="c1"># 交差エントロピー誤差を計算</span>
        <span class="k">return</span> <span class="n">loss</span> 
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">net</span> <span class="o">=</span> <span class="n">simplenet</span><span class="p">()</span>
<span class="n">p</span> <span class="o">=</span> <span class="n">net</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[ 3.07523529  1.92089652 -0.2923073 ]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">net</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">NameError</span><span class="g g-Whitespace">                                 </span>Traceback (most recent call last)
<span class="n">Cell</span> <span class="n">In</span><span class="p">[</span><span class="mi">22</span><span class="p">],</span> <span class="n">line</span> <span class="mi">1</span>
<span class="ne">----&gt; </span><span class="mi">1</span> <span class="n">net</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>

<span class="nn">Cell In[20], line 10,</span> in <span class="ni">simplenet.loss</span><span class="nt">(self, x, t)</span>
<span class="g g-Whitespace">      </span><span class="mi">8</span> <span class="n">z</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="g g-Whitespace">      </span><span class="mi">9</span> <span class="n">y</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">z</span><span class="p">)</span> <span class="c1"># ソフトマックス関数による正規化</span>
<span class="ne">---&gt; </span><span class="mi">10</span> <span class="n">loss</span> <span class="o">=</span> <span class="n">cross_entropy_error</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span> <span class="c1"># 交差エントロピー誤差を計算</span>
<span class="g g-Whitespace">     </span><span class="mi">11</span> <span class="k">return</span> <span class="n">loss</span>

<span class="ne">NameError</span>: name &#39;cross_entropy_error&#39; is not defined
</pre></div>
</div>
</div>
</div>
<p>続いて、勾配を求めてみましょう。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># 損失メソッドを実行する関数を作成</span>
<span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">W</span><span class="p">):</span>
    <span class="c1"># 損失メソッドを実行</span>
    <span class="k">return</span> <span class="n">net</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># 損失を計算</span>
<span class="n">L</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">W</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">L</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>3.6674507891066104
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># 重みの勾配を計算</span>
<span class="n">dW</span> <span class="o">=</span> <span class="n">numerical_gradient</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">net</span><span class="o">.</span><span class="n">W</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">dW</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[ 0.44452826  0.14014461 -0.58467287]
 [ 0.66679239  0.21021692 -0.87700931]]
</pre></div>
</div>
</div>
</div>
<p>これで重みの勾配<span class="math notranslate nohighlight">\(\frac{\partial L}{\partial \mathbf{W}}\)</span>を得られました。</p>
<p>その中身を見ると、例えば、<span class="math notranslate nohighlight">\(\frac{\partial L}{\partial \mathbf{W_{1,1}}}\)</span>はおよそ<span class="math notranslate nohighlight">\(0.44\)</span>ということは、<span class="math notranslate nohighlight">\(w_{1,1}\)</span>を<span class="math notranslate nohighlight">\(h\)</span>だけ増やすと損失関数の値は<span class="math notranslate nohighlight">\(0.44h\)</span>だけ増加することを意味します。</p>
<p>そのため、損失関数の値を減らすために、<span class="math notranslate nohighlight">\(w_{1,1}\)</span>はマイナス方向へ更新するのが良いことがわかりました。</p>
<p>パラメータの勾配が得られたということは、パラメータの学習を行えるようになったということです。</p>
</section>
</section>
</section>
<section id="id20">
<h2>2層ニューラルネットワークの実装<a class="headerlink" href="#id20" title="Link to this heading">#</a></h2>
<p>これまでに勉強した、「損失関数」、「ミニバッチ」、「勾配」、「勾配下降法」をまとめて、ニューラルネットワークの学習手順を確認します。</p>
<ul class="simple">
<li><p>ミニバッチ: データセットからミニバッチをランダムに取り出す。ここでは、そのミニバッチの損失関数の値を減らすことを目的とする。</p></li>
<li><p>勾配の算出：各重みパラメータの勾配を求める。</p></li>
<li><p>パラメーターの更新：重みパラメータを勾配方向に微少量だけ更新する。</p></li>
<li><p>収束するまで繰り返す</p></li>
</ul>
<p>ここでは、2層のニューラルネットワークの計算と最適化プロセスを確認しましょう。</p>
<section id="id21">
<h3>数式の確認<a class="headerlink" href="#id21" title="Link to this heading">#</a></h3>
<section id="id22">
<h4>入力層<a class="headerlink" href="#id22" title="Link to this heading">#</a></h4>
<p>ニューラルネットワークの入力<span class="math notranslate nohighlight">\(\mathbf{X}\)</span>、第<span class="math notranslate nohighlight">\(1\)</span>層の重み<span class="math notranslate nohighlight">\(\mathbf{W^{(1)}}\)</span>と<span class="math notranslate nohighlight">\(\mathbf{b}^{(1)}\)</span>を次の形状とします。</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{X}
    = \begin{pmatrix}
          x_{0,0} &amp; x_{0,1} &amp; \cdots &amp; x_{0,D-1} \\
          x_{1,0} &amp; x_{1,1} &amp; \cdots &amp; x_{1,D-1} \\
          \vdots &amp; \vdots &amp; \ddots &amp; \cdots \\
          x_{N-1,0} &amp; x_{N-1,1} &amp; \cdots &amp; x_{N-1,D-1}
      \end{pmatrix}
,\ 
\mathbf{W}^{(1)}
    = \begin{pmatrix}
          w_{0,0} &amp; w_{0,1} &amp; \cdots &amp; w_{0,H-1} \\
          w_{1,0} &amp; w_{1,1} &amp; \cdots &amp; w_{1,H-1} \\
          \vdots &amp;\vdots &amp; \ddots &amp; \vdots \\
          w_{D-1,0} &amp; w_{D-1,1} &amp; \cdots &amp; w_{D-1,H-1}
      \end{pmatrix}
,\ 
\mathbf{b}^{(1)}
    = \begin{pmatrix}
          b_0 &amp; b_1 &amp; \cdots &amp; b_{H-1}
      \end{pmatrix}
\end{split}\]</div>
<p>ここで、<span class="math notranslate nohighlight">\(\mathbf{N}\)</span>はバッチサイズ、<span class="math notranslate nohighlight">\(\mathbf{D}\)</span>は各データ<span class="math notranslate nohighlight">\(\mathbf{x}_n = (x_{n,0}, \cdots, x_{n,D-1})\)</span>の要素数、<span class="math notranslate nohighlight">\(\mathbf{H}\)</span>は中間層のニューロン数です。</p>
</section>
<section id="id23">
<h4>隠れ層<a class="headerlink" href="#id23" title="Link to this heading">#</a></h4>
<p>第<span class="math notranslate nohighlight">\(1\)</span>層の重み付き和<span class="math notranslate nohighlight">\(\mathbf{A}^{(1)}\)</span>を計算します。</p>
<div class="math notranslate nohighlight">
\[
\mathbf{A}^{(1)}
    = \mathbf{X} \mathbf{W}^{(1)} + \mathbf{B}^{(1)}
\]</div>
<p><span class="math notranslate nohighlight">\(\mathbf{N} \times \mathbf{D}\)</span>と<span class="math notranslate nohighlight">\(\mathbf{D} \times \mathbf{H}\)</span>の行列の積なので、計算結果は<span class="math notranslate nohighlight">\(\mathbf{N} \times \mathbf{H}\)</span>の行列になります。</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{A}^{(1)}
    = \begin{pmatrix}
          a_{0,0} &amp; a_{0,1} &amp; \cdots &amp; a_{0,H-1} \\
          a_{1,0} &amp; a_{1,1} &amp; \cdots &amp; a_{1,H-1} \\
          \vdots &amp; \vdots &amp; \ddots &amp; \cdots \\
          a_{N-1,0} &amp; a_{N-1,1} &amp; \cdots &amp; a_{N-1,H-1}
      \end{pmatrix}
\end{split}\]</div>
<p>重み付き和<span class="math notranslate nohighlight">\(\mathbf{A}^{(1)}\)</span>の各要素をシグモイド関数により活性化します。</p>
<div class="math notranslate nohighlight">
\[
z_{n,h}
    = \mathrm{sigmoid}(a_{n,h})
\]</div>
<p>ただ、活性化関数は形状(<span class="math notranslate nohighlight">\(\mathbf{N} \times \mathbf{H}\)</span>)に影響しません。第1層の出力の結果は、</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{Z}
    = \begin{pmatrix}
          z_{0,0} &amp; z_{0,1} &amp; \cdots &amp; z_{0,H-1} \\
          z_{1,0} &amp; z_{1,1} &amp; \cdots &amp; z_{1,H-1} \\
          \vdots &amp; \vdots &amp; \ddots &amp; \cdots \\
          z_{N-1,0} &amp; z_{N-1,1} &amp; \cdots &amp; z_{N-1,H-1}
      \end{pmatrix}
\end{split}\]</div>
<p>になります。</p>
</section>
<section id="id24">
<h4>出力層<a class="headerlink" href="#id24" title="Link to this heading">#</a></h4>
<p>次に、第<span class="math notranslate nohighlight">\(2\)</span>層の重み<span class="math notranslate nohighlight">\(\mathbf{W^{(2)}}\)</span>と<span class="math notranslate nohighlight">\(\mathbf{b}^{(2)}\)</span>は以下のように形状しています。</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{W}^{(2)}
    = \begin{pmatrix}
          w_{0,0} &amp; w_{0,1} &amp; \cdots &amp; w_{0,K-1} \\
          w_{1,0} &amp; w_{1,1} &amp; \cdots &amp; w_{1,K-1} \\
          \vdots &amp;\vdots &amp; \ddots &amp; \vdots \\
          w_{H-1,0} &amp; w_{H-1,1} &amp; \cdots &amp; w_{H-1,K-1}
      \end{pmatrix}
,\ 
\mathbf{b}^{(2)}
    = \begin{pmatrix}
          b_0 &amp; b_1 &amp; \cdots &amp; b_{K-1}
      \end{pmatrix}
\end{split}\]</div>
<p>ここで、<span class="math notranslate nohighlight">\(\mathbf{H}\)</span>は中間層のニューロン数、<span class="math notranslate nohighlight">\(\mathbf{K}\)</span>は出力層のクラス数です。</p>
<p>第2層の重み付き和<span class="math notranslate nohighlight">\(\mathbf{A}^{(2)}\)</span>は</p>
<div class="math notranslate nohighlight">
\[
\mathbf{A}^{(2)}
    = \mathbf{Z} \mathbf{W}^{(2)} + \mathbf{B}^{(2)}
\]</div>
<p><span class="math notranslate nohighlight">\(\mathbf{N} \times \mathbf{H}\)</span>と<span class="math notranslate nohighlight">\(\mathbf{H} \times \mathbf{K}\)</span>の行列の積なので、計算結果は<span class="math notranslate nohighlight">\(\mathbf{N} \times \mathbf{K}\)</span>の行列になります。</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{A}^{(2)}
    = \begin{pmatrix}
          a_{0,0} &amp; a_{0,1} &amp; \cdots &amp; a_{0,K-1} \\
          a_{1,0} &amp; a_{1,1} &amp; \cdots &amp; a_{1,K-1} \\
          \vdots &amp; \vdots &amp; \ddots &amp; \cdots \\
          a_{N-1,0} &amp; a_{N-1,1} &amp; \cdots &amp; a_{N-1,K-1}
      \end{pmatrix}
\end{split}\]</div>
<p>ここで、ソフトマックス関数により各データの重み付き和<span class="math notranslate nohighlight">\(a_{n}^{(2)}\)</span>を活性化して、ニューラルネットワークの出力<span class="math notranslate nohighlight">\(y_n\)</span>とします。</p>
<div class="math notranslate nohighlight">
\[
\mathbf{y}_n
    = \mathrm{softmax}(\mathbf{a}_n^{(2)})
\]</div>
<p><span class="math notranslate nohighlight">\(\mathbf{Y}\)</span>でニューラルネットワークの出力を表します。<span class="math notranslate nohighlight">\(n\)</span>番目のデータに関する出力<span class="math notranslate nohighlight">\(\mathbf{y}_n\)</span>は、<span class="math notranslate nohighlight">\(0 \leq y_{n,k} \leq 1\)</span>、<span class="math notranslate nohighlight">\(\sum_{k=0}^{K-1} y_{n,k} = 1\)</span>に正規化されており、<span class="math notranslate nohighlight">\(n\)</span>番目の入力データ<span class="math notranslate nohighlight">\(\mathbf{x}_n\)</span>がどのクラスのかを表す確率分布として扱えるのでした。</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{Y}
    = \begin{pmatrix}
          y_{0,0} &amp; y_{0,1} &amp; \cdots &amp; y_{0,K-1} \\
          y_{1,0} &amp; y_{1,1} &amp; \cdots &amp; y_{1,K-1} \\
          \vdots &amp; \vdots &amp; \ddots &amp; \cdots \\
          y_{N-1,0} &amp; y_{N-1,1} &amp; \cdots &amp; y_{N-1,K-1}
      \end{pmatrix}
\end{split}\]</div>
</section>
<section id="id25">
<h4>損失の計算<a class="headerlink" href="#id25" title="Link to this heading">#</a></h4>
<p><span class="math notranslate nohighlight">\(N\)</span>個のデータに関する教師データ<span class="math notranslate nohighlight">\(\mathbf{T}\)</span>は、出力<span class="math notranslate nohighlight">\(\mathbf{Y}\)</span>と同じ形状になります。</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{T}
    = \begin{pmatrix}
          t_{0,0} &amp; t_{0,1} &amp; \cdots &amp; t_{0,K-1} \\
          t_{1,0} &amp; t_{1,1} &amp; \cdots &amp; t_{1,K-1} \\
          \vdots &amp; \vdots &amp; \ddots &amp; \cdots \\
          t_{N-1,0} &amp; t_{N-1,1} &amp; \cdots &amp; t_{N-1,K-1}
      \end{pmatrix}
\end{split}\]</div>
<p>特に、分類問題の場合、各データの教師データ<span class="math notranslate nohighlight">\(t_n\)</span>は、、正解ラベルが<span class="math notranslate nohighlight">\(1\)</span>でそれ以外が<span class="math notranslate nohighlight">\(0\)</span>といった形になります。</p>
<p>(平均)交差エントロピー誤差を計算して、損失<span class="math notranslate nohighlight">\(L\)</span>とします。</p>
<div class="math notranslate nohighlight">
\[
L   = - \frac{1}{N}
        \sum_{n=0}^{N-1} \sum_{k=0}^{K-1}
          t_{n,k} \log y_{n,k}
\]</div>
</section>
<section id="id26">
<h4>勾配の計算<a class="headerlink" href="#id26" title="Link to this heading">#</a></h4>
<p>損失を求めるまでの計算を1つの関数とみなして、重みの勾配<span class="math notranslate nohighlight">\(\frac{\partial L}{\partial \mathbf{W}}\)</span>とバイアスの勾配<span class="math notranslate nohighlight">\(\frac{\partial L}{\partial \mathbf{b}}\)</span>を求めます。</p>
<p>第<span class="math notranslate nohighlight">\(1\)</span>層のパラメータ<span class="math notranslate nohighlight">\(\mathbf{W}^{(1)},\ \mathbf{b}^{(1)}\)</span>を<span class="math notranslate nohighlight">\(\frac{\partial L}{\partial \mathbf{W}^{(1)}},\ \frac{\partial L}{\partial \mathbf{b}^{(1)}}\)</span>で表します。</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\frac{\partial L}{\partial \mathbf{W}^{(1)}}
    = \begin{pmatrix}
          \frac{\partial L}{\partial w_{0,0}} &amp; \frac{\partial L}{\partial w_{0,1}} &amp; \cdots &amp; \frac{\partial L}{\partial w_{0,H-1}} \\
          \frac{\partial L}{\partial w_{1,0}} &amp; \frac{\partial L}{\partial w_{a,1}} &amp; \cdots &amp; \frac{\partial L}{\partial w_{1,H-1}} \\
          \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
          \frac{\partial L}{\partial w_{D-1,0}} &amp; \frac{\partial L}{\partial w_{D-1,1}} &amp; \cdots &amp; \frac{\partial L}{\partial w_{D-1,H-1}} \\
      \end{pmatrix}
,\ 
\frac{\partial L}{\partial \mathbf{b}^{(1)}}
    = \begin{pmatrix}
          \frac{\partial L}{\partial b_0} &amp; \frac{\partial L}{\partial b_1} &amp; \cdots &amp; \frac{\partial L}{\partial b_{H-1}}
      \end{pmatrix}\end{split}\]</div>
<p>同様に、第<span class="math notranslate nohighlight">\(2\)</span>層のパラメータ<span class="math notranslate nohighlight">\(\mathbf{W}^{(2)},\ \mathbf{b}^{(2)}\)</span>を<span class="math notranslate nohighlight">\(\frac{\partial L}{\partial \mathbf{W}^{(2)}},\ \frac{\partial L}{\partial \mathbf{b}^{(2)}}\)</span>で表します。</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\frac{\partial L}{\partial \mathbf{W}^{(2)}}
    = \begin{pmatrix}
          \frac{\partial L}{\partial w_{0,0}} &amp; \frac{\partial L}{\partial w_{0,1}} &amp; \cdots &amp; \frac{\partial L}{\partial w_{0,K-1}} \\
          \frac{\partial L}{\partial w_{1,0}} &amp; \frac{\partial L}{\partial w_{a,1}} &amp; \cdots &amp; \frac{\partial L}{\partial w_{1,K-1}} \\
          \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
          \frac{\partial L}{\partial w_{H-1,0}} &amp; \frac{\partial L}{\partial w_{H-1,1}} &amp; \cdots &amp; \frac{\partial L}{\partial w_{H-1,K-1}} \\
      \end{pmatrix}
,\ 
\frac{\partial L}{\partial \mathbf{b}^{(2)}}
    = \begin{pmatrix}
          \frac{\partial L}{\partial b_0} &amp; \frac{\partial L}{\partial b_1} &amp; \cdots &amp; \frac{\partial L}{\partial b_{K-1}}
      \end{pmatrix}\end{split}\]</div>
<p>各要素は、それぞれパラメータの対応する要素の偏微分です。</p>
</section>
<section id="id27">
<h4>パラメータの更新<a class="headerlink" href="#id27" title="Link to this heading">#</a></h4>
<p>各パラメータの勾配、<span class="math notranslate nohighlight">\(\frac{\partial L}{\partial \mathbf{W}},\ \frac{\partial L}{\partial \mathbf{b}}\)</span>を用いて、勾配降下法によりパラメータ<span class="math notranslate nohighlight">\(\mathbf{W},\ \mathbf{b}\)</span>を更新します。</p>
<p>更新後のパラメータを<span class="math notranslate nohighlight">\(\mathbf{W}^{(\mathrm{new})},\ \mathbf{b}^{(\mathrm{new})}\)</span>とすると、更新式は次の式で表せます。</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbf{W}^{(\mathrm{new})}
   &amp;= \mathbf{W}
      - \eta \frac{\partial L}{\partial \mathbf{W}}
\\
\mathbf{b}^{(\mathrm{new})}
   &amp;= \mathbf{b}
      - \eta \frac{\partial L}{\partial \mathbf{b}}
\end{aligned}
\end{split}\]</div>
<p>各要素に注目すると、それぞれ次の計算をしています。</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
w_{h,k}^{(\mathrm{new})}
   &amp;= w_{h,k} - \eta \frac{\partial L}{\partial w_{h,k}}
\\
b_k^{(\mathrm{new})}
   &amp;= b_k - \eta \frac{\partial L}{\partial b_k}
\end{aligned}
\end{split}\]</div>
</section>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./notebook"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="math_basis2.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">数学基礎</p>
      </div>
    </a>
    <a class="right-next"
       href="backpropagation.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">誤差逆伝播法</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">ニューラルネットワークの構造</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">パーセプトロン</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">活性化関数</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">ニューラルネットワークの仕組み</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">ニューラルネットワークの計算</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">記号の説明</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">各層における信号伝達</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id9">数値を見ながら計算の流れを確認</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id10">出力層の設計</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id11">ソフトマックス関数</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id12">ニューラルネットワークの学習</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id13">損失関数</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id14">平均二乗誤差</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id15">交差エントロピー</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id16">損失関数の最適化</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id17">勾配法</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id18">勾配下降法の実装</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#f-x-x-2"><span class="math notranslate nohighlight">\(f(x)=x^2\)</span>に対する最適化</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#example-f-x-0-x-1-x-0-2-x-1-2">Example: <span class="math notranslate nohighlight">\(f(x_0,x_1)=x_0^2+x_1^2\)</span>に対する最適化</a></li>
</ul>
</li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id19">ニューラルネットワークに対する勾配</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id20">2層ニューラルネットワークの実装</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id21">数式の確認</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id22">入力層</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id23">隠れ層</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id24">出力層</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id25">損失の計算</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id26">勾配の計算</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id27">パラメータの更新</a></li>
</ul>
</li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By 呂　沢宇
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=bd9e20870c6007c4c509"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=bd9e20870c6007c4c509"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>