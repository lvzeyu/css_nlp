{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前章では、「カウントベースの手法」によって単語分散表現を得ました。具体的には、単語の共起行列を作り、その行列に対してSVDを適用することで、密なベクトくー 単語分散表現ーを獲得したのです。\n",
    "\n",
    "しかし、カウントベースの手法にはいくつかの問題点があります。\n",
    "\n",
    "- 大規模なコーパスを扱う場合、巨大な共起行列に対してSVDを計算することが難しい。\n",
    "- コーパスの全体から一回の学習で単語分散表現を獲得していますので、新しい単語が追加される場合、再度最初から学習を行う必要があり、単語分散表現更新の効率が低い。\n",
    "\n",
    "「カウントベースの手法」に代わる強力な手法として「推論ベース」の手法が挙げられます。特に、Mikolov et al. {cite}`mikolov-etal-2013-linguistic` {cite}`NIPS2013_9aa42b31`　によって提案されたword2vecの有用性が多くの自然言語処理タスクにおいて示されてきたのです。\n",
    "\n",
    "本章では、word2vecの仕組みについて説明し、それを実装することで理解を深めます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 推論ベース手法とニューラルネットワーク\n",
    "\n",
    "推論ベースの手法は、ミニバッチで学習する形で、ニューラルネットワークを用いて、重みを繰り返し更新することで単語分散表現を獲得します。\n",
    "\n",
    "\n",
    "![](./Figure/inference.png)\n",
    "\n",
    "### 推論ベース手法の設計\n",
    "\n",
    "推論ベース手法では、```you 【？】 goodbye and I say hello .```のような、周囲の単語が与えられたときに、```【？】```にどのような単語が出現するのかを推測する推論問題を繰り返し解くことで、単語の出現バターンを学習します。\n",
    "\n",
    "つまり、コンテキスト情報を入力として受け取り、各単語の出現する確率を出力する「モデル」を作成することは目標になります。ここで、正しい推測ができるように、コーパスを使って、ニューラルネットワークモデルの学習を行います。そして、その学習の結果として、単語の分散表現を得られます。\n",
    "\n",
    "![](./Figure/inference2.png)\n",
    "\n",
    "### one-hot表現\n",
    "\n",
    "ニューラルネットワークで単語を処理するには、それを「固定長のベクトル」に変換する必要があります。\n",
    "\n",
    "そのための方法の一つは、単語をone-hot表現へと変換することです。one-hot表現とは、ベクトルの要素の中で一つだけが$1$で、残りは全て$0$であるようなベクトルと言います。\n",
    "\n",
    "単語をone-hot表現に変換するには、語彙数分の要素を持つベクトルを用意して、単語IDの該当する箇所を$1$に、残りは全て$0$に設定します。\n",
    "\n",
    "\n",
    "![](./Figure/one-hot.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CBOW（continuous bag-of-words）モデル\n",
    "\n",
    "CBOWモデルは、コンテキストからターゲットを推測することを目的としたニューラルネットワークです。このCBOWモデルで、できるだけ正確な推測ができるように訓練することで、単語の分散表現を取得することができます。\n",
    "\n",
    "ここで、例として、コンテキスト```[\"you\",\"goodbye\"]```からターゲット```\"say\"```を予測するタスクを考えます。\n",
    "\n",
    "![](./Figure/cbow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 入力層から中間層(エンコード)\n",
    "\n",
    "one-hotエンコーディングで、単語を固定長のベクトルに変換するすることができます。\n",
    "\n",
    "単語をベクトルで表すことができれば、そのベクトルはニューラルネットワークを構成する「レイヤ」によって処理することができるようになりました。\n",
    "\n",
    "コンテキストを$\\mathbf{c}$、重みを$\\mathbf{W}$とし、それぞれ次の形状とします。\n",
    "\n",
    "```{margin}\n",
    "イメージしやすいように、ここでは添字を対応する単語で表すことにします。ただし「.(ピリオド)」については「priod」とします。\n",
    "```\n",
    "\n",
    "$$\n",
    "\\mathbf{c}\n",
    "    = \\begin{pmatrix}\n",
    "          c_{\\mathrm{you}} & c_{\\mathrm{say}} & c_{\\mathrm{goodbye}} & c_{\\mathrm{and}} & c_{\\mathrm{I}} & c_{\\mathrm{hello}} & c_{\\mathrm{period}}\n",
    "      \\end{pmatrix}\n",
    ",\\ \n",
    "\\mathbf{W}\n",
    "    = \\begin{pmatrix}\n",
    "          w_{\\mathrm{you},1} & w_{\\mathrm{you},2} & w_{\\mathrm{you},3} \\\\\n",
    "          w_{\\mathrm{say},1} & w_{\\mathrm{say},2} & w_{\\mathrm{say},3} \\\\\n",
    "          w_{\\mathrm{goodbye},1} & w_{\\mathrm{goodbye},2} & w_{\\mathrm{goodbye},3} \\\\\n",
    "          w_{\\mathrm{and},1} & w_{\\mathrm{and},2} & w_{\\mathrm{and},3} \\\\\n",
    "          w_{\\mathrm{I},1} & w_{\\mathrm{I},2} & w_{\\mathrm{I},3} \\\\\n",
    "          w_{\\mathrm{hello},1} & w_{\\mathrm{hello},2} & w_{\\mathrm{hello},3} \\\\\n",
    "          w_{\\mathrm{period},1} & w_{\\mathrm{period},2} & w_{\\mathrm{period},3} \\\\\n",
    "      \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "コンテキストの要素数(列数)と重みの行数が、単語の種類数に対応します。\n",
    "\n",
    "コンテキスト(単語)はone-hot表現として扱うため、例えば「you」の場合は\n",
    "\n",
    "$$\n",
    "\\mathbf{c}_{\\mathrm{you}}\n",
    "    = \\begin{pmatrix}\n",
    "          1 & 0 & 0 & 0 & 0 & 0 & 0\n",
    "      \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "とすることで、単語「you」を表現できます。\n",
    "\n",
    "重み付き和$\\mathbf{h}$は、行列の積で求められます。\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf{h}\n",
    "   &= \\mathbf{c}_{\\mathrm{you}}\n",
    "      \\mathbf{W}\n",
    "\\\\\n",
    "   &= \\begin{pmatrix}\n",
    "          h_1 & h_2 & h_3\n",
    "      \\end{pmatrix}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$h_1$の計算を詳しく見ると、次のようになります。\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "h_1\n",
    "   &= c_{\\mathrm{you}} w_{\\mathrm{you},1}\n",
    "      + c_{\\mathrm{say}} w_{\\mathrm{say},1}\n",
    "      + c_{\\mathrm{goodbye}} w_{\\mathrm{goodbye},1}\n",
    "      + c_{\\mathrm{and}} w_{\\mathrm{and},1}\n",
    "      + c_{\\mathrm{I}} w_{\\mathrm{I},1}\n",
    "      + c_{\\mathrm{hello}} w_{\\mathrm{hello},1}\n",
    "      + c_{\\mathrm{period}} w_{\\mathrm{period},1}\n",
    "\\\\\n",
    "   &= 1 w_{\\mathrm{you},1}\n",
    "      + 0 w_{\\mathrm{say},1}\n",
    "      + 0 w_{\\mathrm{goodbye},1}\n",
    "      + 0 w_{\\mathrm{and},1}\n",
    "      + 0 w_{\\mathrm{I},1}\n",
    "      + 0 w_{\\mathrm{hello},1}\n",
    "      + 0 w_{\\mathrm{period},1}\n",
    "\\\\\n",
    "   &= w_{\\mathrm{you},1}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "コンテキストと重みの対応する(同じ単語に関する)要素を掛けて、全ての単語で和をとります。しかしコンテキストは、$c_{you}$以外の要素が$0$なので、対応する重みの値の影響は消えていまします。また$c_{you}$は$1$なので、対応する重みの値$w_{\\mathrm{you},1}$がそのまま中間層のニューロンに伝播します。\n",
    "\n",
    "残りの2つの要素も同様に計算できるので、重み付き和\n",
    "\n",
    "$$\n",
    "\\mathbf{h}\n",
    "    = \\begin{pmatrix}\n",
    "          w_{\\mathrm{you},1} & w_{\\mathrm{you},2} & w_{\\mathrm{you},3}\n",
    "      \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "は、単語「you」に関する重みの値となります。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "コンテキストの形状：(1, 7)\n",
      "重み\n",
      "[[ 0.88017787 -1.14164433 -0.16026237]\n",
      " [ 0.10290618  0.94854785  1.00786091]\n",
      " [-0.69365455  1.05141111 -0.28107061]\n",
      " [-0.46715976  0.02720009  0.00501715]\n",
      " [ 1.88551675  0.36829858  0.12531265]\n",
      " [-0.0687483   1.4502785   0.47945892]\n",
      " [-0.29803183 -1.61602693 -0.57121356]]\n",
      "重み付き和\n",
      "[[ 0.88017787 -1.14164433 -0.16026237]]\n",
      "重み付き和の形状：(1, 3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 適当にコンテキスト(one-hot表現)を指定\n",
    "c = np.array([[1, 0, 0, 0, 0, 0, 0]])\n",
    "print(f\"コンテキストの形状：{c.shape}\")\n",
    "\n",
    "# 重みをランダムに生成\n",
    "W = np.random.randn(7, 3)\n",
    "print(f\"重み\\n{W}\")\n",
    "\n",
    "# 重み付き和を計算\n",
    "h = np.dot(c, W)\n",
    "print(f\"重み付き和\\n{h}\")\n",
    "print(f\"重み付き和の形状：{h.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "コンテキストに複数な単語がある場合、入力層も複数になります。このとき、中間層にあるニューロンは、各入力層の全結合による変換後の値が平均されたものになります。\n",
    "\n",
    "中間層のニューロンの数を入力層よりも減らすことによって、中間層には、単語を予測するために必要な情報が\"コンパクト\"に収められて、結果としては密なベクトル表現が得られます。このとき、この中間層の情報は、人間には理解できない「ブラックボックス」ような状態になります。この作業は、「エンコード」と言います。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 中間層から出力層(デコード)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "中間層の情報から目的の結果を得る作業は、「デコード」と言います。ここでは、中間層のニューロンの値$\\mathbf{h}$を各単語に対応した値になるように、つまり要素(行)数が単語の種類数となるように再度変換したものを、CBOWモデルの出力とします。\n",
    "\n",
    "出力層の重みを\n",
    "\n",
    "$$\n",
    "\\mathbf{W}_{\\mathrm{out}}\n",
    "    = \\begin{pmatrix}\n",
    "          w_{1,\\mathrm{you}} & w_{1,\\mathrm{say}} & w_{1,\\mathrm{goodbye}} & w_{1,\\mathrm{and}} &\n",
    "          w_{1,\\mathrm{I}} & w_{1,\\mathrm{hello}} & w_{1,\\mathrm{period}} \\\\\n",
    "          w_{2,\\mathrm{you}} & w_{2,\\mathrm{say}} & w_{2,\\mathrm{goodbye}} & w_{2,\\mathrm{and}} &\n",
    "          w_{2,\\mathrm{I}} & w_{2,\\mathrm{hello}} & w_{2,\\mathrm{period}} \\\\\n",
    "          w_{3,\\mathrm{you}} & w_{3,\\mathrm{say}} & w_{3,\\mathrm{goodbye}} & w_{3,\\mathrm{and}} &\n",
    "          w_{3,\\mathrm{I}} & w_{3,\\mathrm{hello}} & w_{3\\mathrm{period}} \\\\\n",
    "      \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "とします。行数が中間層のニューロン数、列数が単語の種類数になります。\n",
    "\n",
    "出力層も全結合層とすると、最終的な出力は\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf{s}\n",
    "   &= \\mathbf{h}\n",
    "      \\mathbf{W}_{\\mathrm{out}}\n",
    "\\\\\n",
    "   &= \\begin{pmatrix}\n",
    "          s_{\\mathrm{you}} & s_{\\mathrm{say}} & s_{\\mathrm{goodbye}} & s_{\\mathrm{and}} &\n",
    "          s_{\\mathrm{I}} & s_{\\mathrm{hello}} & s_{\\mathrm{period}}\n",
    "      \\end{pmatrix}\n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "例えば、「you」に関する要素の計算は、\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "s_{\\mathrm{you}}\n",
    "   &= \\frac{1}{2} (w_{\\mathrm{you},1} + w_{\\mathrm{goodbye},1}) w_{1,\\mathrm{you}}\n",
    "      + \\frac{1}{2} (w_{\\mathrm{you},2} + w_{\\mathrm{goodbye},2}) w_{2,\\mathrm{you}}\n",
    "      + \\frac{1}{2} (w_{\\mathrm{you},3} + w_{\\mathrm{goodbye},3}) w_{3,\\mathrm{you}}\n",
    "\\\\\n",
    "   &= \\frac{1}{2}\n",
    "      \\sum_{i=1}^3\n",
    "          (w_{\\mathrm{you},i} + w_{\\mathrm{goodbye},i}) w_{i,\\mathrm{you}}\n",
    "\\end{aligned}\n",
    "\n",
    "$$\n",
    "\n",
    "コンテキストに対応する入力層の重みの平均と「you」に関する出力の重みの積になります。\n",
    "\n",
    "他の要素(単語)についても同様に計算できるので、最終的な出力は\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf{s}\n",
    "   &= \\begin{pmatrix}\n",
    "          s_{\\mathrm{you}} & s_{\\mathrm{say}} & s_{\\mathrm{goodbye}} & s_{\\mathrm{and}} &\n",
    "          s_{\\mathrm{I}} & s_{\\mathrm{hello}} & s_{\\mathrm{period}}\n",
    "      \\end{pmatrix}\n",
    "\\\\\n",
    "   &= \\begin{pmatrix}\n",
    "          \\frac{1}{2} \\sum_{i=1}^3 (w_{\\mathrm{you},i} + w_{\\mathrm{goodbye},i}) w_{i,\\mathrm{you}} &\n",
    "          \\frac{1}{2} \\sum_{i=1}^3 (w_{\\mathrm{you},i} + w_{\\mathrm{goodbye},i}) w_{i,\\mathrm{say}} &\n",
    "          \\cdots &\n",
    "          \\frac{1}{2} \\sum_{i=1}^3 (w_{\\mathrm{you},i} + w_{\\mathrm{goodbye},i}) w_{i,\\mathrm{hello}} &\n",
    "          \\frac{1}{2} \\sum_{i=1}^3 (w_{\\mathrm{you},i} + w_{\\mathrm{goodbye},i}) w_{i,\\mathrm{period}}\n",
    "      \\end{pmatrix}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "となります。\n",
    "\n",
    "ここで、出力層のニューロンは各単語に対応し、各単語の「スコア」と言います。\n",
    "\n",
    "「スコア」の値が高ければ高いほど、それに対応する単語の出現確率も高くなり、ターゲットの単語であるとして採用します。そのため、スコアを求める処理を推論処理と言います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-1.1176,  2.0091,  0.3479]]),\n",
       " tensor([[ 1.2594, -2.4558,  0.3923]]),\n",
       " tensor([[ 0.0709, -0.2233,  0.3701]]),\n",
       " tensor([[ 0.3330,  0.2530,  0.0310, -0.0290, -0.6210, -0.2400,  0.2620]]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "# Define the context data\n",
    "c0 = torch.tensor([[1, 0, 0, 0, 0, 0, 0]], dtype=torch.float32) # you\n",
    "c1 = torch.tensor([[0, 0, 1, 0, 0, 0, 0]], dtype=torch.float32) # goodbye\n",
    "\n",
    "# Initialize weights randomly\n",
    "W_in = torch.randn(7, 3, requires_grad=False)  # Input layer weights\n",
    "W_out = torch.randn(3, 7, requires_grad=False) # Output layer weights\n",
    "\n",
    "# Define the layers using PyTorch's functional API\n",
    "def in_layer(x, W):\n",
    "    return torch.matmul(x, W)\n",
    "\n",
    "def out_layer(h, W):\n",
    "    return torch.matmul(h, W)\n",
    "\n",
    "# Forward pass through the input layers\n",
    "h0 = in_layer(c0, W_in) # you\n",
    "h1 = in_layer(c1, W_in) # goodbye\n",
    "h = 0.5 * (h0 + h1)\n",
    "\n",
    "# Forward pass through the output layer (scores)\n",
    "s = out_layer(h, W_out)\n",
    "\n",
    "# Print the outputs\n",
    "h0, h1, h, torch.round(s, decimals=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````{tab-set}\n",
    "```{tab-item} 課題\n",
    "正解は「say」として、Softmax関数によってスコア``s``を確率として扱えるように変換し、そして、正規化した値と教師ラベルを用いて損失を求めなさい。\n",
    "```\n",
    "\n",
    "```{tab-item} ヒント\n",
    "正解は「say」の場合、教師ラベルは``[0, 1, 0, 0, 0, 0, 0]``になります。\n",
    "```\n",
    "\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### word2vecの重みと分散表現"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与えられたコンテキストに対して単語を予測するときに、「良い重み」のネットワークがあれば、「確率」を表すニューロンにおいて、正解に対応するニューロンが高くなっていることが期待できます。そして、大規模コーパスを使って得られる単語の分散表現は、単語の意味や文法のルールにおいて、人間の直感と合致するケースが多く見られます。\n",
    "\n",
    "\n",
    "word2vecモデルの学習で行うことが、正しい予測ができるように重みを調整することです。つまり、「コンテキストから出現単語」を予測するという偽タスクをニューラルネットで解いてきましたが、目的はニューラルネットの重みを求めることになります。\n",
    "\n",
    "もっと具体的に言えば、word2vecで使用されるネットワークには二つの重みがあります。それは、入力層の重み$\\mathbf{W_{in}}$と、出力層の重み$\\mathbf{W_{out}}$です。それでは、どちらの重みを使えば良いでしょうか？\n",
    "\n",
    "1. 入力側の重みを利用する\n",
    "2. 出力側の重みを利用する\n",
    "3. 二つの重みの両方を利用する\n",
    "\n",
    "Word2Vecモデルに関しては、多くの研究や応用例で、入力層の重みを単語のベクトル表現として使用さており、良好なパフォーマンスを示しています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vecモデルの実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習データの準備\n",
    "\n",
    "#### コンテキストとターゲット\n",
    "\n",
    "Word2Vecモデルためのニューラルネットワークでは、「コンテキスト」を入力した時に、「ターゲット」が出現する確率を高くになるように学習を行います。\n",
    "\n",
    "そのため、コーパスから「コンテキスト」と「ターゲット」が対応するデータを作成する必要があります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前処理関数の実装\n",
    "def preprocess(text):\n",
    "    # 前処理\n",
    "    text = text.lower() # 小文字に変換\n",
    "    text = text.replace('.', ' .') # ピリオドの前にスペースを挿入\n",
    "    words = text.split(' ') # 単語ごとに分割\n",
    "    \n",
    "    # ディクショナリを初期化\n",
    "    word_to_id = {}\n",
    "    id_to_word = {}\n",
    "    \n",
    "    # 未収録の単語をディクショナリに格納\n",
    "    for word in words:\n",
    "        if word not in word_to_id: # 未収録の単語のとき\n",
    "            # 次の単語のidを取得\n",
    "            new_id = len(word_to_id)\n",
    "            \n",
    "            # 単語をキーとして単語IDを格納\n",
    "            word_to_id[word] = new_id\n",
    "            \n",
    "            # 単語IDをキーとして単語を格納\n",
    "            id_to_word[new_id] = word\n",
    "    \n",
    "    # 単語IDリストを作成\n",
    "    corpus = [word_to_id[w] for w in words]\n",
    "    \n",
    "    return corpus, word_to_id, id_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'you': 0, 'say': 1, 'goodbye': 2, 'and': 3, 'i': 4, 'hello': 5, '.': 6}\n",
      "{0: 'you', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'i', 5: 'hello', 6: '.'}\n",
      "[0, 1, 2, 3, 4, 1, 5, 6]\n"
     ]
    }
   ],
   "source": [
    "# テキストを設定\n",
    "text = 'You say goodbye and I say hello.'\n",
    "\n",
    "# 前処理\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "print(word_to_id)\n",
    "print(id_to_word)\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "テキストの単語を単語IDに変換した```corpus```からターゲットを抽出します。\n",
    "\n",
    "ターゲットはコンテキストの中央の単語なので、```corpus```の始めと終わりのウインドウサイズ分の単語は含めません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 1, 5]\n"
     ]
    }
   ],
   "source": [
    "# ウインドウサイズを指定\n",
    "window_size = 1\n",
    "\n",
    "# ターゲットを抽出\n",
    "target = corpus[window_size:-window_size]\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ターゲットの単語に対して、for文で前後ウィンドウサイズの範囲の単語を順番に抽出し```cs```に格納します。\n",
    "\n",
    "つまりウィンドウサイズを$1$とすると、```corpus```におけるターゲットのインデックス```idx```に対して、1つ前(```idx - window_size```)から1つ後(```idx + window_size```)までの範囲の単語を順番に```cs```格納します。ただしターゲット自体の単語はコンテキストに含めません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n",
      "[0, 2]\n",
      "[[0, 2]]\n"
     ]
    }
   ],
   "source": [
    "# コンテキストを初期化(受け皿を作成)\n",
    "contexts = []\n",
    "\n",
    "# 1つ目のターゲットのインデックス\n",
    "idx = window_size\n",
    "\n",
    "# 1つ目のターゲットのコンテキストを初期化(受け皿を作成)\n",
    "cs = []\n",
    "\n",
    "# 1つ目のターゲットのコンテキストを1単語ずつ格納\n",
    "for t in range(-window_size, window_size + 1):\n",
    "    \n",
    "    # tがターゲットのインデックスのとき処理しない\n",
    "    if t == 0:\n",
    "        continue\n",
    "    \n",
    "    # コンテキストを格納\n",
    "    cs.append(corpus[idx + t])\n",
    "    print(cs)\n",
    "\n",
    "# 1つ目のターゲットのコンテキストを格納\n",
    "contexts.append(cs)\n",
    "print(contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# コンテキストとターゲットの作成関数の実装\n",
    "def create_contexts_target(corpus, window_size=1):\n",
    "    \n",
    "    # ターゲットを抽出\n",
    "    target = corpus[window_size:-window_size]\n",
    "    \n",
    "    # コンテキストを初期化\n",
    "    contexts = []\n",
    "    \n",
    "    # ターゲットごとにコンテキストを格納\n",
    "    for idx in range(window_size, len(corpus) - window_size):\n",
    "        \n",
    "        # 現在のターゲットのコンテキストを初期化\n",
    "        cs = []\n",
    "        \n",
    "        # 現在のターゲットのコンテキストを1単語ずつ格納\n",
    "        for t in range(-window_size, window_size + 1):\n",
    "            \n",
    "            # 0番目の要素はターゲットそのものなので処理を省略\n",
    "            if t == 0:\n",
    "                continue\n",
    "            \n",
    "            # コンテキストを格納\n",
    "            cs.append(corpus[idx + t])\n",
    "            \n",
    "        # 現在のターゲットのコンテキストのセットを格納\n",
    "        contexts.append(cs)\n",
    "    \n",
    "    # NumPy配列に変換\n",
    "    return np.array(contexts), np.array(target) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 2]\n",
      " [1 3]\n",
      " [2 4]\n",
      " [3 1]\n",
      " [4 5]\n",
      " [1 6]]\n",
      "[1 2 3 4 1 5]\n"
     ]
    }
   ],
   "source": [
    "# コンテキストとターゲットを作成\n",
    "contexts, target = create_contexts_target(corpus, window_size=1)\n",
    "print(contexts)\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PytorchでCBOWモデルの実装\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embeddingレイヤ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先ほど、理解しやすいone-hot表現でコンテキストを変換する方法を説明しましたが、大規模なコーパスで学習する際、one-hot表現の次元数も大きくになって、非効率な学習の原因になります。\n",
    "\n",
    "ただ、one-hot表現による計算は、単に行列の特定の行を抜き出すことだけですから、同じ機能を持つレイヤで入れ替えることは可能です。このような、重みパラメータから「単語IDに該当する行(ベクトル)」を抜き出すためのレイヤは「Embeddingレイヤ」と言います。\n",
    "\n",
    "PyTorchで提供されるモジュール```nn.Embedding```を使うと、簡単にEmbeddingレイヤを実装することができます。\n",
    "\n",
    "例えば、語彙に6つの単語があり、各埋め込みベクトルの次元数を3に設定した場合、nn.Embeddingの定義は以下のようになります。\n",
    "\n",
    "そして、"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = nn.Embedding(6, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "もしインデックス2のトークンの埋め込みを取得したい場合、次のようにします："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4987, -1.8682, -0.5487]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = torch.tensor([2], dtype=torch.long)\n",
    "embedding = embedding_layer(index)\n",
    "embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CBOWモデルの構築"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CBOWモデルの定義\n",
    "class SimpleCBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size):\n",
    "        super(SimpleCBOW, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.linear1 = nn.Linear(embedding_size, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs)\n",
    "        out = torch.sum(embeds, dim=0).view((1, -1))\n",
    "        out = self.linear1(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 2]\n",
      " [1 3]\n",
      " [2 4]\n",
      " [3 1]\n",
      " [4 5]\n",
      " [1 6]]\n",
      "[1 2 3 4 1 5]\n"
     ]
    }
   ],
   "source": [
    "# コンテキストとターゲットを作成\n",
    "contexts, targets = create_contexts_target(corpus, window_size=1)\n",
    "print(contexts)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3 4 1 5]\n"
     ]
    }
   ],
   "source": [
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 11.971852421760559\n",
      "Epoch 1, Loss: 10.9270738363266\n",
      "Epoch 2, Loss: 10.04607492685318\n",
      "Epoch 3, Loss: 9.31147849559784\n",
      "Epoch 4, Loss: 8.69989687204361\n",
      "Epoch 5, Loss: 8.187074899673462\n",
      "Epoch 6, Loss: 7.751776933670044\n",
      "Epoch 7, Loss: 7.377147078514099\n",
      "Epoch 8, Loss: 7.05041640996933\n",
      "Epoch 9, Loss: 6.762041449546814\n",
      "Epoch 10, Loss: 6.504855155944824\n",
      "Epoch 11, Loss: 6.273402035236359\n",
      "Epoch 12, Loss: 6.063456773757935\n",
      "Epoch 13, Loss: 5.871697813272476\n",
      "Epoch 14, Loss: 5.695473223924637\n",
      "Epoch 15, Loss: 5.532641172409058\n",
      "Epoch 16, Loss: 5.381453633308411\n",
      "Epoch 17, Loss: 5.240472435951233\n",
      "Epoch 18, Loss: 5.108502894639969\n",
      "Epoch 19, Loss: 4.984549641609192\n",
      "Epoch 20, Loss: 4.867771923542023\n",
      "Epoch 21, Loss: 4.757462441921234\n",
      "Epoch 22, Loss: 4.653015643358231\n",
      "Epoch 23, Loss: 4.553913861513138\n",
      "Epoch 24, Loss: 4.459709823131561\n",
      "Epoch 25, Loss: 4.370016157627106\n",
      "Epoch 26, Loss: 4.284491837024689\n",
      "Epoch 27, Loss: 4.202840059995651\n",
      "Epoch 28, Loss: 4.1247954070568085\n",
      "Epoch 29, Loss: 4.050122678279877\n",
      "Epoch 30, Loss: 3.978610783815384\n",
      "Epoch 31, Loss: 3.9100694060325623\n",
      "Epoch 32, Loss: 3.8443251252174377\n",
      "Epoch 33, Loss: 3.781222388148308\n",
      "Epoch 34, Loss: 3.720616325736046\n",
      "Epoch 35, Loss: 3.662375047802925\n",
      "Epoch 36, Loss: 3.606377422809601\n",
      "Epoch 37, Loss: 3.5525088608264923\n",
      "Epoch 38, Loss: 3.5006657242774963\n",
      "Epoch 39, Loss: 3.450750172138214\n",
      "Epoch 40, Loss: 3.4026696383953094\n",
      "Epoch 41, Loss: 3.3563393354415894\n",
      "Epoch 42, Loss: 3.311679095029831\n",
      "Epoch 43, Loss: 3.2686130553483963\n",
      "Epoch 44, Loss: 3.2270692586898804\n",
      "Epoch 45, Loss: 3.1869812309741974\n",
      "Epoch 46, Loss: 3.148285210132599\n",
      "Epoch 47, Loss: 3.110920861363411\n",
      "Epoch 48, Loss: 3.0748311281204224\n",
      "Epoch 49, Loss: 3.0399614572525024\n",
      "Epoch 50, Loss: 3.006261244416237\n",
      "Epoch 51, Loss: 2.9736818373203278\n",
      "Epoch 52, Loss: 2.942175731062889\n",
      "Epoch 53, Loss: 2.9116996228694916\n",
      "Epoch 54, Loss: 2.88221175968647\n",
      "Epoch 55, Loss: 2.8536717146635056\n",
      "Epoch 56, Loss: 2.8260406851768494\n",
      "Epoch 57, Loss: 2.7992838621139526\n",
      "Epoch 58, Loss: 2.773365303874016\n",
      "Epoch 59, Loss: 2.748252719640732\n",
      "Epoch 60, Loss: 2.7239139527082443\n",
      "Epoch 61, Loss: 2.7003197073936462\n",
      "Epoch 62, Loss: 2.677440382540226\n",
      "Epoch 63, Loss: 2.655249208211899\n",
      "Epoch 64, Loss: 2.6337198689579964\n",
      "Epoch 65, Loss: 2.6128264516592026\n",
      "Epoch 66, Loss: 2.5925453305244446\n",
      "Epoch 67, Loss: 2.5728545263409615\n",
      "Epoch 68, Loss: 2.5537308529019356\n",
      "Epoch 69, Loss: 2.5351538583636284\n",
      "Epoch 70, Loss: 2.517103761434555\n",
      "Epoch 71, Loss: 2.4995605051517487\n",
      "Epoch 72, Loss: 2.482506789267063\n",
      "Epoch 73, Loss: 2.465923771262169\n",
      "Epoch 74, Loss: 2.4497954100370407\n",
      "Epoch 75, Loss: 2.434105545282364\n",
      "Epoch 76, Loss: 2.418838791549206\n",
      "Epoch 77, Loss: 2.403980068862438\n",
      "Epoch 78, Loss: 2.389514595270157\n",
      "Epoch 79, Loss: 2.375429719686508\n",
      "Epoch 80, Loss: 2.3617121502757072\n",
      "Epoch 81, Loss: 2.3483489602804184\n",
      "Epoch 82, Loss: 2.335327871143818\n",
      "Epoch 83, Loss: 2.3226379677653313\n",
      "Epoch 84, Loss: 2.3102679923176765\n",
      "Epoch 85, Loss: 2.298206612467766\n",
      "Epoch 86, Loss: 2.286445014178753\n",
      "Epoch 87, Loss: 2.274972155690193\n",
      "Epoch 88, Loss: 2.263779617846012\n",
      "Epoch 89, Loss: 2.252857528626919\n",
      "Epoch 90, Loss: 2.2421975135803223\n",
      "Epoch 91, Loss: 2.2317911982536316\n",
      "Epoch 92, Loss: 2.221630275249481\n",
      "Epoch 93, Loss: 2.211707040667534\n",
      "Epoch 94, Loss: 2.202014245092869\n",
      "Epoch 95, Loss: 2.192545346915722\n",
      "Epoch 96, Loss: 2.183291718363762\n",
      "Epoch 97, Loss: 2.1742486879229546\n",
      "Epoch 98, Loss: 2.1654086112976074\n",
      "Epoch 99, Loss: 2.1567654609680176\n"
     ]
    }
   ],
   "source": [
    "# パラメータの設定\n",
    "embedding_size = 10\n",
    "learning_rate = 0.01\n",
    "epochs = 100\n",
    "\n",
    "# モデルのインスタンス化\n",
    "model = SimpleCBOW(vocab_size, embedding_size)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 学習の実行\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for context, target in zip(contexts, targets):\n",
    "        # コンテキストとターゲットをテンソルに変換\n",
    "        context_tensor = torch.tensor(context, dtype=torch.long)\n",
    "        target_tensor = torch.tensor(target, dtype=torch.long)\n",
    "        \n",
    "        # 勾配情報をリセット\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 順伝播\n",
    "        log_probs = model(context_tensor)\n",
    "        \n",
    "        # 損失の計算\n",
    "        loss = loss_function(log_probs.view(1, -1), target_tensor.view(1))\n",
    "        \n",
    "        # 逆伝播\n",
    "        loss.backward()\n",
    "        \n",
    "        # パラメータの更新\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f'Epoch {epoch}, Loss: {total_loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyterbook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
