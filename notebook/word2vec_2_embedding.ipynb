{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前章では、「カウントベースの手法」によって単語分散表現を得ました。具体的には、単語の共起行列を作り、その行列に対してSVDを適用することで、密なベクトくー 単語分散表現ーを獲得したのです。\n",
    "\n",
    "しかし、カウントベースの手法にはいくつかの問題点があります。\n",
    "\n",
    "- 大規模なコーパスを扱う場合、巨大な共起行列に対してSVDを計算することが難しい。\n",
    "- コーパスの全体から一回の学習で単語分散表現を獲得していますので、新しい単語が追加される場合、再度最初から学習を行う必要があり、単語分散表現更新の効率が低い。\n",
    "\n",
    "「カウントベースの手法」に代わる強力な手法として「推論ベース」の手法が挙げられます。特に、Mikolov et al. {cite}`mikolov-etal-2013-linguistic` {cite}`NIPS2013_9aa42b31`　によって提案されたword2vecの有用性が多くの自然言語処理タスクにおいて示されてきたのです。\n",
    "\n",
    "本章では、word2vecの仕組みについて説明し、それを実装することで理解を深めます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 推論ベース手法とニューラルネットワーク\n",
    "\n",
    "推論ベースの手法は、ミニバッチで学習する形で、ニューラルネットワークを用いて、重みを繰り返し更新することで単語分散表現を獲得します。\n",
    "\n",
    "\n",
    "![](./Figure/inference.png)\n",
    "\n",
    "### 推論ベース手法の設計\n",
    "\n",
    "推論ベース手法では、```you 【？】 goodbye and I say hello .```のような、周囲の単語が与えられたときに、```【？】```にどのような単語が出現するのかを推測する推論問題を繰り返し解くことで、単語の出現バターンを学習します。\n",
    "\n",
    "つまり、コンテキスト情報を入力として受け取り、各単語の出現する確率を出力する「モデル」を作成することは目標になります。ここで、正しい推測ができるように、コーパスを使って、ニューラルネットワークモデルの学習を行います。そして、その学習の結果として、単語の分散表現を得られます。\n",
    "\n",
    "![](./Figure/inference2.png)\n",
    "\n",
    "### one-hot表現\n",
    "\n",
    "ニューラルネットワークで単語を処理するには、それを「固定長のベクトル」に変換する必要があります。\n",
    "\n",
    "そのための方法の一つは、単語をone-hot表現へと変換することです。one-hot表現とは、ベクトルの要素の中で一つだけが$1$で、残りは全て$0$であるようなベクトルと言います。\n",
    "\n",
    "単語をone-hot表現に変換するには、語彙数分の要素を持つベクトルを用意して、単語IDの該当する箇所を$1$に、残りは全て$0$に設定します。\n",
    "\n",
    "\n",
    "![](./Figure/one-hot.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CBOW（continuous bag-of-words）モデル\n",
    "\n",
    "CBOWモデルは、コンテキストからターゲットを推測することを目的としたニューラルネットワークです。このCBOWモデルで、できるだけ正確な推測ができるように訓練することで、単語の分散表現を取得することができます。\n",
    "\n",
    "ここで、例として、コンテキスト```[\"you\",\"goodbye\"]```からターゲット```\"say\"```を予測するタスクを考えます。\n",
    "\n",
    "![](./Figure/cbow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 入力層から中間層(エンコード)\n",
    "\n",
    "one-hotエンコーディングで、単語を固定長のベクトルに変換するすることができます。\n",
    "\n",
    "単語をベクトルで表すことができれば、そのベクトルはニューラルネットワークを構成する「レイヤ」によって処理することができるようになりました。\n",
    "\n",
    "コンテキストを$\\mathbf{c}$、重みを$\\mathbf{W}$とし、それぞれ次の形状とします。\n",
    "\n",
    "```{margin}\n",
    "イメージしやすいように、ここでは添字を対応する単語で表すことにします。ただし「.(ピリオド)」については「priod」とします。\n",
    "```\n",
    "\n",
    "$$\n",
    "\\mathbf{c}\n",
    "    = \\begin{pmatrix}\n",
    "          c_{\\mathrm{you}} & c_{\\mathrm{say}} & c_{\\mathrm{goodbye}} & c_{\\mathrm{and}} & c_{\\mathrm{I}} & c_{\\mathrm{hello}} & c_{\\mathrm{period}}\n",
    "      \\end{pmatrix}\n",
    ",\\ \n",
    "\\mathbf{W}\n",
    "    = \\begin{pmatrix}\n",
    "          w_{\\mathrm{you},1} & w_{\\mathrm{you},2} & w_{\\mathrm{you},3} \\\\\n",
    "          w_{\\mathrm{say},1} & w_{\\mathrm{say},2} & w_{\\mathrm{say},3} \\\\\n",
    "          w_{\\mathrm{goodbye},1} & w_{\\mathrm{goodbye},2} & w_{\\mathrm{goodbye},3} \\\\\n",
    "          w_{\\mathrm{and},1} & w_{\\mathrm{and},2} & w_{\\mathrm{and},3} \\\\\n",
    "          w_{\\mathrm{I},1} & w_{\\mathrm{I},2} & w_{\\mathrm{I},3} \\\\\n",
    "          w_{\\mathrm{hello},1} & w_{\\mathrm{hello},2} & w_{\\mathrm{hello},3} \\\\\n",
    "          w_{\\mathrm{period},1} & w_{\\mathrm{period},2} & w_{\\mathrm{period},3} \\\\\n",
    "      \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "コンテキストの要素数(列数)と重みの行数が、単語の種類数に対応します。\n",
    "\n",
    "コンテキスト(単語)はone-hot表現として扱うため、例えば「you」の場合は\n",
    "\n",
    "$$\n",
    "\\mathbf{c}_{\\mathrm{you}}\n",
    "    = \\begin{pmatrix}\n",
    "          1 & 0 & 0 & 0 & 0 & 0 & 0\n",
    "      \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "とすることで、単語「you」を表現できます。\n",
    "\n",
    "重み付き和$\\mathbf{h}$は、行列の積で求められます。\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf{h}\n",
    "   &= \\mathbf{c}_{\\mathrm{you}}\n",
    "      \\mathbf{W}\n",
    "\\\\\n",
    "   &= \\begin{pmatrix}\n",
    "          h_1 & h_2 & h_3\n",
    "      \\end{pmatrix}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$h_1$の計算を詳しく見ると、次のようになります。\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "h_1\n",
    "   &= c_{\\mathrm{you}} w_{\\mathrm{you},1}\n",
    "      + c_{\\mathrm{say}} w_{\\mathrm{say},1}\n",
    "      + c_{\\mathrm{goodbye}} w_{\\mathrm{goodbye},1}\n",
    "      + c_{\\mathrm{and}} w_{\\mathrm{and},1}\n",
    "      + c_{\\mathrm{I}} w_{\\mathrm{I},1}\n",
    "      + c_{\\mathrm{hello}} w_{\\mathrm{hello},1}\n",
    "      + c_{\\mathrm{period}} w_{\\mathrm{period},1}\n",
    "\\\\\n",
    "   &= 1 w_{\\mathrm{you},1}\n",
    "      + 0 w_{\\mathrm{say},1}\n",
    "      + 0 w_{\\mathrm{goodbye},1}\n",
    "      + 0 w_{\\mathrm{and},1}\n",
    "      + 0 w_{\\mathrm{I},1}\n",
    "      + 0 w_{\\mathrm{hello},1}\n",
    "      + 0 w_{\\mathrm{period},1}\n",
    "\\\\\n",
    "   &= w_{\\mathrm{you},1}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "コンテキストと重みの対応する(同じ単語に関する)要素を掛けて、全ての単語で和をとります。しかしコンテキストは、$c_{you}$以外の要素が$0$なので、対応する重みの値の影響は消えていまします。また$c_{you}$は$1$なので、対応する重みの値$w_{\\mathrm{you},1}$がそのまま中間層のニューロンに伝播します。\n",
    "\n",
    "残りの2つの要素も同様に計算できるので、重み付き和\n",
    "\n",
    "$$\n",
    "\\mathbf{h}\n",
    "    = \\begin{pmatrix}\n",
    "          w_{\\mathrm{you},1} & w_{\\mathrm{you},2} & w_{\\mathrm{you},3}\n",
    "      \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "は、単語「you」に関する重みの値となります。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "コンテキストの形状：(1, 7)\n",
      "重み\n",
      "[[-0.82442318  0.90600941  0.9241718 ]\n",
      " [-0.42512631 -0.23285085  0.58274328]\n",
      " [ 0.39604055  0.83321188 -1.11293102]\n",
      " [-2.28395881 -0.52467955  0.26106726]\n",
      " [ 0.15591635  1.93327041 -2.29409049]\n",
      " [-0.052742    0.07993214 -0.65729455]\n",
      " [-1.50153645  0.59617421 -0.60090422]]\n",
      "重み付き和\n",
      "[[-0.82442318  0.90600941  0.9241718 ]]\n",
      "重み付き和の形状：(1, 3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 適当にコンテキスト(one-hot表現)を指定\n",
    "c = np.array([[1, 0, 0, 0, 0, 0, 0]])\n",
    "print(f\"コンテキストの形状：{c.shape}\")\n",
    "\n",
    "# 重みをランダムに生成\n",
    "W = np.random.randn(7, 3)\n",
    "print(f\"重み\\n{W}\")\n",
    "\n",
    "# 重み付き和を計算\n",
    "h = np.dot(c, W)\n",
    "print(f\"重み付き和\\n{h}\")\n",
    "print(f\"重み付き和の形状：{h.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "コンテキストに複数な単語がある場合、入力層も複数になります。このとき、中間層にあるニューロンは、各入力層の全結合による変換後の値が平均されたものになります。\n",
    "\n",
    "中間層のニューロンの数を入力層よりも減らすことによって、中間層には、単語を予測するために必要な情報が\"コンパクト\"に収められて、結果としては密なベクトル表現が得られます。このとき、この中間層の情報は、人間には理解できない「ブラックボックス」ような状態になります。この作業は、「エンコード」と言います。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 中間層から出力層(デコード)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "中間層の情報から目的の結果を得る作業は、「デコード」と言います。ここでは、中間層のニューロンの値$\\mathbf{h}$を各単語に対応した値になるように、つまり要素(行)数が単語の種類数となるように再度変換したものを、CBOWモデルの出力とします。\n",
    "\n",
    "出力層の重みを\n",
    "\n",
    "$$\n",
    "\\mathbf{W}_{\\mathrm{out}}\n",
    "    = \\begin{pmatrix}\n",
    "          w_{1,\\mathrm{you}} & w_{1,\\mathrm{say}} & w_{1,\\mathrm{goodbye}} & w_{1,\\mathrm{and}} &\n",
    "          w_{1,\\mathrm{I}} & w_{1,\\mathrm{hello}} & w_{1,\\mathrm{period}} \\\\\n",
    "          w_{2,\\mathrm{you}} & w_{2,\\mathrm{say}} & w_{2,\\mathrm{goodbye}} & w_{2,\\mathrm{and}} &\n",
    "          w_{2,\\mathrm{I}} & w_{2,\\mathrm{hello}} & w_{2,\\mathrm{period}} \\\\\n",
    "          w_{3,\\mathrm{you}} & w_{3,\\mathrm{say}} & w_{3,\\mathrm{goodbye}} & w_{3,\\mathrm{and}} &\n",
    "          w_{3,\\mathrm{I}} & w_{3,\\mathrm{hello}} & w_{3\\mathrm{period}} \\\\\n",
    "      \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "とします。行数が中間層のニューロン数、列数が単語の種類数になります。\n",
    "\n",
    "出力層も全結合層とすると、最終的な出力は\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf{s}\n",
    "   &= \\mathbf{h}\n",
    "      \\mathbf{W}_{\\mathrm{out}}\n",
    "\\\\\n",
    "   &= \\begin{pmatrix}\n",
    "          s_{\\mathrm{you}} & s_{\\mathrm{say}} & s_{\\mathrm{goodbye}} & s_{\\mathrm{and}} &\n",
    "          s_{\\mathrm{I}} & s_{\\mathrm{hello}} & s_{\\mathrm{period}}\n",
    "      \\end{pmatrix}\n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "例えば、「you」に関する要素の計算は、\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "s_{\\mathrm{you}}\n",
    "   &= \\frac{1}{2} (w_{\\mathrm{you},1} + w_{\\mathrm{goodbye},1}) w_{1,\\mathrm{you}}\n",
    "      + \\frac{1}{2} (w_{\\mathrm{you},2} + w_{\\mathrm{goodbye},2}) w_{2,\\mathrm{you}}\n",
    "      + \\frac{1}{2} (w_{\\mathrm{you},3} + w_{\\mathrm{goodbye},3}) w_{3,\\mathrm{you}}\n",
    "\\\\\n",
    "   &= \\frac{1}{2}\n",
    "      \\sum_{i=1}^3\n",
    "          (w_{\\mathrm{you},i} + w_{\\mathrm{goodbye},i}) w_{i,\\mathrm{you}}\n",
    "\\end{aligned}\n",
    "\n",
    "$$\n",
    "\n",
    "コンテキストに対応する入力層の重みの平均と「you」に関する出力の重みの積になります。\n",
    "\n",
    "他の要素(単語)についても同様に計算できるので、最終的な出力は\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf{s}\n",
    "   &= \\begin{pmatrix}\n",
    "          s_{\\mathrm{you}} & s_{\\mathrm{say}} & s_{\\mathrm{goodbye}} & s_{\\mathrm{and}} &\n",
    "          s_{\\mathrm{I}} & s_{\\mathrm{hello}} & s_{\\mathrm{period}}\n",
    "      \\end{pmatrix}\n",
    "\\\\\n",
    "   &= \\begin{pmatrix}\n",
    "          \\frac{1}{2} \\sum_{i=1}^3 (w_{\\mathrm{you},i} + w_{\\mathrm{goodbye},i}) w_{i,\\mathrm{you}} &\n",
    "          \\frac{1}{2} \\sum_{i=1}^3 (w_{\\mathrm{you},i} + w_{\\mathrm{goodbye},i}) w_{i,\\mathrm{say}} &\n",
    "          \\cdots &\n",
    "          \\frac{1}{2} \\sum_{i=1}^3 (w_{\\mathrm{you},i} + w_{\\mathrm{goodbye},i}) w_{i,\\mathrm{hello}} &\n",
    "          \\frac{1}{2} \\sum_{i=1}^3 (w_{\\mathrm{you},i} + w_{\\mathrm{goodbye},i}) w_{i,\\mathrm{period}}\n",
    "      \\end{pmatrix}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "となります。\n",
    "\n",
    "ここで、出力層のニューロンは各単語に対応し、各単語の「スコア」と言います。\n",
    "\n",
    "「スコア」の値が高ければ高いほど、それに対応する単語の出現確率も高くなり、ターゲットの単語であるとして採用します。そのため、スコアを求める処理を推論処理と言います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1.4005,  0.5182, -0.2709]]),\n",
       " tensor([[ 0.9881,  1.0239, -0.5802]]),\n",
       " tensor([[ 1.1943,  0.7711, -0.4256]]),\n",
       " tensor([[ 1.9680, -0.4000,  0.5640, -2.5090,  0.1770,  1.0630, -0.0090]]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "# Define the context data\n",
    "c0 = torch.tensor([[1, 0, 0, 0, 0, 0, 0]], dtype=torch.float32) # you\n",
    "c1 = torch.tensor([[0, 0, 1, 0, 0, 0, 0]], dtype=torch.float32) # goodbye\n",
    "\n",
    "# Initialize weights randomly\n",
    "W_in = torch.randn(7, 3, requires_grad=False)  # Input layer weights\n",
    "W_out = torch.randn(3, 7, requires_grad=False) # Output layer weights\n",
    "\n",
    "# Define the layers using PyTorch's functional API\n",
    "def in_layer(x, W):\n",
    "    return torch.matmul(x, W)\n",
    "\n",
    "def out_layer(h, W):\n",
    "    return torch.matmul(h, W)\n",
    "\n",
    "# Forward pass through the input layers\n",
    "h0 = in_layer(c0, W_in) # you\n",
    "h1 = in_layer(c1, W_in) # goodbye\n",
    "h = 0.5 * (h0 + h1)\n",
    "\n",
    "# Forward pass through the output layer (scores)\n",
    "s = out_layer(h, W_out)\n",
    "\n",
    "# Print the outputs\n",
    "h0, h1, h, torch.round(s, decimals=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````{tab-set}\n",
    "```{tab-item} 課題\n",
    "正解は「say」として、Softmax関数によってスコア``s``を確率として扱えるように変換し、そして、正規化した値と教師ラベルを用いて損失を求めなさい。\n",
    "```\n",
    "\n",
    "```{tab-item} ヒント\n",
    "正解は「say」の場合、教師ラベルは``[0, 1, 0, 0, 0, 0, 0]``になります。\n",
    "```\n",
    "\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### word2vecの重みと分散表現"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与えられたコンテキストに対して単語を予測するときに、「良い重み」のネットワークがあれば、「確率」を表すニューロンにおいて、正解に対応するニューロンが高くなっていることが期待できます。そして、大規模コーパスを使って得られる単語の分散表現は、単語の意味や文法のルールにおいて、人間の直感と合致するケースが多く見られます。\n",
    "\n",
    "\n",
    "word2vecモデルの学習で行うことが、正しい予測ができるように重みを調整することです。つまり、「コンテキストから出現単語」を予測するという偽タスクをニューラルネットで解いてきましたが、目的はニューラルネットの重みを求めることになります。\n",
    "\n",
    "もっと具体的に言えば、word2vecで使用されるネットワークには二つの重みがあります。それは、入力層の重み$\\mathbf{W_{in}}$と、出力層の重み$\\mathbf{W_{out}}$です。それでは、どちらの重みを使えば良いでしょうか？\n",
    "\n",
    "1. 入力側の重みを利用する\n",
    "2. 出力側の重みを利用する\n",
    "3. 二つの重みの両方を利用する\n",
    "\n",
    "Word2Vecモデルに関しては、多くの研究や応用例で、入力層の重みを単語のベクトル表現として使用さており、良好なパフォーマンスを示しています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vecモデルの実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習データの準備\n",
    "\n",
    "#### コンテキストとターゲット\n",
    "\n",
    "Word2Vecモデルためのニューラルネットワークでは、「コンテキスト」を入力した時に、「ターゲット」が出現する確率を高くになるように学習を行います。\n",
    "\n",
    "そのため、コーパスから「コンテキスト」と「ターゲット」が対応するデータを作成する必要があります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前処理関数の実装\n",
    "def preprocess(text):\n",
    "    # 前処理\n",
    "    text = text.lower() # 小文字に変換\n",
    "    text = text.replace('.', ' .') # ピリオドの前にスペースを挿入\n",
    "    words = text.split(' ') # 単語ごとに分割\n",
    "    \n",
    "    # ディクショナリを初期化\n",
    "    word_to_id = {}\n",
    "    id_to_word = {}\n",
    "    \n",
    "    # 未収録の単語をディクショナリに格納\n",
    "    for word in words:\n",
    "        if word not in word_to_id: # 未収録の単語のとき\n",
    "            # 次の単語のidを取得\n",
    "            new_id = len(word_to_id)\n",
    "            \n",
    "            # 単語をキーとして単語IDを格納\n",
    "            word_to_id[word] = new_id\n",
    "            \n",
    "            # 単語IDをキーとして単語を格納\n",
    "            id_to_word[new_id] = word\n",
    "    \n",
    "    # 単語IDリストを作成\n",
    "    corpus = [word_to_id[w] for w in words]\n",
    "    \n",
    "    return corpus, word_to_id, id_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'you': 0, 'say': 1, 'goodbye': 2, 'and': 3, 'i': 4, 'hello': 5, '.': 6}\n",
      "{0: 'you', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'i', 5: 'hello', 6: '.'}\n",
      "[0, 1, 2, 3, 4, 1, 5, 6]\n"
     ]
    }
   ],
   "source": [
    "# テキストを設定\n",
    "text = 'You say goodbye and I say hello.'\n",
    "\n",
    "# 前処理\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "print(word_to_id)\n",
    "print(id_to_word)\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "テキストの単語を単語IDに変換した```corpus```からターゲットを抽出します。\n",
    "\n",
    "ターゲットはコンテキストの中央の単語なので、```corpus```の始めと終わりのウインドウサイズ分の単語は含めません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 1, 5]\n"
     ]
    }
   ],
   "source": [
    "# ウインドウサイズを指定\n",
    "window_size = 1\n",
    "\n",
    "# ターゲットを抽出\n",
    "target = corpus[window_size:-window_size]\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ターゲットの単語に対して、for文で前後ウィンドウサイズの範囲の単語を順番に抽出し```cs```に格納します。\n",
    "\n",
    "つまりウィンドウサイズを$1$とすると、```corpus```におけるターゲットのインデックス```idx```に対して、1つ前(```idx - window_size```)から1つ後(```idx + window_size```)までの範囲の単語を順番に```cs```格納します。ただしターゲット自体の単語はコンテキストに含めません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n",
      "[0, 2]\n",
      "[[0, 2]]\n"
     ]
    }
   ],
   "source": [
    "# コンテキストを初期化(受け皿を作成)\n",
    "contexts = []\n",
    "\n",
    "# 1つ目のターゲットのインデックス\n",
    "idx = window_size\n",
    "\n",
    "# 1つ目のターゲットのコンテキストを初期化(受け皿を作成)\n",
    "cs = []\n",
    "\n",
    "# 1つ目のターゲットのコンテキストを1単語ずつ格納\n",
    "for t in range(-window_size, window_size + 1):\n",
    "    \n",
    "    # tがターゲットのインデックスのとき処理しない\n",
    "    if t == 0:\n",
    "        continue\n",
    "    \n",
    "    # コンテキストを格納\n",
    "    cs.append(corpus[idx + t])\n",
    "    print(cs)\n",
    "\n",
    "# 1つ目のターゲットのコンテキストを格納\n",
    "contexts.append(cs)\n",
    "print(contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# コンテキストとターゲットの作成関数の実装\n",
    "def create_contexts_target(corpus, window_size=1):\n",
    "    \n",
    "    # ターゲットを抽出\n",
    "    target = corpus[window_size:-window_size]\n",
    "    \n",
    "    # コンテキストを初期化\n",
    "    contexts = []\n",
    "    \n",
    "    # ターゲットごとにコンテキストを格納\n",
    "    for idx in range(window_size, len(corpus) - window_size):\n",
    "        \n",
    "        # 現在のターゲットのコンテキストを初期化\n",
    "        cs = []\n",
    "        \n",
    "        # 現在のターゲットのコンテキストを1単語ずつ格納\n",
    "        for t in range(-window_size, window_size + 1):\n",
    "            \n",
    "            # 0番目の要素はターゲットそのものなので処理を省略\n",
    "            if t == 0:\n",
    "                continue\n",
    "            \n",
    "            # コンテキストを格納\n",
    "            cs.append(corpus[idx + t])\n",
    "            \n",
    "        # 現在のターゲットのコンテキストのセットを格納\n",
    "        contexts.append(cs)\n",
    "    \n",
    "    # NumPy配列に変換\n",
    "    return np.array(contexts), np.array(target) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 2]\n",
      " [1 3]\n",
      " [2 4]\n",
      " [3 1]\n",
      " [4 5]\n",
      " [1 6]]\n",
      "[1 2 3 4 1 5]\n"
     ]
    }
   ],
   "source": [
    "# コンテキストとターゲットを作成\n",
    "contexts, targets = create_contexts_target(corpus, window_size=1)\n",
    "print(contexts)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PytorchでCBOWモデルの実装\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embeddingレイヤ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先ほど、理解しやすいone-hot表現でコンテキストを変換する方法を説明しましたが、大規模なコーパスで学習する際、one-hot表現の次元数も大きくになって、非効率な学習の原因になります。\n",
    "\n",
    "ただ、one-hot表現による計算は、単に行列の特定の行を抜き出すことだけですから、同じ機能を持つレイヤで入れ替えることは可能です。このような、重みパラメータから「単語IDに該当する行(ベクトル)」を抜き出すためのレイヤは「Embeddingレイヤ」と言います。\n",
    "\n",
    "PyTorchで提供されるモジュール```nn.Embedding```を使うと、簡単にEmbeddingレイヤを実装することができます。\n",
    "\n",
    "例えば、語彙に6つの単語があり、各埋め込みベクトルの次元数を3に設定した場合、nn.Embeddingの定義は以下のようになります。\n",
    "\n",
    "そして、"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = nn.Embedding(6, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "もしインデックス2のトークンの埋め込みを取得したい場合、次のようにします："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3195,  2.2811,  1.1859],\n",
       "         [ 0.2258,  0.4615, -0.5164]]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = torch.tensor([[1,2]], dtype=torch.long)\n",
    "embedding = embedding_layer(inputs)\n",
    "embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "埋め込みベクトルの和を取って、入力層から中間層までにエンコードの機能を実装できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0937,  2.7425,  0.6695]], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out=torch.sum(embedding, dim=1)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear1 = nn.Linear(3, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0842, -1.1985, -1.7372, -2.1813, -3.1823, -3.5150]],\n",
       "       grad_fn=<LogSoftmaxBackward0>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.log_softmax(linear1(out), dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ミニバッチ化データセットの作成\n",
    "\n",
    "Word2Vecも含めて、深層学習によって学習を行う際には、ミニバッチ化して学習させることが一般的です。\n",
    "\n",
    "pytorchで提供されている```DataSet```と```DataLoader```という機能を用いてミニバッチ化を簡単に実現できます。\n",
    "\n",
    "##### DataSet\n",
    "\n",
    "DataSetは，元々のデータを全て持っていて、ある番号を指定されると、その番号の入出力のペアをただ一つ返します。クラスを使って実装します。\n",
    "\n",
    "DataSetを実装する際には、クラスのメンバ関数として```__len__()```と```__getitem__()```を必ず作ります．\n",
    "\n",
    "- ```__len__()```は、```len()```を使ったときに呼ばれる関数です。\n",
    "- ```__getitem__()```は、```array[i]```のようにインデックスを使って要素を参照するときに呼ばれる関数です。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOWDataset(Dataset):\n",
    "    def __init__(self, contexts, targets):\n",
    "        self.contexts = contexts\n",
    "        self.targets = targets\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.contexts[idx], self.targets[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert contexts and targets to tensors\n",
    "contexts_tensor = torch.tensor(contexts, dtype=torch.long).to(device)\n",
    "targets_tensor = torch.tensor(targets, dtype=torch.long).to(device)\n",
    "\n",
    "# Create the dataset\n",
    "dataset = CBOWDataset(contexts_tensor, targets_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "全データ数: 6\n",
      "4番目のデータ: (tensor([3, 1], device='cuda:0'), tensor(4, device='cuda:0'))\n",
      "4~5番目のデータ: (tensor([[3, 1],\n",
      "        [4, 5]], device='cuda:0'), tensor([4, 1], device='cuda:0'))\n"
     ]
    }
   ],
   "source": [
    "print('全データ数:',len(dataset))\n",
    "print('4番目のデータ:',dataset[3]) \n",
    "print('4~5番目のデータ:',dataset[3:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DataLoader\n",
    "\n",
    "```torch.utils.data```モジュールには、データのシャッフとミニバッチの整形に役立つ```DataLoader```というクラスが用意されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DataLoader\n",
    "batch_size = 2  # You can adjust the batch size\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[2, 4],\n",
      "        [0, 2]], device='cuda:0'), tensor([3, 1], device='cuda:0')]\n",
      "[tensor([[4, 5],\n",
      "        [1, 3]], device='cuda:0'), tensor([1, 2], device='cuda:0')]\n",
      "[tensor([[1, 6],\n",
      "        [3, 1]], device='cuda:0'), tensor([5, 4], device='cuda:0')]\n"
     ]
    }
   ],
   "source": [
    "for data in data_loader:\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CBOWモデルの構築"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ```self.embeddings = nn.Embedding(vocab_size, embedding_size)```: 語彙の各単語に対して```embedding_size```次元のベクトルを割り当てる埋め込み層を作成します。\n",
    "- ```self.linear1 = nn.Linear(embedding_size, vocab_size)```: 埋め込みベクトルを受け取り、語彙のサイズに対応する出力を生成します。\n",
    "- ```embeds = self.embeddings(inputs)```:入力された単語のインデックスに基づいて、埋め込み層から対応するベクトルを取得します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size):\n",
    "        super(SimpleCBOW, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.linear1 = nn.Linear(embedding_size, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs)\n",
    "        out = torch.sum(embeds, dim=1)\n",
    "        out = self.linear1(out)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Total loss: 6.988481760025024\n",
      "Epoch 1, Total loss: 6.806377410888672\n",
      "Epoch 2, Total loss: 6.633864879608154\n",
      "Epoch 3, Total loss: 6.472071528434753\n",
      "Epoch 4, Total loss: 6.328179717063904\n",
      "Epoch 5, Total loss: 6.169561147689819\n",
      "Epoch 6, Total loss: 6.027028322219849\n",
      "Epoch 7, Total loss: 5.891311407089233\n",
      "Epoch 8, Total loss: 5.760920882225037\n",
      "Epoch 9, Total loss: 5.637338161468506\n",
      "Epoch 10, Total loss: 5.5156025886535645\n",
      "Epoch 11, Total loss: 5.405279636383057\n",
      "Epoch 12, Total loss: 5.3040771484375\n",
      "Epoch 13, Total loss: 5.1860363483428955\n",
      "Epoch 14, Total loss: 5.088816165924072\n",
      "Epoch 15, Total loss: 4.9916582107543945\n",
      "Epoch 16, Total loss: 4.89832878112793\n",
      "Epoch 17, Total loss: 4.8065489530563354\n",
      "Epoch 18, Total loss: 4.723245859146118\n",
      "Epoch 19, Total loss: 4.635175704956055\n",
      "Epoch 20, Total loss: 4.564026355743408\n",
      "Epoch 21, Total loss: 4.476324915885925\n",
      "Epoch 22, Total loss: 4.393905282020569\n",
      "Epoch 23, Total loss: 4.320005893707275\n",
      "Epoch 24, Total loss: 4.266999244689941\n",
      "Epoch 25, Total loss: 4.185740947723389\n",
      "Epoch 26, Total loss: 4.110549807548523\n",
      "Epoch 27, Total loss: 4.0481603145599365\n",
      "Epoch 28, Total loss: 3.9914798736572266\n",
      "Epoch 29, Total loss: 3.9273629188537598\n",
      "Epoch 30, Total loss: 3.8739067912101746\n",
      "Epoch 31, Total loss: 3.7993518114089966\n",
      "Epoch 32, Total loss: 3.745306432247162\n",
      "Epoch 33, Total loss: 3.6924952268600464\n",
      "Epoch 34, Total loss: 3.646523952484131\n",
      "Epoch 35, Total loss: 3.585528016090393\n",
      "Epoch 36, Total loss: 3.54123055934906\n",
      "Epoch 37, Total loss: 3.48138165473938\n",
      "Epoch 38, Total loss: 3.4405620098114014\n",
      "Epoch 39, Total loss: 3.3769712448120117\n",
      "Epoch 40, Total loss: 3.344673991203308\n",
      "Epoch 41, Total loss: 3.2937464714050293\n",
      "Epoch 42, Total loss: 3.255847692489624\n",
      "Epoch 43, Total loss: 3.1929121017456055\n",
      "Epoch 44, Total loss: 3.1570370197296143\n",
      "Epoch 45, Total loss: 3.1198480129241943\n",
      "Epoch 46, Total loss: 3.0817267894744873\n",
      "Epoch 47, Total loss: 3.0253578424453735\n",
      "Epoch 48, Total loss: 2.994727849960327\n",
      "Epoch 49, Total loss: 2.955052137374878\n",
      "Epoch 50, Total loss: 2.914321780204773\n",
      "Epoch 51, Total loss: 2.8864685893058777\n",
      "Epoch 52, Total loss: 2.8387924432754517\n",
      "Epoch 53, Total loss: 2.816656231880188\n",
      "Epoch 54, Total loss: 2.7657448053359985\n",
      "Epoch 55, Total loss: 2.7407021522521973\n",
      "Epoch 56, Total loss: 2.7087987661361694\n",
      "Epoch 57, Total loss: 2.6844704151153564\n",
      "Epoch 58, Total loss: 2.6449644565582275\n",
      "Epoch 59, Total loss: 2.6171027421951294\n",
      "Epoch 60, Total loss: 2.5727465748786926\n",
      "Epoch 61, Total loss: 2.5573624968528748\n",
      "Epoch 62, Total loss: 2.5137729048728943\n",
      "Epoch 63, Total loss: 2.4899107217788696\n",
      "Epoch 64, Total loss: 2.4717594385147095\n",
      "Epoch 65, Total loss: 2.433053970336914\n",
      "Epoch 66, Total loss: 2.4140374064445496\n",
      "Epoch 67, Total loss: 2.3916066884994507\n",
      "Epoch 68, Total loss: 2.366456985473633\n",
      "Epoch 69, Total loss: 2.3429169058799744\n",
      "Epoch 70, Total loss: 2.307699143886566\n",
      "Epoch 71, Total loss: 2.293011426925659\n",
      "Epoch 72, Total loss: 2.2697808742523193\n",
      "Epoch 73, Total loss: 2.237609386444092\n",
      "Epoch 74, Total loss: 2.211161494255066\n",
      "Epoch 75, Total loss: 2.1889111697673798\n",
      "Epoch 76, Total loss: 2.1777905225753784\n",
      "Epoch 77, Total loss: 2.1605153679847717\n",
      "Epoch 78, Total loss: 2.1388895511627197\n",
      "Epoch 79, Total loss: 2.0986939668655396\n",
      "Epoch 80, Total loss: 2.0948925018310547\n",
      "Epoch 81, Total loss: 2.0766595005989075\n",
      "Epoch 82, Total loss: 2.0602009892463684\n",
      "Epoch 83, Total loss: 2.0383423566818237\n",
      "Epoch 84, Total loss: 2.015181601047516\n",
      "Epoch 85, Total loss: 1.9973920285701752\n",
      "Epoch 86, Total loss: 1.9878840446472168\n",
      "Epoch 87, Total loss: 1.9508140683174133\n",
      "Epoch 88, Total loss: 1.9538979530334473\n",
      "Epoch 89, Total loss: 1.9297703504562378\n",
      "Epoch 90, Total loss: 1.9012849628925323\n",
      "Epoch 91, Total loss: 1.9028303027153015\n",
      "Epoch 92, Total loss: 1.8733902275562286\n",
      "Epoch 93, Total loss: 1.8727006912231445\n",
      "Epoch 94, Total loss: 1.8570311069488525\n",
      "Epoch 95, Total loss: 1.8284960091114044\n",
      "Epoch 96, Total loss: 1.8314759731292725\n",
      "Epoch 97, Total loss: 1.8142567574977875\n",
      "Epoch 98, Total loss: 1.8030870854854584\n",
      "Epoch 99, Total loss: 1.7870511412620544\n",
      "Epoch 100, Total loss: 1.7767792642116547\n",
      "Epoch 101, Total loss: 1.7574611902236938\n",
      "Epoch 102, Total loss: 1.748590111732483\n",
      "Epoch 103, Total loss: 1.7208065390586853\n",
      "Epoch 104, Total loss: 1.7237398624420166\n",
      "Epoch 105, Total loss: 1.7105964720249176\n",
      "Epoch 106, Total loss: 1.7019947171211243\n",
      "Epoch 107, Total loss: 1.6696474254131317\n",
      "Epoch 108, Total loss: 1.6613187193870544\n",
      "Epoch 109, Total loss: 1.646991342306137\n",
      "Epoch 110, Total loss: 1.6566587686538696\n",
      "Epoch 111, Total loss: 1.6457946598529816\n",
      "Epoch 112, Total loss: 1.6174585819244385\n",
      "Epoch 113, Total loss: 1.6223495602607727\n",
      "Epoch 114, Total loss: 1.6121387481689453\n",
      "Epoch 115, Total loss: 1.6026344895362854\n",
      "Epoch 116, Total loss: 1.5921743214130402\n",
      "Epoch 117, Total loss: 1.5641924738883972\n",
      "Epoch 118, Total loss: 1.572550117969513\n",
      "Epoch 119, Total loss: 1.5658894777297974\n",
      "Epoch 120, Total loss: 1.5508646070957184\n",
      "Epoch 121, Total loss: 1.5427683591842651\n",
      "Epoch 122, Total loss: 1.5394942462444305\n",
      "Epoch 123, Total loss: 1.527772068977356\n",
      "Epoch 124, Total loss: 1.5008396804332733\n",
      "Epoch 125, Total loss: 1.5082443356513977\n",
      "Epoch 126, Total loss: 1.5021345913410187\n",
      "Epoch 127, Total loss: 1.496408075094223\n",
      "Epoch 128, Total loss: 1.4879338145256042\n",
      "Epoch 129, Total loss: 1.475215345621109\n",
      "Epoch 130, Total loss: 1.4675228595733643\n",
      "Epoch 131, Total loss: 1.4482731521129608\n",
      "Epoch 132, Total loss: 1.458148866891861\n",
      "Epoch 133, Total loss: 1.4331899583339691\n",
      "Epoch 134, Total loss: 1.44262033700943\n",
      "Epoch 135, Total loss: 1.4361761212348938\n",
      "Epoch 136, Total loss: 1.411525011062622\n",
      "Epoch 137, Total loss: 1.401078999042511\n",
      "Epoch 138, Total loss: 1.414537489414215\n",
      "Epoch 139, Total loss: 1.4076247215270996\n",
      "Epoch 140, Total loss: 1.3996496498584747\n",
      "Epoch 141, Total loss: 1.3901880383491516\n",
      "Epoch 142, Total loss: 1.3712856769561768\n",
      "Epoch 143, Total loss: 1.363778978586197\n",
      "Epoch 144, Total loss: 1.3713312149047852\n",
      "Epoch 145, Total loss: 1.3658958971500397\n",
      "Epoch 146, Total loss: 1.3592911660671234\n",
      "Epoch 147, Total loss: 1.3393923044204712\n",
      "Epoch 148, Total loss: 1.3517257571220398\n",
      "Epoch 149, Total loss: 1.3287510126829147\n",
      "Epoch 150, Total loss: 1.3400940299034119\n",
      "Epoch 151, Total loss: 1.3343506455421448\n",
      "Epoch 152, Total loss: 1.32508784532547\n",
      "Epoch 153, Total loss: 1.3034524619579315\n",
      "Epoch 154, Total loss: 1.3009913563728333\n",
      "Epoch 155, Total loss: 1.312993735074997\n",
      "Epoch 156, Total loss: 1.2894545197486877\n",
      "Epoch 157, Total loss: 1.3008731603622437\n",
      "Epoch 158, Total loss: 1.2966298162937164\n",
      "Epoch 159, Total loss: 1.2923845946788788\n",
      "Epoch 160, Total loss: 1.2873218059539795\n",
      "Epoch 161, Total loss: 1.281117469072342\n",
      "Epoch 162, Total loss: 1.2605297565460205\n",
      "Epoch 163, Total loss: 1.2730720490217209\n",
      "Epoch 164, Total loss: 1.2689925283193588\n",
      "Epoch 165, Total loss: 1.262378215789795\n",
      "Epoch 166, Total loss: 1.257857084274292\n",
      "Epoch 167, Total loss: 1.2520751655101776\n",
      "Epoch 168, Total loss: 1.249254122376442\n",
      "Epoch 169, Total loss: 1.2461862564086914\n",
      "Epoch 170, Total loss: 1.2422760426998138\n",
      "Epoch 171, Total loss: 1.2363606989383698\n",
      "Epoch 172, Total loss: 1.2158631831407547\n",
      "Epoch 173, Total loss: 1.2286382019519806\n",
      "Epoch 174, Total loss: 1.2250518947839737\n",
      "Epoch 175, Total loss: 1.2012259811162949\n",
      "Epoch 176, Total loss: 1.2163141965866089\n",
      "Epoch 177, Total loss: 1.2107664197683334\n",
      "Epoch 178, Total loss: 1.2092834115028381\n",
      "Epoch 179, Total loss: 1.2031553387641907\n",
      "Epoch 180, Total loss: 1.1842111498117447\n",
      "Epoch 181, Total loss: 1.1985220462083817\n",
      "Epoch 182, Total loss: 1.1948480606079102\n",
      "Epoch 183, Total loss: 1.1723250299692154\n",
      "Epoch 184, Total loss: 1.187122255563736\n",
      "Epoch 185, Total loss: 1.183771163225174\n",
      "Epoch 186, Total loss: 1.1779111623764038\n",
      "Epoch 187, Total loss: 1.1762387454509735\n",
      "Epoch 188, Total loss: 1.1725916266441345\n",
      "Epoch 189, Total loss: 1.1689845025539398\n",
      "Epoch 190, Total loss: 1.1659347862005234\n",
      "Epoch 191, Total loss: 1.1634437143802643\n",
      "Epoch 192, Total loss: 1.1602091193199158\n",
      "Epoch 193, Total loss: 1.1568200290203094\n",
      "Epoch 194, Total loss: 1.153291642665863\n",
      "Epoch 195, Total loss: 1.1308256685733795\n",
      "Epoch 196, Total loss: 1.1454861462116241\n",
      "Epoch 197, Total loss: 1.1443446576595306\n",
      "Epoch 198, Total loss: 1.14181187748909\n",
      "Epoch 199, Total loss: 1.1205499172210693\n",
      "Epoch 200, Total loss: 1.1350953727960587\n",
      "Epoch 201, Total loss: 1.132478654384613\n",
      "Epoch 202, Total loss: 1.1297808587551117\n",
      "Epoch 203, Total loss: 1.126030370593071\n",
      "Epoch 204, Total loss: 1.124094381928444\n",
      "Epoch 205, Total loss: 1.1015050560235977\n",
      "Epoch 206, Total loss: 1.098761260509491\n",
      "Epoch 207, Total loss: 1.1158288419246674\n",
      "Epoch 208, Total loss: 1.1112976670265198\n",
      "Epoch 209, Total loss: 1.1098506301641464\n",
      "Epoch 210, Total loss: 1.107458084821701\n",
      "Epoch 211, Total loss: 1.105276733636856\n",
      "Epoch 212, Total loss: 1.0845544189214706\n",
      "Epoch 213, Total loss: 1.0983774065971375\n",
      "Epoch 214, Total loss: 1.096824198961258\n",
      "Epoch 215, Total loss: 1.0951649993658066\n",
      "Epoch 216, Total loss: 1.0729203671216965\n",
      "Epoch 217, Total loss: 1.0896624624729156\n",
      "Epoch 218, Total loss: 1.0878255069255829\n",
      "Epoch 219, Total loss: 1.0834849178791046\n",
      "Epoch 220, Total loss: 1.0825480222702026\n",
      "Epoch 221, Total loss: 1.0803962051868439\n",
      "Epoch 222, Total loss: 1.0779205411672592\n",
      "Epoch 223, Total loss: 1.0754320621490479\n",
      "Epoch 224, Total loss: 1.0738089680671692\n",
      "Epoch 225, Total loss: 1.0713385045528412\n",
      "Epoch 226, Total loss: 1.0693302974104881\n",
      "Epoch 227, Total loss: 1.0672558844089508\n",
      "Epoch 228, Total loss: 1.0649985522031784\n",
      "Epoch 229, Total loss: 1.062255397439003\n",
      "Epoch 230, Total loss: 1.0590787678956985\n",
      "Epoch 231, Total loss: 1.0404119119048119\n",
      "Epoch 232, Total loss: 1.0558690130710602\n",
      "Epoch 233, Total loss: 1.0528937429189682\n",
      "Epoch 234, Total loss: 1.0524500012397766\n",
      "Epoch 235, Total loss: 1.0502608716487885\n",
      "Epoch 236, Total loss: 1.0480947047472\n",
      "Epoch 237, Total loss: 1.0464985966682434\n",
      "Epoch 238, Total loss: 1.0249250009655952\n",
      "Epoch 239, Total loss: 1.041339635848999\n",
      "Epoch 240, Total loss: 1.022362969815731\n",
      "Epoch 241, Total loss: 1.0384410321712494\n",
      "Epoch 242, Total loss: 1.0369592159986496\n",
      "Epoch 243, Total loss: 1.0350343510508537\n",
      "Epoch 244, Total loss: 1.0331851243972778\n",
      "Epoch 245, Total loss: 1.031339980661869\n",
      "Epoch 246, Total loss: 1.029377356171608\n",
      "Epoch 247, Total loss: 1.0277438014745712\n",
      "Epoch 248, Total loss: 1.0075893849134445\n",
      "Epoch 249, Total loss: 1.004627674818039\n",
      "Epoch 250, Total loss: 1.0220107585191727\n",
      "Epoch 251, Total loss: 1.0023347735404968\n",
      "Epoch 252, Total loss: 1.0001515001058578\n",
      "Epoch 253, Total loss: 1.016030266880989\n",
      "Epoch 254, Total loss: 1.015798158943653\n",
      "Epoch 255, Total loss: 1.01404570043087\n",
      "Epoch 256, Total loss: 1.0124057680368423\n",
      "Epoch 257, Total loss: 1.0103136599063873\n",
      "Epoch 258, Total loss: 1.0091360062360764\n",
      "Epoch 259, Total loss: 1.0071159973740578\n",
      "Epoch 260, Total loss: 1.004947230219841\n",
      "Epoch 261, Total loss: 1.0046818181872368\n",
      "Epoch 262, Total loss: 1.0024256184697151\n",
      "Epoch 263, Total loss: 0.9827643781900406\n",
      "Epoch 264, Total loss: 0.9998449832201004\n",
      "Epoch 265, Total loss: 0.979696586728096\n",
      "Epoch 266, Total loss: 0.9781888574361801\n",
      "Epoch 267, Total loss: 0.9940996617078781\n",
      "Epoch 268, Total loss: 0.9938240870833397\n",
      "Epoch 269, Total loss: 0.9924596548080444\n",
      "Epoch 270, Total loss: 0.9911676943302155\n",
      "Epoch 271, Total loss: 0.9895694330334663\n",
      "Epoch 272, Total loss: 0.9880641996860504\n",
      "Epoch 273, Total loss: 0.9865303784608841\n",
      "Epoch 274, Total loss: 0.9665869995951653\n",
      "Epoch 275, Total loss: 0.9648084715008736\n",
      "Epoch 276, Total loss: 0.9815981388092041\n",
      "Epoch 277, Total loss: 0.9807592257857323\n",
      "Epoch 278, Total loss: 0.9601621478796005\n",
      "Epoch 279, Total loss: 0.9784651547670364\n",
      "Epoch 280, Total loss: 0.9762771725654602\n",
      "Epoch 281, Total loss: 0.9749808311462402\n",
      "Epoch 282, Total loss: 0.9740957915782928\n",
      "Epoch 283, Total loss: 0.973260797560215\n",
      "Epoch 284, Total loss: 0.9720542877912521\n",
      "Epoch 285, Total loss: 0.9703622460365295\n",
      "Epoch 286, Total loss: 0.9692447707056999\n",
      "Epoch 287, Total loss: 0.9679971635341644\n",
      "Epoch 288, Total loss: 0.947325199842453\n",
      "Epoch 289, Total loss: 0.9657416194677353\n",
      "Epoch 290, Total loss: 0.9635512381792068\n",
      "Epoch 291, Total loss: 0.9436783790588379\n",
      "Epoch 292, Total loss: 0.9621831178665161\n",
      "Epoch 293, Total loss: 0.9417765885591507\n",
      "Epoch 294, Total loss: 0.9594291150569916\n",
      "Epoch 295, Total loss: 0.9582735523581505\n",
      "Epoch 296, Total loss: 0.9573607593774796\n",
      "Epoch 297, Total loss: 0.9563589245080948\n",
      "Epoch 298, Total loss: 0.9552024081349373\n",
      "Epoch 299, Total loss: 0.9351429864764214\n",
      "Epoch 300, Total loss: 0.9340073019266129\n",
      "Epoch 301, Total loss: 0.9510410949587822\n",
      "Epoch 302, Total loss: 0.9508607387542725\n",
      "Epoch 303, Total loss: 0.9495673924684525\n",
      "Epoch 304, Total loss: 0.9484760239720345\n",
      "Epoch 305, Total loss: 0.9277173578739166\n",
      "Epoch 306, Total loss: 0.9463780745863914\n",
      "Epoch 307, Total loss: 0.9454727917909622\n",
      "Epoch 308, Total loss: 0.9444250613451004\n",
      "Epoch 309, Total loss: 0.9428337141871452\n",
      "Epoch 310, Total loss: 0.9414926171302795\n",
      "Epoch 311, Total loss: 0.9413334652781487\n",
      "Epoch 312, Total loss: 0.9401360303163528\n",
      "Epoch 313, Total loss: 0.9384718388319016\n",
      "Epoch 314, Total loss: 0.938128724694252\n",
      "Epoch 315, Total loss: 0.9370748028159142\n",
      "Epoch 316, Total loss: 0.9361704438924789\n",
      "Epoch 317, Total loss: 0.9351012855768204\n",
      "Epoch 318, Total loss: 0.9342397972941399\n",
      "Epoch 319, Total loss: 0.9141833558678627\n",
      "Epoch 320, Total loss: 0.9324320554733276\n",
      "Epoch 321, Total loss: 0.9313062876462936\n",
      "Epoch 322, Total loss: 0.9304265826940536\n",
      "Epoch 323, Total loss: 0.9288131445646286\n",
      "Epoch 324, Total loss: 0.909411147236824\n",
      "Epoch 325, Total loss: 0.92694241553545\n",
      "Epoch 326, Total loss: 0.9075320512056351\n",
      "Epoch 327, Total loss: 0.9258656948804855\n",
      "Epoch 328, Total loss: 0.9240559339523315\n",
      "Epoch 329, Total loss: 0.923953503370285\n",
      "Epoch 330, Total loss: 0.9230100512504578\n",
      "Epoch 331, Total loss: 0.9214040637016296\n",
      "Epoch 332, Total loss: 0.9210964068770409\n",
      "Epoch 333, Total loss: 0.9204264879226685\n",
      "Epoch 334, Total loss: 0.9195575639605522\n",
      "Epoch 335, Total loss: 0.9186855480074883\n",
      "Epoch 336, Total loss: 0.9171000421047211\n",
      "Epoch 337, Total loss: 0.897190771996975\n",
      "Epoch 338, Total loss: 0.8969417735934258\n",
      "Epoch 339, Total loss: 0.9147349819540977\n",
      "Epoch 340, Total loss: 0.9143877252936363\n",
      "Epoch 341, Total loss: 0.9136749655008316\n",
      "Epoch 342, Total loss: 0.9126698970794678\n",
      "Epoch 343, Total loss: 0.9121558666229248\n",
      "Epoch 344, Total loss: 0.9105451107025146\n",
      "Epoch 345, Total loss: 0.909756563603878\n",
      "Epoch 346, Total loss: 0.9096596986055374\n",
      "Epoch 347, Total loss: 0.908995658159256\n",
      "Epoch 348, Total loss: 0.8882702514529228\n",
      "Epoch 349, Total loss: 0.9072803035378456\n",
      "Epoch 350, Total loss: 0.9065468497574329\n",
      "Epoch 351, Total loss: 0.9057724624872208\n",
      "Epoch 352, Total loss: 0.8857061266899109\n",
      "Epoch 353, Total loss: 0.903989851474762\n",
      "Epoch 354, Total loss: 0.9036034196615219\n",
      "Epoch 355, Total loss: 0.902584120631218\n",
      "Epoch 356, Total loss: 0.9017075598239899\n",
      "Epoch 357, Total loss: 0.9012339562177658\n",
      "Epoch 358, Total loss: 0.9006522297859192\n",
      "Epoch 359, Total loss: 0.8997431844472885\n",
      "Epoch 360, Total loss: 0.8797570466995239\n",
      "Epoch 361, Total loss: 0.8790291473269463\n",
      "Epoch 362, Total loss: 0.8976403474807739\n",
      "Epoch 363, Total loss: 0.8969605937600136\n",
      "Epoch 364, Total loss: 0.8763905502855778\n",
      "Epoch 365, Total loss: 0.8955340087413788\n",
      "Epoch 366, Total loss: 0.8749990612268448\n",
      "Epoch 367, Total loss: 0.8935926705598831\n",
      "Epoch 368, Total loss: 0.8932263106107712\n",
      "Epoch 369, Total loss: 0.8928244858980179\n",
      "Epoch 370, Total loss: 0.872742410749197\n",
      "Epoch 371, Total loss: 0.89144516736269\n",
      "Epoch 372, Total loss: 0.8713922202587128\n",
      "Epoch 373, Total loss: 0.8707206472754478\n",
      "Epoch 374, Total loss: 0.8892702609300613\n",
      "Epoch 375, Total loss: 0.8889384530484676\n",
      "Epoch 376, Total loss: 0.8687477856874466\n",
      "Epoch 377, Total loss: 0.8873253017663956\n",
      "Epoch 378, Total loss: 0.8868411332368851\n",
      "Epoch 379, Total loss: 0.8863682746887207\n",
      "Epoch 380, Total loss: 0.8855762034654617\n",
      "Epoch 381, Total loss: 0.8653416484594345\n",
      "Epoch 382, Total loss: 0.8843904733657837\n",
      "Epoch 383, Total loss: 0.8642856851220131\n",
      "Epoch 384, Total loss: 0.8634681403636932\n",
      "Epoch 385, Total loss: 0.8628481030464172\n",
      "Epoch 386, Total loss: 0.8816812112927437\n",
      "Epoch 387, Total loss: 0.881344847381115\n",
      "Epoch 388, Total loss: 0.8612131029367447\n",
      "Epoch 389, Total loss: 0.8800703510642052\n",
      "Epoch 390, Total loss: 0.879552785307169\n",
      "Epoch 391, Total loss: 0.8789236769080162\n",
      "Epoch 392, Total loss: 0.8782427757978439\n",
      "Epoch 393, Total loss: 0.8580572679638863\n",
      "Epoch 394, Total loss: 0.8770819678902626\n",
      "Epoch 395, Total loss: 0.8566692546010017\n",
      "Epoch 396, Total loss: 0.8755660802125931\n",
      "Epoch 397, Total loss: 0.8752640634775162\n",
      "Epoch 398, Total loss: 0.8749329335987568\n",
      "Epoch 399, Total loss: 0.8743186891078949\n",
      "Epoch 400, Total loss: 0.8738206550478935\n",
      "Epoch 401, Total loss: 0.8535000160336494\n",
      "Epoch 402, Total loss: 0.8527282774448395\n",
      "Epoch 403, Total loss: 0.8717963322997093\n",
      "Epoch 404, Total loss: 0.8717048168182373\n",
      "Epoch 405, Total loss: 0.870615541934967\n",
      "Epoch 406, Total loss: 0.8703656978905201\n",
      "Epoch 407, Total loss: 0.8502243682742119\n",
      "Epoch 408, Total loss: 0.8695630505681038\n",
      "Epoch 409, Total loss: 0.8491544649004936\n",
      "Epoch 410, Total loss: 0.8684370443224907\n",
      "Epoch 411, Total loss: 0.8679106198251247\n",
      "Epoch 412, Total loss: 0.8673917427659035\n",
      "Epoch 413, Total loss: 0.8665285110473633\n",
      "Epoch 414, Total loss: 0.8663757219910622\n",
      "Epoch 415, Total loss: 0.8658310398459435\n",
      "Epoch 416, Total loss: 0.8651402369141579\n",
      "Epoch 417, Total loss: 0.8646745458245277\n",
      "Epoch 418, Total loss: 0.8443194180727005\n",
      "Epoch 419, Total loss: 0.8638517707586288\n",
      "Epoch 420, Total loss: 0.8633615858852863\n",
      "Epoch 421, Total loss: 0.8628426268696785\n",
      "Epoch 422, Total loss: 0.8623799905180931\n",
      "Epoch 423, Total loss: 0.8617302104830742\n",
      "Epoch 424, Total loss: 0.8614127337932587\n",
      "Epoch 425, Total loss: 0.8610069379210472\n",
      "Epoch 426, Total loss: 0.8604572862386703\n",
      "Epoch 427, Total loss: 0.8599295914173126\n",
      "Epoch 428, Total loss: 0.8396278843283653\n",
      "Epoch 429, Total loss: 0.8588798567652702\n",
      "Epoch 430, Total loss: 0.858545109629631\n",
      "Epoch 431, Total loss: 0.8581808395683765\n",
      "Epoch 432, Total loss: 0.8575458526611328\n",
      "Epoch 433, Total loss: 0.8570948094129562\n",
      "Epoch 434, Total loss: 0.8567413948476315\n",
      "Epoch 435, Total loss: 0.8561372831463814\n",
      "Epoch 436, Total loss: 0.855841051787138\n",
      "Epoch 437, Total loss: 0.8552330061793327\n",
      "Epoch 438, Total loss: 0.8545511066913605\n",
      "Epoch 439, Total loss: 0.8544714376330376\n",
      "Epoch 440, Total loss: 0.8339589908719063\n",
      "Epoch 441, Total loss: 0.8535754531621933\n",
      "Epoch 442, Total loss: 0.8530829846858978\n",
      "Epoch 443, Total loss: 0.8527228087186813\n",
      "Epoch 444, Total loss: 0.8325117379426956\n",
      "Epoch 445, Total loss: 0.8518899530172348\n",
      "Epoch 446, Total loss: 0.8514316603541374\n",
      "Epoch 447, Total loss: 0.8508523553609848\n",
      "Epoch 448, Total loss: 0.8503294214606285\n",
      "Epoch 449, Total loss: 0.8502580299973488\n",
      "Epoch 450, Total loss: 0.8298082500696182\n",
      "Epoch 451, Total loss: 0.8493553139269352\n",
      "Epoch 452, Total loss: 0.8489148691296577\n",
      "Epoch 453, Total loss: 0.8482616990804672\n",
      "Epoch 454, Total loss: 0.8481892198324203\n",
      "Epoch 455, Total loss: 0.8477291315793991\n",
      "Epoch 456, Total loss: 0.8472920432686806\n",
      "Epoch 457, Total loss: 0.8469164222478867\n",
      "Epoch 458, Total loss: 0.8463412746787071\n",
      "Epoch 459, Total loss: 0.8461143597960472\n",
      "Epoch 460, Total loss: 0.8258507363498211\n",
      "Epoch 461, Total loss: 0.8450667671859264\n",
      "Epoch 462, Total loss: 0.8449173122644424\n",
      "Epoch 463, Total loss: 0.8243863619863987\n",
      "Epoch 464, Total loss: 0.8438217304646969\n",
      "Epoch 465, Total loss: 0.8437811210751534\n",
      "Epoch 466, Total loss: 0.8433906510472298\n",
      "Epoch 467, Total loss: 0.8429914936423302\n",
      "Epoch 468, Total loss: 0.8423820063471794\n",
      "Epoch 469, Total loss: 0.8421714529395103\n",
      "Epoch 470, Total loss: 0.8418800048530102\n",
      "Epoch 471, Total loss: 0.8413486927747726\n",
      "Epoch 472, Total loss: 0.841194823384285\n",
      "Epoch 473, Total loss: 0.8407460525631905\n",
      "Epoch 474, Total loss: 0.8403661586344242\n",
      "Epoch 475, Total loss: 0.8198410049080849\n",
      "Epoch 476, Total loss: 0.8394309170544147\n",
      "Epoch 477, Total loss: 0.8191079311072826\n",
      "Epoch 478, Total loss: 0.8389300107955933\n",
      "Epoch 479, Total loss: 0.8186382539570332\n",
      "Epoch 480, Total loss: 0.838229276239872\n",
      "Epoch 481, Total loss: 0.8377984911203384\n",
      "Epoch 482, Total loss: 0.8372156694531441\n",
      "Epoch 483, Total loss: 0.83723034709692\n",
      "Epoch 484, Total loss: 0.836597453802824\n",
      "Epoch 485, Total loss: 0.8364005088806152\n",
      "Epoch 486, Total loss: 0.8361133188009262\n",
      "Epoch 487, Total loss: 0.8156879022717476\n",
      "Epoch 488, Total loss: 0.8351470828056335\n",
      "Epoch 489, Total loss: 0.8149984367191792\n",
      "Epoch 490, Total loss: 0.8347673937678337\n",
      "Epoch 491, Total loss: 0.8344125524163246\n",
      "Epoch 492, Total loss: 0.834020234644413\n",
      "Epoch 493, Total loss: 0.8336177915334702\n",
      "Epoch 494, Total loss: 0.8333172872662544\n",
      "Epoch 495, Total loss: 0.8330280184745789\n",
      "Epoch 496, Total loss: 0.8327411971986294\n",
      "Epoch 497, Total loss: 0.8324439227581024\n",
      "Epoch 498, Total loss: 0.8320886790752411\n",
      "Epoch 499, Total loss: 0.8317657485604286\n"
     ]
    }
   ],
   "source": [
    "# パラメータの設定\n",
    "embedding_size = 10\n",
    "learning_rate = 0.01\n",
    "epochs = 500\n",
    "vocab_size = len(word_to_id)\n",
    "\n",
    "# モデルのインスタンス化\n",
    "model = SimpleCBOW(vocab_size, embedding_size).to(device)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "# Training loop with batch processing\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for i, (context_batch, target_batch) in enumerate(data_loader):\n",
    "        # Zero out the gradients from the last step\n",
    "        model.zero_grad()\n",
    "        # Forward pass through the model\n",
    "        log_probs = model(context_batch)\n",
    "        # Compute the loss\n",
    "        loss = loss_function(log_probs, target_batch)\n",
    "        # Backward pass to compute gradients\n",
    "        loss.backward()\n",
    "        # Update the model parameters\n",
    "        optimizer.step()\n",
    "        # Accumulate the loss\n",
    "        total_loss += loss.item()\n",
    "    # Log the total loss for the epoch\n",
    "    print(f'Epoch {epoch}, Total loss: {total_loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{margin}\n",
    "``nn.CrossEntropyLoss``ターゲットラベルをクラスのインデックスとして受け取り、内部で必要な変換を行いますので、ターゲットをワンホットエンコーディングに変換する必要はありません。\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モデルの入力層の重みが単語分散表現であり、$単語 \\times  埋め込み次元数$の形の行列になります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 10])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embeddings.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: you\n",
      "Vector: [ 0.70055836  1.0866567  -1.064358    0.66675586 -0.7163387   1.6130689\n",
      "  0.83074427 -0.49878228  0.34294105  1.0148984 ]\n",
      "\n",
      "Word: say\n",
      "Vector: [-1.5901022  -0.00219009 -0.6646933   0.45157602  0.56254256 -1.0936656\n",
      "  0.23256147  0.58017635 -0.24702871 -0.16118881]\n",
      "\n",
      "Word: goodbye\n",
      "Vector: [-2.269208   -0.7624368  -0.53659636 -2.4982815  -0.45344758 -0.18367144\n",
      "  0.98493576 -0.5040312   0.82145077 -0.14498714]\n",
      "\n",
      "Word: and\n",
      "Vector: [ 0.56028974 -0.44652066 -0.7096378   0.22628564  2.331173    1.0436808\n",
      "  0.7705056   0.01509575  0.4359598   1.1922593 ]\n",
      "\n",
      "Word: i\n",
      "Vector: [-1.9440092   1.2316376  -1.3823694   1.2243642   0.61890274  0.3146721\n",
      "  0.6694679   0.34261543  0.7801544   0.1767771 ]\n",
      "\n",
      "Word: hello\n",
      "Vector: [ 0.39397553  0.39634743 -1.4382983   0.27155522  0.11106181  0.9662054\n",
      " -0.38220018 -0.5216776   0.8878775  -0.9981149 ]\n",
      "\n",
      "Word: .\n",
      "Vector: [-0.9190593   0.43809733 -0.02275043 -0.28098708 -0.47820055 -0.4780515\n",
      " -0.36750537  1.1174918  -0.20671348  1.7928059 ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "word_embeddings = model.embeddings.weight.data\n",
    "\n",
    "# 各単語とそれに対応する分散表現を表示\n",
    "for word, idx in word_to_id.items():\n",
    "    vector = word_embeddings[idx].cpu().numpy()\n",
    "    print(f\"Word: {word}\")\n",
    "    print(f\"Vector: {vector}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyterbook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
