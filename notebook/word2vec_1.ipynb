{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 単語分散表現\n",
    "\n",
    "単語分散表現とは、単語の意味を低次元の実数値ベクトルで表現することです。\n",
    "\n",
    "機械学習・深層学習モデルは、ベクトル（数値の配列）を入力として受け取ります。テキストを扱う際、最初に決めなければならないのは、文字列を機械学習モデルに入力する前に、数値に変換する（あるいはテキストを「ベクトル化」する）ための戦略です。\n",
    "\n",
    "単語の持つ性質や意味をよく反映するベクトル表現を獲得することは、機械学習・深層学習を自然言語処理で活用するために重要なプロセスです。\n",
    "\n",
    "- 類似性: ある概念を表現する際に、ほかの概念との共通点や類似性と紐づけながら、ベクトル空間上に表現します。\n",
    "- 単語類推: 分散表現では異なる概念を表現するベクトル同士での計算が可能です\n",
    "\n",
    "\n",
    "\n",
    "![類似性](./Figure/word2vec.png)\n",
    "\n",
    "![単語類推](./Figure/city.png)\n",
    "\n",
    "\n",
    "\n",
    "単語ベクトルへの変換には様々なアプローチが存在します。最初は、(1)人の手によって作られたシソーラス（類語辞書）を利用する手法について簡単の見ていきます。続いて、統計情報から単語を表現する手法ーカウントベースの手法ーについて説明します。この方法は、言語のモデル化を理解することに役に立つと考られます。そして、ニューラルネットワークを用いた手法(具体的には、word2vwcと呼ばれる手法)を扱います。\n",
    "\n",
    "1. シソーラスによる手法\n",
    "    - 人の手によって作られたシソーラス（類語辞書）を利用する手法\n",
    "2. カウントベースの手法(今週)\n",
    "    - 統計情報から単語を表現する手法\n",
    "3. 推論ベースの手法(来週)\n",
    "    - ニューラルネットワークを用いた手法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## シソーラスによる手法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "「単語の意味」を表すためには、人の手によって単語の意味を定義することが考えられます。\n",
    "\n",
    "シソーラス(thesaurus)と呼ばれるタイプの辞書は、単語間の関係を異表記・類義語・上位下位といった関係性を用いて、単語間の関連を定義できます。\n",
    "\n",
    "$$\n",
    "car = auto \\ automobile \\ machine \\ motorcar\n",
    "$$\n",
    "\n",
    "![](./Figure/thesaurus.png)\n",
    "\n",
    "\n",
    "この「単語ネットーワーク」を利用することで、コンピュータに単語間の関連性を伝えることができます。しかし、この手法には大きな欠点が存在します。\n",
    "\n",
    "- 人の作業コストが高い\n",
    "- 時代の変化に対応するのが困難\n",
    "    - 言語は常に進化しており、新しい単語や意味が生まれては消えていくので、シソーラスを最新の状態に保つのは難しいです。\n",
    "- 単語の些細なかニュアンスを表現できない\n",
    "    - 単語が持つ複数の意味を区別することは難しい\n",
    "    - 単語間の関連性はシソーラスでは静的なものであり、動的な文脈や知識の流れを反映しきれないことがあります"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## カウントベースの手法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "コーパスには、自然言語に対する人の「知識」ー文章の書き方、単語の選び方、単語の意味ーがふんだんに含まれています。カウントベースの手法の目標は、そのような人の知識が詰まったコーパスから自動的に抽出することにあります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### コーパスの前処理\n",
    "\n",
    "コーパスに対して、テキストデータを単語に分割し、その分割した単語をID化にすることが必要されます。\n",
    "\n",
    "単語のID化とは、テキストデータを機械学習モデルなどで処理する際に、単語を一意の整数値（ID）に変換するプロセスを指します。これは、テキストデータをベクトルや行列の形でモデルに入力するための前処理として行われます。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "例として、簡単なテキストを用意します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'You say goodbye and I say hello.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you say goodbye and i say hello.\n",
      "you say goodbye and i say hello .\n",
      "['you', 'say', 'goodbye', 'and', 'i', 'say', 'hello', '.']\n"
     ]
    }
   ],
   "source": [
    "# 小文字に変換\n",
    "text = text.lower()\n",
    "print(text)\n",
    "\n",
    "# ピリオドの前にスペースを挿入\n",
    "text = text.replace('.', ' .')\n",
    "print(text)\n",
    "\n",
    "# 単語ごとに分割\n",
    "words = text.split(' ')\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これで、元の文章を単語リストとして利用できるようになりました。これに基づいて、分割した単語と、単語ごとに通し番号を割り振ったIDを2つのディクショナリに格納します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ディクショナリを初期化\n",
    "word_to_id = {}\n",
    "id_to_word = {}\n",
    "\n",
    "# 未収録の単語をディクショナリに格納\n",
    "for word in words:\n",
    "    if word not in word_to_id: # 未収録の単語のとき\n",
    "        # 次の単語のidを取得\n",
    "        new_id = len(word_to_id)\n",
    "        \n",
    "        # 単語IDを格納\n",
    "        word_to_id[word] = new_id\n",
    "        \n",
    "        # 単語を格納\n",
    "        id_to_word[new_id] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'you', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'i', 5: 'hello', 6: '.'}\n",
      "hello\n",
      "{'you': 0, 'say': 1, 'goodbye': 2, 'and': 3, 'i': 4, 'hello': 5, '.': 6}\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "# 単語IDを指定すると単語を返す\n",
    "print(id_to_word)\n",
    "print(id_to_word[5])\n",
    "\n",
    "# 単語を指定すると単語IDを返す\n",
    "print(word_to_id)\n",
    "print(word_to_id['hello'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最後に、単語リストから単語IDリストに変換します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 1 5 6]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# リストに変換\n",
    "corpus = [word_to_id[word] for word in words]\n",
    "\n",
    "# NumPy配列に変換\n",
    "corpus = np.array(corpus)\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上の処理を```preprocess()```という関数として、まとめて実装することにします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前処理関数の実装\n",
    "def preprocess(text):\n",
    "    # 前処理\n",
    "    text = text.lower() # 小文字に変換\n",
    "    text = text.replace('.', ' .') # ピリオドの前にスペースを挿入\n",
    "    words = text.split(' ') # 単語ごとに分割\n",
    "    \n",
    "    # ディクショナリを初期化\n",
    "    word_to_id = {}\n",
    "    id_to_word = {}\n",
    "    \n",
    "    # 未収録の単語をディクショナリに格納\n",
    "    for word in words:\n",
    "        if word not in word_to_id: # 未収録の単語のとき\n",
    "            # 次の単語のidを取得\n",
    "            new_id = len(word_to_id)\n",
    "            \n",
    "            # 単語をキーとして単語IDを格納\n",
    "            word_to_id[word] = new_id\n",
    "            \n",
    "            # 単語IDをキーとして単語を格納\n",
    "            id_to_word[new_id] = word\n",
    "    \n",
    "    # 単語IDリストを作成\n",
    "    corpus = [word_to_id[w] for w in words]\n",
    "    \n",
    "    return corpus, word_to_id, id_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'you', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'i', 5: 'hello', 6: '.'}\n",
      "{'you': 0, 'say': 1, 'goodbye': 2, 'and': 3, 'i': 4, 'hello': 5, '.': 6}\n",
      "[0, 1, 2, 3, 4, 1, 5, 6]\n"
     ]
    }
   ],
   "source": [
    "# テキストを設定\n",
    "text = 'You say goodbye and I say hello.'\n",
    "\n",
    "# 単語と単語IDに関する変数を取得\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "print(id_to_word)\n",
    "print(word_to_id)\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分布仮説\n",
    "\n",
    "分布仮説（Distributional Hypothesis）は、言語学や自然言語処理の分野で重要な考え方で、単語の意味は、周囲の単語(コンテキスト)によって形成されるというものです。\n",
    "\n",
    "- 単語は、その単語が出現する文脈の集合によって意味が形成されるとされます。同じ文脈で出現する単語は、意味が似ていると考えられます。\n",
    "- 単語Aと単語Bが多くの共通の文脈で使用される場合、これらの単語は意味的に関連があると見なされます。\n",
    "\n",
    "この仮説は、単語の意味を捉えるためのモデルを作成する際に基本的な原則となっています。\n",
    "\n",
    "![](./Figure/DH.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 共起行列\n",
    "\n",
    "分布仮説に基づいた単語ベクトル化の方法を考える際、一番素直な方法は、周囲の単語を\"カウント\"することです。つまり、ある単語に着目した場合、その周囲どのような単語がどれだけ現れるのかをカウントし、それを集計するのです。\n",
    "\n",
    "ここでは、\"*You say goodbye and I say hello.*\"という文章について、ウィンドウサイズを$1$とする場合、そのコンテキストに含まれる単語の頻度をカウントしてみます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{figure} ./Figure/co-occur.png\n",
    "---\n",
    "align: center\n",
    "---\n",
    "各単語について、そのコンテキストに含まれす単語の頻度\n",
    "```\n",
    "\n",
    "\n",
    "- 「You」の周辺単語は「say」のみであり、「say」にのみコンテキストの目印として共起した回数の$1$をカウントします\n",
    "- 「say」については文字列中に2回現れていることに注意すると、$[1, 0, 1, 0, 1, 1, 0]$とベクトル表記できます\n",
    "\n",
    "全ての単語に対して、共起する単語をまとめたものを共起行列と呼ばれます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "単語の種類数: 7\n",
      "総単語数: 8\n"
     ]
    }
   ],
   "source": [
    "# ウィンドウサイズを指定\n",
    "wndow_size = 1\n",
    "\n",
    "# 単語の種類数を取得\n",
    "vocab_size = len(word_to_id)\n",
    "print(f\"単語の種類数: {vocab_size}\")\n",
    "\n",
    "# 総単語数を取得\n",
    "corpus_size = len(corpus)\n",
    "print(f\"総単語数: {corpus_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]]\n",
      "(7, 7)\n"
     ]
    }
   ],
   "source": [
    "# 共起行列を初期化\n",
    "co_matrix = np.zeros((vocab_size, vocab_size), dtype=np.int32)\n",
    "print(co_matrix)\n",
    "print(co_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "コーパスの7語目の単語「hello」に注目してみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "hello\n"
     ]
    }
   ],
   "source": [
    "# 単語インデックスを指定\n",
    "idx = 6\n",
    "\n",
    "# 指定した単語のIDを取得\n",
    "word_id = corpus[idx]\n",
    "print(word_id)\n",
    "print(id_to_word[word_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "# 左隣のインデックス\n",
    "left_idx = idx - 1\n",
    "print(left_idx)\n",
    "\n",
    "# 右隣のインデックス\n",
    "right_idx = idx + 1\n",
    "print(right_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "say\n",
      "[[0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# 左隣の単語IDを取得\n",
    "left_word_id = corpus[left_idx]\n",
    "print(left_word_id)\n",
    "print(id_to_word[left_word_id])\n",
    "\n",
    "# 共起行列に記録(加算)\n",
    "co_matrix[word_id, left_word_id] += 1\n",
    "print(co_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      ".\n",
      "[[0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 0]]\n",
      "[0 1 0 0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "# 右隣の単語IDを取得\n",
    "right_word_id = corpus[right_idx]\n",
    "print(right_word_id)\n",
    "print(id_to_word[right_word_id])\n",
    "\n",
    "# 共起行列に記録(加算)\n",
    "co_matrix[word_id, right_word_id] += 1\n",
    "print(co_matrix)\n",
    "\n",
    "# 対象の単語ベクトル\n",
    "print(co_matrix[word_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "処理を共起行列を作成する関数として実装します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 共起行列作成関数の実装\n",
    "def create_co_matrix(corpus, vocab_size, window_size=1):\n",
    "    \n",
    "    # 総単語数を取得\n",
    "    corpus_size = len(corpus)\n",
    "    \n",
    "    # 共起行列を初期化\n",
    "    co_matrix = np.zeros((vocab_size, vocab_size), dtype=np.int32)\n",
    "    \n",
    "    # 1語ずつ処理\n",
    "    for idx, word_id in enumerate(corpus):\n",
    "        \n",
    "        # ウィンドウサイズまでの要素を順番に処理\n",
    "        for i in range(1, window_size + 1):\n",
    "            # 範囲内のインデックスを計算\n",
    "            left_idx = idx - i\n",
    "            right_idx = idx + i\n",
    "            \n",
    "            # 左側の単語の処理\n",
    "            if left_idx >= 0: # 対象の単語が最初の単語でないとき\n",
    "                # 単語IDを取得\n",
    "                left_word_id = corpus[left_idx]\n",
    "                \n",
    "                # 共起行列にカウント\n",
    "                co_matrix[word_id, left_word_id] += 1\n",
    "            \n",
    "            # 右側の単語の処理\n",
    "            if right_idx < corpus_size: # 対象の単語が最後の単語でないとき\n",
    "                # 単語IDを取得\n",
    "                right_word_id = corpus[right_idx]\n",
    "                \n",
    "                # 共起行列にカウント\n",
    "                co_matrix[word_id, right_word_id] += 1\n",
    "    \n",
    "    return co_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ベクトル間の類似度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "共起行列によって単語をベクトルで表すことができました。単語の意味を「計算」する手法として、ベクトル間の類似度の計測する方法について見ていきます。\n",
    "\n",
    "様々な方法がありますが、単語のベクトル表現の類似度に関して、コサイン類似度がよく用いられます。\n",
    "\n",
    "コサイン類似度とは、2つのベクトルを$\\mathbf{x} = (x_1, x_2, \\cdots, x_n), \\mathbf{y} = (y_1, y_2, \\cdots, y_n)$として、次の式で定義されます。\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathrm{similarity}(\\mathbf{x}, \\mathbf{y})\n",
    "   &= \\frac{\n",
    "          \\mathbf{x} \\cdot \\mathbf{y}\n",
    "      }{\n",
    "          \\|\\mathbf{x}\\| \\|\\mathbf{y}\\|\n",
    "      }\n",
    "\\\\\n",
    "   &= \\frac{\n",
    "          x_1 y_1 + x_2 y_2 + \\cdots + x_n y_n\n",
    "      }{\n",
    "          \\sqrt{x_1^2 + x_2^2 + \\cdots + x_n^2}\n",
    "          \\sqrt{y_1^2 + y_2^2 + \\cdots + y_n^2}\n",
    "      }\n",
    "\\\\\n",
    "   &= \\frac{\n",
    "          \\sum_{n} x_n y_n\n",
    "      }{\n",
    "          \\sqrt{\\sum_{n} x_n^2}\n",
    "          \\sqrt{\\sum_{n} y_n^2}\n",
    "      }\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "- 分子はベクトルの内積\n",
    "- 分母は各ベクトルの「ノルム」(ベクトルの大きさ)があります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{margin}\n",
    "epsは、$0$除算とならないための微小な値です。通常、このような小さな値は浮動小数点の「丸の誤差」により、他の値に\"吸収\"されますので、最終の計算結果に影響を与えません。\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# コサイン類似度の実装\n",
    "def cos_similarity(x, y, eps=1e-8):\n",
    "    # コサイン類似度を計算:式(2.1)\n",
    "    nx = x / (np.sqrt(np.sum(x**2)) + eps)\n",
    "    ny = y / (np.sqrt(np.sum(y**2)) + eps)\n",
    "    return np.dot(nx, ny)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyterbook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
