{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前章では、「カウントベースの手法」によって単語分散表現を得ました。具体的には、単語の共起行列を作り、その行列に対してSVDを適用することで、密なベクトくー 単語分散表現ーを獲得したのです。\n",
    "\n",
    "しかし、カウントベースの手法にはいくつかの問題点があります。\n",
    "\n",
    "- 大規模なコーパスを扱う場合、巨大な共起行列に対してSVDを計算することが難しい。\n",
    "- コーパスの全体から一回の学習で単語分散表現を獲得していますので、新しい単語が追加される場合、再度最初から学習を行う必要があり、単語分散表現更新の効率が低い。\n",
    "\n",
    "「カウントベースの手法」に代わる強力な手法として「推論ベース」の手法が挙げられます。特に、Mikolov et al. {cite}`mikolov-etal-2013-linguistic` {cite}`NIPS2013_9aa42b31`　によって提案されたword2vecの有用性が多くの自然言語処理タスクにおいて示されてきたのです。\n",
    "\n",
    "本章では、word2vecの仕組みについて説明し、それを実装することで理解を深めます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 推論ベース手法とニューラルネットワーク\n",
    "\n",
    "推論ベースの手法は、ミニバッチで学習する形で、ニューラルネットワークを用いて、重みを繰り返し更新することで単語分散表現を獲得します。\n",
    "\n",
    "\n",
    "![](./Figure/inference.png)\n",
    "\n",
    "### 推論ベース手法の設計\n",
    "\n",
    "推論ベース手法では、```you 【？】 goodbye and I say hello .```のような、周囲の単語が与えられたときに、```【？】```にどのような単語が出現するのかを推測する推論問題を繰り返し解くことで、単語の出現バターンを学習します。\n",
    "\n",
    "つまり、コンテキスト情報を入力として受け取り、各単語の出現する確率を出力する「モデル」を作成することは目標になります。ここで、正しい推測ができるように、コーパスを使って、ニューラルネットワークモデルの学習を行います。そして、その学習の結果として、単語の分散表現を得られます。\n",
    "\n",
    "![](./Figure/inference2.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{checkitout}推論タスク\n",
    "<div style=\"display: flex; justify-content: space-between; flex-wrap: wrap; gap: 8px;\">\n",
    "  <img src=\"./Figure/window_prob1-min.png\" alt=\"Fig1\">\n",
    "  <img src=\"./Figure/window_prob2-min.png\" alt=\"Fig2\">\n",
    "</div>\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### one-hot表現\n",
    "\n",
    "ニューラルネットワークで単語を処理するには、それを「固定長のベクトル」に変換する必要があります。\n",
    "\n",
    "そのための方法の一つは、単語をone-hot表現へと変換することです。one-hot表現とは、ベクトルの要素の中で一つだけが$1$で、残りは全て$0$であるようなベクトルと言います。\n",
    "\n",
    "単語をone-hot表現に変換するには、語彙数分の要素を持つベクトルを用意して、単語IDの該当する箇所を$1$に、残りは全て$0$に設定します。\n",
    "\n",
    "\n",
    "![](./Figure/one-hot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CBOW（continuous bag-of-words）モデル\n",
    "\n",
    "CBOWモデルは、コンテキストからターゲットを推測することを目的としたニューラルネットワークです。このCBOWモデルで、できるだけ正確な推測ができるように訓練することで、単語の分散表現を取得することができます。\n",
    "\n",
    "ここで、例として、コンテキスト```[\"you\",\"goodbye\"]```からターゲット```\"say\"```を予測するタスクを考えます。\n",
    "\n",
    "![](./Figure/cbow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 入力層から中間層(エンコード)\n",
    "\n",
    "one-hotエンコーディングで、単語を固定長のベクトルに変換するすることができます。\n",
    "\n",
    "単語をベクトルで表すことができれば、そのベクトルはニューラルネットワークを構成する「レイヤ」によって処理することができるようになりました。\n",
    "\n",
    "コンテキストを$\\mathbf{c}$、重みを$\\mathbf{W}$とし、それぞれ次の形状とします。\n",
    "\n",
    "```{margin}\n",
    "イメージしやすいように、ここでは添字を対応する単語で表すことにします。ただし「.(ピリオド)」については「priod」とします。\n",
    "```\n",
    "\n",
    "$$\n",
    "\\mathbf{c}\n",
    "    = \\begin{pmatrix}\n",
    "          c_{\\mathrm{you}} & c_{\\mathrm{say}} & c_{\\mathrm{goodbye}} & c_{\\mathrm{and}} & c_{\\mathrm{I}} & c_{\\mathrm{hello}} & c_{\\mathrm{period}}\n",
    "      \\end{pmatrix}\n",
    ",\\ \n",
    "\\mathbf{W}\n",
    "    = \\begin{pmatrix}\n",
    "          w_{\\mathrm{you},1} & w_{\\mathrm{you},2} & w_{\\mathrm{you},3} \\\\\n",
    "          w_{\\mathrm{say},1} & w_{\\mathrm{say},2} & w_{\\mathrm{say},3} \\\\\n",
    "          w_{\\mathrm{goodbye},1} & w_{\\mathrm{goodbye},2} & w_{\\mathrm{goodbye},3} \\\\\n",
    "          w_{\\mathrm{and},1} & w_{\\mathrm{and},2} & w_{\\mathrm{and},3} \\\\\n",
    "          w_{\\mathrm{I},1} & w_{\\mathrm{I},2} & w_{\\mathrm{I},3} \\\\\n",
    "          w_{\\mathrm{hello},1} & w_{\\mathrm{hello},2} & w_{\\mathrm{hello},3} \\\\\n",
    "          w_{\\mathrm{period},1} & w_{\\mathrm{period},2} & w_{\\mathrm{period},3} \\\\\n",
    "      \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "コンテキストの要素数(列数)と重みの行数が、単語の種類数に対応します。\n",
    "\n",
    "コンテキスト(単語)はone-hot表現として扱うため、例えば「you」の場合は\n",
    "\n",
    "$$\n",
    "\\mathbf{c}_{\\mathrm{you}}\n",
    "    = \\begin{pmatrix}\n",
    "          1 & 0 & 0 & 0 & 0 & 0 & 0\n",
    "      \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "とすることで、単語「you」を表現できます。\n",
    "\n",
    "重み付き和$\\mathbf{h}$は、行列の積で求められます。\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf{h}\n",
    "   &= \\mathbf{c}_{\\mathrm{you}}\n",
    "      \\mathbf{W}\n",
    "\\\\\n",
    "   &= \\begin{pmatrix}\n",
    "          h_1 & h_2 & h_3\n",
    "      \\end{pmatrix}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$h_1$の計算を詳しく見ると、次のようになります。\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "h_1\n",
    "   &= c_{\\mathrm{you}} w_{\\mathrm{you},1}\n",
    "      + c_{\\mathrm{say}} w_{\\mathrm{say},1}\n",
    "      + c_{\\mathrm{goodbye}} w_{\\mathrm{goodbye},1}\n",
    "      + c_{\\mathrm{and}} w_{\\mathrm{and},1}\n",
    "      + c_{\\mathrm{I}} w_{\\mathrm{I},1}\n",
    "      + c_{\\mathrm{hello}} w_{\\mathrm{hello},1}\n",
    "      + c_{\\mathrm{period}} w_{\\mathrm{period},1}\n",
    "\\\\\n",
    "   &= 1 w_{\\mathrm{you},1}\n",
    "      + 0 w_{\\mathrm{say},1}\n",
    "      + 0 w_{\\mathrm{goodbye},1}\n",
    "      + 0 w_{\\mathrm{and},1}\n",
    "      + 0 w_{\\mathrm{I},1}\n",
    "      + 0 w_{\\mathrm{hello},1}\n",
    "      + 0 w_{\\mathrm{period},1}\n",
    "\\\\\n",
    "   &= w_{\\mathrm{you},1}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "コンテキストと重みの対応する(同じ単語に関する)要素を掛けて、全ての単語で和をとります。しかしコンテキストは、$c_{you}$以外の要素が$0$なので、対応する重みの値の影響は消えていまします。また$c_{you}$は$1$なので、対応する重みの値$w_{\\mathrm{you},1}$がそのまま中間層のニューロンに伝播します。\n",
    "\n",
    "残りの2つの要素も同様に計算できるので、重み付き和\n",
    "\n",
    "$$\n",
    "\\mathbf{h}\n",
    "    = \\begin{pmatrix}\n",
    "          w_{\\mathrm{you},1} & w_{\\mathrm{you},2} & w_{\\mathrm{you},3}\n",
    "      \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "は、単語「you」に関する重みの値となります。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "コンテキストの形状：(1, 7)\n",
      "重み\n",
      "[[-0.26905357  0.15754355  0.26362719]\n",
      " [-0.05649976 -0.19133707 -0.46970514]\n",
      " [ 1.25937735 -0.8312994   0.91840232]\n",
      " [-0.44383637  1.71926509  2.00003426]\n",
      " [ 1.30260472  0.2873453  -0.75457985]\n",
      " [ 0.42182388  1.24156842  2.98676491]\n",
      " [-1.90253657 -0.67307653 -0.43790344]]\n",
      "重み付き和\n",
      "[[-0.26905357  0.15754355  0.26362719]]\n",
      "重み付き和の形状：(1, 3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 適当にコンテキスト(one-hot表現)を指定\n",
    "c = np.array([[1, 0, 0, 0, 0, 0, 0]])\n",
    "print(f\"コンテキストの形状：{c.shape}\")\n",
    "\n",
    "# 重みをランダムに生成\n",
    "W = np.random.randn(7, 3)\n",
    "print(f\"重み\\n{W}\")\n",
    "\n",
    "# 重み付き和を計算\n",
    "h = np.dot(c, W)\n",
    "print(f\"重み付き和\\n{h}\")\n",
    "print(f\"重み付き和の形状：{h.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "コンテキストに複数な単語がある場合、入力層も複数になります。このとき、中間層にあるニューロンは、各入力層の全結合による変換後の値が平均されたものになります。\n",
    "\n",
    "中間層のニューロンの数を入力層よりも減らすことによって、中間層には、単語を予測するために必要な情報が\"コンパクト\"に収められて、結果としては密なベクトル表現が得られます。このとき、この中間層の情報は、人間には理解できない「ブラックボックス」ような状態になります。この作業は、「エンコード」と言います。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 中間層から出力層(デコード)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "中間層の情報から目的の結果を得る作業は、「デコード」と言います。ここでは、中間層のニューロンの値$\\mathbf{h}$を各単語に対応した値になるように、つまり要素(行)数が単語の種類数となるように再度変換したものを、CBOWモデルの出力とします。\n",
    "\n",
    "出力層の重みを\n",
    "\n",
    "$$\n",
    "\\mathbf{W}_{\\mathrm{out}}\n",
    "    = \\begin{pmatrix}\n",
    "          w_{1,\\mathrm{you}} & w_{1,\\mathrm{say}} & w_{1,\\mathrm{goodbye}} & w_{1,\\mathrm{and}} &\n",
    "          w_{1,\\mathrm{I}} & w_{1,\\mathrm{hello}} & w_{1,\\mathrm{period}} \\\\\n",
    "          w_{2,\\mathrm{you}} & w_{2,\\mathrm{say}} & w_{2,\\mathrm{goodbye}} & w_{2,\\mathrm{and}} &\n",
    "          w_{2,\\mathrm{I}} & w_{2,\\mathrm{hello}} & w_{2,\\mathrm{period}} \\\\\n",
    "          w_{3,\\mathrm{you}} & w_{3,\\mathrm{say}} & w_{3,\\mathrm{goodbye}} & w_{3,\\mathrm{and}} &\n",
    "          w_{3,\\mathrm{I}} & w_{3,\\mathrm{hello}} & w_{3\\mathrm{period}} \\\\\n",
    "      \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "とします。行数が中間層のニューロン数、列数が単語の種類数になります。\n",
    "\n",
    "出力層も全結合層とすると、最終的な出力は\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf{s}\n",
    "   &= \\mathbf{h}\n",
    "      \\mathbf{W}_{\\mathrm{out}}\n",
    "\\\\\n",
    "   &= \\begin{pmatrix}\n",
    "          s_{\\mathrm{you}} & s_{\\mathrm{say}} & s_{\\mathrm{goodbye}} & s_{\\mathrm{and}} &\n",
    "          s_{\\mathrm{I}} & s_{\\mathrm{hello}} & s_{\\mathrm{period}}\n",
    "      \\end{pmatrix}\n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "例えば、「you」に関する要素の計算は、\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "s_{\\mathrm{you}}\n",
    "   &= \\frac{1}{2} (w_{\\mathrm{you},1} + w_{\\mathrm{goodbye},1}) w_{1,\\mathrm{you}}\n",
    "      + \\frac{1}{2} (w_{\\mathrm{you},2} + w_{\\mathrm{goodbye},2}) w_{2,\\mathrm{you}}\n",
    "      + \\frac{1}{2} (w_{\\mathrm{you},3} + w_{\\mathrm{goodbye},3}) w_{3,\\mathrm{you}}\n",
    "\\\\\n",
    "   &= \\frac{1}{2}\n",
    "      \\sum_{i=1}^3\n",
    "          (w_{\\mathrm{you},i} + w_{\\mathrm{goodbye},i}) w_{i,\\mathrm{you}}\n",
    "\\end{aligned}\n",
    "\n",
    "$$\n",
    "\n",
    "コンテキストに対応する入力層の重みの平均と「you」に関する出力の重みの積になります。\n",
    "\n",
    "他の要素(単語)についても同様に計算できるので、最終的な出力は\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf{s}\n",
    "   &= \\begin{pmatrix}\n",
    "          s_{\\mathrm{you}} & s_{\\mathrm{say}} & s_{\\mathrm{goodbye}} & s_{\\mathrm{and}} &\n",
    "          s_{\\mathrm{I}} & s_{\\mathrm{hello}} & s_{\\mathrm{period}}\n",
    "      \\end{pmatrix}\n",
    "\\\\\n",
    "   &= \\begin{pmatrix}\n",
    "          \\frac{1}{2} \\sum_{i=1}^3 (w_{\\mathrm{you},i} + w_{\\mathrm{goodbye},i}) w_{i,\\mathrm{you}} &\n",
    "          \\frac{1}{2} \\sum_{i=1}^3 (w_{\\mathrm{you},i} + w_{\\mathrm{goodbye},i}) w_{i,\\mathrm{say}} &\n",
    "          \\cdots &\n",
    "          \\frac{1}{2} \\sum_{i=1}^3 (w_{\\mathrm{you},i} + w_{\\mathrm{goodbye},i}) w_{i,\\mathrm{hello}} &\n",
    "          \\frac{1}{2} \\sum_{i=1}^3 (w_{\\mathrm{you},i} + w_{\\mathrm{goodbye},i}) w_{i,\\mathrm{period}}\n",
    "      \\end{pmatrix}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "となります。\n",
    "\n",
    "ここで、出力層のニューロンは各単語に対応し、各単語の「スコア」と言います。\n",
    "\n",
    "「スコア」の値が高ければ高いほど、それに対応する単語の出現確率も高くなり、ターゲットの単語であるとして採用します。そのため、スコアを求める処理を推論処理と言います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.5066, 1.7745, 1.2321]]),\n",
       " tensor([[-0.5417,  0.5844,  0.6926]]),\n",
       " tensor([[-0.0176,  1.1794,  0.9624]]),\n",
       " tensor([[ 3.3000, -0.2030, -2.6140,  2.6170, -0.8370, -1.4140,  1.3780]]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "# Define the context data\n",
    "c0 = torch.tensor([[1, 0, 0, 0, 0, 0, 0]], dtype=torch.float32) # you\n",
    "c1 = torch.tensor([[0, 0, 1, 0, 0, 0, 0]], dtype=torch.float32) # goodbye\n",
    "\n",
    "# Initialize weights randomly\n",
    "W_in = torch.randn(7, 3, requires_grad=False)  # Input layer weights\n",
    "W_out = torch.randn(3, 7, requires_grad=False) # Output layer weights\n",
    "\n",
    "# Define the layers using PyTorch's functional API\n",
    "def in_layer(x, W):\n",
    "    return torch.matmul(x, W)\n",
    "\n",
    "def out_layer(h, W):\n",
    "    return torch.matmul(h, W)\n",
    "\n",
    "# Forward pass through the input layers\n",
    "h0 = in_layer(c0, W_in) # you\n",
    "h1 = in_layer(c1, W_in) # goodbye\n",
    "h = 0.5 * (h0 + h1)\n",
    "\n",
    "# Forward pass through the output layer (scores)\n",
    "s = out_layer(h, W_out)\n",
    "\n",
    "# Print the outputs\n",
    "h0, h1, h, torch.round(s, decimals=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````{tab-set}\n",
    "```{tab-item} 課題\n",
    "正解は「say」として、Softmax関数によってスコア``s``を確率として扱えるように変換し、そして、正規化した値と教師ラベルを用いて損失を求めなさい。\n",
    "```\n",
    "\n",
    "```{tab-item} ヒント\n",
    "正解は「say」の場合、教師ラベルは``[0, 1, 0, 0, 0, 0, 0]``になります。\n",
    "```\n",
    "\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### word2vecの重みと分散表現"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与えられたコンテキストに対して単語を予測するときに、「良い重み」のネットワークがあれば、「確率」を表すニューロンにおいて、正解に対応するニューロンが高くなっていることが期待できます。そして、大規模コーパスを使って得られる単語の分散表現は、単語の意味や文法のルールにおいて、人間の直感と合致するケースが多く見られます。\n",
    "\n",
    "\n",
    "word2vecモデルの学習で行うことが、正しい予測ができるように重みを調整することです。つまり、「コンテキストから出現単語」を予測するという偽タスクをニューラルネットで解いてきましたが、目的はニューラルネットの重みを求めることになります。\n",
    "\n",
    "もっと具体的に言えば、word2vecで使用されるネットワークには二つの重みがあります。それは、入力層の重み$\\mathbf{W_{in}}$と、出力層の重み$\\mathbf{W_{out}}$です。それでは、どちらの重みを使えば良いでしょうか？\n",
    "\n",
    "1. 入力側の重みを利用する\n",
    "2. 出力側の重みを利用する\n",
    "3. 二つの重みの両方を利用する\n",
    "\n",
    "Word2Vecモデルに関しては、多くの研究や応用例で、入力層の重みを単語のベクトル表現として使用さており、良好なパフォーマンスを示しています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vecモデルの実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習データの準備\n",
    "\n",
    "#### コンテキストとターゲット\n",
    "\n",
    "Word2Vecモデルためのニューラルネットワークでは、「コンテキスト」を入力した時に、「ターゲット」が出現する確率を高くになるように学習を行います。\n",
    "\n",
    "そのため、コーパスから「コンテキスト」と「ターゲット」が対応するデータを作成する必要があります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前処理関数の実装\n",
    "def preprocess(text):\n",
    "    # 前処理\n",
    "    text = text.lower() # 小文字に変換\n",
    "    text = text.replace('.', ' .') # ピリオドの前にスペースを挿入\n",
    "    words = text.split(' ') # 単語ごとに分割\n",
    "    \n",
    "    # ディクショナリを初期化\n",
    "    word_to_id = {}\n",
    "    id_to_word = {}\n",
    "    \n",
    "    # 未収録の単語をディクショナリに格納\n",
    "    for word in words:\n",
    "        if word not in word_to_id: # 未収録の単語のとき\n",
    "            # 次の単語のidを取得\n",
    "            new_id = len(word_to_id)\n",
    "            \n",
    "            # 単語をキーとして単語IDを格納\n",
    "            word_to_id[word] = new_id\n",
    "            \n",
    "            # 単語IDをキーとして単語を格納\n",
    "            id_to_word[new_id] = word\n",
    "    \n",
    "    # 単語IDリストを作成\n",
    "    corpus = [word_to_id[w] for w in words]\n",
    "    \n",
    "    return corpus, word_to_id, id_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'you': 0, 'say': 1, 'goodbye': 2, 'and': 3, 'i': 4, 'hello': 5, '.': 6}\n",
      "{0: 'you', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'i', 5: 'hello', 6: '.'}\n",
      "[0, 1, 2, 3, 4, 1, 5, 6]\n"
     ]
    }
   ],
   "source": [
    "# テキストを設定\n",
    "text = 'You say goodbye and I say hello.'\n",
    "\n",
    "# 前処理\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "print(word_to_id)\n",
    "print(id_to_word)\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "テキストの単語を単語IDに変換した```corpus```からターゲットを抽出します。\n",
    "\n",
    "ターゲットはコンテキストの中央の単語なので、```corpus```の始めと終わりのウインドウサイズ分の単語は含めません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 1, 5]\n"
     ]
    }
   ],
   "source": [
    "# ウインドウサイズを指定\n",
    "window_size = 1\n",
    "\n",
    "# ターゲットを抽出\n",
    "target = corpus[window_size:-window_size]\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ターゲットの単語に対して、for文で前後ウィンドウサイズの範囲の単語を順番に抽出し```cs```に格納します。\n",
    "\n",
    "つまりウィンドウサイズを$1$とすると、```corpus```におけるターゲットのインデックス```idx```に対して、1つ前(```idx - window_size```)から1つ後(```idx + window_size```)までの範囲の単語を順番に```cs```格納します。ただしターゲット自体の単語はコンテキストに含めません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n",
      "[0, 2]\n",
      "[[0, 2]]\n"
     ]
    }
   ],
   "source": [
    "# コンテキストを初期化(受け皿を作成)\n",
    "contexts = []\n",
    "\n",
    "# 1つ目のターゲットのインデックス\n",
    "idx = window_size\n",
    "\n",
    "# 1つ目のターゲットのコンテキストを初期化(受け皿を作成)\n",
    "cs = []\n",
    "\n",
    "# 1つ目のターゲットのコンテキストを1単語ずつ格納\n",
    "for t in range(-window_size, window_size + 1):\n",
    "    \n",
    "    # tがターゲットのインデックスのとき処理しない\n",
    "    if t == 0:\n",
    "        continue\n",
    "    \n",
    "    # コンテキストを格納\n",
    "    cs.append(corpus[idx + t])\n",
    "    print(cs)\n",
    "\n",
    "# 1つ目のターゲットのコンテキストを格納\n",
    "contexts.append(cs)\n",
    "print(contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# コンテキストとターゲットの作成関数の実装\n",
    "def create_contexts_target(corpus, window_size=1):\n",
    "    \n",
    "    # ターゲットを抽出\n",
    "    target = corpus[window_size:-window_size]\n",
    "    \n",
    "    # コンテキストを初期化\n",
    "    contexts = []\n",
    "    \n",
    "    # ターゲットごとにコンテキストを格納\n",
    "    for idx in range(window_size, len(corpus) - window_size):\n",
    "        \n",
    "        # 現在のターゲットのコンテキストを初期化\n",
    "        cs = []\n",
    "        \n",
    "        # 現在のターゲットのコンテキストを1単語ずつ格納\n",
    "        for t in range(-window_size, window_size + 1):\n",
    "            \n",
    "            # 0番目の要素はターゲットそのものなので処理を省略\n",
    "            if t == 0:\n",
    "                continue\n",
    "            \n",
    "            # コンテキストを格納\n",
    "            cs.append(corpus[idx + t])\n",
    "            \n",
    "        # 現在のターゲットのコンテキストのセットを格納\n",
    "        contexts.append(cs)\n",
    "    \n",
    "    # NumPy配列に変換\n",
    "    return np.array(contexts), np.array(target) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 2]\n",
      " [1 3]\n",
      " [2 4]\n",
      " [3 1]\n",
      " [4 5]\n",
      " [1 6]]\n",
      "[1 2 3 4 1 5]\n"
     ]
    }
   ],
   "source": [
    "# コンテキストとターゲットを作成\n",
    "contexts, target = create_contexts_target(corpus, window_size=1)\n",
    "print(contexts)\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### one-hot表現への変換\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "単語IDを要素とするコンテキストとターゲットをone-hot表現のコンテキストとターゲットに変換する関数を実装します。\n",
    "\n",
    "基本的な処理は、単語の種類数個の$0$を要素とするベクトルを作成し、単語ID番目の要素だけを$1$に置き換えます。\n",
    "\n",
    "ターゲットは、要素数がターゲット数のベクトルです。変換後は、ターゲット数の行数、単語の種類数の列数の2次元配列になります。つまり、行が各ターゲットの単語、列が各単語IDに対応します。そして行ごとに1つだけ、値が$1$の要素を持ちます。\n",
    "\n",
    "-  ```np.zeros()```で変換後の形状の2次元配列を作成し、for文で行ごとに単語ID番目の要素を1を代入します。\n",
    "- ```enumerate()```で引数に渡したリストの要素とその要素のインデックスを出力します。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3 4 1 5]\n",
      "(6,)\n",
      "[[0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]]\n",
      "[[0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0]\n",
      " [0 0 0 1 0 0 0]\n",
      " [0 0 0 0 1 0 0]\n",
      " [0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0]]\n",
      "(6, 7)\n"
     ]
    }
   ],
   "source": [
    "# ターゲットを確認\n",
    "print(target)\n",
    "print(target.shape)\n",
    "\n",
    "# ターゲットの単語数を取得\n",
    "N = target.shape[0]\n",
    "\n",
    "# 単語の種類数を取得\n",
    "vocab_size = len(word_to_id)\n",
    "\n",
    "# 全ての要素が0の変換後の形状の2次元配列を作成\n",
    "one_hot = np.zeros((N, vocab_size), dtype=np.int32)\n",
    "print(one_hot)\n",
    "\n",
    "# 単語ID番目の要素を1に置換\n",
    "for idx, word_id in enumerate(target):\n",
    "    one_hot[idx, word_id] = 1\n",
    "print(one_hot)\n",
    "print(one_hot.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "コンテキストは、0次元目の要素数がターゲット数、1次元目の要素数がウィンドウサイズの$2$倍の2次元配列です。\n",
    "\n",
    "- ```np.zeros()```で形状が```(N, C, vocab_size)```である配列を作成し、単語ID番目の要素を1に置換します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 2]\n",
      " [1 3]\n",
      " [2 4]\n",
      " [3 1]\n",
      " [4 5]\n",
      " [1 6]]\n",
      "(6, 2)\n",
      "全ての要素が0の変換後の形状の3次元配列を作成\n",
      "[[[0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0]]\n",
      "\n",
      " [[0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0]]\n",
      "\n",
      " [[0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0]]\n",
      "\n",
      " [[0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0]]\n",
      "\n",
      " [[0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0]]\n",
      "\n",
      " [[0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0]]]\n",
      "単語ID番目の要素を1に置換\n",
      "[[[1 0 0 0 0 0 0]\n",
      "  [0 0 1 0 0 0 0]]\n",
      "\n",
      " [[0 1 0 0 0 0 0]\n",
      "  [0 0 0 1 0 0 0]]\n",
      "\n",
      " [[0 0 1 0 0 0 0]\n",
      "  [0 0 0 0 1 0 0]]\n",
      "\n",
      " [[0 0 0 1 0 0 0]\n",
      "  [0 1 0 0 0 0 0]]\n",
      "\n",
      " [[0 0 0 0 1 0 0]\n",
      "  [0 0 0 0 0 1 0]]\n",
      "\n",
      " [[0 1 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 1]]]\n",
      "(6, 2, 7)\n"
     ]
    }
   ],
   "source": [
    "# コンテキストを確認\n",
    "print(contexts)\n",
    "print(contexts.shape)\n",
    "\n",
    "# ターゲットの単語数を取得\n",
    "N = contexts.shape[0]\n",
    "\n",
    "# コンテキストサイズを取得\n",
    "C = contexts.shape[1]\n",
    "\n",
    "# 単語の種類数を取得\n",
    "vocab_size = len(word_to_id)\n",
    "\n",
    "# 全ての要素が0の変換後の形状の3次元配列を作成\n",
    "one_hot = np.zeros((N, C, vocab_size), dtype=np.int32)\n",
    "print(\"全ての要素が0の変換後の形状の3次元配列を作成\")\n",
    "print(one_hot)\n",
    "\n",
    "# 単語ID番目の要素を1に置換\n",
    "for idx_0, word_ids in enumerate(contexts): # 0次元方向\n",
    "    for idx_1, word_id in enumerate(word_ids): # 1次元方向\n",
    "        one_hot[idx_0, idx_1, word_id] = 1\n",
    "\n",
    "print(\"単語ID番目の要素を1に置換\")\n",
    "print(one_hot)\n",
    "print(one_hot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot表現への変換関数の実装\n",
    "def convert_one_hot(corpus, vocab_size):\n",
    "    \n",
    "    # ターゲットの単語数を取得\n",
    "    N = corpus.shape[0]\n",
    "    \n",
    "    # one-hot表現に変換\n",
    "    if corpus.ndim == 1: # 1次元配列のとき\n",
    "        \n",
    "        # 変換後の形状の2次元配列を作成\n",
    "        one_hot = np.zeros((N, vocab_size), dtype=np.int32)\n",
    "        \n",
    "        # 単語ID番目の要素を1に置換\n",
    "        for idx, word_id in enumerate(corpus):\n",
    "            one_hot[idx, word_id] = 1\n",
    "    \n",
    "    elif corpus.ndim == 2: # 2次元配列のとき\n",
    "        \n",
    "        # コンテキストサイズを取得\n",
    "        C = corpus.shape[1]\n",
    "        \n",
    "        # 変換後の形状の3次元配列を作成\n",
    "        one_hot = np.zeros((N, C, vocab_size), dtype=np.int32)\n",
    "        \n",
    "        # 単語ID番目の要素を1に置換\n",
    "        for idx_0, word_ids in enumerate(corpus): # 0次元方向\n",
    "            for idx_1, word_id in enumerate(word_ids): # 1次元方向\n",
    "                one_hot[idx_0, idx_1, word_id] = 1\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CBOWモデルの実装\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleCBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, embedding_size):\n",
    "        super(SimpleCBOW, self).__init__()\n",
    "        # 入力層と中間層の重み\n",
    "        self.in_layer = nn.Embedding(vocab_size, embedding_size)\n",
    "        # 中間層の次元を調整するための線形層\n",
    "        self.middle_layer = nn.Linear(embedding_size, hidden_size)\n",
    "        # 中間層と出力層の重み\n",
    "        self.out_layer = nn.Linear(hidden_size, vocab_size)\n",
    "        # 単語の分散表現を初期化\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        # 重みを標準正規分布で初期化\n",
    "        self.in_layer.weight.data.normal_(0, 1)\n",
    "        self.middle_layer.weight.data.normal_(0, 1)\n",
    "        self.out_layer.weight.data.normal_(0, 1)\n",
    "\n",
    "    def forward(self, contexts):\n",
    "        # 入力層から中間層への順伝播\n",
    "        # contextsは周囲の単語のインデックスのバッチ\n",
    "        embeds = self.in_layer(contexts)  # 埋め込みレイヤーを適用\n",
    "        h = embeds.mean(dim=1)  # 埋め込みの平均を取る\n",
    "        h = self.middle_layer(h)  # 中間層に適用\n",
    "        # 中間層から出力層への順伝播\n",
    "        out = self.out_layer(h)\n",
    "        return out\n",
    "\n",
    "    def loss(self, contexts, target):\n",
    "        # 順伝播\n",
    "        out = self.forward(contexts)\n",
    "        # 損失の計算\n",
    "        loss = F.cross_entropy(out, target)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=SimpleCBOW(6, 20, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.6963,  4.0685, -1.4509, -1.8220,  2.3745,  0.9844],\n",
       "         [-0.2198,  0.3583,  0.3574, -1.3417, -1.7221, -0.0400],\n",
       "         [ 1.6963,  4.0685, -1.4509, -1.8220,  2.3745,  0.9844],\n",
       "         [-0.2198,  0.3583,  0.3574, -1.3417, -1.7221, -0.0400],\n",
       "         [-0.2198,  0.3583,  0.3574, -1.3417, -1.7221, -0.0400],\n",
       "         [-0.2198,  0.3583,  0.3574, -1.3417, -1.7221, -0.0400],\n",
       "         [-0.2198,  0.3583,  0.3574, -1.3417, -1.7221, -0.0400]],\n",
       "\n",
       "        [[-0.2198,  0.3583,  0.3574, -1.3417, -1.7221, -0.0400],\n",
       "         [ 1.6963,  4.0685, -1.4509, -1.8220,  2.3745,  0.9844],\n",
       "         [-0.2198,  0.3583,  0.3574, -1.3417, -1.7221, -0.0400],\n",
       "         [ 1.6963,  4.0685, -1.4509, -1.8220,  2.3745,  0.9844],\n",
       "         [-0.2198,  0.3583,  0.3574, -1.3417, -1.7221, -0.0400],\n",
       "         [-0.2198,  0.3583,  0.3574, -1.3417, -1.7221, -0.0400],\n",
       "         [-0.2198,  0.3583,  0.3574, -1.3417, -1.7221, -0.0400]],\n",
       "\n",
       "        [[-0.2198,  0.3583,  0.3574, -1.3417, -1.7221, -0.0400],\n",
       "         [-0.2198,  0.3583,  0.3574, -1.3417, -1.7221, -0.0400],\n",
       "         [ 1.6963,  4.0685, -1.4509, -1.8220,  2.3745,  0.9844],\n",
       "         [-0.2198,  0.3583,  0.3574, -1.3417, -1.7221, -0.0400],\n",
       "         [ 1.6963,  4.0685, -1.4509, -1.8220,  2.3745,  0.9844],\n",
       "         [-0.2198,  0.3583,  0.3574, -1.3417, -1.7221, -0.0400],\n",
       "         [-0.2198,  0.3583,  0.3574, -1.3417, -1.7221, -0.0400]],\n",
       "\n",
       "        [[-0.2198,  0.3583,  0.3574, -1.3417, -1.7221, -0.0400],\n",
       "         [ 1.6963,  4.0685, -1.4509, -1.8220,  2.3745,  0.9844],\n",
       "         [-0.2198,  0.3583,  0.3574, -1.3417, -1.7221, -0.0400],\n",
       "         [ 1.6963,  4.0685, -1.4509, -1.8220,  2.3745,  0.9844],\n",
       "         [-0.2198,  0.3583,  0.3574, -1.3417, -1.7221, -0.0400],\n",
       "         [-0.2198,  0.3583,  0.3574, -1.3417, -1.7221, -0.0400],\n",
       "         [-0.2198,  0.3583,  0.3574, -1.3417, -1.7221, -0.0400]],\n",
       "\n",
       "        [[-0.2198,  0.3583,  0.3574, -1.3417, -1.7221, -0.0400],\n",
       "         [-0.2198,  0.3583,  0.3574, -1.3417, -1.7221, -0.0400],\n",
       "         [-0.2198,  0.3583,  0.3574, -1.3417, -1.7221, -0.0400],\n",
       "         [-0.2198,  0.3583,  0.3574, -1.3417, -1.7221, -0.0400],\n",
       "         [ 1.6963,  4.0685, -1.4509, -1.8220,  2.3745,  0.9844],\n",
       "         [ 1.6963,  4.0685, -1.4509, -1.8220,  2.3745,  0.9844],\n",
       "         [-0.2198,  0.3583,  0.3574, -1.3417, -1.7221, -0.0400]],\n",
       "\n",
       "        [[-0.2198,  0.3583,  0.3574, -1.3417, -1.7221, -0.0400],\n",
       "         [ 1.6963,  4.0685, -1.4509, -1.8220,  2.3745,  0.9844],\n",
       "         [-0.2198,  0.3583,  0.3574, -1.3417, -1.7221, -0.0400],\n",
       "         [-0.2198,  0.3583,  0.3574, -1.3417, -1.7221, -0.0400],\n",
       "         [-0.2198,  0.3583,  0.3574, -1.3417, -1.7221, -0.0400],\n",
       "         [-0.2198,  0.3583,  0.3574, -1.3417, -1.7221, -0.0400],\n",
       "         [ 1.6963,  4.0685, -1.4509, -1.8220,  2.3745,  0.9844]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.forward(torch.tensor(contexts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'you': 0, 'say': 1, 'goodbye': 2, 'and': 3, 'i': 4, 'hello': 5, '.': 6}\n",
      "{0: 'you', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'i', 5: 'hello', 6: '.'}\n",
      "[0, 1, 2, 3, 4, 1, 5, 6]\n"
     ]
    }
   ],
   "source": [
    "# テキストを設定\n",
    "text = 'You say goodbye and I say hello.'\n",
    "\n",
    "# 前処理\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "print(word_to_id)\n",
    "print(id_to_word)\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "[[0 2]\n",
      " [1 3]\n",
      " [2 4]\n",
      " [3 1]\n",
      " [4 5]\n",
      " [1 6]]\n",
      "(6, 2)\n",
      "[1 2 3 4 1 5]\n",
      "(6,)\n",
      "[[[1 0 0 0 0 0 0]\n",
      "  [0 0 1 0 0 0 0]]\n",
      "\n",
      " [[0 1 0 0 0 0 0]\n",
      "  [0 0 0 1 0 0 0]]\n",
      "\n",
      " [[0 0 1 0 0 0 0]\n",
      "  [0 0 0 0 1 0 0]]\n",
      "\n",
      " [[0 0 0 1 0 0 0]\n",
      "  [0 1 0 0 0 0 0]]\n",
      "\n",
      " [[0 0 0 0 1 0 0]\n",
      "  [0 0 0 0 0 1 0]]\n",
      "\n",
      " [[0 1 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 1]]]\n",
      "(6, 2, 7)\n",
      "[[0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0]\n",
      " [0 0 0 1 0 0 0]\n",
      " [0 0 0 0 1 0 0]\n",
      " [0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0]]\n",
      "(6, 7)\n"
     ]
    }
   ],
   "source": [
    "# ウインドウサイズ\n",
    "window_size = 1\n",
    "\n",
    "# 単語の種類数を取得\n",
    "vocab_size = len(word_to_id)\n",
    "print(vocab_size)\n",
    "\n",
    "# コンテキストとターゲットを作成\n",
    "contexts, target = create_contexts_target(corpus, window_size)\n",
    "print(contexts)\n",
    "print(contexts.shape)\n",
    "print(target)\n",
    "print(target.shape)\n",
    "\n",
    "# one-hot表現に変換\n",
    "contexts = convert_one_hot(contexts, vocab_size)\n",
    "target = convert_one_hot(target, vocab_size)\n",
    "print(contexts)\n",
    "print(contexts.shape)\n",
    "print(target)\n",
    "print(target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = nn.Embedding(6, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "out=model (torch.tensor(contexts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-1.0733, -1.1314,  0.7917],\n",
       "          [-0.1738, -0.3991, -0.0342],\n",
       "          [-0.1738, -0.3991, -0.0342],\n",
       "          [-0.1738, -0.3991, -0.0342],\n",
       "          [-0.1738, -0.3991, -0.0342],\n",
       "          [-0.1738, -0.3991, -0.0342],\n",
       "          [-0.1738, -0.3991, -0.0342]],\n",
       "\n",
       "         [[-0.1738, -0.3991, -0.0342],\n",
       "          [-0.1738, -0.3991, -0.0342],\n",
       "          [-1.0733, -1.1314,  0.7917],\n",
       "          [-0.1738, -0.3991, -0.0342],\n",
       "          [-0.1738, -0.3991, -0.0342],\n",
       "          [-0.1738, -0.3991, -0.0342],\n",
       "          [-0.1738, -0.3991, -0.0342]]],\n",
       "\n",
       "\n",
       "        [[[-0.1738, -0.3991, -0.0342],\n",
       "          [-1.0733, -1.1314,  0.7917],\n",
       "          [-0.1738, -0.3991, -0.0342],\n",
       "          [-0.1738, -0.3991, -0.0342],\n",
       "          [-0.1738, -0.3991, -0.0342],\n",
       "          [-0.1738, -0.3991, -0.0342],\n",
       "          [-0.1738, -0.3991, -0.0342]],\n",
       "\n",
       "         [[-0.1738, -0.3991, -0.0342],\n",
       "          [-0.1738, -0.3991, -0.0342],\n",
       "          [-0.1738, -0.3991, -0.0342],\n",
       "          [-1.0733, -1.1314,  0.7917],\n",
       "          [-0.1738, -0.3991, -0.0342],\n",
       "          [-0.1738, -0.3991, -0.0342],\n",
       "          [-0.1738, -0.3991, -0.0342]]],\n",
       "\n",
       "\n",
       "        [[[-0.1738, -0.3991, -0.0342],\n",
       "          [-0.1738, -0.3991, -0.0342],\n",
       "          [-1.0733, -1.1314,  0.7917],\n",
       "          [-0.1738, -0.3991, -0.0342],\n",
       "          [-0.1738, -0.3991, -0.0342],\n",
       "          [-0.1738, -0.3991, -0.0342],\n",
       "          [-0.1738, -0.3991, -0.0342]],\n",
       "\n",
       "         [[-0.1738, -0.3991, -0.0342],\n",
       "          [-0.1738, -0.3991, -0.0342],\n",
       "          [-0.1738, -0.3991, -0.0342],\n",
       "          [-0.1738, -0.3991, -0.0342],\n",
       "          [-1.0733, -1.1314,  0.7917],\n",
       "          [-0.1738, -0.3991, -0.0342],\n",
       "          [-0.1738, -0.3991, -0.0342]]],\n",
       "\n",
       "\n",
       "        [[[-0.1738, -0.3991, -0.0342],\n",
       "          [-0.1738, -0.3991, -0.0342],\n",
       "          [-0.1738, -0.3991, -0.0342],\n",
       "          [-1.0733, -1.1314,  0.7917],\n",
       "          [-0.1738, -0.3991, -0.0342],\n",
       "          [-0.1738, -0.3991, -0.0342],\n",
       "          [-0.1738, -0.3991, -0.0342]],\n",
       "\n",
       "         [[-0.1738, -0.3991, -0.0342],\n",
       "          [-1.0733, -1.1314,  0.7917],\n",
       "          [-0.1738, -0.3991, -0.0342],\n",
       "          [-0.1738, -0.3991, -0.0342],\n",
       "          [-0.1738, -0.3991, -0.0342],\n",
       "          [-0.1738, -0.3991, -0.0342],\n",
       "          [-0.1738, -0.3991, -0.0342]]],\n",
       "\n",
       "\n",
       "        [[[-0.1738, -0.3991, -0.0342],\n",
       "          [-0.1738, -0.3991, -0.0342],\n",
       "          [-0.1738, -0.3991, -0.0342],\n",
       "          [-0.1738, -0.3991, -0.0342],\n",
       "          [-1.0733, -1.1314,  0.7917],\n",
       "          [-0.1738, -0.3991, -0.0342],\n",
       "          [-0.1738, -0.3991, -0.0342]],\n",
       "\n",
       "         [[-0.1738, -0.3991, -0.0342],\n",
       "          [-0.1738, -0.3991, -0.0342],\n",
       "          [-0.1738, -0.3991, -0.0342],\n",
       "          [-0.1738, -0.3991, -0.0342],\n",
       "          [-0.1738, -0.3991, -0.0342],\n",
       "          [-1.0733, -1.1314,  0.7917],\n",
       "          [-0.1738, -0.3991, -0.0342]]],\n",
       "\n",
       "\n",
       "        [[[-0.1738, -0.3991, -0.0342],\n",
       "          [-1.0733, -1.1314,  0.7917],\n",
       "          [-0.1738, -0.3991, -0.0342],\n",
       "          [-0.1738, -0.3991, -0.0342],\n",
       "          [-0.1738, -0.3991, -0.0342],\n",
       "          [-0.1738, -0.3991, -0.0342],\n",
       "          [-0.1738, -0.3991, -0.0342]],\n",
       "\n",
       "         [[-0.1738, -0.3991, -0.0342],\n",
       "          [-0.1738, -0.3991, -0.0342],\n",
       "          [-0.1738, -0.3991, -0.0342],\n",
       "          [-0.1738, -0.3991, -0.0342],\n",
       "          [-0.1738, -0.3991, -0.0342],\n",
       "          [-0.1738, -0.3991, -0.0342],\n",
       "          [-1.0733, -1.1314,  0.7917]]]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = embeds.mean(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 7, 3])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = nn.Embedding(10, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.LongTensor([[1, 2, 4, 5], [4, 3, 2, 9]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super(SimpleCBOW, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_size)  # 単語埋め込み\n",
    "        self.linear1 = nn.Linear(embedding_size, hidden_size)  # 中間層\n",
    "        self.linear2 = nn.Linear(hidden_size, vocab_size)  # 出力層\n",
    "    \n",
    "    def forward(self, contexts):\n",
    "        # contextは [batch_size, context_window * 2]\n",
    "        embeds = self.embeddings(contexts)  # [batch_size, context_window * 2, embedding_size]\n",
    "        print(embeds.shape)\n",
    "        embeds = embeds.sum(dim=1)  # [batch_size, embedding_size]\n",
    "        print(embeds.shape)\n",
    "        h = self.linear1(embeds)  # [batch_size, hidden_size]\n",
    "        h = F.relu(h)  # 活性化関数\n",
    "        out = self.linear2(h)  # [batch_size, vocab_size]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=SimpleCBOW(6, 20, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 7, 6])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.forward(torch.tensor(contexts)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-9.3958e+00, -1.9546e+01, -1.3012e+01, -9.7609e-01, -2.8456e+01,\n",
       "          -4.7300e-01],\n",
       "         [-7.3104e+00, -2.8340e+01, -1.1410e+01, -1.4202e-02, -3.5601e+01,\n",
       "          -4.3108e+00],\n",
       "         [-9.3958e+00, -1.9546e+01, -1.3012e+01, -9.7609e-01, -2.8456e+01,\n",
       "          -4.7300e-01],\n",
       "         [-7.3104e+00, -2.8340e+01, -1.1410e+01, -1.4202e-02, -3.5601e+01,\n",
       "          -4.3108e+00],\n",
       "         [-7.3104e+00, -2.8340e+01, -1.1410e+01, -1.4202e-02, -3.5601e+01,\n",
       "          -4.3108e+00],\n",
       "         [-7.3104e+00, -2.8340e+01, -1.1410e+01, -1.4202e-02, -3.5601e+01,\n",
       "          -4.3108e+00],\n",
       "         [-7.3104e+00, -2.8340e+01, -1.1410e+01, -1.4202e-02, -3.5601e+01,\n",
       "          -4.3108e+00]],\n",
       "\n",
       "        [[-7.3104e+00, -2.8340e+01, -1.1410e+01, -1.4202e-02, -3.5601e+01,\n",
       "          -4.3108e+00],\n",
       "         [-9.3958e+00, -1.9546e+01, -1.3012e+01, -9.7609e-01, -2.8456e+01,\n",
       "          -4.7300e-01],\n",
       "         [-7.3104e+00, -2.8340e+01, -1.1410e+01, -1.4202e-02, -3.5601e+01,\n",
       "          -4.3108e+00],\n",
       "         [-9.3958e+00, -1.9546e+01, -1.3012e+01, -9.7609e-01, -2.8456e+01,\n",
       "          -4.7300e-01],\n",
       "         [-7.3104e+00, -2.8340e+01, -1.1410e+01, -1.4202e-02, -3.5601e+01,\n",
       "          -4.3108e+00],\n",
       "         [-7.3104e+00, -2.8340e+01, -1.1410e+01, -1.4202e-02, -3.5601e+01,\n",
       "          -4.3108e+00],\n",
       "         [-7.3104e+00, -2.8340e+01, -1.1410e+01, -1.4202e-02, -3.5601e+01,\n",
       "          -4.3108e+00]],\n",
       "\n",
       "        [[-7.3104e+00, -2.8340e+01, -1.1410e+01, -1.4202e-02, -3.5601e+01,\n",
       "          -4.3108e+00],\n",
       "         [-7.3104e+00, -2.8340e+01, -1.1410e+01, -1.4202e-02, -3.5601e+01,\n",
       "          -4.3108e+00],\n",
       "         [-9.3958e+00, -1.9546e+01, -1.3012e+01, -9.7609e-01, -2.8456e+01,\n",
       "          -4.7300e-01],\n",
       "         [-7.3104e+00, -2.8340e+01, -1.1410e+01, -1.4202e-02, -3.5601e+01,\n",
       "          -4.3108e+00],\n",
       "         [-9.3958e+00, -1.9546e+01, -1.3012e+01, -9.7609e-01, -2.8456e+01,\n",
       "          -4.7300e-01],\n",
       "         [-7.3104e+00, -2.8340e+01, -1.1410e+01, -1.4202e-02, -3.5601e+01,\n",
       "          -4.3108e+00],\n",
       "         [-7.3104e+00, -2.8340e+01, -1.1410e+01, -1.4202e-02, -3.5601e+01,\n",
       "          -4.3108e+00]],\n",
       "\n",
       "        [[-7.3104e+00, -2.8340e+01, -1.1410e+01, -1.4202e-02, -3.5601e+01,\n",
       "          -4.3108e+00],\n",
       "         [-9.3958e+00, -1.9546e+01, -1.3012e+01, -9.7609e-01, -2.8456e+01,\n",
       "          -4.7300e-01],\n",
       "         [-7.3104e+00, -2.8340e+01, -1.1410e+01, -1.4202e-02, -3.5601e+01,\n",
       "          -4.3108e+00],\n",
       "         [-9.3958e+00, -1.9546e+01, -1.3012e+01, -9.7609e-01, -2.8456e+01,\n",
       "          -4.7300e-01],\n",
       "         [-7.3104e+00, -2.8340e+01, -1.1410e+01, -1.4202e-02, -3.5601e+01,\n",
       "          -4.3108e+00],\n",
       "         [-7.3104e+00, -2.8340e+01, -1.1410e+01, -1.4202e-02, -3.5601e+01,\n",
       "          -4.3108e+00],\n",
       "         [-7.3104e+00, -2.8340e+01, -1.1410e+01, -1.4202e-02, -3.5601e+01,\n",
       "          -4.3108e+00]],\n",
       "\n",
       "        [[-7.3104e+00, -2.8340e+01, -1.1410e+01, -1.4202e-02, -3.5601e+01,\n",
       "          -4.3108e+00],\n",
       "         [-7.3104e+00, -2.8340e+01, -1.1410e+01, -1.4202e-02, -3.5601e+01,\n",
       "          -4.3108e+00],\n",
       "         [-7.3104e+00, -2.8340e+01, -1.1410e+01, -1.4202e-02, -3.5601e+01,\n",
       "          -4.3108e+00],\n",
       "         [-7.3104e+00, -2.8340e+01, -1.1410e+01, -1.4202e-02, -3.5601e+01,\n",
       "          -4.3108e+00],\n",
       "         [-9.3958e+00, -1.9546e+01, -1.3012e+01, -9.7609e-01, -2.8456e+01,\n",
       "          -4.7300e-01],\n",
       "         [-9.3958e+00, -1.9546e+01, -1.3012e+01, -9.7609e-01, -2.8456e+01,\n",
       "          -4.7300e-01],\n",
       "         [-7.3104e+00, -2.8340e+01, -1.1410e+01, -1.4202e-02, -3.5601e+01,\n",
       "          -4.3108e+00]],\n",
       "\n",
       "        [[-7.3104e+00, -2.8340e+01, -1.1410e+01, -1.4202e-02, -3.5601e+01,\n",
       "          -4.3108e+00],\n",
       "         [-9.3958e+00, -1.9546e+01, -1.3012e+01, -9.7609e-01, -2.8456e+01,\n",
       "          -4.7300e-01],\n",
       "         [-7.3104e+00, -2.8340e+01, -1.1410e+01, -1.4202e-02, -3.5601e+01,\n",
       "          -4.3108e+00],\n",
       "         [-7.3104e+00, -2.8340e+01, -1.1410e+01, -1.4202e-02, -3.5601e+01,\n",
       "          -4.3108e+00],\n",
       "         [-7.3104e+00, -2.8340e+01, -1.1410e+01, -1.4202e-02, -3.5601e+01,\n",
       "          -4.3108e+00],\n",
       "         [-7.3104e+00, -2.8340e+01, -1.1410e+01, -1.4202e-02, -3.5601e+01,\n",
       "          -4.3108e+00],\n",
       "         [-9.3958e+00, -1.9546e+01, -1.3012e+01, -9.7608e-01, -2.8456e+01,\n",
       "          -4.7300e-01]]], grad_fn=<LogSoftmaxBackward0>)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.forward(torch.tensor(contexts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyterbook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
