{"version":"1","records":[{"hierarchy":{"lvl1":"計算社会科学と自然言語処理"},"type":"lvl1","url":"/intro","position":0},{"hierarchy":{"lvl1":"計算社会科学と自然言語処理"},"content":"計算社会科学と自然言語処理\n\nイントロダクション\n\nガイダンス\n\n基礎知識\n\n自然言語処理の基礎\n\n機械学習の基本概念\n\n数学基礎\n\nニューラルネットワーク\n\nニューラルネットワーク\n\n誤差逆伝播法\n\n学習に関するテクニック\n\nPyTorch\n\nPytorch\n\n単語分散表現\n\n単語分散表現\n\nword2vec\n\nGensimによるWord2Vecの学習と使用\n\nWord2Vecが人文・社会科学研究における応用\n\nRNN\n\nRNNの基礎\n\nLSTM\n\nLSTMの実装\n\nLSTMによる文書分類\n\nSeq2seq\n\nTransformer\n\nAttention\n\nSelf-Attention\n\nTransformerアーキテクチャ\n\nBERT\n\nBERTによるセンチメント分析\n\nBERTopic\n\n大規模言語モデル\n\nGPT\n\n大規模言語モデルの基本\n\nLLMSの応用(1)：Langchainの基礎","type":"content","url":"/intro","position":1},{"hierarchy":{"lvl1":"BERT"},"type":"lvl1","url":"/bert","position":0},{"hierarchy":{"lvl1":"BERT"},"content":"\n\n2018年Googleが発表したBERT（Bidirectional Encoder Representations from Transformers）は、エンコーダ構成のTransformersを採用し、先行するトークン列と後続するトークン列の双方向性から文脈を捉えます。\n\nBERTは、Wikipediaと7,000冊の書籍を合わせた大規模なコーパスを使って事前学習されたモデルを、下流タスクのデータセットでファインチューングすることで、様々なタスクの性能を大幅に改善できることが示されました。\n\n事前学習: 大規模なコーパスを用いて、特定なタスクを学習することで、広範な言語データからパターンを学習し、汎用的な言語理解の能力を身につける。\n\nファインチューング：、特定のタスクや領域に特化した小さなデータセットを用いて、事前学習したモデルを微調整します。この微調整により、モデルは特定のタスクや領域に適応し、高い精度を達成することが可能です。\n\n","type":"content","url":"/bert","position":1},{"hierarchy":{"lvl1":"BERT","lvl2":"Contextualized Embedding"},"type":"lvl2","url":"/bert#contextualized-embedding","position":2},{"hierarchy":{"lvl1":"BERT","lvl2":"Contextualized Embedding"},"content":"\n\n\n\n","type":"content","url":"/bert#contextualized-embedding","position":3},{"hierarchy":{"lvl1":"BERT","lvl2":"BERTの事前学習"},"type":"lvl2","url":"/bert#bert","position":4},{"hierarchy":{"lvl1":"BERT","lvl2":"BERTの事前学習"},"content":"","type":"content","url":"/bert#bert","position":5},{"hierarchy":{"lvl1":"BERT","lvl3":"BERTの構造","lvl2":"BERTの事前学習"},"type":"lvl3","url":"/bert#bert-1","position":6},{"hierarchy":{"lvl1":"BERT","lvl3":"BERTの構造","lvl2":"BERTの事前学習"},"content":"\n\nBERTは、Transformerのエンコーダ部分をベースにしたモデルになります。\n\n複数のエンコーダ層を積み重ねることで、より複雑で長い文脈を処理できるようになり、幅広い自然言語処理タスクで高い性能を発揮しています。\n\n","type":"content","url":"/bert#bert-1","position":7},{"hierarchy":{"lvl1":"BERT","lvl3":"入力表現","lvl2":"BERTの事前学習"},"type":"lvl3","url":"/bert#id","position":8},{"hierarchy":{"lvl1":"BERT","lvl3":"入力表現","lvl2":"BERTの事前学習"},"content":"\n\nBERTで入力を作成する際、入力の開始を表す[CLS]トークンと、入力の区切りを表す[SEP]トークンという二つの特殊トークンが使われます。\n\nまたよく使われる特殊トークンとして、マスクタスクための[MASK]トークン、vocabularyに含まれていないことを示す[UNK]トークンがあります。\n\n入力トークン埋め込みと位置埋め込み以外、それぞれのテキストの範囲を区別しやくするためにsegment embeddingという埋め込みが導入されています。\n\nまとめると、BERTの入力埋め込みx_iは、トークン埋め込み、位置埋め込み、segment埋め込みより加算されます。\n\n","type":"content","url":"/bert#id","position":9},{"hierarchy":{"lvl1":"BERT","lvl3":"BERTの出力","lvl2":"BERTの事前学習"},"type":"lvl3","url":"/bert#bert-2","position":10},{"hierarchy":{"lvl1":"BERT","lvl3":"BERTの出力","lvl2":"BERTの事前学習"},"content":"\n\nBERTの出力は、入力文の各トークンに対応する文脈化された埋め込みベクトル（contextual embeddings）です。これらの埋め込みは、モデルが入力文全体の文脈を考慮しながら、各トークンの意味を表現したものです。\n\nNote\n\nBERTの出力の中で、入力文の最初に挿入される特殊トークン [CLS] に対応する埋め込みは、文全体の情報を要約するように設計されています。\n\n","type":"content","url":"/bert#bert-2","position":11},{"hierarchy":{"lvl1":"BERT","lvl3":"マスク言語モデリング","lvl2":"BERTの事前学習"},"type":"lvl3","url":"/bert#id-1","position":12},{"hierarchy":{"lvl1":"BERT","lvl3":"マスク言語モデリング","lvl2":"BERTの事前学習"},"content":"マスク言語モデリングは、トークンの穴埋め問題を解けるタスクです。具体的には、ランダムにトークン列中のトークンを隠して、その周辺の単語からマスクされた単語を予測することが求められます。\n\nここで、先行するトークンと後続トークンの双方の情報が使われていますので、全体の文脈を捉える学習を実現しています。\n\n","type":"content","url":"/bert#id-1","position":13},{"hierarchy":{"lvl1":"BERT","lvl3":"次文予測","lvl2":"BERTの事前学習"},"type":"lvl3","url":"/bert#id-2","position":14},{"hierarchy":{"lvl1":"BERT","lvl3":"次文予測","lvl2":"BERTの事前学習"},"content":"次文予測タスクでは、2つの文が与えられ、一方が他方の直後に来るかどうかを判定することが求められます。\n\n","type":"content","url":"/bert#id-2","position":15},{"hierarchy":{"lvl1":"BERT","lvl2":"ファインチューング(Fine-Tuning)"},"type":"lvl2","url":"/bert#id-fine-tuning","position":16},{"hierarchy":{"lvl1":"BERT","lvl2":"ファインチューング(Fine-Tuning)"},"content":"様々なタスクにおいて、事前学習モデルをもとにしてそれぞれのタスクに特化したモデルを作るためのステップはファインチューングです。\n\n事前学習タスクから下流タスクに切り替える時、モデルの最後のレイヤーをタスクに適したものに置き換える必要があります。この最後の層はヘッドと呼ばれ、タスクに固有な部分です。\n\n残りの部分はボディと呼ばれ、タスクに依存しない事前学習された部分であり(トークン埋め込み層やTransformer層が含まれ)、一般的な言語の理解を行うための基本的な情報を含んでいます。\n\n例えば、テキスト分類の場合、追加層としては、BERTの最後の隠れ層からの出力に冒頭のSpecial token[CLS]を全結合層（Dense Layer）に経由し、各カテゴリーに属する確率を出力します。それは、[CLS]トークンは入力テキスト全体の文脈情報を集約すると考えるためです。\n\nNote\n\nテキスト分類タスクにおいて、[CLS]トークンと全結合層（Dense Layer）を使用するのが一般的ですが、他の追加層の設計を用いることももちろん可能です。\n\n","type":"content","url":"/bert#id-fine-tuning","position":17},{"hierarchy":{"lvl1":"BERT","lvl3":"Huggingface transformerを使う","lvl2":"ファインチューング(Fine-Tuning)"},"type":"lvl3","url":"/bert#huggingface-transformer","position":18},{"hierarchy":{"lvl1":"BERT","lvl3":"Huggingface transformerを使う","lvl2":"ファインチューング(Fine-Tuning)"},"content":"転移学習は事前学習済みモデルを新しいタスクに再利用するといった強みがあります。そのため、事前学習済みのモデルを素早く共有、ロードすることは重要です。\n\nHugging Face Hubは、モデル、データセットとデモを備えたプラットフォームです。\n\nHuggingface transformerは、自然言語処理を中心に最先端のTransformerベースのモデルを効率に利用するためのオープンソースライブラリです。\n\n多数の事前学習済みモデル: ライブラリは、BERT、GPT-2、RoBERTa、T5、DistilBERTなど、さまざまな有名なNLPモデルの事前学習済みバージョンを提供しています。\n\nモデルの利用の簡易化: 事前学習済みのモデルを簡単にダウンロードし、特定のタスクにファインチューニングするための高レベルのAPIを提供しています。\n\nTokenizers: ほとんどのモデルには、テキストデータをモデルが扱える形式に変換するためのトークナイザが付属しています。これはテキストの前処理を簡単に行うためのツールです。\n\nModel Hubの統合: Hugging FaceのModel Hubと直接統合されており、コミュニティによって共有されている数千もの事前学習済みモデルに簡単にアクセスできます。","type":"content","url":"/bert#huggingface-transformer","position":19},{"hierarchy":{"lvl1":"BERT","lvl4":"pipline","lvl3":"Huggingface transformerを使う","lvl2":"ファインチューング(Fine-Tuning)"},"type":"lvl4","url":"/bert#pipline","position":20},{"hierarchy":{"lvl1":"BERT","lvl4":"pipline","lvl3":"Huggingface transformerを使う","lvl2":"ファインチューング(Fine-Tuning)"},"content":"pipelineというクラスで、特定のタスクを実行するために事前学習されたモデルとトークンナイザーを統合し、簡単に使用することができます。\n\n#!pip install fugashi\n#!pip install unidic-lite\n\n\n\nimport pandas as pd\nfrom transformers import pipeline\n\nfill_mask = pipeline(\n    \"fill-mask\",\n    model=\"cl-tohoku/bert-base-japanese-v3\"\n)\n\n\n\n\n\nmasked_text = \"東北大学は[MASK]市に位置しています。\"\noutput = fill_mask(masked_text)\n\n\n\ndisplay(pd.DataFrame(output))\n\n\n\nsummarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n\n\n\nARTICLE = \"\"\" New York (CNN)When Liana Barrientos was 23 years old, she got married in Westchester County, New York.\nA year later, she got married again in Westchester County, but to a different man and without divorcing her first husband.\nOnly 18 days after that marriage, she got hitched yet again. Then, Barrientos declared \"I do\" five more times, sometimes only within two weeks of each other.\nIn 2010, she married once more, this time in the Bronx. In an application for a marriage license, she stated it was her \"first and only\" marriage.\nBarrientos, now 39, is facing two criminal counts of \"offering a false instrument for filing in the first degree,\" referring to her false statements on the\n2010 marriage license application, according to court documents.\nProsecutors said the marriages were part of an immigration scam.\nOn Friday, she pleaded not guilty at State Supreme Court in the Bronx, according to her attorney, Christopher Wright, who declined to comment further.\nAfter leaving court, Barrientos was arrested and charged with theft of service and criminal trespass for allegedly sneaking into the New York subway through an emergency exit, said Detective\nAnnette Markowski, a police spokeswoman. In total, Barrientos has been married 10 times, with nine of her marriages occurring between 1999 and 2002.\nAll occurred either in Westchester County, Long Island, New Jersey or the Bronx. She is believed to still be married to four men, and at one time, she was married to eight men at once, prosecutors say.\nProsecutors said the immigration scam involved some of her husbands, who filed for permanent residence status shortly after the marriages.\nAny divorces happened only after such filings were approved. It was unclear whether any of the men will be prosecuted.\nThe case was referred to the Bronx District Attorney\\'s Office by Immigration and Customs Enforcement and the Department of Homeland Security\\'s\nInvestigation Division. Seven of the men are from so-called \"red-flagged\" countries, including Egypt, Turkey, Georgia, Pakistan and Mali.\nHer eighth husband, Rashid Rajput, was deported in 2006 to his native Pakistan after an investigation by the Joint Terrorism Task Force.\nIf convicted, Barrientos faces up to four years in prison.  Her next court appearance is scheduled for May 18.\n\"\"\"\nprint(summarizer(ARTICLE, max_length=130, min_length=30, do_sample=False))\n\n\n\nHuggingface Hubで日本語のセンチメント分類ためのモデルを探し、piplineでセンチメント分類器を実装しなさい。\n以下のテキストに対する分類結果を確認しよう。\n\nこの製品は全く役に立ちませんでした\n\n今日はいい天気ですね\n\n世界経済も、米国が12月に続き３月にも追加利上げを実施するなど、先進国を中心に回復の動きとなりました\n\n","type":"content","url":"/bert#pipline","position":21},{"hierarchy":{"lvl1":"BERT","lvl2":"まとめ : word2vecからBERTまで"},"type":"lvl2","url":"/bert#id-word2vec-bert","position":22},{"hierarchy":{"lvl1":"BERT","lvl2":"まとめ : word2vecからBERTまで"},"content":"人間が使う自然言語をコンピュータに処理させるため、言語を数値形式で表現するモデリングのプロセスが必要とされ、どのようなモデルを採用するかによって分析の方向性は異なっています。\n\n最も基本的なモデリングアプローチとして、文書を単語の集合とそれぞれの単語の頻度情報に変換するバグオブワーズ(bag of words)があげられます。この手法は、文書の基本的な内容を捉えるのに有効であるが、単語の順番や意味のニュアンスなどの情報はすべて捨象されています。\n\nより複雑な言語の特性を捉えるために、大量のコーパスを使った学習により、言語の文法や意味構造など多くの情報を埋め込んだ高度なモデルが期待されています。word2vecをはじめとする単語分散表現モデルは、単語を「意味」情報を表現したベクトルにマッピングすることができます。\n\nword2vecに単語分散表現の学習では、「単語の意味は、その単語の周囲の単語（文脈）によって決まる」という分布仮説に基づく手法が用いられます。この仮説にしたがうモデルでは、ある単語がどのような文脈で生じやすいかということをある程度考慮し、単語間の関係をベクトルで表現することができます。\n\nただ、word2vecではいくつの欠点があります。特に、word2vecではあくまで1単語1ベクトルでしたが、実際のケースでは、文脈によって単語の意味が変わることがありますので、文脈に依存する分散表現が望ましいです。\n\nテキストの「文脈」を表現するための言語モデルが開発されました。\n\nRNNとLSTMではテキストデータの時系列的な性質を捉え、文中の単語の順序や時間的な関連性をモデルが学習できるようになります。ただ、\n\n長距離の依存関係を効果的に対処できでいない\n\n計算コストが高い\n\nSelf-Attenttionでは、すべての単語間の関係を並列に計算することで、長距離の依存関係を効果的に捉えます。\n\nTransformerをベースにしたモデルでは、単語が出現する具体的な文脈に基づいてその単語の埋め込みを生成することができます。","type":"content","url":"/bert#id-word2vec-bert","position":23},{"hierarchy":{"lvl1":"GPT"},"type":"lvl1","url":"/gpt","position":0},{"hierarchy":{"lvl1":"GPT"},"content":"GPT(Generative Pretrained Transformer)はTransformerベースの言語モデルです。ChatGPT などの生成系 AI アプリケーションの基礎となっている人工知能 (AI) の重要な新技術です。GPT モデルにより、アプリケーションは人間のようにテキストやコンテンツ (画像、音楽など) を作成したり、会話形式で質問に答えたりすることができます。さまざまな業界の組織が、Q&A ボット、テキスト要約、コンテンツ生成、検索に GPT モデルと生成系 AI を使用しています。\n\nGPTはOpenAIによって定期的に新しいバージョンが公開されていますが、ここではGPT-2について解説します。\n\n","type":"content","url":"/gpt","position":1},{"hierarchy":{"lvl1":"GPT","lvl2":"入力表現"},"type":"lvl2","url":"/gpt#id","position":2},{"hierarchy":{"lvl1":"GPT","lvl2":"入力表現"},"content":"\n\nGPTの入力は、入力トークン列に対応するトークン埋め込みe_{w_i}と位置埋め込むp_iを加算した埋め込み列です。x_i=e_{w_i}+p_i\n\n","type":"content","url":"/gpt#id","position":3},{"hierarchy":{"lvl1":"GPT","lvl2":"事前学習"},"type":"lvl2","url":"/gpt#id-1","position":4},{"hierarchy":{"lvl1":"GPT","lvl2":"事前学習"},"content":"\n\nGPTの事前学習タスクは、入力されたトークン列の次のトークンを予測することです。ここで、GPTはデコーダ構成のTransformerを用います。\n\nGPTはオリジナルのTransformerにいくつかの改装を行いました。\n\n学習に用いるトークン列w_1,w_2,...,w_Nにおけるのトークンw_iを予測することを考えます。GPTでは、予測確率を使った負の対数尤度を損失関数として事前学習を行います。\\zeta(\\theta)=- \\sum_i log P(w_i|w_{i-K},....w_{i-1},\\theta)\n\nここで、\\thetaはモデルに含まれるすべてのパラメータを表します。\n\n学習時にはMasked Self-Attention機構が導入され、入力トークン列の各位置において次のトークンを予測して学習が行われます。\n\n","type":"content","url":"/gpt#id-1","position":5},{"hierarchy":{"lvl1":"GPT","lvl2":"ファインチューング"},"type":"lvl2","url":"/gpt#id-2","position":6},{"hierarchy":{"lvl1":"GPT","lvl2":"ファインチューング"},"content":"GPTの事前学習済みモデルに、下流タスクに合わせて変換するためのヘッドを追加し、下流タスクのデータセットを用いてモデル全体を調整します。\n\nGPTは下流タスクを解く際、特殊トークンを用いて入力テキストを拡張します。\n\nテキスト分類のタスクにおいては、文書の最初に<s>、最後に<e>が追加されます。\n\n自然言語推論のタスクにおいては、テキストの境界に$が挿入されます。\n\n","type":"content","url":"/gpt#id-2","position":7},{"hierarchy":{"lvl1":"GPT","lvl3":"Huggingface transformerを使う","lvl2":"ファインチューング"},"type":"lvl3","url":"/gpt#huggingface-transformer","position":8},{"hierarchy":{"lvl1":"GPT","lvl3":"Huggingface transformerを使う","lvl2":"ファインチューング"},"content":"\n\nfrom transformers import pipeline\n#!pip install sentencepiece\n#!pip install protobuf\ngenerator = pipeline(\"text-generation\", model=\"abeja/gpt2-large-japanese\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ngenerated = generator(\n    \"東北大学は\",\n    max_length=100,\n    do_sample=True,\n    num_return_sequences=3,\n    top_p=0.95,\n    top_k=50,\n    pad_token_id=3\n)\nprint(*generated, sep=\"\\n\")\n\n","type":"content","url":"/gpt#huggingface-transformer","position":9},{"hierarchy":{"lvl1":"ニューラルネットワーク"},"type":"lvl1","url":"/nn2","position":0},{"hierarchy":{"lvl1":"ニューラルネットワーク"},"content":"\n\nニューラルネットワークは、人間の脳に似た層状構造で相互接続されたノードやニューロンを使用するの計算モデルです。\n\nニューラルネットワークは、画像認識、自然言語処理、音声認識など、さまざまな領域で広く利用されています。特に、大量のデータと計算能力が利用可能になった近年、ディープニューラルネットワーク(DNN)の研究や応用が急速に進展しています。\n\n","type":"content","url":"/nn2","position":1},{"hierarchy":{"lvl1":"ニューラルネットワーク","lvl2":"ニューラルネットワークの構造"},"type":"lvl2","url":"/nn2#id","position":2},{"hierarchy":{"lvl1":"ニューラルネットワーク","lvl2":"ニューラルネットワークの構造"},"content":"","type":"content","url":"/nn2#id","position":3},{"hierarchy":{"lvl1":"ニューラルネットワーク","lvl3":"パーセプトロン","lvl2":"ニューラルネットワークの構造"},"type":"lvl3","url":"/nn2#id-1","position":4},{"hierarchy":{"lvl1":"ニューラルネットワーク","lvl3":"パーセプトロン","lvl2":"ニューラルネットワークの構造"},"content":"パーセプトロンとは、複数の入力を受け取り、重み付けして、1つの信号を出力するアルゴリズムです。\n\n例えば,x_1とx_2の2つの入力を受け取り、yを出力するパーセプトロンを考えます。\n\nw_1やw_2は各入力の「重み」を表すパラメータで、各入力の重要性をコントロールします。\n\nbはバイアス\n\n\n\nパーセプトロンの「○」で表されている部分は、ニューロンやノードと呼びます。\n\n","type":"content","url":"/nn2#id-1","position":5},{"hierarchy":{"lvl1":"ニューラルネットワーク","lvl3":"活性化関数","lvl2":"ニューラルネットワークの構造"},"type":"lvl3","url":"/nn2#id-2","position":6},{"hierarchy":{"lvl1":"ニューラルネットワーク","lvl3":"活性化関数","lvl2":"ニューラルネットワークの構造"},"content":"活性化関数はニューラルネットワークの各層において、入力データに対して非線形性を導入するために使用される関数です。\n\n例えば、関数の入力(パーセプトロンだと重み付き和)が0以下のとき0を、0より大きいとき1を出力することが考えます。y   = \\begin{cases}\n          0 \\quad (w_1 x_1 + w_2 x_2 + b \\leq 0) \\\\\n          1 \\quad (w_1 x_1 + w_2 x_2 + b > 0)\n      \\end{cases}\n\n出力に関する計算数式を分解すると、y   = h(a)h(a)\n    = \\begin{cases}\n          0 \\quad (a \\leq 0) \\\\\n          1 \\quad (a > 0)\n      \\end{cases}\n\nで書けます。つまり、入力の重み付き和の結果がaというノードになり、そして活性化関数h()によってyという出力が計算されます。\n\n\n\n活性化関数があるパーセプトロン\n\n活性化関数の主な役割は、入力の加重和に非線形な変換を加えることで、ニューラルネットワークが複雑なパターンを学習できるようにすることです。\n\n例えば、線形変換のみで下図右の丸で表される観測データからxとyの関係を近似した場合、点線のような直線が得られたとします。これでは、一部のデータについてはあまりよく当てはまっていないのが分かります。\n\nしかし、もし図右の実線のような曲線を表現することができれば、両者の関係をより適切に表現することができます。\n\n活性化関数にはいくつか種類があり、異なる特性や用途を持っています。\n\nシグモイド関数\n\n任意の値を0から1に変換します\n\nReLU\n\n負の入力は0として、0もしくは正の入力はそのまま出力\n\n\n\n活性化関数の種類\n\n","type":"content","url":"/nn2#id-2","position":7},{"hierarchy":{"lvl1":"ニューラルネットワーク","lvl3":"ニューラルネットワークの仕組み","lvl2":"ニューラルネットワークの構造"},"type":"lvl3","url":"/nn2#id-3","position":8},{"hierarchy":{"lvl1":"ニューラルネットワーク","lvl3":"ニューラルネットワークの仕組み","lvl2":"ニューラルネットワークの構造"},"content":"ニューラルネットワークの仕組みは下の図で表さます。左側から、最初の層を入力層 (input layer)、最後の層を出力層 (output layer)といいます。\n\nその間にある層は中間層 （intermediate layer) もしくは隠れ層 (hidden layer) といいます。中間層において、層の数を増やすことによって、ディープニューラルネットワークを実現することができます。\n\nニューラルネットワークは、層から層へ、値を変換していきます。 そのため、ニューラルネットワークとはこの変換がいくつも連なってできる一つの大きな関数だと考えることができます。 従って、基本的には、入力を受け取って、何か出力を返すものです。 そして、どのようなデータを入力し、どのような出力を作りたいかによって、入力層と出力層のノード数が決定されます。\n\n\n\n","type":"content","url":"/nn2#id-3","position":9},{"hierarchy":{"lvl1":"ニューラルネットワーク","lvl2":"ニューラルネットワークの計算"},"type":"lvl2","url":"/nn2#id-4","position":10},{"hierarchy":{"lvl1":"ニューラルネットワーク","lvl2":"ニューラルネットワークの計算"},"content":"\n\nそれでは、下図に示す3層ニューラルネットワークを例として、入力から出力への計算のについて解説を行います。\n\n","type":"content","url":"/nn2#id-4","position":11},{"hierarchy":{"lvl1":"ニューラルネットワーク","lvl3":"記号の説明","lvl2":"ニューラルネットワークの計算"},"type":"lvl3","url":"/nn2#id-5","position":12},{"hierarchy":{"lvl1":"ニューラルネットワーク","lvl3":"記号の説明","lvl2":"ニューラルネットワークの計算"},"content":"ニューラルネットワークの計算を説明するにあたって、導入される記号の定義から始めます。\n\n入力層のx_1とx_2ニューロンから、次層のニューロンa_1^{(1)}への信号伝達を見ていきます。\n\nw_{12}^{(1)} は前層の2番目のニューロン(x_2)から次層の1番目のニューロン(a_1^{(1)})への重みであることを意味します。\n\n右上(1)は第1層の重みということ意味します\n\n右下12ような数字の並びは、次層のニューロン(1)と前層のニューロンのインデックス番号(2)から構成されます\n\na_1^{(1)}は第1層1番目のニューロンであることを意味します。\n\n右上(1)は第1層のニューロンということ意味します\n\n右下1は1番目のニューロンということ意味します\n\n\n","type":"content","url":"/nn2#id-5","position":13},{"hierarchy":{"lvl1":"ニューラルネットワーク","lvl3":"各層における信号伝達","lvl2":"ニューラルネットワークの計算"},"type":"lvl3","url":"/nn2#id-6","position":14},{"hierarchy":{"lvl1":"ニューラルネットワーク","lvl3":"各層における信号伝達","lvl2":"ニューラルネットワークの計算"},"content":"\n\nまず、入力層から「第1層の1番目のニューロン」への信号伝達を見ていきます。ここでは。バイアス項も追加し、a_1^{(1)}を以下の数式で計算します。a_1^{(1)}= w_{11}^{(1)}x_{1} + w_{12}^{(1)}x_{2} + b_1^{(1)}\n\n同じ形で、第1層におけるすべでのニューロンの計算式を書けます。\\begin{split}\\begin{cases}\n    a_1^{(1)} = w_{11}^{(1)}x_{1} + w_{12}^{(1)}x_{2} + b_1^{(1)} \\\\\n    a_2^{(1)} = w_{21}^{(1)}x_{1} + w_{22}^{(1)}x_{2} + b_2^{(1)} \\\\\n    a_3^{(1)} = w_{31}^{(1)}x_{1} + w_{32}^{(1)}x_{2} + b_3^{(1)}\n\\end{cases}\\end{split}\n\n行列で第1層におけるニューロンの計算式をまとめて表すことができます。\n\n入力 \\mathbf{X}=\\begin{pmatrix} x_1 & x_2 \\end{pmatrix}\n\nバイアス \\mathbf{B} = \\begin{pmatrix} b_{1}^{(1)} & b_{2}^{(1)} & b_{3}^{(1)} \\end{pmatrix}\n\n重み\\begin{split} \\mathbf{W} = \\begin{pmatrix}\n    w_{11}^{(1)} & w_{21}^{(1)} & w_{31}^{(1)} \\\\\n    w_{12}^{(1)} & w_{22}^{(1)} & w_{32}^{(1)}\n\\end{pmatrix}\\end{split}\n\n入力・バイアスと重みの総和: \\mathbf{A} = \\begin{pmatrix}\n  a_1^{(1)} & a_2^{(1)} & a_3^{(1)}\n\\end{pmatrix}\\mathbf{A}^{(1)}\n     = \\mathbf{X} \\mathbf{W}^{(1)} + \\mathbf{B}^{(1)}\n\nさらに、活性化関数を導入します。入力・バイアスと重みの総和をaで表し、活性化関数h()による変換された結果をzで表すことにします。\n\n\n","type":"content","url":"/nn2#id-6","position":15},{"hierarchy":{"lvl1":"ニューラルネットワーク","lvl3":"数値を見ながら計算の流れを確認","lvl2":"ニューラルネットワークの計算"},"type":"lvl3","url":"/nn2#id-7","position":16},{"hierarchy":{"lvl1":"ニューラルネットワーク","lvl3":"数値を見ながら計算の流れを確認","lvl2":"ニューラルネットワークの計算"},"content":"それでは、NumPyの多次元配列を使って、入力 x_1,x_2,x_3から出力が計算される過程を確認してみましょう。入力、重み、バイアスは適当な値を設定します。\n\nimport numpy as np\nX = np.array([1.0, 0.5])\nW1 = np.array([[0.1, 0.3, 0.5],[0.2, 0.4, 0.6]])\nB1 = np.array([0.1, 0.2, 0.3])\nprint(r\"入力の形状: {}\".format(X.shape))\nprint(r\"重みの形状: {}\".format(W1.shape))\nprint(r\"バイアスの形状: {}\".format(B1.shape))\n\n\n\n第一層隠れ層で重み付きとバイアスの総和を計算し、活性化関数で変換された結果を返します。\n\nA1 = np.dot(X, W1) + B1\n\n\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\n\n\nZ1 = sigmoid(A1)\n\n\n\n続いて、同じ形で第1層から第2層目への信号伝達を行います。\n\nW2 = np.array([[0.1, 0.4],[0.2, 0.5],[0.3, 0.6]])\nB2 = np.array([0.1, 0.2])\n\n\n\nA2 = np.dot(Z1, W2) + B2\nZ2 = sigmoid(A2)\n\n\n\n最後に、第2層から出力層への信号を行います。出力層の活性化関数は、恒等関数を用います。\n\nW3 = np.array([[0.1, 0.3],[0.2, 0.4]])\nB3 = np.array([0.1, 0.2])\n\n\n\nA3 = np.dot(Z2, W3) + B3 # Y = A3\n\n\n\n","type":"content","url":"/nn2#id-7","position":17},{"hierarchy":{"lvl1":"ニューラルネットワーク","lvl3":"出力層の設計","lvl2":"ニューラルネットワークの計算"},"type":"lvl3","url":"/nn2#id-8","position":18},{"hierarchy":{"lvl1":"ニューラルネットワーク","lvl3":"出力層の設計","lvl2":"ニューラルネットワークの計算"},"content":"ニューラルネットワークは、分類問題と回帰問題の両方に用いることができます。ただし、分類問題と回帰問題のどちらに用いるかで、出力層の活性化関数を変更する必要があります。\n\n回帰問題\n\nモデルの出力は連続的な実数値でなければなりません。そのため、出力層に活性化関数を使用しないか、出力の範囲を制限しない活性化関数を使用します。\n\n分類問題\n\n分類問題では、モデルの出力を特定のクラスに分類するため、出力は確率として解釈できる必要があります。一般的には クラス数と同じだけのノードを出力層に用意しておき、各ノードがあるクラスに入力が属する確率を表すようにします。 このため、全出力ノードの値の合計が1になるよう正規化します。 これには、要素ごとに適用される活性化関数ではなく、層ごとに活性値を計算する別の関数を用いる必要があります。 そのような目的に使用される代表的な関数には、ソフトマックス関数があります。","type":"content","url":"/nn2#id-8","position":19},{"hierarchy":{"lvl1":"ニューラルネットワーク","lvl4":"ソフトマックス関数","lvl3":"出力層の設計","lvl2":"ニューラルネットワークの計算"},"type":"lvl4","url":"/nn2#id-9","position":20},{"hierarchy":{"lvl1":"ニューラルネットワーク","lvl4":"ソフトマックス関数","lvl3":"出力層の設計","lvl2":"ニューラルネットワークの計算"},"content":"ソフトマックス関数は複数値からなるベクトルを入力し、それを正規化したベクトルを出力します。ソフトマックス関数は、次の式で定義されます。y_k = \\frac{\\exp(a_k)}{\\sum_{k'=0}^{K-1} \\exp(a_{k'})}\n\nK個の要素\\mathbf{a} = (a_0, a_1, \\cdots, a_{K-1})を入力して、0 \\leq y_k \\leq 1、\\sum_{k=0}^{K-1} y_k = 1となる\\mathbf{y} = (y_0, y_1, \\cdots, y_{K-1})を出力します。つまり、ソフトマックス関数を適用することで、各成分は区間 (0, 1) に収まり、全ての成分の和が 1 になるため、「確率」として解釈できるようになります。\n\n実装の際、指数関数の計算のため容易に大きな値になり、計算結果はinfが返ってきますので、数値が不安定になってしまう「オーバーフロー」問題を対応するため。入力の最大値を引くことで、正しく計算するようにする方法が採用されています。\n\nimport numpy as np\ndef softmax(x):\n    c = np.max(x)\n    exp_x = np.exp(x - c)\n    sum_exp_x = np.sum(exp_x)\n    return exp_x / sum_exp_x\n\n\n\nprint(\"活性化関数に適用する前に: {}\".format(A3))\nprint(\"最終出力: {}\".format(softmax(A3)))\n\n\n\n","type":"content","url":"/nn2#id-9","position":21},{"hierarchy":{"lvl1":"ニューラルネットワーク","lvl2":"ニューラルネットワークの学習"},"type":"lvl2","url":"/nn2#id-10","position":22},{"hierarchy":{"lvl1":"ニューラルネットワーク","lvl2":"ニューラルネットワークの学習"},"content":"重回帰分析では、最小二乗法などの推定方法で行列計算や微分方程式を用いて解を導出することができます。つまり、実際の数値を使うことなく変数のまま、解（最適なパラメータ）を求めることができました。このように、変数のままで解を求めることを解析的に解くと言い、その答えのことを解析解 (analytical solution) と呼びます。\n\nしかし、ニューラルネットワークで表現されるような複雑な関数の場合、パラメータの数は数億に及ぶこともありますので、最適解を解析的に解くことはほとんどの場合困難です。そのため、別の方法を考える必要があります。具体的には、解析的に解く方法に対し、計算機を使って繰り返し数値計算を行って解を求めることを数値的に解くといい、求まった解は数値解 (numerical solution) と呼ばれます。\n\nニューラルネットワークでは、基本的に数値的な手法によって最適なパラメータを求めます。","type":"content","url":"/nn2#id-10","position":23},{"hierarchy":{"lvl1":"ニューラルネットワーク","lvl3":"損失関数","lvl2":"ニューラルネットワークの学習"},"type":"lvl3","url":"/nn2#id-11","position":24},{"hierarchy":{"lvl1":"ニューラルネットワーク","lvl3":"損失関数","lvl2":"ニューラルネットワークの学習"},"content":"損失関数（Loss function）とは、「正解値」と、モデルによる出力された「予測値」とのズレの大きさ（これを「Loss：損失」と呼ぶ）を計算するための関数です。損失関数の値は、学習アルゴリズムがモデルのパラメータを調整する際の指標となります。","type":"content","url":"/nn2#id-11","position":25},{"hierarchy":{"lvl1":"ニューラルネットワーク","lvl4":"平均二乗誤差","lvl3":"損失関数","lvl2":"ニューラルネットワークの学習"},"type":"lvl4","url":"/nn2#id-12","position":26},{"hierarchy":{"lvl1":"ニューラルネットワーク","lvl4":"平均二乗誤差","lvl3":"損失関数","lvl2":"ニューラルネットワークの学習"},"content":"平均二乗誤差 (mean squared error) は、回帰問題を解きたい場合によく用いられる目的関数です。 重回帰分析の解説中に紹介した二乗和誤差と似ていますが、各データ点における誤差の総和をとるだけでなく、それをデータ数で割って、誤差の平均値を計算している点が異なります。L = \\frac{1}{N} \\sum_{n=1}^N (t_n - y_n)^2\n\nここで、Nはサンプルサイズ、y_nはn個目のデータに対するニューラルネットワークの出力値、t_nはn個目のデータに対する望ましい正解の値です。","type":"content","url":"/nn2#id-12","position":27},{"hierarchy":{"lvl1":"ニューラルネットワーク","lvl4":"交差エントロピー","lvl3":"損失関数","lvl2":"ニューラルネットワークの学習"},"type":"lvl4","url":"/nn2#id-13","position":28},{"hierarchy":{"lvl1":"ニューラルネットワーク","lvl4":"交差エントロピー","lvl3":"損失関数","lvl2":"ニューラルネットワークの学習"},"content":"交差エントロピー (cross entropy) は、分類問題を解きたい際によく用いられる目的関数です。\n\n例として、Kクラスの分類問題を考えてみましょう。 ある入力xが与えられたとき、ニューラルネットワークの出力層にK個のノードがあり、それぞれがこの入力がk番目のクラスに属する確率y_k = p(y=k|x)\n\nを表しているとします。 これは、入力xが与えられたという条件のもとで、予測クラスを意味するyがkであるような確率、を表す条件付き確率です。\n\nここで、xが所属するクラスの正解が、{\\bf t} = \\begin{bmatrix} t_1 & t_2 & \\dots & t_K \\end{bmatrix}^{\\rm T}\n\nというベクトルで与えられているとします。 ただし、このベクトルはt_k (k=1,2,...,K) のいずれか一つだけが1であり、それ以外は0であるようなベクトルであるとします。\n\nそして、この一つだけ値が1となっている要素は、その要素のインデックスに対応したクラスが正解であることを意味します。\n\n以上を用いて、交差エントロピーは以下のように定義されます。- \\sum_{k=1}^{K}t_{k}\\log y_{k}\n\nこれは、t_kが k=1,2,...,K のうち正解クラスである一つのkの値でだけ1となるので、正解クラスであるようなkでの\\log y_{k}を取り出して−1を掛けているのと同じです。 また、N個すべてのサンプルを考慮すると、交差エントロピーは以下になりますL = - \\sum_{n=1}^{N} \\sum_{k=1}^{K}t_{n, k}\\log y_{n, k}\n\n\n\nAverage loss\n\n3クラス分類問題を考えます。\n\n予測はy=(0.1,0.2,0.3)、真のラベルはt=(0,0,1)の場合、交差エントロピーの計算式を書いてください。\n\nt = np.array([0, 0, 1])\ny = np.array([0.1, 0.2, 0.5])\n\n\n\nNote\n\n損失関数は、すべての訓練データを対象として求める必要がありますが、場合によるすべてのデータを一気に計算するのは現実ではありません。そこで、データの中から一部を選びだし、つまりミニバッチごとに学習を行います。このような手法をミニバッチ学習と言います。\n\n","type":"content","url":"/nn2#id-13","position":29},{"hierarchy":{"lvl1":"ニューラルネットワーク","lvl3":"損失関数の最適化","lvl2":"ニューラルネットワークの学習"},"type":"lvl3","url":"/nn2#id-14","position":30},{"hierarchy":{"lvl1":"ニューラルネットワーク","lvl3":"損失関数の最適化","lvl2":"ニューラルネットワークの学習"},"content":"","type":"content","url":"/nn2#id-14","position":31},{"hierarchy":{"lvl1":"ニューラルネットワーク","lvl4":"勾配法","lvl3":"損失関数の最適化","lvl2":"ニューラルネットワークの学習"},"type":"lvl4","url":"/nn2#id-15","position":32},{"hierarchy":{"lvl1":"ニューラルネットワーク","lvl4":"勾配法","lvl3":"損失関数の最適化","lvl2":"ニューラルネットワークの学習"},"content":"下の図は，パラメータwを変化させた際の損失関数Lの値を表しています。損失関数の値を最小にするようなパラメータの値を求めることで、ニューラルネットワークを訓練します。ただ、実際のニューラルネットワークの目的関数は、多次元で、かつもっと複雑な形をしていることがほとんどです。 そこで、勾配を利用して関数の最小値を探す勾配法がよく用いられます。\n\n勾配は、各地点における関数の傾きであり、関数の値が最も急速に変化する方向と大きさを示します。\n\n今はLの値を小さくしたいわけです。勾配の反対方向に進むことで関数の値を最も減らせることができますので、勾配の情報を手がかりに、できるだけ小さな値となる関数の場所を探します。\n\n損失を求めるまでの計算を1つの関数とみなして、重みの勾配\\frac{\\partial L}{\\partial \\mathbf{W}}ととバイアスの勾配\\frac{\\partial L}{\\partial \\mathbf{b}}を求めます。各要素は、それぞれパラメータの対応する要素の偏微分です。各パラメータの勾配\\frac{\\partial L}{\\partial \\mathbf{W}}、\\frac{\\partial L}{\\partial \\mathbf{b}}を用いて、勾配降下法によりパラメータ\\mathbf{W},\\ \\mathbf{b}を更新します。\\begin{aligned}\n\\mathbf{W}^{(\\mathrm{new})}\n   &= \\mathbf{W}\n      - \\eta \\frac{\\partial L}{\\partial \\mathbf{W}}\n\\\\\n\\mathbf{b}^{(\\mathrm{new})}\n   &= \\mathbf{b}\n      - \\eta \\frac{\\partial L}{\\partial \\mathbf{b}}\n\\end{aligned}\n\n\\etaは学習率と言います。1回の学習で、どれだけパラメータを更新するか、ということを決めます。\n\n","type":"content","url":"/nn2#id-15","position":33},{"hierarchy":{"lvl1":"ニューラルネットワーク","lvl4":"勾配下降法の実装","lvl3":"損失関数の最適化","lvl2":"ニューラルネットワークの学習"},"type":"lvl4","url":"/nn2#id-16","position":34},{"hierarchy":{"lvl1":"ニューラルネットワーク","lvl4":"勾配下降法の実装","lvl3":"損失関数の最適化","lvl2":"ニューラルネットワークの学習"},"content":"","type":"content","url":"/nn2#id-16","position":35},{"hierarchy":{"lvl1":"ニューラルネットワーク","lvl5":"f(x)=x^2に対する最適化","lvl4":"勾配下降法の実装","lvl3":"損失関数の最適化","lvl2":"ニューラルネットワークの学習"},"type":"lvl5","url":"/nn2#f-x-x-2","position":36},{"hierarchy":{"lvl1":"ニューラルネットワーク","lvl5":"f(x)=x^2に対する最適化","lvl4":"勾配下降法の実装","lvl3":"損失関数の最適化","lvl2":"ニューラルネットワークの学習"},"content":"\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# 関数とその勾配\ndef function_f(x):\n    return x ** 2\n\ndef numerical_gradient(f, x, h=1e-5):\n    return (f(x + h) - f(x - h)) / (2 * h)\n\n\n\ndef gradient_descent(initial_x, learning_rate, num_iterations):\n    x = initial_x\n    x_history = [x]\n    \n    for i in range(num_iterations):\n        grad = numerical_gradient(function_f, x)\n        x = x - learning_rate * grad\n        x_history.append(x)\n        if i % 10 == 0:\n            print(\"Iteration {}: x = {}, f(x) = {}\".format(i, x, function_f(x)))\n    \n    return x_history\n\n\n\n# パラメータ設定\ninitial_x = 5.0\nlearning_rate = 0.1\nnum_iterations = 100\nx_history = gradient_descent(initial_x, learning_rate, num_iterations)\n\n\n\n# プロット\nx = np.linspace(-initial_x-1, initial_x+1, 400)\ny = function_f(x)\n\nplt.figure(figsize=(8,5))\nplt.plot(x, y, '-b', label='$f(x) = x^2$')\nplt.scatter(x_history, [function_f(i) for i in x_history], c='red', label='Gradient Descent Steps')\nplt.title('Gradient Descent Visualization')\nplt.xlabel('$x$')\nplt.ylabel('$f(x)$')\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n勾配法でf(x)=2x^2-10x-80の最小値を求めます。\n\n勾配降下法のアルゴリズムを実装する。\n\nアルゴリズムを使って関数の最小値を求める。\n\n","type":"content","url":"/nn2#f-x-x-2","position":37},{"hierarchy":{"lvl1":"ニューラルネットワーク","lvl5":"Example: f(x_0,x_1)=x_0^2+x_1^2に対する最適化","lvl4":"勾配下降法の実装","lvl3":"損失関数の最適化","lvl2":"ニューラルネットワークの学習"},"type":"lvl5","url":"/nn2#example-f-x-0-x-1-x-0-2-x-1-2","position":38},{"hierarchy":{"lvl1":"ニューラルネットワーク","lvl5":"Example: f(x_0,x_1)=x_0^2+x_1^2に対する最適化","lvl4":"勾配下降法の実装","lvl3":"損失関数の最適化","lvl2":"ニューラルネットワークの学習"},"content":"今度は、変数が複数(2つ)あるの関数に対する最適化を実装してみます。複数の変数からなる関数の微分は偏微分といいます。\n\n偏微分の場合、複数ある変数の中でターゲットとする変数を一つに絞り、他の変数はある値に固定します。\n\n# 関数定義\ndef function_f(x):\n    return x[0]**2 + x[1]**2\n# 勾配\ndef numerical_gradient(f, x, h = 1e-4):\n    grad = np.zeros_like(x)\n    # x の各成分について順番にループを回し、数値的に勾配を計算します。\n    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n    while not it.finished:\n        idx = it.multi_index\n        tmp_val = x[idx]\n        x[idx] = tmp_val + h\n        fxh1 = f(x) # f(x+h)\n\n        x[idx] = tmp_val - h \n        fxh2 = f(x) # f(x-h)\n\n        grad[idx] = (fxh1 - fxh2) / (2*h)\n\n        x[idx] = tmp_val \n        it.iternext()   \n\n    return grad\n\ndef gradient_descent(f, init_x, lr=0.01, num_iterations=100):\n    x = init_x\n    x_history = [x.copy()]\n\n    for i in range(num_iterations):\n        grad = numerical_gradient(f, x)\n        x -= lr * grad\n        x_history.append(x.copy())\n\n    return np.array(x_history)\n\n\n\nnumerical_gradientちう関数で、配列の各要素に対して数値微分を求めます。例えば、点(-3,4)での勾配を求めてみます。\n\n勾配が示す方向は、各場所において関数の値を最も減らす方向であり、その方向に晋ことで関数の値を最も減らせることができます。つまり、勾配の情報を手がかりに、進む方向を決めるべきでしょう。\n\n# 勾配の計算\nnumerical_gradient(function_f, np.array([-3.0, 4.0]))\n\n\n\n# Parameters\ninit_x = np.array([-3.0, 4.0])\nlr = 0.1\nnum_iterations = 20\n\n# Run gradient descent\nx_history = gradient_descent(function_f, init_x, lr, num_iterations)\n\n# Generate mesh data for visualization\nx = np.linspace(-4.5, 4.5, 200)\ny = np.linspace(-4.5, 4.5, 200)\nX, Y = np.meshgrid(x, y)\nZ = X**2 + Y**2\n\nfig = plt.figure(figsize=(15, 8))\n# 1st subplot: 3D plot\nax1 = fig.add_subplot(121, projection='3d')\nax1.plot_surface(X, Y, Z, cmap='viridis', alpha=0.6)\nax1.contour(X, Y, Z, zdir='z', offset=0, cmap='viridis', linestyles='dashed')\nZ_history = np.array([function_f(x) for x in x_history])\nax1.plot(x_history[:, 0], x_history[:, 1], Z_history, 'o-', color='red')\nfor i in range(1, len(x_history)):\n    ax1.quiver(x_history[i-1][0], x_history[i-1][1], Z_history[i-1], \n               x_history[i][0]-x_history[i-1][0], \n               x_history[i][1]-x_history[i-1][1], \n               Z_history[i]-Z_history[i-1], \n               color=\"blue\", arrow_length_ratio=0.05)\nax1.set_xlabel(\"$X_0$\")\nax1.set_ylabel(\"$X_1$\")\nax1.set_zlabel(\"$f(X_0, X_1)$\")\nax1.set_title(\"3D Visualization\")\n\n# 2nd subplot: 2D contour plot\nax2 = fig.add_subplot(122)\ncontour = ax2.contour(X, Y, Z, levels=10, colors='black', linestyles='dashed')\nax2.clabel(contour)\nfor i in range(1, len(x_history)):\n    ax2.quiver(x_history[i-1][0], x_history[i-1][1], \n               x_history[i][0]-x_history[i-1][0], \n               x_history[i][1]-x_history[i-1][1], \n               angles=\"xy\", scale_units=\"xy\", scale=1, color=\"blue\")\nax2.plot(x_history[:, 0], x_history[:, 1], 'o-', color='red')\nax2.set_xlim(-4.5, 4.5)\nax2.set_ylim(-4.5, 4.5)\nax2.set_xlabel(\"$X_0$\")\nax2.set_ylabel(\"$X_1$\")\nax2.set_title(\"2D Visualization\")\n\n# Display the figure\nplt.tight_layout()\nplt.show()\n\n\n\n","type":"content","url":"/nn2#example-f-x-0-x-1-x-0-2-x-1-2","position":39},{"hierarchy":{"lvl1":"ニューラルネットワーク","lvl4":"ニューラルネットワークに対する勾配","lvl3":"損失関数の最適化","lvl2":"ニューラルネットワークの学習"},"type":"lvl4","url":"/nn2#id-17","position":40},{"hierarchy":{"lvl1":"ニューラルネットワーク","lvl4":"ニューラルネットワークに対する勾配","lvl3":"損失関数の最適化","lvl2":"ニューラルネットワークの学習"},"content":"ニューラルネットワークにおいて、重みパラメータの勾配を求める計算を確認します。\n\nここで、形状が2 \\times 3の重み\\mathbf{W}を持つニューラルネットワークがあり、損失関数をLで表すことを考えましょう。この場合、勾配は\\frac{\\partial L}{\\partial \\mathbf{W}}で表すことができます。\n\n\\mathbf{W}\n    = \\begin{pmatrix}\n          w_{0,0} & w_{0,1} & w_{0,2} \\\\\n          w_{1,0} & w_{1,1} & w_{1,2}\n      \\end{pmatrix},\n\n\\frac{\\partial L}{\\partial \\mathbf{W}}\n    = \\begin{pmatrix}\n          \\frac{\\partial L}{\\partial w_{0,0}} & \\frac{\\partial L}{\\partial w_{0,1}} & \\frac{\\partial L}{\\partial w_{0,2}} \\\\\n          \\frac{\\partial L}{\\partial w_{1,0}} & \\frac{\\partial L}{\\partial w_{1,1}} & \\frac{\\partial L}{\\partial w_{0,2}}\n      \\end{pmatrix}\n\n# (仮の)入力データを作成\nx = np.array([0.6, 0.9])\nprint(x)\n\n# (仮の)教師データを作成\nt = np.array([0, 0, 1])\nprint(t)\n\n\n\nnp.random.seed(0) # 乱数のシードを固定\nclass simplenet:\n    def __init__(self):\n        self.W = np.random.randn(2, 3)# 重みを初期化する関数を定義\n    def predict(self, x):\n        return np.dot(x, self.W) # 重み付き和を計算\n    def loss(self, x, t):\n        z = self.predict(x)\n        y = softmax(z) # ソフトマックス関数による正規化\n        loss = cross_entropy_error(y, t) # 交差エントロピー誤差を計算\n        return loss \n\n\n\nnet = simplenet()\np = net.predict(x)\nprint(p)\n\n\n\nnet.loss(x, t)\n\n\n\n続いて、勾配を求めてみましょう。\n\n# 損失メソッドを実行する関数を作成\ndef f(W):\n    # 損失メソッドを実行\n    return net.loss(x, t)\n\n\n\n# 損失を計算\nL = f(net.W)\nprint(L)\n\n\n\n# 重みの勾配を計算\ndW = numerical_gradient(f, net.W)\nprint(dW)\n\n\n\nこれで重みの勾配\\frac{\\partial L}{\\partial \\mathbf{W}}を得られました。\n\nその中身を見ると、例えば、\\frac{\\partial L}{\\partial \\mathbf{W_{1,1}}}はおよそ0.44ということは、w_{1,1}をhだけ増やすと損失関数の値は0.44hだけ増加することを意味します。\n\nそのため、損失関数の値を減らすために、w_{1,1}はマイナス方向へ更新するのが良いことがわかりました。\n\nパラメータの勾配が得られたということは、パラメータの学習を行えるようになったということです。\n\n","type":"content","url":"/nn2#id-17","position":41},{"hierarchy":{"lvl1":"ニューラルネットワーク","lvl2":"2層ニューラルネットワークの実装"},"type":"lvl2","url":"/nn2#id-2-1","position":42},{"hierarchy":{"lvl1":"ニューラルネットワーク","lvl2":"2層ニューラルネットワークの実装"},"content":"これまでに勉強した、「損失関数」、「ミニバッチ」、「勾配」、「勾配下降法」をまとめて、ニューラルネットワークの学習手順を確認します。\n\nミニバッチ: データセットからミニバッチをランダムに取り出す。ここでは、そのミニバッチの損失関数の値を減らすことを目的とする。\n\n勾配の算出：各重みパラメータの勾配を求める。\n\nパラメーターの更新：重みパラメータを勾配方向に微少量だけ更新する。\n\n収束するまで繰り返す\n\nここでは、2層のニューラルネットワークの計算と最適化プロセスを確認しましょう。","type":"content","url":"/nn2#id-2-1","position":43},{"hierarchy":{"lvl1":"ニューラルネットワーク","lvl3":"数式の確認","lvl2":"2層ニューラルネットワークの実装"},"type":"lvl3","url":"/nn2#id-18","position":44},{"hierarchy":{"lvl1":"ニューラルネットワーク","lvl3":"数式の確認","lvl2":"2層ニューラルネットワークの実装"},"content":"","type":"content","url":"/nn2#id-18","position":45},{"hierarchy":{"lvl1":"ニューラルネットワーク","lvl4":"入力層","lvl3":"数式の確認","lvl2":"2層ニューラルネットワークの実装"},"type":"lvl4","url":"/nn2#id-19","position":46},{"hierarchy":{"lvl1":"ニューラルネットワーク","lvl4":"入力層","lvl3":"数式の確認","lvl2":"2層ニューラルネットワークの実装"},"content":"ニューラルネットワークの入力\\mathbf{X}、第1層の重み\\mathbf{W^{(1)}}と\\mathbf{b}^{(1)}を次の形状とします。\\mathbf{X}\n    = \\begin{pmatrix}\n          x_{0,0} & x_{0,1} & \\cdots & x_{0,D-1} \\\\\n          x_{1,0} & x_{1,1} & \\cdots & x_{1,D-1} \\\\\n          \\vdots & \\vdots & \\ddots & \\cdots \\\\\n          x_{N-1,0} & x_{N-1,1} & \\cdots & x_{N-1,D-1}\n      \\end{pmatrix}\n,\\ \n\\mathbf{W}^{(1)}\n    = \\begin{pmatrix}\n          w_{0,0} & w_{0,1} & \\cdots & w_{0,H-1} \\\\\n          w_{1,0} & w_{1,1} & \\cdots & w_{1,H-1} \\\\\n          \\vdots &\\vdots & \\ddots & \\vdots \\\\\n          w_{D-1,0} & w_{D-1,1} & \\cdots & w_{D-1,H-1}\n      \\end{pmatrix}\n,\\ \n\\mathbf{b}^{(1)}\n    = \\begin{pmatrix}\n          b_0 & b_1 & \\cdots & b_{H-1}\n      \\end{pmatrix}\n\nここで、\\mathbf{N}はバッチサイズ、\\mathbf{D}は各データ\\mathbf{x}_n = (x_{n,0}, \\cdots, x_{n,D-1})の要素数、\\mathbf{H}は中間層のニューロン数です。","type":"content","url":"/nn2#id-19","position":47},{"hierarchy":{"lvl1":"ニューラルネットワーク","lvl4":"隠れ層","lvl3":"数式の確認","lvl2":"2層ニューラルネットワークの実装"},"type":"lvl4","url":"/nn2#id-20","position":48},{"hierarchy":{"lvl1":"ニューラルネットワーク","lvl4":"隠れ層","lvl3":"数式の確認","lvl2":"2層ニューラルネットワークの実装"},"content":"第1層の重み付き和\\mathbf{A}^{(1)}を計算します。\\mathbf{A}^{(1)}\n    = \\mathbf{X} \\mathbf{W}^{(1)} + \\mathbf{B}^{(1)}\n\n\\mathbf{N} \\times \\mathbf{D}と\\mathbf{D} \\times \\mathbf{H}の行列の積なので、計算結果は\\mathbf{N} \\times \\mathbf{H}の行列になります。\\mathbf{A}^{(1)}\n    = \\begin{pmatrix}\n          a_{0,0} & a_{0,1} & \\cdots & a_{0,H-1} \\\\\n          a_{1,0} & a_{1,1} & \\cdots & a_{1,H-1} \\\\\n          \\vdots & \\vdots & \\ddots & \\cdots \\\\\n          a_{N-1,0} & a_{N-1,1} & \\cdots & a_{N-1,H-1}\n      \\end{pmatrix}\n\n重み付き和\\mathbf{A}^{(1)}の各要素をシグモイド関数により活性化します。z_{n,h}\n    = \\mathrm{sigmoid}(a_{n,h})\n\nただ、活性化関数は形状(\\mathbf{N} \\times \\mathbf{H})に影響しません。第1層の出力の結果は、\\mathbf{Z}\n    = \\begin{pmatrix}\n          z_{0,0} & z_{0,1} & \\cdots & z_{0,H-1} \\\\\n          z_{1,0} & z_{1,1} & \\cdots & z_{1,H-1} \\\\\n          \\vdots & \\vdots & \\ddots & \\cdots \\\\\n          z_{N-1,0} & z_{N-1,1} & \\cdots & z_{N-1,H-1}\n      \\end{pmatrix}\n\nになります。","type":"content","url":"/nn2#id-20","position":49},{"hierarchy":{"lvl1":"ニューラルネットワーク","lvl4":"出力層","lvl3":"数式の確認","lvl2":"2層ニューラルネットワークの実装"},"type":"lvl4","url":"/nn2#id-21","position":50},{"hierarchy":{"lvl1":"ニューラルネットワーク","lvl4":"出力層","lvl3":"数式の確認","lvl2":"2層ニューラルネットワークの実装"},"content":"次に、第2層の重み\\mathbf{W^{(2)}}と\\mathbf{b}^{(2)}は以下のように形状しています。\\mathbf{W}^{(2)}\n    = \\begin{pmatrix}\n          w_{0,0} & w_{0,1} & \\cdots & w_{0,K-1} \\\\\n          w_{1,0} & w_{1,1} & \\cdots & w_{1,K-1} \\\\\n          \\vdots &\\vdots & \\ddots & \\vdots \\\\\n          w_{H-1,0} & w_{H-1,1} & \\cdots & w_{H-1,K-1}\n      \\end{pmatrix}\n,\\ \n\\mathbf{b}^{(2)}\n    = \\begin{pmatrix}\n          b_0 & b_1 & \\cdots & b_{K-1}\n      \\end{pmatrix}\n\nここで、\\mathbf{H}は中間層のニューロン数、\\mathbf{K}は出力層のクラス数です。\n\n第2層の重み付き和\\mathbf{A}^{(2)}は\\mathbf{A}^{(2)}\n    = \\mathbf{Z} \\mathbf{W}^{(2)} + \\mathbf{B}^{(2)}\n\n\\mathbf{N} \\times \\mathbf{H}と\\mathbf{H} \\times \\mathbf{K}の行列の積なので、計算結果は\\mathbf{N} \\times \\mathbf{K}の行列になります。\\mathbf{A}^{(2)}\n    = \\begin{pmatrix}\n          a_{0,0} & a_{0,1} & \\cdots & a_{0,K-1} \\\\\n          a_{1,0} & a_{1,1} & \\cdots & a_{1,K-1} \\\\\n          \\vdots & \\vdots & \\ddots & \\cdots \\\\\n          a_{N-1,0} & a_{N-1,1} & \\cdots & a_{N-1,K-1}\n      \\end{pmatrix}\n\nここで、ソフトマックス関数により各データの重み付き和a_{n}^{(2)}を活性化して、ニューラルネットワークの出力y_nとします。\\mathbf{y}_n\n    = \\mathrm{softmax}(\\mathbf{a}_n^{(2)})\n\n\\mathbf{Y}でニューラルネットワークの出力を表します。n番目のデータに関する出力\\mathbf{y}_nは、0 \\leq y_{n,k} \\leq 1、\\sum_{k=0}^{K-1} y_{n,k} = 1に正規化されており、n番目の入力データ\\mathbf{x}_nがどのクラスのかを表す確率分布として扱えるのでした。\\mathbf{Y}\n    = \\begin{pmatrix}\n          y_{0,0} & y_{0,1} & \\cdots & y_{0,K-1} \\\\\n          y_{1,0} & y_{1,1} & \\cdots & y_{1,K-1} \\\\\n          \\vdots & \\vdots & \\ddots & \\cdots \\\\\n          y_{N-1,0} & y_{N-1,1} & \\cdots & y_{N-1,K-1}\n      \\end{pmatrix}\n\n","type":"content","url":"/nn2#id-21","position":51},{"hierarchy":{"lvl1":"ニューラルネットワーク","lvl4":"損失の計算","lvl3":"数式の確認","lvl2":"2層ニューラルネットワークの実装"},"type":"lvl4","url":"/nn2#id-22","position":52},{"hierarchy":{"lvl1":"ニューラルネットワーク","lvl4":"損失の計算","lvl3":"数式の確認","lvl2":"2層ニューラルネットワークの実装"},"content":"N個のデータに関する教師データ\\mathbf{T}は、出力\\mathbf{Y}と同じ形状になります。\\mathbf{T}\n    = \\begin{pmatrix}\n          t_{0,0} & t_{0,1} & \\cdots & t_{0,K-1} \\\\\n          t_{1,0} & t_{1,1} & \\cdots & t_{1,K-1} \\\\\n          \\vdots & \\vdots & \\ddots & \\cdots \\\\\n          t_{N-1,0} & t_{N-1,1} & \\cdots & t_{N-1,K-1}\n      \\end{pmatrix}\n\n特に、分類問題の場合、各データの教師データt_nは、、正解ラベルが1でそれ以外が0といった形になります。\n\n(平均)交差エントロピー誤差を計算して、損失Lとします。L   = - \\frac{1}{N}\n        \\sum_{n=0}^{N-1} \\sum_{k=0}^{K-1}\n          t_{n,k} \\log y_{n,k}\n\n","type":"content","url":"/nn2#id-22","position":53},{"hierarchy":{"lvl1":"ニューラルネットワーク","lvl4":"勾配の計算","lvl3":"数式の確認","lvl2":"2層ニューラルネットワークの実装"},"type":"lvl4","url":"/nn2#id-23","position":54},{"hierarchy":{"lvl1":"ニューラルネットワーク","lvl4":"勾配の計算","lvl3":"数式の確認","lvl2":"2層ニューラルネットワークの実装"},"content":"損失を求めるまでの計算を1つの関数とみなして、重みの勾配\\frac{\\partial L}{\\partial \\mathbf{W}}とバイアスの勾配\\frac{\\partial L}{\\partial \\mathbf{b}}を求めます。\n\n第1層のパラメータ\\mathbf{W}^{(1)},\\ \\mathbf{b}^{(1)}を\\frac{\\partial L}{\\partial \\mathbf{W}^{(1)}},\\ \\frac{\\partial L}{\\partial \\mathbf{b}^{(1)}}で表します。\\frac{\\partial L}{\\partial \\mathbf{W}^{(1)}}\n    = \\begin{pmatrix}\n          \\frac{\\partial L}{\\partial w_{0,0}} & \\frac{\\partial L}{\\partial w_{0,1}} & \\cdots & \\frac{\\partial L}{\\partial w_{0,H-1}} \\\\\n          \\frac{\\partial L}{\\partial w_{1,0}} & \\frac{\\partial L}{\\partial w_{a,1}} & \\cdots & \\frac{\\partial L}{\\partial w_{1,H-1}} \\\\\n          \\vdots & \\vdots & \\ddots & \\vdots \\\\\n          \\frac{\\partial L}{\\partial w_{D-1,0}} & \\frac{\\partial L}{\\partial w_{D-1,1}} & \\cdots & \\frac{\\partial L}{\\partial w_{D-1,H-1}} \\\\\n      \\end{pmatrix}\n,\\ \n\\frac{\\partial L}{\\partial \\mathbf{b}^{(1)}}\n    = \\begin{pmatrix}\n          \\frac{\\partial L}{\\partial b_0} & \\frac{\\partial L}{\\partial b_1} & \\cdots & \\frac{\\partial L}{\\partial b_{H-1}}\n      \\end{pmatrix}\n\n同様に、第2層のパラメータ\\mathbf{W}^{(2)},\\ \\mathbf{b}^{(2)}を\\frac{\\partial L}{\\partial \\mathbf{W}^{(2)}},\\ \\frac{\\partial L}{\\partial \\mathbf{b}^{(2)}}で表します。\\frac{\\partial L}{\\partial \\mathbf{W}^{(2)}}\n    = \\begin{pmatrix}\n          \\frac{\\partial L}{\\partial w_{0,0}} & \\frac{\\partial L}{\\partial w_{0,1}} & \\cdots & \\frac{\\partial L}{\\partial w_{0,K-1}} \\\\\n          \\frac{\\partial L}{\\partial w_{1,0}} & \\frac{\\partial L}{\\partial w_{a,1}} & \\cdots & \\frac{\\partial L}{\\partial w_{1,K-1}} \\\\\n          \\vdots & \\vdots & \\ddots & \\vdots \\\\\n          \\frac{\\partial L}{\\partial w_{H-1,0}} & \\frac{\\partial L}{\\partial w_{H-1,1}} & \\cdots & \\frac{\\partial L}{\\partial w_{H-1,K-1}} \\\\\n      \\end{pmatrix}\n,\\ \n\\frac{\\partial L}{\\partial \\mathbf{b}^{(2)}}\n    = \\begin{pmatrix}\n          \\frac{\\partial L}{\\partial b_0} & \\frac{\\partial L}{\\partial b_1} & \\cdots & \\frac{\\partial L}{\\partial b_{K-1}}\n      \\end{pmatrix}\n\n各要素は、それぞれパラメータの対応する要素の偏微分です。\n\n","type":"content","url":"/nn2#id-23","position":55},{"hierarchy":{"lvl1":"ニューラルネットワーク","lvl4":"パラメータの更新","lvl3":"数式の確認","lvl2":"2層ニューラルネットワークの実装"},"type":"lvl4","url":"/nn2#id-24","position":56},{"hierarchy":{"lvl1":"ニューラルネットワーク","lvl4":"パラメータの更新","lvl3":"数式の確認","lvl2":"2層ニューラルネットワークの実装"},"content":"各パラメータの勾配、\\frac{\\partial L}{\\partial \\mathbf{W}},\\ \\frac{\\partial L}{\\partial \\mathbf{b}}を用いて、勾配降下法によりパラメータ\\mathbf{W},\\ \\mathbf{b}を更新します。\n\n更新後のパラメータを\\mathbf{W}^{(\\mathrm{new})},\\ \\mathbf{b}^{(\\mathrm{new})}とすると、更新式は次の式で表せます。\\begin{aligned}\n\\mathbf{W}^{(\\mathrm{new})}\n   &= \\mathbf{W}\n      - \\eta \\frac{\\partial L}{\\partial \\mathbf{W}}\n\\\\\n\\mathbf{b}^{(\\mathrm{new})}\n   &= \\mathbf{b}\n      - \\eta \\frac{\\partial L}{\\partial \\mathbf{b}}\n\\end{aligned}\n\n各要素に注目すると、それぞれ次の計算をしています。\\begin{aligned}\nw_{h,k}^{(\\mathrm{new})}\n   &= w_{h,k} - \\eta \\frac{\\partial L}{\\partial w_{h,k}}\n\\\\\nb_k^{(\\mathrm{new})}\n   &= b_k - \\eta \\frac{\\partial L}{\\partial b_k}\n\\end{aligned}","type":"content","url":"/nn2#id-24","position":57},{"hierarchy":{"lvl1":"学習に関するテクニック"},"type":"lvl1","url":"/sgd","position":0},{"hierarchy":{"lvl1":"学習に関するテクニック"},"content":"\n\nここまで勉強したように、ニューラルネットワークの学習は損失関数の最小化に帰着されます。これは高次元パラメータ空間において、複雑な目的関数の良い最適解を見出そうとする、非常に難しい問題です。\n\nさらに、深いネットーワークでは、パラメータの数が膨大になり、事態はより深刻になってきます。\n\nこれを解決に導くため、様々なテクニックが組み合わされて利用されています。\n\n","type":"content","url":"/sgd","position":1},{"hierarchy":{"lvl1":"学習に関するテクニック","lvl2":"確率的勾配降下法"},"type":"lvl2","url":"/sgd#id","position":2},{"hierarchy":{"lvl1":"学習に関するテクニック","lvl2":"確率的勾配降下法"},"content":"\n\n勾配降下法で損失関数を最適化する方法としては、学習データの全てのサンプルに対する損失関数を求め、それを最小化することが考えられます。つまり、各サンプル１個に関する損失L_nの和の勾配を用いてwを更新する方法を、バッチ学習(batch learning)と呼ぶことにします。\n\nこれに対して、一定数のサンプルの集合単位で重みを更新するのが、計算効率性の観点からより現実的な選択です。このサンプルの集合のことをミニバッチ(minibatch)と呼びます。\n\nなお、このミニバッチを用いた勾配降下法は確率的勾配降下法(stochastic gradient descent)と呼ばれます。\n\nここで、ミニバッチ1つをD_tと書きます。添字はt回目の更新ごとにそのサンプル集合が変わることを表します。ミニバッチのリストD_1,D2,...を考え、１つずつ順番に用いて重みを更新します。つまりt回目にはD_tが含む全サンプルに対する損失の総和L_t(w)=\\frac{1}{N_t} \\sum_{n \\in D_t} E_n(w)\n\nを計算し、その勾配の方向にパラメータを一度だけ更新します。なお、N_tはこのミニバッチが含むサンプル数です。\n\nミニバッチをD_1,D2,...と順番に取り出して使い、一巡したら再度、最初からD_1,D2,...の順に取り出して使います。ここで、一巡することをエポック(epoch)と呼びます。\n\nニューラルネットワークの学習では一般に、数エポックから数十、あるいはそれ以上のエポック数分、上の過程を繰り返します。\n\nNote\n\nミニバッチのサイズを決める系統的な方法はありませんが、大体10〜100サンプルとすることが一般です。\n\n","type":"content","url":"/sgd#id","position":3},{"hierarchy":{"lvl1":"学習に関するテクニック","lvl2":"過学習問題の制御"},"type":"lvl2","url":"/sgd#id-1","position":4},{"hierarchy":{"lvl1":"学習に関するテクニック","lvl2":"過学習問題の制御"},"content":"過学習（overfitting）とは、機械モデルが学習データに対して過度に適合してしまい、新しいデータに対して良いパフォーマンスを発揮できなくなる状態をいいます。機械モデルの目的は、学習データには含まれないデータであっても、正しく識別できる汎化性能が望まれます。\n\nそのため、過学習を抑制するテクニックが重要になってくるのです。","type":"content","url":"/sgd#id-1","position":5},{"hierarchy":{"lvl1":"学習に関するテクニック","lvl3":"重みの減衰","lvl2":"過学習問題の制御"},"type":"lvl3","url":"/sgd#id-2","position":6},{"hierarchy":{"lvl1":"学習に関するテクニック","lvl3":"重みの減衰","lvl2":"過学習問題の制御"},"content":"過学習抑制のためによく用いられる手法として、重みの減衰という方法があります。\n\n過学習は、重みパラメータが大きな値を取ることによって発生することが多くのため、この手法は、学習の過程において、大きな重みをもつことに対してペナルティを課することで、過学習を抑制しようということです。\n\n典型例が、損失関数に重みの二乗和(L2ノルムの二乗)を加えるL2正則化(L2 regularization)です。L_t(w)=\\frac{1}{N_t} \\sum_{n \\in D_t} E_n(w)+ \\frac{\\lambda}{2}||w||^2\n\nここで、\\lambdaは正則化の効果の強さを制御するパラメータです。\n\nNote\n\nパラメータを制約することに限らず、過剰適合を回避する目的を持つ方法を広く正則化と呼ぶこともあります。\n\n\n\nつまり、損失関数に\\frac{\\lambda}{2}||w||^2を加算します。\\frac{\\lambda}{2}||w||^2はペナルティとして、重み自身の大きさに比例して大きくなるため、重みが大きくなることを抑制することができます。\n\nこのことから、この方法は重みの減衰(weight decay)とも呼ばれます。\n\n","type":"content","url":"/sgd#id-2","position":7},{"hierarchy":{"lvl1":"学習に関するテクニック","lvl3":"Dropout","lvl2":"過学習問題の制御"},"type":"lvl3","url":"/sgd#dropout","position":8},{"hierarchy":{"lvl1":"学習に関するテクニック","lvl3":"Dropout","lvl2":"過学習問題の制御"},"content":"Dropoutは、ネットワークの隠れ層のニューロンを学習時ランダムに選別して削除することで過学習を抑制する手法です。\n\nDropoutの狙いは、学習時ネットワークの自由度を強制的に小さくし、過学習を避けることです。\n\nまたこうすることは、Dropoutによってニューロンをランダムに無効化したネットワークを多数、独立の学習し、推論時にそれらの結果を平均すると同じ効果があると考えられています。複数のネットワークの平均をとると推論の精度が一般的に向上することが知られており、Dropoutはこれと同じ効果をより小さい計算コストで実現していると考えることができます。\n\n\n\n","type":"content","url":"/sgd#dropout","position":9},{"hierarchy":{"lvl1":"学習に関するテクニック","lvl2":"学習率の制御"},"type":"lvl2","url":"/sgd#id-3","position":10},{"hierarchy":{"lvl1":"学習に関するテクニック","lvl2":"学習率の制御"},"content":"ニューラルネットワークの学習では、学習率の値が重要になります。学習率が大きくなりすぎると学習は収束せず、小さくしすぎると学習が非常に遅くなる、または局所的な最適解にとどまってしまい、真の最適解にたどり着けない可能性があります。\n\n","type":"content","url":"/sgd#id-3","position":11},{"hierarchy":{"lvl1":"学習に関するテクニック","lvl3":"モメンタム","lvl2":"学習率の制御"},"type":"lvl3","url":"/sgd#id-4","position":12},{"hierarchy":{"lvl1":"学習に関するテクニック","lvl3":"モメンタム","lvl2":"学習率の制御"},"content":"SGDにおいてのパラメータ更新を安定化し、収束性を改善するモメンタム(momentum)という方法があり、SGDではほとんど常に採用されます。\n\nモメンタムは、重みを更新時、重みの修正量に前回の重みの修正量の何割を加算する方法です。つまり、ミニバッチD_{t-1}に対する重みを修正量をv_t=w_t-w_{t-1}を書くと、ミニバッチD_tに対する更新をw_{t+1}=w_t-\\epsilon \\Delta L_{t}+ \\mu v_t\n\nと定めます。\n\nモメンタム法は、過去の勾配の移動平均を蓄積し、持続的にその勾配の方向性を進むようにします。これにより、より効率的に最小値を探索できるようになります。\n\n","type":"content","url":"/sgd#id-4","position":13},{"hierarchy":{"lvl1":"学習に関するテクニック","lvl3":"更新幅の適応的調整","lvl2":"学習率の制御"},"type":"lvl3","url":"/sgd#id-5","position":14},{"hierarchy":{"lvl1":"学習に関するテクニック","lvl3":"更新幅の適応的調整","lvl2":"学習率の制御"},"content":"学習率を、学習の開始から終了まで固定したままとするとのは稀で、多くの場合時間と共に変化させます。重みの更新幅を適応的に調節する方法として、AdaGradという方法があります。\n\nAdaGradは、パラメータの要素ごとに適応的に学習率を調整しながら学習を行う手法です。\n\nNote\n\nAdaGradのAdaはAdaptiveに由来します。\n\nAdaGradの更新方法を数式で表しますとh \\leftarrow h+ \\frac{\\partial L}{\\partial W} \\odot \\frac{\\partial L}{\\partial W}W \\leftarrow W - \\eta \\frac{1}{\\sqrt{h}} \\frac{\\partial L}{\\partial W}\n\nここで、hは、学習開始から現在に至る全更新について、勾配の二乗を加算したものです。(\\odotは行列要素ごとの掛け算を意味します)\n\nそして、パラメータを更新する際に、\\frac{1}{\\sqrt{h}}を乗算することで、学習のスケールを調整します。\n\nこの調整により、パラメータの要素の中に変動の大きい勾配に対しては小さい学習率、変動の少ない勾配に対しては大きい学習率を適用ことを意味します。\n\nAdaGradでの更新では、学習が進めれば進めるほど、更新度合いは小さくなります。つまり、勾配の累積によって自動的に学習率が調整されます。初期学習率を適切に設定すれば、後の調整を大幅に減らすことができ、異なるパラメータに対して異なるスピードで学習が進むのを許容します。\n\nNote\n\n累積された勾配の二乗和が大きくなると、学習率が非常に小さくなり、最終的に学習が停滞することがあります。AdaGradの学習率が時間とともに過剰に減少してしまうという問題を解決するために、RMSPropといった手法が開発されました。これらは、AdaGradの利点を保ちながら、学習率の減少を制御することでより良い結果を得られるようになっています。\n\n","type":"content","url":"/sgd#id-5","position":15},{"hierarchy":{"lvl1":"学習に関するテクニック","lvl3":"Adam","lvl2":"学習率の制御"},"type":"lvl3","url":"/sgd#adam","position":16},{"hierarchy":{"lvl1":"学習に関するテクニック","lvl3":"Adam","lvl2":"学習率の制御"},"content":"Adamは、AdaGrad法にさらにモメンタムを導入したもので、実装上によく使われている方法になります。\n\nここでは、その理論の説明を避けますが、直感的には、モメンタム法とAdaGrad法の利点を組み合わせることで、効率的にパラーメータ空間を探索することが期待されます。\n\n\n\nさらに、「重みの減衰項」をAdamに導入したAdamWという手法もあります。それがより良い結果をもたらす場合があります。","type":"content","url":"/sgd#adam","position":17},{"hierarchy":{"lvl1":"Attention"},"type":"lvl1","url":"/attention","position":0},{"hierarchy":{"lvl1":"Attention"},"content":"近年では、BERT、GPTなど大規模言語モデルが目を見張るような成果を出しています。これらの大規模言語モデルでは、基本的にはTransformerというというアーキテクチャが応用されています。\n\nこのTransformerアーキテクチャの起源をたどると、2017年にGoogleの研究者たちによって発表された論文「Attention Is All You Need」に行き着きます。このアーキテクチャは、入力された各単語が、他のすべての単語にどの程度「注意」を払うべきかを学習します。これにより、文脈に応じた単語の重要性を柔軟に捉えることができるようになりました。\n\n論文タイトルの通りで、Transformerを理解するためにはAttention機構を理解しなければなりませんので、今回はAttention機構の仕組みについて説明します。\n\n","type":"content","url":"/attention","position":1},{"hierarchy":{"lvl1":"Attention","lvl2":"seq2seqの問題点"},"type":"lvl2","url":"/attention#seq2seq","position":2},{"hierarchy":{"lvl1":"Attention","lvl2":"seq2seqの問題点"},"content":"\n\n今まで説明した通り、seq2seqでは二つのRNNを連結することで、時系列データを別の時系列データに変換させることができました。その変換が、Encoderで「固定長のベクトル」に情報をエンコードし、Decoderに渡すことより実現されています。\n\nしかし、「固定長のベクトル」というところに大きな問題が潜んでいます。固定長ベクトルというのはベクトルの大きさが決まってしまっているということで、入力の大きさに関係なく、常にある一定のサイズのベクトルに変換されます。場合による、長い入力を無理やり「固定長のベクトル」へと押し込むことにより、必要な情報はベクトルからはみだす可能性があります。\n\n実に、seq2seqでは長文、特に学習コーパスの文より長い文に対応することが困難になる場合があり、実際に入力文が長くなるにつれて急速に精度が悪化することが示されました。\n\n","type":"content","url":"/attention#seq2seq","position":3},{"hierarchy":{"lvl1":"Attention","lvl2":"Attentionの仕組み"},"type":"lvl2","url":"/attention#attention","position":4},{"hierarchy":{"lvl1":"Attention","lvl2":"Attentionの仕組み"},"content":"\n\nseq2seqが抱えていた問題を解決することに向けて、認知科学から注意（Attention）という概念からヒントを得られました。\n\n人間の認知モデルにおいて、注意とは「特定の対象に感覚や意識を集中させることで、不必要な情報を排除、必要な情報を選択し、限りある知的資源を効果的に配分するもの」という位置付けになっています。人間が情報を処理する際、特定の物事を注視していた、みたいに「注意」は無意識的に調節されるのが一般的です。\n\nseq2seqは人間と同じように、必要な情報だけ「注意」を向けさせることは、Attention機構の目的になります。\n\nAttention機構の詳細を説明する前に、まず全体の枠組みを示したいと思います。\n\nseq2seqにAttention機構を追加することを考えます。\n\nAtention機構では、EncoderのLSTMライヤの各時点の隠れ状態(hs)を受け取ります。\n\n各時点の隠れ状態(hs)に基づいて、必要な情報を選び出し、それをその先の出力の生成に使います。\n\n","type":"content","url":"/attention#attention","position":5},{"hierarchy":{"lvl1":"Attention","lvl2":"Attentionの計算"},"type":"lvl2","url":"/attention#attention-1","position":6},{"hierarchy":{"lvl1":"Attention","lvl2":"Attentionの計算"},"content":"","type":"content","url":"/attention#attention-1","position":7},{"hierarchy":{"lvl1":"Attention","lvl3":"Attention Weight","lvl2":"Attentionの計算"},"type":"lvl3","url":"/attention#attention-weight","position":8},{"hierarchy":{"lvl1":"Attention","lvl3":"Attention Weight","lvl2":"Attentionの計算"},"content":"「注意を向け」という操作を数学的な演算を置き換えると、各単語の重要度を表す「重み」(a)を利用することが考えられます。ここで、aの各要素は0 ~ 1のスカラあり、その総和は1になります。一般的には、Attention Weightと呼ばれます。\n\nAttention Weight計算の流れを見ていきましょう。\n\nNote\n\n「Key」と「Query」はプログラミング、およびデータベース管理の文脈で一般的に使用される用語です。\n\nKeyはデータ構造（特に辞書やマップ）内のデータアイテムを一意に識別するために使用されます。\n\nQueryは、データを取得、操作、または更新するためにデータベースや検索エンジンに対して行われるリクエストです。\n\nQueryとKeyの生成\n\nKey: 注目の対象、ここではエンコーダの各時点の隠れ状態を指すk_i=W_K \\cdot h_i, for \\ i=1 \\ to \\ m\n\nQuery: 注目したいターゲット、ここではデコーダの現在の隠れ状態を指すq_t = W_Q \\cdot s_t\n\nここで、W_KとW_Qは学習する必要があるのパラメーターになります。\n\nDecoderのLSTMレイヤの隠れ状態ベクトルとEncoderの各時点の隠れ状態h_tとどれだけ\"似ているか\"を数値で表す際、ベクトルの「内積」が使用されています。a_i^t = k_i^T q_t, for \\ i=1 \\ to \\ m\n\nソフトマックス関数を適用し、すべてのウェイトの合計が1になるように正規化します。Softmax([a_1, \\cdot \\cdot \\cdot, a_m])\n\n","type":"content","url":"/attention#attention-weight","position":9},{"hierarchy":{"lvl1":"Attention","lvl3":"コンテキストベクトル","lvl2":"Attentionの計算"},"type":"lvl3","url":"/attention#id","position":10},{"hierarchy":{"lvl1":"Attention","lvl3":"コンテキストベクトル","lvl2":"Attentionの計算"},"content":"各単語の重要度を表すaと各単語のベクトルから、その重み付き和を求めることで、目的とするコンテキストベクトルを求めます。c_t=a_1^t h_1+\\cdot \\cdot \\cdot +a_m^th_m\n\nここで、各単語のベクトルはattention機構においてvalueとして定義されています。\n\nNote\n\nKeyとValueは、Attention機構において異なる機能を持つものですが、しばしば同じ入力データに基づいて生成されます。Keyはクエリとの関連性を決定するために使用され、Valueはコンテキストベクトルを形成するために使用されます。","type":"content","url":"/attention#id","position":11},{"hierarchy":{"lvl1":"Attention","lvl3":"Attentionレイヤ付きのseq2seq","lvl2":"Attentionの計算"},"type":"lvl3","url":"/attention#attention-seq2seq","position":12},{"hierarchy":{"lvl1":"Attention","lvl3":"Attentionレイヤ付きのseq2seq","lvl2":"Attentionの計算"},"content":"Attentionレイヤ付きのseq2seqモデルにおいて、Attentionレイヤの出力ーコンテキストベクトルーが次時点のLSTMレイヤの入力へと接続されています。\nつまり、現在の隠れ状態とコンテキストベクトルを組み合わせて、次の出力トークンを生成します。\n\nそのような構成にすることで、各デコードステップでエンコーダの出力から最も関連する情報が抽出され、それを用いてデコーダが次の出力トークンをより「正確」に生成できます。\n\n","type":"content","url":"/attention#attention-seq2seq","position":13},{"hierarchy":{"lvl1":"誤差逆伝播法"},"type":"lvl1","url":"/backpropagation","position":0},{"hierarchy":{"lvl1":"誤差逆伝播法"},"content":"\n\nこれまでは、ニューラルネットワークの各パラメータについての目的関数の数値微分を計算することで勾配の計算を求める方法を説明しました。\n\n勾配ベクトルの各成分は、各層の結合重みと各ユニットのバイアスでの損失関数の微分です。\n\nしかし、ニューラルネットワークの層数が多くなると、数値微分の計算は膨大な時間がかかるでしょう。さらに、特に入力に近い深い層のパラメータほど、計算の手間が多くなります。\n\nここで、パラメータの勾配の計算を効率よく行う手法である「誤差逆伝播法」について学びます。\n\n","type":"content","url":"/backpropagation","position":1},{"hierarchy":{"lvl1":"誤差逆伝播法","lvl2":"連鎖律"},"type":"lvl2","url":"/backpropagation#id","position":2},{"hierarchy":{"lvl1":"誤差逆伝播法","lvl2":"連鎖律"},"content":"\n\n複数の関数によって構成される関数を合成関数と呼びます。\\begin{align}\nz &= t^2 \\\\\nt &= x + y\n\\end{align}\n\n合成関数の微分は、「tに関するzの微分\\frac{\\partial z}{\\partial t}」と「xに関するtの微分\\frac{\\partial t}{\\partial 1}」の積のように、それぞれの関数の微分の積で求められます。\\frac{\\partial z}{\\partial x}\n    = \\frac{\\partial z}{\\partial t}\n      \\frac{\\partial t}{\\partial x}\n\n連鎖律に対するよくある誤解\\frac{dy}{dx} = \\frac{dy}{du} \\cdot \\frac{du}{dx}\n\nに対して、「du が消えるから当然だ」のように考えがちが、その思うのは誤りです。\nなぜなら、\\frac{dy}{dx}、\\frac{dy}{du}、\\frac{du}{dx} はそれぞれ極限を含む定義式であり、単なる分数ではないからです\\boxed{\n\\frac{dy}{du}\\Big|_{u = u_0}\n= f'(u_0)\n= \\lim_{\\Delta u \\to 0}\n\\frac{f(u_0 + \\Delta u) - f(u_0)}{\\Delta u}\n}\\boxed{\n\\frac{du}{dx}\\Big|_{x = x_0}\n= g'(x_0)\n= \\lim_{\\Delta x \\to 0}\n\\frac{g(x_0 + \\Delta x) - g(x_0)}{\\Delta x}\n}\n\n微分の定義は：f'(x_0) = \\lim_{\\Delta x \\to 0} \\frac{f(x_0 + \\Delta x) - f(x_0)}{\\Delta x}\n\nこれを使うと、\ny=f(u), u=g(x) の場合、\\frac{dy}{dx}\n= \\lim_{\\Delta x \\to 0} \\frac{f(g(x+\\Delta x)) - f(g(x))}{\\Delta x}\n\nこの極限の中で、\\Delta u = g(x+\\Delta x) - g(x)\n\nを代入して丁寧に展開すると、最終的に\\frac{dy}{dx} = f'(u) \\cdot g'(x)\n\nが導かれます。\n\nつまり、この関係は「分数を掛けた」わけではなく、極限に関する計算によって得られるものです。\n\n","type":"content","url":"/backpropagation#id","position":3},{"hierarchy":{"lvl1":"誤差逆伝播法","lvl2":"線形変換の逆伝播の導出"},"type":"lvl2","url":"/backpropagation#id-1","position":4},{"hierarchy":{"lvl1":"誤差逆伝播法","lvl2":"線形変換の逆伝播の導出"},"content":"入力データ\\mathbf{x}は(N \\times D)の行列、\\mathbf{W}は(D \\times H)の行列、\\mathbf{b}は要素数Hのベクトルと考え、線形変換の計算は以下の式で表します。\n\n\\begin{aligned}\n\\mathbf{y}\n   &= \\mathbf{x} \\mathbf{W} + \\mathbf{b}\n\\\\\n   &= \\begin{pmatrix}\n          x_{0,0} & x_{0,1} & \\cdots & x_{0,D-1} \\\\\n          x_{1,0} & x_{1,1} & \\cdots & x_{1,D-1} \\\\\n          \\vdots & \\vdots & \\ddots & \\vdots \\\\\n          x_{N-1,0} & x_{N-1,1} & \\cdots & x_{N-1,D-1}\n      \\end{pmatrix}\n      \\begin{pmatrix}\n          w_{0,0} & w_{0,1} & \\cdots & w_{0,H-1} \\\\\n          w_{1,0} & w_{1,1} & \\cdots & w_{1,H-1} \\\\\n          \\vdots & \\vdots & \\ddots & \\vdots \\\\\n          w_{D-1,0} & w_{D-1,1} & \\cdots & w_{D-1,H-1}\n      \\end{pmatrix}\n      + \\begin{pmatrix}\n          b_0 & b_1 & \\cdots & b_{H-1}\n        \\end{pmatrix}\n\\\\\n   &= \\begin{pmatrix}\n          \\sum_{d=0}^{D-1} x_{0,d} w_{d,0} + b_0 & \n          \\sum_{d=0}^{D-1} x_{0,d} w_{d,1} + b_1 & \n          \\cdots & \n          \\sum_{d=0}^{D-1} x_{0,d} w_{d,H-1} + b_{H-1} \\\\\n          \\sum_{d=0}^{D-1} x_{1,d} w_{d,0} + b_0 & \n          \\sum_{d=0}^{D-1} x_{1,d} w_{d,1} + b_1 & \n          \\cdots & \n          \\sum_{d=0}^{D-1} x_{1,d} w_{d,H-1} + b_{H-1}  \\\\\n          \\vdots & \\vdots & \\ddots & \\vdots \\\\\n          \\sum_{d=0}^{D-1} x_{N-1,d} w_{d,0} + b_0 & \n          \\sum_{d=0}^{D-1} x_{N-1,d} w_{d,1} + b_1 & \n          \\cdots & \n          \\sum_{d=0}^{D-1} x_{N-1,d} w_{d,H-1} + b_{H-1} \n      \\end{pmatrix}\n\\\\\n   &= \\begin{pmatrix}\n          y_{0,0} & y_{0,1} & \\cdots & y_{0,H-1} \\\\\n          y_{1,0} & y_{1,1} & \\cdots & y_{1,H-1} \\\\\n          \\vdots & \\vdots & \\ddots & \\vdots \\\\\n          y_{N-1,0} & y_{N-1,1} & \\cdots & y_{N-1,H-1}\n      \\end{pmatrix}\n\\end{aligned}\n\nここで、「n番目の出力データのh番目の項y_{n,h}」は、y_{n,h}\n    = \\sum_{d=0}^{D-1} x_{n,d} w_{d,h} + b_h\n\nで計算できるのが分かります。\n\n","type":"content","url":"/backpropagation#id-1","position":5},{"hierarchy":{"lvl1":"誤差逆伝播法","lvl3":"重みの勾配","lvl2":"線形変換の逆伝播の導出"},"type":"lvl3","url":"/backpropagation#id-2","position":6},{"hierarchy":{"lvl1":"誤差逆伝播法","lvl3":"重みの勾配","lvl2":"線形変換の逆伝播の導出"},"content":"\n\n例えば、損失関数に二乗誤差L = \\frac{1}{2}||y(x)-d||^2\n\nを選んだとき、連鎖律より、\\frac{\\partial L}{\\partial w_{d,h}}は次の式で求められます\\frac{\\partial L}{\\partial w_{d,h}}\n    = \\sum_{n=0}^{N-1}\n          \\frac{\\partial L}{\\partial y_{n,h}}\n          \\frac{\\partial y_{n,h}}{\\partial w_{d,h}}\n\n\\frac{\\partial L}{\\partial y_{n,h}}は、y_{n,h}に関するLの微分です。\n\n\\frac{\\partial y_{n,h}}{\\partial w_{d,h}}は、w_{d,h}に関するy_{n,h}の微分です。\n\nここで、\\frac{\\partial y_{n,h}}{\\partial w_{d,h}}は、\n\n\\begin{aligned}\n\\frac{\\partial y_{n,h}}{\\partial w_{d,h}}\n   &= \\frac{\\partial}{\\partial w_{d,h}} \\left\\{\n          \\sum_{d=0}^{D-1} x_{n,d} w_{d,h} + b_h\n      \\right\\}\n\\\\\n   &= \\frac{\\partial}{\\partial x_{n,d}} \\Bigl\\{\n          x_{n,0} w_{0,h} + \\cdots + x_{n,d} w_{d,h} + \\cdots + x_{n,D-1} w_{D-1,h} + b_h\n      \\Bigr\\}\n\\\\\n   &= 0 + \\cdots + x_{n,d} + \\cdots + 0 + 0\n\\\\\n   &= x_{n,d}\n\\end{aligned}\n\nになりますため、\\frac{\\partial L}{\\partial w_{d,h}}\n    = \\sum_{n=0}^{N-1}\n          \\frac{\\partial L}{\\partial y_{n,h}}\n          x_{n,d}\n\n","type":"content","url":"/backpropagation#id-2","position":7},{"hierarchy":{"lvl1":"誤差逆伝播法","lvl3":"バイアスの勾配","lvl2":"線形変換の逆伝播の導出"},"type":"lvl3","url":"/backpropagation#id-3","position":8},{"hierarchy":{"lvl1":"誤差逆伝播法","lvl3":"バイアスの勾配","lvl2":"線形変換の逆伝播の導出"},"content":"同じく連鎖律より、\\frac{\\partial L}{\\partial b_h}は次の式で求められます。\\frac{\\partial L}{\\partial b_h}\n    = \\sum_{n=0}^{N-1}\n          \\frac{\\partial L}{\\partial y_{n,h}}\n          \\frac{\\partial y_{n,h}}{\\partial b_h}\\begin{aligned}\n\\frac{\\partial y_{n,h}}{\\partial b_h}\n   &= \\frac{\\partial}{\\partial w_{d,h}} \\left\\{\n          \\sum_{d=0}^{D-1} x_{n,d} w_{d,h} + b_h\n      \\right\\}\n\\\\\n   &= 0 + 1\n\\\\\n   &= 1\n\\end{aligned}\n\nまとめると、\\frac{\\partial L}{\\partial b_h}\n    = \\sum_{n=0}^{N-1}\n          \\frac{\\partial L}{\\partial y_{n,h}}\n\n","type":"content","url":"/backpropagation#id-3","position":9},{"hierarchy":{"lvl1":"誤差逆伝播法","lvl2":"ニューラルネットワークにおける誤差逆伝播法の計算例"},"type":"lvl2","url":"/backpropagation#id-4","position":10},{"hierarchy":{"lvl1":"誤差逆伝播法","lvl2":"ニューラルネットワークにおける誤差逆伝播法の計算例"},"content":"連鎖律より勾配を計算する考え方をニューラルネットワークにも適用する計算例を見ましょう。\n\n具体的には、ニューラルネットワークを構成する関数が持つパラメータについての目的関数の勾配を、順伝播で通った経路を逆向きにたどるようにして途中の関数の勾配の掛け算によって求めます。\n\n補足\n\nニューラルネットワークには、活性化関数によて変換し、次の層へ伝播するといった計算の流れになりますが、逆伝播による勾配を計算できる原理は変わらないです。\n\nここから、手計算を通じて誤差逆伝播法の実装を理解しましよう。\n\n入力i_{1} = 0.05,i_{2} = 0.10\n\n初期パラメータw_{1} = 0.15,w_{2} = 0.20,w_{3} = 0.25,w_{4} = 0.30w_{5} = 0.40,w_{6} = 0.45,w_{7} = 0.50,w_{8} = 0.55\n\n活性化関数: シグモイド関数h(x)\n    = \\frac{1}{1 + \\exp(-x)}\n\n教師データo_{1} = 0.01,o_{2} = 0.99\n\n目的関数は平均二乗誤差関数を用いることにします。L = \\dfrac{1}{N} \\sum_{n=1}^{N} (t_{n} - y_{n})^2\n\n\n\nニューラルネットワークの実装例\n\n","type":"content","url":"/backpropagation#id-4","position":11},{"hierarchy":{"lvl1":"誤差逆伝播法","lvl3":"順伝播の流れ","lvl2":"ニューラルネットワークにおける誤差逆伝播法の計算例"},"type":"lvl3","url":"/backpropagation#id-5","position":12},{"hierarchy":{"lvl1":"誤差逆伝播法","lvl3":"順伝播の流れ","lvl2":"ニューラルネットワークにおける誤差逆伝播法の計算例"},"content":"\n\nimport numpy as np\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\n\n\nnet_h1= (0.15)*(0.05)+(0.2)*(0.1)+0.35\nprint(\"net_h1={}\".format(net_h1))\n\n\n\nnet_h2= (0.25)*(0.05)+(0.3)*(0.1)+0.35\nprint(\"out_h2={}\".format(net_h2))\n\n\n\nnet_o1 = (0.4)*net_h1+(0.45)*net_h2+0.6\nout_o1= sigmoid(net_o1)\nprint(\"out_o1={}\".format(out_o1))\nnet_o2 = (0.5)*net_h1+(0.55)*net_h2+0.6\nout_o2= sigmoid(net_o2)\nprint(\"out_o2={}\".format(out_o2))\n\n\n\nL_1 = 0.5 * np.square(0.01-out_o1)\nL_2 = 0.5 * np.square(0.99-out_o2)\nL = L_1+L_2\nprint(\"Loss={}\".format(L))\n\n\n\n例えば、w_5の勾配を計算する際には、\n\n\n\n誤差逆伝播法でw_5の勾配を求める\\frac{\\partial L}{\\partial w_5} = \\frac{\\partial L}{\\partial out_{o1}}\\frac{\\partial out_{o1}}{\\partial net_{o1}}\\frac{\\partial net_{o1}}{\\partial w_5}\n\n\\frac{\\partial L}{\\partial out_{o1}}を計算するL= \\frac{1}{2}(target_{o_{1}}-out_{o_{1}})^2+\\frac{1}{2}(target_{o_{2}}-out_{o_{2}})^2\n\n合成関数の微分g(f(x))= g^{\\prime}(f(x))f^{\\prime}(x)によって\\frac{\\partial L}{\\partial out_{o1}}= 2*\\frac{1}{2}(target_{o_{1}}-out_{o_{1}})*-1+0\n\nd_out_o1 = -(0.01-out_o1)\nprint(\"d_out_o1={}\".format(d_out_o1))\n\n\n\n\\frac{\\partial out_{o1}}{\\partial net_{o1}}を計算するout_{o1}= sigmod(net_{o_{1}})\n\nSigmoid関数の微分は f^{\\prime}(x)=f(x)(1-f(x)) なので\\frac{\\partial out_{o1}}{\\partial net_{o1}}= out_{o1}(1-out_{o1})\n\nシグモイド関数の勾配の証明\\begin{aligned}\n\\frac{d y}{d x}\n   &= \\frac{d}{d x} \\Bigl\\{\n          \\frac{1}{1 + \\exp(-x)}\n      \\Bigr\\}\n\\\\\n   &= - \\frac{1}{(1 + \\exp(-x))^2}\n        \\frac{d}{d x} \\Bigl\\{\n            1 + \\exp(-x)\n        \\Bigr\\}\n\\\\\n   &= - \\frac{1}{(1 + \\exp(-x))^2} \\Bigl(\n            - \\exp(-x)\n        \\Bigr)\n\\\\\n   &= \\frac{\\exp(-x)}{(1 + \\exp(-x))^2}\n\\\\\n  &= \\frac{1}{1 + \\exp(-x)}\n      \\frac{\\exp(-x)}{1 + \\exp(-x)}\n\\\\\n   &= \\frac{1}{1 + \\exp(-x)}\n      \\frac{1 + \\exp(-x) - 1}{1 + \\exp(-x)}\n\\\\\n   &= \\frac{1}{1 + \\exp(-x)} \\left(\n          \\frac{1 + \\exp(-x)}{1 + \\exp(-x)}\n          - \\frac{1}{1 + \\exp(-x)}\n      \\right)\n\\\\\n   &= y (1 - y)\n    \\end{aligned}\n\nd_net_o1 = out_o1*(1-out_o1)\nprint(\"d_net_o1={}\".format(d_net_o1))\n\n\n\n\\frac{\\partial net_{o1}}{\\partial w_5}を計算するnet_{o_{1}}=w_{5}*net_{h_{1}}+w_{6}*net_{h_{2}}+b_{2}*1\\frac{\\partial net_{o1}}{\\partial w_5}= net_{h_{1}}\n\nd_w5= d_out_o1*d_net_o1*net_h1\nprint(\"d_w5={}\".format(d_w5))\n\n\n\nパラメータを更新するw_5^+ = w_{5}- \\eta \\frac{\\partial {L}}{\\partial w_5}\n\n","type":"content","url":"/backpropagation#id-5","position":13},{"hierarchy":{"lvl1":"誤差逆伝播法","lvl3":"多層ネットワークへの一般化","lvl2":"ニューラルネットワークにおける誤差逆伝播法の計算例"},"type":"lvl3","url":"/backpropagation#id-6","position":14},{"hierarchy":{"lvl1":"誤差逆伝播法","lvl3":"多層ネットワークへの一般化","lvl2":"ニューラルネットワークにおける誤差逆伝播法の計算例"},"content":"\n\n以上の計算例には、２層ネットワークの場合の損失関数の勾配を誤差逆伝播法で計算できました。各層での勾配計算がチェーンルールに従っているため、層数が増えても理論的には同じ流れで処理できますため、任意の層数のネットワークに拡張することができます。\n\n入力：学習サンプルx_nおよび目標出力d_nのベア\n\n出力：損失関数L_n(w)の各層lのパラメータについての微分\\frac{\\partial {L}}{\\partial w_{ji}^{l}}\n\n順伝播: 入力データが層を通過して出力に達するまで順番に計算が行われます。\n\n損失の計算: ネットワークの最終出力（予測値）と実際のラベルとの誤差（損失）を損失関数で計算します (\\delta_j^{L}=z_j-d_j)\n\n逆伝播: 各隠れ層では、l(=L-1, L-2,...)での\\delta^{(l)}を、\\delta^{(l)}= (\\frac{\\partial z^{(l+1)}} {\\partial z^{(l)}} \\cdot \\delta^{(l+1)}) \\cdot f'(z^{l})のように、次の層からの勾配を逆伝播させ、チェーンルールを使って計算します。\n\nパラメータの更新:逆伝播によって計算された勾配\\delta^{(l)}を用いて、各層の重みとバイアスを更新します。","type":"content","url":"/backpropagation#id-6","position":15},{"hierarchy":{"lvl1":"BERTによるセンチメント分析"},"type":"lvl1","url":"/bert-sentiment","position":0},{"hierarchy":{"lvl1":"BERTによるセンチメント分析"},"content":"","type":"content","url":"/bert-sentiment","position":1},{"hierarchy":{"lvl1":"BERTによるセンチメント分析","lvl2":"転移学習とファインチューニング"},"type":"lvl2","url":"/bert-sentiment#id","position":2},{"hierarchy":{"lvl1":"BERTによるセンチメント分析","lvl2":"転移学習とファインチューニング"},"content":"転移学習は、あるタスクの学習で得られた知識を、他の関連するタスクの学習に適用する手法を指します。一般的には、以下のステップで行われることが多いです：\n\n事前学習: 事前学習モデル（pre-trained models)とは、大規模なデータセットを用いて訓練した学習済みモデルのことです。一般的に、大量のデータ（例えば、インターネット上のテキストデータ）を使用して、モデルを事前に学習します。この時点でのモデルは、言語の汎用的な特徴や構造を捉えることができます。\n\nファインチューニング(fine-tuning): 事前学習モデルを、特定のタスクのデータ（例えば、感情分析や質問応答）でファインチューニングします。事前学習モデルでは汎用的な特徴をあらかじめ学習しておきますので、手元にある学習データが小規模でも高精度な認識性能を達成することが知られています。\n\n","type":"content","url":"/bert-sentiment#id","position":3},{"hierarchy":{"lvl1":"BERTによるセンチメント分析","lvl2":"センチメント分析の実装"},"type":"lvl2","url":"/bert-sentiment#id-1","position":4},{"hierarchy":{"lvl1":"BERTによるセンチメント分析","lvl2":"センチメント分析の実装"},"content":"\n\nGoogle ColabdでGPUを使用する\n\n「ランタイム」→「ランタイムのタイプを変更」→「ハードウェア アクセラレータ」→「GPU」\n\n!nvidia-smi\n\n\n\n","type":"content","url":"/bert-sentiment#id-1","position":5},{"hierarchy":{"lvl1":"BERTによるセンチメント分析","lvl3":"データセット","lvl2":"センチメント分析の実装"},"type":"lvl3","url":"/bert-sentiment#id-2","position":6},{"hierarchy":{"lvl1":"BERTによるセンチメント分析","lvl3":"データセット","lvl2":"センチメント分析の実装"},"content":"\n\n","type":"content","url":"/bert-sentiment#id-2","position":7},{"hierarchy":{"lvl1":"BERTによるセンチメント分析","lvl4":"Hugging Faceからサンプルデータの取得","lvl3":"データセット","lvl2":"センチメント分析の実装"},"type":"lvl4","url":"/bert-sentiment#hugging-face","position":8},{"hierarchy":{"lvl1":"BERTによるセンチメント分析","lvl4":"Hugging Faceからサンプルデータの取得","lvl3":"データセット","lvl2":"センチメント分析の実装"},"content":"Hugging Faceのには色々なデータセットが用意されております。ここでは、多言語のセンチメントデータセットを例として使用することにします。その中に、英語と日本語のサプセットが含まれます。\n\n!pip -q install \"datasets<4.0.0\"\n\n\n\nfrom datasets import load_dataset\n#dataset = load_dataset(\"tyqiangz/multilingual-sentiments\", \"japanese\")\ndataset = load_dataset(\"tyqiangz/multilingual-sentiments\", \"english\")\n\n\n\n","type":"content","url":"/bert-sentiment#hugging-face","position":9},{"hierarchy":{"lvl1":"BERTによるセンチメント分析","lvl4":"Weights & Biases (wandb)","lvl3":"データセット","lvl2":"センチメント分析の実装"},"type":"lvl4","url":"/bert-sentiment#weights-biases-wandb","position":10},{"hierarchy":{"lvl1":"BERTによるセンチメント分析","lvl4":"Weights & Biases (wandb)","lvl3":"データセット","lvl2":"センチメント分析の実装"},"content":"Weights & Biases (wandb)は、機械学習の実験を追跡・可視化・管理するためのプラットフォームです。\n\n主な機能：\n\n実験追跡: 学習の進捗、損失、精度などのメトリクスをリアルタイムで記録・可視化\n\nハイパーパラメータ管理: 異なる設定での実験結果を比較\n\nモデル管理: 学習済みモデルのバージョン管理と共有\n\nここでは、モデルの学習過程を追跡するためにwandbを使用します。wandb.login()でアカウントにログインし、実験結果をクラウドに保存します。\n\n!pip install wandb\nimport wandb\nwandb.login()\n\n\n\n\n\n\n\nimport os\nos.environ[\"WANDB_PROJECT\"]=\"sentiment_analysis\"\n\n\n\n","type":"content","url":"/bert-sentiment#weights-biases-wandb","position":11},{"hierarchy":{"lvl1":"BERTによるセンチメント分析","lvl4":"サンプルデータの確認","lvl3":"データセット","lvl2":"センチメント分析の実装"},"type":"lvl4","url":"/bert-sentiment#id-3","position":12},{"hierarchy":{"lvl1":"BERTによるセンチメント分析","lvl4":"サンプルデータの確認","lvl3":"データセット","lvl2":"センチメント分析の実装"},"content":"取得したデータセットの中身を確認します。\n\nデータセットはこのようにtrain, validation, testに分かれています。\n[‘text’, ‘source’, ‘label’]といった情報を持っています。\n\ndataset\n\n\n\ndataset.set_format(type=\"pandas\")\ntrain_df = dataset[\"train\"][:]\ntrain_df.head(5)\n\n\n\ndataset[\"train\"].features\n\n\n\nimport matplotlib.pyplot as plt\ntrain_df[\"label\"].value_counts(ascending=True).plot(kind=\"barh\", title=\"Train Dataset\")\n\n\n\n\n\n","type":"content","url":"/bert-sentiment#id-3","position":13},{"hierarchy":{"lvl1":"BERTによるセンチメント分析","lvl4":"テキストの確認","lvl3":"データセット","lvl2":"センチメント分析の実装"},"type":"lvl4","url":"/bert-sentiment#id-4","position":14},{"hierarchy":{"lvl1":"BERTによるセンチメント分析","lvl4":"テキストの確認","lvl3":"データセット","lvl2":"センチメント分析の実装"},"content":"Transformerモデルは、最大コンテキストサイズ(maximum context size)と呼ばれる最大入力系列長があります。\n\nモデルのコンテキストサイズより長いテキストは切り捨てる必要があり、切り捨てたテキストに重要な情報が含まれている場合、性能の低下につながることがあります。\n\ntrain_df[\"text_length\"]=train_df[\"text\"].str.len()\n\n\n\ntrain_df.boxplot(column=\"text_length\", by=\"label\", figsize=(12, 6))\n\n\n\n\n\n\n","type":"content","url":"/bert-sentiment#id-4","position":15},{"hierarchy":{"lvl1":"BERTによるセンチメント分析","lvl3":"トークン化","lvl2":"センチメント分析の実装"},"type":"lvl3","url":"/bert-sentiment#id-5","position":16},{"hierarchy":{"lvl1":"BERTによるセンチメント分析","lvl3":"トークン化","lvl2":"センチメント分析の実装"},"content":"コンピュータは、入力として生の文字列を受け取ることができません。その代わりに、テキストがトークン化され、数値ベクトルとしてエンコードされていることが想定しています。\n\nトークン化は、文字列をモデルで使用される最小単位に分解するステップです。\n\nTransformerライブラリー は便利なAutoTokenizerクラスを提供しており、事前学習済みモデルに関連つけられたトークナイザーを素早く使用することができます。\n\n","type":"content","url":"/bert-sentiment#id-5","position":17},{"hierarchy":{"lvl1":"BERTによるセンチメント分析","lvl4":"トークナイザの動作確認","lvl3":"トークン化","lvl2":"センチメント分析の実装"},"type":"lvl4","url":"/bert-sentiment#id-6","position":18},{"hierarchy":{"lvl1":"BERTによるセンチメント分析","lvl4":"トークナイザの動作確認","lvl3":"トークン化","lvl2":"センチメント分析の実装"},"content":"\n\ntokenizerテキストを数値形式（トークン）に変換します。\n\n入力テキストをトークンに分割します\n\n特殊トークンが自動的に付加されます\n\nトークンをトークンIDに変換します\n\nfrom transformers import AutoTokenizer\nmodel_ckpt = \"distilbert-base-uncased\"\ntokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n\n\n\ntrain_df[\"text\"][0]\n\n\n\nsample_text_encoded = tokenizer(train_df[\"text\"][0])\nsample_text_encoded\n\n\n\n結果にinput_idsとattention_maskが含まれます。\n\ninput_ids: 数字にエンコードされたトークン\n\nattention_mask: モデルで有効なトークンかどうかを判別するためのマスクです。無効なトークン（例えば、PADなど）に対しては、attention_maskを\nとして処理します。\n\n各batchにおいて、入力系列はbatch内最大系列長までpaddingされます。\n\nトークナイザの結果は数字にエンコードされているため、トークン文字列を得るには、convert_ids_to_tokensを用います。\n\n文の開始が[CLS]、文の終了が[SEP]という特殊なトークンとなっています。\n\ntokens = tokenizer.convert_ids_to_tokens(sample_text_encoded.input_ids)\nprint(tokens)\n\n\n\n","type":"content","url":"/bert-sentiment#id-6","position":19},{"hierarchy":{"lvl1":"BERTによるセンチメント分析","lvl4":"データセット全体のトークン化","lvl3":"トークン化","lvl2":"センチメント分析の実装"},"type":"lvl4","url":"/bert-sentiment#id-7","position":20},{"hierarchy":{"lvl1":"BERTによるセンチメント分析","lvl4":"データセット全体のトークン化","lvl3":"トークン化","lvl2":"センチメント分析の実装"},"content":"\n\ndef tokenize(batch):\n    return tokenizer(batch[\"text\"], padding=True, truncation=True)\n\n\n\ndataset.reset_format()\n\n\n\ndataset_encoded = dataset.map(tokenize, batched=True, batch_size=None)\n\n\n\nimport pandas as pd\nsample_encoded = dataset_encoded[\"train\"][0]\npd.DataFrame(\n    [sample_encoded[\"input_ids\"]\n     , sample_encoded[\"attention_mask\"]\n     , tokenizer.convert_ids_to_tokens(sample_encoded[\"input_ids\"])],\n    ['input_ids', 'attention_mask', \"tokens\"]\n).T\n\n\n\n","type":"content","url":"/bert-sentiment#id-7","position":21},{"hierarchy":{"lvl1":"BERTによるセンチメント分析","lvl3":"分類器の実装","lvl2":"センチメント分析の実装"},"type":"lvl3","url":"/bert-sentiment#id-8","position":22},{"hierarchy":{"lvl1":"BERTによるセンチメント分析","lvl3":"分類器の実装","lvl2":"センチメント分析の実装"},"content":"","type":"content","url":"/bert-sentiment#id-8","position":23},{"hierarchy":{"lvl1":"BERTによるセンチメント分析","lvl4":"事前学習モデルの導入","lvl3":"分類器の実装","lvl2":"センチメント分析の実装"},"type":"lvl4","url":"/bert-sentiment#id-9","position":24},{"hierarchy":{"lvl1":"BERTによるセンチメント分析","lvl4":"事前学習モデルの導入","lvl3":"分類器の実装","lvl2":"センチメント分析の実装"},"content":"Transformerライブラリは事前学習モデルの使用ためAutoModelクラスを提供します。\n\nAutoModelクラスはトークンエンコーディングを埋め込みに変換し、エンコーダスタックを経由して最後の隠れ状態を返します。\n\nimport torch\nfrom transformers import AutoModel\n\n# GPUある場合はGPUを使う\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = AutoModel.from_pretrained(model_ckpt).to(device)\n\n\n\n最初に、文字列をエンコーダしてトークンをPyTorchのテンソルに変換する必要があります。\n\n結果として得られるテンソルは[batch_size,n_tokens]という形状です。\n\ntext = \"this is a test\"\ninputs = tokenizer(text, return_tensors=\"pt\")\nprint(f\"Input tensor shape: {inputs['input_ids'].size()}\")\n\n\n\n得られるテンソルをモデルの入力として渡します。\n\nモデルと同じデバイス(GPU or CPU)に設置します。\n\n計算のメモリを減らせるため、torch.no_grad()で、勾配の自動計算を無効します。\n\n出力には隠れ状態、損失、アテンションのオブジェクトが含まれます。\n\ninputs = {k:v.to(device) for k,v in inputs.items()}\nwith torch.no_grad():\n    outputs = model(**inputs)\nprint(outputs)\n\n\n\n隠れた状態テンソルを見ると、その形状は [batch_size, n_tokens, hidden_dim] であることがわかります。つまり、6つの入力トークンのそれぞれに対して、768次元のベクトルが返されます。\n\noutputs.last_hidden_state.size()\n\n\n\n分類タスクでは、[CLS] トークンに関連する隠れた状態を入力特徴として使用するのが一般的な方法です。このトークンは各シーケンスの始まりに現れるため、次のように outputs.last_hidden_state に単純にインデックスを付けることで抽出できます。\n\noutputs.last_hidden_state[:,0].size()\n\n\n\n\n最後の隠れ状態を取得する方法がわかりましたので、データ全体に対して処理を行うため、これまでのステップを関数でまとめます。\n\nそして、データ全体に適用し、すべてのテキストの隠れ状態を抽出します。\n\ndef extract_hidden_states(batch):\n    # Place model inputs on the GPU\n    inputs = {k:v.to(device) for k,v in batch.items() \n              if k in tokenizer.model_input_names}\n    # Extract last hidden states\n    with torch.no_grad():\n        last_hidden_state = model(**inputs).last_hidden_state\n    # Return vector for [CLS] token\n    return {\"hidden_state\": last_hidden_state[:,0].cpu().numpy()}\n\n\n\ndataset_encoded.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\",\"label\"])\n\n\n\n\ndataset_hidden=dataset_encoded.map(extract_hidden_states, batched=True, batch_size=16)\n\n\n\n","type":"content","url":"/bert-sentiment#id-9","position":25},{"hierarchy":{"lvl1":"BERTによるセンチメント分析","lvl4":"AutoModelForSequenceClassificationのファインチューニング","lvl3":"分類器の実装","lvl2":"センチメント分析の実装"},"type":"lvl4","url":"/bert-sentiment#automodelforsequenceclassification","position":26},{"hierarchy":{"lvl1":"BERTによるセンチメント分析","lvl4":"AutoModelForSequenceClassificationのファインチューニング","lvl3":"分類器の実装","lvl2":"センチメント分析の実装"},"content":"transformerライブラリは、ファインチューニングのタスクに応じてAPIを提供しています。\n\n分類タスクの場合、AutoModelの代わりにAutoModelForSequenceClassificationを使用します。\n\nAutoModelForSequenceClassificationが事前学習済みモデルの出力の上に分類器ヘッドを持っており、モデルの設定がより簡単になります。\n\nfrom transformers import AutoModelForSequenceClassification\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nnum_labels = 3\n\nmodel = (AutoModelForSequenceClassification\n    .from_pretrained(model_ckpt, num_labels=num_labels)\n    .to(device))\n\n\n\nmodel\n\n\n\ninputs = tokenizer(\"I purchased these boots to use both for everyday wear and when riding my motorcycle.\", return_tensors=\"pt\") # pytorch tensorに変換するためにreturn_tensors=\"pt\"を指定\ninputs = {k: v.to(device) for k, v in inputs.items()}\nwith torch.no_grad():\n    outputs = model(**inputs)\nprint(outputs)\n\n\n\n","type":"content","url":"/bert-sentiment#automodelforsequenceclassification","position":27},{"hierarchy":{"lvl1":"BERTによるセンチメント分析","lvl4":"学習の準備","lvl3":"分類器の実装","lvl2":"センチメント分析の実装"},"type":"lvl4","url":"/bert-sentiment#id-10","position":28},{"hierarchy":{"lvl1":"BERTによるセンチメント分析","lvl4":"学習の準備","lvl3":"分類器の実装","lvl2":"センチメント分析の実装"},"content":"学習時に性能指標を与える必要があるため、それを関数化して定義しておきます。\n\nfrom sklearn.metrics import accuracy_score, f1_score\n\ndef compute_metrics(pred):\n    labels = pred.label_ids\n    preds = pred.predictions.argmax(-1)\n    f1 = f1_score(labels, preds, average=\"weighted\")\n    acc = accuracy_score(labels, preds)\n    return {\"accuracy\": acc, \"f1\": f1}\n\n\n\n学習を効率化するために、transformerライブラリのTrainer APIを使用します。\n\nTrainerクラスを初期化する際には、TrainingArgumentsという訓練に関する様々な設定値の集合を引数に与えることで、訓練の設定に関する細かい調整が可能です。\n\n\nfrom transformers import TrainingArguments\n\nbatch_size = 16\nlogging_steps = len(dataset_encoded[\"train\"]) // batch_size\nmodel_name = \"sample-text-classification-bert\"\n\ntraining_args = TrainingArguments(\n    output_dir=model_name,\n    num_train_epochs=2,\n    learning_rate=2e-5,\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n    weight_decay=0.01,\n    eval_strategy=\"epoch\",\n    disable_tqdm=False,\n    logging_steps=logging_steps,\n    push_to_hub=False,\n    log_level=\"error\",\n    report_to=\"wandb\",  # wandbに記録\n    run_name=\"bert-sentiment-analysis_20260107\"  # wandbのrun名\n)\n\n\n\nTrainerクラスで実行します。\n\nCUDAエラーのデバッグについて:\n\nCUDA_LAUNCH_BLOCKING=1: GPU操作を同期実行にし、エラーの正確な発生箇所を特定\n\nTORCH_USE_CUDA_DSA: デバイス側のアサーションを有効化してより詳細なエラー情報を取得\n\nモデルとデータの整合性を事前に確認することで、ラベル数のミスマッチなどの問題を早期発見\n\nfrom transformers import Trainer\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    compute_metrics=compute_metrics,\n    train_dataset=dataset_encoded[\"train\"],\n    eval_dataset=dataset_encoded[\"validation\"],\n    tokenizer=tokenizer\n)\ntrainer.train()\n\n\n\n","type":"content","url":"/bert-sentiment#id-10","position":29},{"hierarchy":{"lvl1":"BERTによるセンチメント分析","lvl3":"学習済みモデルの使用","lvl2":"センチメント分析の実装"},"type":"lvl3","url":"/bert-sentiment#id-11","position":30},{"hierarchy":{"lvl1":"BERTによるセンチメント分析","lvl3":"学習済みモデルの使用","lvl2":"センチメント分析の実装"},"content":"","type":"content","url":"/bert-sentiment#id-11","position":31},{"hierarchy":{"lvl1":"BERTによるセンチメント分析","lvl4":"モデル精度の検証","lvl3":"学習済みモデルの使用","lvl2":"センチメント分析の実装"},"type":"lvl4","url":"/bert-sentiment#id-12","position":32},{"hierarchy":{"lvl1":"BERTによるセンチメント分析","lvl4":"モデル精度の検証","lvl3":"学習済みモデルの使用","lvl2":"センチメント分析の実装"},"content":"学習済みのモデルを他のデータセットに適用します。\n\npreds_output = trainer.predict(dataset_encoded[\"test\"])\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n\ny_preds = np.argmax(preds_output.predictions, axis=1)\ny_valid = np.array(dataset_encoded[\"test\"][\"label\"])\nlabels = dataset_encoded[\"train\"].features[\"label\"].names\n\ndef plot_confusion_matrix(y_preds, y_true, labels):\n    cm = confusion_matrix(y_true, y_preds, normalize=\"true\")\n    fig, ax = plt.subplots(figsize=(6, 6))\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n    disp.plot(cmap=\"Blues\", values_format=\".2f\", ax=ax, colorbar=False)\n    plt.title(\"Normalized confusion matrix\")\n    plt.show()\n\nplot_confusion_matrix(y_preds, y_valid, labels)\n\n\n\n","type":"content","url":"/bert-sentiment#id-12","position":33},{"hierarchy":{"lvl1":"BERTによるセンチメント分析","lvl4":"モデル保存","lvl3":"学習済みモデルの使用","lvl2":"センチメント分析の実装"},"type":"lvl4","url":"/bert-sentiment#id-13","position":34},{"hierarchy":{"lvl1":"BERTによるセンチメント分析","lvl4":"モデル保存","lvl3":"学習済みモデルの使用","lvl2":"センチメント分析の実装"},"content":"\n\nid2label = {}\nfor i in range(dataset[\"train\"].features[\"label\"].num_classes):\n    id2label[i] = dataset[\"train\"].features[\"label\"].int2str(i)\n\nlabel2id = {}\nfor i in range(dataset[\"train\"].features[\"label\"].num_classes):\n    label2id[dataset[\"train\"].features[\"label\"].int2str(i)] = i\n\ntrainer.model.config.id2label = id2label\ntrainer.model.config.label2id = label2id\n\n\n\ntrainer.save_model(f\"./Data/sample-text-classification-bert\")\n\n\n\n","type":"content","url":"/bert-sentiment#id-13","position":35},{"hierarchy":{"lvl1":"BERTによるセンチメント分析","lvl4":"学習済みモデルの読み込み","lvl3":"学習済みモデルの使用","lvl2":"センチメント分析の実装"},"type":"lvl4","url":"/bert-sentiment#id-14","position":36},{"hierarchy":{"lvl1":"BERTによるセンチメント分析","lvl4":"学習済みモデルの読み込み","lvl3":"学習済みモデルの使用","lvl2":"センチメント分析の実装"},"content":"\n\nnew_tokenizer = AutoTokenizer\\\n    .from_pretrained(f\"./Data/sample-text-classification-bert\")\n\nnew_model = (AutoModelForSequenceClassification\n    .from_pretrained(f\"./Data/sample-text-classification-bert\")\n    .to(device))\n\n\n\nサンプルテキストで推論の結果を確認します。\n\ndef id2label(x):\n    label_dict={0:\"positive\",1:\"neutral\",2:\"negative\"}\n    return label_dict[x]\n\n\n\ntext1=\"this week is not going as i had hoped\"\ntext2=\"awe i love you too!!!! 1 am here i miss you\"\n\n\n\n\ninputs = new_tokenizer(text1, return_tensors=\"pt\")\n\nnew_model.eval()\n\nwith torch.no_grad():\n    outputs = new_model(\n        inputs[\"input_ids\"].to(device), \n        inputs[\"attention_mask\"].to(device),\n    )\noutputs.logits\n\ny_preds = np.argmax(outputs.logits.to('cpu').detach().numpy().copy(), axis=1)\ny_preds = [id2label(x) for x in y_preds]\ny_preds\n\n\n\ninputs = new_tokenizer(text2, return_tensors=\"pt\")\n\nnew_model.eval()\n\nwith torch.no_grad():\n    outputs = new_model(\n        inputs[\"input_ids\"].to(device), \n        inputs[\"attention_mask\"].to(device),\n    )\noutputs.logits\n\ny_preds = np.argmax(outputs.logits.to('cpu').detach().numpy().copy(), axis=1)\ny_preds = [id2label(x) for x in y_preds]\ny_preds\n\n","type":"content","url":"/bert-sentiment#id-14","position":37},{"hierarchy":{"lvl1":"BERTopic"},"type":"lvl1","url":"/bert-topic","position":0},{"hierarchy":{"lvl1":"BERTopic"},"content":"\n\n社会科学において、定量的な手法によってテキストの内容や構造を明らかにすることは重要な課題である。このような目的を達成するため、トピックモデルがよく用いられます。\n\n一般的には、トピックモデルは、テキストデータを構成する各文書の背後にはトピックと呼ばれる語の集合が存在し、それに基づいて各文書が生成されるという仮定から出発します。そして、そのトピックを抽出することにより、テキストデータ全体の傾向を要約することを可能にするモデルです。また、各文書が、どのトピックから生成されているかという点についても示すことが可能となっています。 代表的なモデルとして、Latent Dirichlet allocation (LDA) が挙げられます。\n\nこのような確率的モデルに対して、文章の分散表現を使用してトピックの抽出に文章のコンテキストまで活用しようとする研究が行われています。ここでは、\n\nBERTopicという手法を解説と実装します\n\n","type":"content","url":"/bert-topic","position":1},{"hierarchy":{"lvl1":"BERTopic","lvl2":"BERTopicの基本概念"},"type":"lvl2","url":"/bert-topic#bertopic","position":2},{"hierarchy":{"lvl1":"BERTopic","lvl2":"BERTopicの基本概念"},"content":"BERTopicのトピック抽出方法は要約すると以下の流れになリます。\n\n学習済みモデルで文書の埋め込みを獲得\n\nBERTopicに使われる習済みモデルは、言語モデルの発展に伴って最先端のモデルを利用することもできます\n\n次元削減の手法で文書の埋め込みの次元数を圧縮\n\n埋め込みベクトルは高次元であるため、より扱いやすい低次元の空間に変換するために次元削減技術を適用します。このステップは、埋め込みベクトルの本質的な特徴を保持しつつ、計算量を減らすことができます\n\n圧縮された文章ベクトルをクラスタリング\n\n類似したテキストやトピックを共通のグループに分類することができます。\n\n(c-TF-IDF による)トピックの代表単語の抽出\n\n各クラスタに対して、それを最もよく表現する単語やフレーズを選択します。これにより、各クラスタを「トピック」として識別し、それぞれのトピックがどのような内容を含んでいるかを理解することができます。\n\nBERTopic では、クラスタ内の単語の重要度を知るために TF-IDF を応用してその重要度を算出しています。\n\nクラスタに含まれる文書を全て結合し 1つの文章として扱い、クラスタ単位の TF-IDF (class-based TF-IDF と呼んでいる) を以下のように計算して、それぞれの単語の重要度としています。\n\nクラスタc内の単語tのclass-based TF-IDF W_{t,c}は以下のようにで得られますW_{t, c} = tf_{t,c} \\cdot \\log \\left( 1 + \\frac{A}{f_t} \\right)\n\ntf_{t,c}: クラスタc内の単語tの単語頻度\n\nf_t: 全クラスタに含まれる単語tの単語頻度\n\nA: クラスタあたりの平均単語数\n\n#!pip install bertopic\n\n\n\n","type":"content","url":"/bert-topic#bertopic","position":3},{"hierarchy":{"lvl1":"BERTopic","lvl2":"BERTopicの実装"},"type":"lvl2","url":"/bert-topic#bertopic-1","position":4},{"hierarchy":{"lvl1":"BERTopic","lvl2":"BERTopicの実装"},"content":"\n\n","type":"content","url":"/bert-topic#bertopic-1","position":5},{"hierarchy":{"lvl1":"BERTopic","lvl4":"サンプルデータ","lvl2":"BERTopicの実装"},"type":"lvl4","url":"/bert-topic#id","position":6},{"hierarchy":{"lvl1":"BERTopic","lvl4":"サンプルデータ","lvl2":"BERTopicの実装"},"content":"\n\nfrom sklearn.datasets import fetch_20newsgroups\ndocs = fetch_20newsgroups(subset='all',  remove=('headers', 'footers', 'quotes'))['data']\n\n\n\ndocs[:5]\n\n\n\n","type":"content","url":"/bert-topic#id","position":7},{"hierarchy":{"lvl1":"BERTopic","lvl4":"モデルの学習","lvl2":"BERTopicの実装"},"type":"lvl4","url":"/bert-topic#id-1","position":8},{"hierarchy":{"lvl1":"BERTopic","lvl4":"モデルの学習","lvl2":"BERTopicの実装"},"content":"\n\nlanguageで学習済みモデルの対応言語を選定します。\n\nenglishの場合、all-MiniLM-L6-v2が使われます。\n\nmultilingual、paraphrase-multilingual-MiniLM-L12-v2が使われます。\n\nドキュメント集合（docs）を使ってトピックモデルを訓練し、ドキュメントごとのトピックとその確率を取得します。\n\nfrom bertopic import BERTopic\n\ntopic_model = BERTopic(language=\"english\", calculate_probabilities=True, verbose=True)\ntopics, probs = topic_model.fit_transform(docs)\n\n\n\n","type":"content","url":"/bert-topic#id-1","position":9},{"hierarchy":{"lvl1":"BERTopic","lvl4":"トピックの抽出","lvl2":"BERTopicの実装"},"type":"lvl4","url":"/bert-topic#id-2","position":10},{"hierarchy":{"lvl1":"BERTopic","lvl4":"トピックの抽出","lvl2":"BERTopicの実装"},"content":"\n\nfreq = topic_model.get_topic_info(); freq.head(5)\n\n\n\nTopic：トピックのID。-1は、特定のトピックに分類されなかったドキュメントを表します。\n\nCount：そのトピックに分類されたドキュメントの数。\n\nName：トピックの名前。これは、そのトピックの主要な単語から生成されます。\n\nRepresentation：そのトピックを表現する主要な単語のリスト。\n\nRepresentative_Docs：そのトピックに関連する代表的なドキュメントの一部。\n\ntopic_model.get_topic(0)  # Select the most frequent topic\n\n\n\nトピックには、様々な属性を呼び出すメソッドが実装されています。\n\nAttribute\n\nDescription\n\ntopics_\n\nThe topics that are generated for each document after training or updating the topic model.\n\nprobabilities_\n\nThe probabilities that are generated for each document if HDBSCAN is used.\n\ntopic_sizes_\n\nThe size of each topic\n\ntopic_mapper_\n\nA class for tracking topics and their mappings anytime they are merged/reduced.\n\ntopic_representations_\n\nThe top n terms per topic and their respective c-TF-IDF values.\n\nc_tf_idf_\n\nThe topic-term matrix as calculated through c-TF-IDF.\n\ntopic_labels_\n\nThe default labels for each topic.\n\ncustom_labels_\n\nCustom labels for each topic as generated through .set_topic_labels.\n\ntopic_embeddings_\n\nThe embeddings for each topic if embedding_model was used.\n\nrepresentative_docs_\n\nThe representative documents for each topic if HDBSCAN is used.\n\n","type":"content","url":"/bert-topic#id-2","position":11},{"hierarchy":{"lvl1":"BERTopic","lvl4":"トピックモデルの可視化","lvl2":"BERTopicの実装"},"type":"lvl4","url":"/bert-topic#id-3","position":12},{"hierarchy":{"lvl1":"BERTopic","lvl4":"トピックモデルの可視化","lvl2":"BERTopicの実装"},"content":"\n\nvisualize_topicsメソッドを呼び出すと、各トピックが2次元空間上にプロットされ、類似のトピックが互いに近くに配置されます。これにより、トピック間の関係と各トピックの重要性（サイズによって示される）を視覚的に理解することができます。\n\n#!pip install --upgrade nbformat\ntopic_model.visualize_topics()\n\n\n\n\n\nトピックの階層関係を確認することができます。\n\ntopic_model.visualize_hierarchy(top_n_topics=50)\n\n\n\n特定のトピック内での用語のc-TF-IDFスコアを表します。スコアが高いほど、その用語はそのトピックにとってより関連性が高い、または特徴的であることを示します。\n\n異なるトピックのバーチャートを比較することで、トピックがその主要用語でどのように異なるかを見ることができます。\n\ntopic_model.visualize_barchart(top_n_topics=5)\n\n\n\n","type":"content","url":"/bert-topic#id-3","position":13},{"hierarchy":{"lvl1":"BERTopic","lvl3":"Topic Representation","lvl2":"BERTopicの実装"},"type":"lvl3","url":"/bert-topic#topic-representation","position":14},{"hierarchy":{"lvl1":"BERTopic","lvl3":"Topic Representation","lvl2":"BERTopicの実装"},"content":"学習されたトピックモデルの「性能」を向上させるために、追加の最適化ステップが行われます。\n\nupdate_topicsメソッドで、既存のトピックモデルが指定するドキュメント集合（docs）に基づいて更新されます。ここでは、n_gram_range=(1, 2)と指定し、モデルが考慮する単語の組み合わせの範囲を1から2に設定することを意味します。つまり、単語単体（1-gram）と単語のペア（2-gram）が考慮されます。\n\ntopic_model.update_topics(docs, n_gram_range=(1, 2))\n\n\n\ntopic_model.get_topic(0)   # We select topic that we viewed before\n\n\n\nreduce_topicsでトピックモデルのトピック数を減らします。トピックモデルが生成するトピックの数を制御するための方法で、トピックの数が多すぎて解釈が難しい場合や、トピック間の区別が不明瞭な場合に有用です。\n\ntopic_model.reduce_topics(docs, nr_topics=60)\n\n\n\n\n\nfind_topicsメソッドで、指定したキーワードに最も関連性の高いトピックを検索します。このメソッドを呼び出すと、指定したキーワードに最も関連性の高いトピックのIDとその関連性のスコアが返されます。\n\nsimilar_topics, similarity = topic_model.find_topics(\"vehicle\", top_n=5); similar_topics\n\n\n\ntopic_model.get_topic(3)\n\n","type":"content","url":"/bert-topic#topic-representation","position":15},{"hierarchy":{"lvl1":"ガイダンス"},"type":"lvl1","url":"/introduction","position":0},{"hierarchy":{"lvl1":"ガイダンス"},"content":"","type":"content","url":"/introduction","position":1},{"hierarchy":{"lvl1":"ガイダンス","lvl2":"自然言語処理"},"type":"lvl2","url":"/introduction#id","position":2},{"hierarchy":{"lvl1":"ガイダンス","lvl2":"自然言語処理"},"content":"自然言語処理（NLP: Natural Language Processing）は、人間が日常的に使っている自然言語をコンピュータに処理させる一連の技術であり、人工知能（AI）の研究分野で中核を成す要素技術の一つといえます。\n\n私たちは普段、自分たちの言語の複雑さについて考えることはありません。人間にとっては、言語は歩くのと同じように、訓練された反復可能な行動であるため、習得しやすく、青年期にはより自然に使用できるようになると言われています。ただ、人間にとって自然なことでも、大量の非構造化データを処理し、正式なルールがないばかりか、現実世界のコンテキストや意図もないコンピューターにとっては、それを成すことは非常に困難です。\n\nなぜ自然言語処理は難しいのか\n\n私たちは普段、自分たちの言語の複雑さについて考えることはありません。人間にとっては、言語は歩くのと同じように、訓練された反復可能な行動であるため、習得しやすく、青年期にはより自然に使用できるようになると言われています。ただ、人間にとって自然なことでも、大量の非構造化データを処理し、正式なルールがないばかりか、現実世界のコンテキストや意図もないコンピューターにとっては、それを成すことは非常に困難です。\n\n近年、自然言語処理技術の急速な進歩に驚きの声が上がっていました。\n\n実際、自然言語処理において昨今の支配的な手法は、隠れマルコフモデル(HMM)、線型サポートベクトルマシン(SVM)やロジスティック回帰など統計的機械学習(statistical machine learning)に基づいていました。\n\n2014年頃、この分野において、\n\nニューラルネットワークという技術が導入され、多くのタスクにより高い性能を達成できました。さらに、これに基づいて、より先進的なモデリング方法の開発も進めました。特に、\n\n再帰的ニューラルネットワーク(RNN)に基づく方法は言語の時系列性質も学習できるになって、様々なタスクにおいて精度向上に大きく貢献しました。\n\n2018年にGoogleが発表した\n\nBERTというモデルでは、大規模なテキストで一般的な言語パターンや文脈を学習します。その後、この事前学習済みモデルを特定のNLPタスクに\n\nファインチューニングすることで、少ないラベル付きデータで高性能を発揮することができます。\n\nそれだけでも驚きでしたが、一般人の中でも話題になる「ChatGPT」をはじめとする生成AIの\n\n大規模な言語モデル(LLM:Large Language Model)の進展により、質問への回答、文章の要約や翻訳、ソフトウエアのプログラミングなど、言語に関わるさまざまなタスクができるようになりました。\n\n高機能化のカギは、\n\n深層学習技術の発展があります。深層学習を用いた自然言語処理には、あらかじめ用意した膨大な文章を使って、\n\n「言語モデル」と呼ばれるシステムを学習させる方法があります。\n言語モデルの実体は簡単な計算式を大量に組み合わせた超巨大な数式といえます。最先端の言語モデルでは、想像を絶するほど大量の文章を使い、パラメータ（数式の係数）が数千億に達するほどの大規模な言語モデルを学習させて使っています。LLMが人間に匹敵するほどの高度な能力を持つ、文章の作成や会話を利用するさまざまな仕事を、コンピュータに任せることが可能になってきました。\n\n\n\n自然言語処理の歴史\n\n","type":"content","url":"/introduction#id","position":3},{"hierarchy":{"lvl1":"ガイダンス","lvl2":"計算社会科学において自然言語処理の応用"},"type":"lvl2","url":"/introduction#id-1","position":4},{"hierarchy":{"lvl1":"ガイダンス","lvl2":"計算社会科学において自然言語処理の応用"},"content":"人間の社会行動に関するデジタル化された高密度・大容量のデータの蓄積を背景に、近年、計算社会科学と呼ばれる新たな学問領域が勃興し、急速な発展を遂げています\n\nLazer et al., 2009\n\nHofman, 2021。\n\n計算社会科学において，テキストデータの収集・分析は広く用いられている研究手法です。ここで、自然言語処理技術の発展が計算社会科学にす新たな可能性をもたらせます。\n\n本講義の目的は、計算社会科学に多く応用された自然言語処理技術を理解し、実問題に適用するための基礎力を身につけることです。\n\nその目的を達成するために、自然言語処理と深層学習の基礎、重要な概念と主な手法(モデル)を学習する。さらに、Python用いて、自然言語処理によく用いられるライブラリとツールを学習しつつ、自然言語処理技術を応用するスキルを修得する。\n\n","type":"content","url":"/introduction#id-1","position":5},{"hierarchy":{"lvl1":"ガイダンス","lvl3":"単語分散表現","lvl2":"計算社会科学において自然言語処理の応用"},"type":"lvl3","url":"/introduction#id-2","position":6},{"hierarchy":{"lvl1":"ガイダンス","lvl3":"単語分散表現","lvl2":"計算社会科学において自然言語処理の応用"},"content":"単語分散表現とは、「文字・単語をベクトル空間に埋め込み、その空間上のひとつの点として捉える」ことを指します。\n\n類似性: ある概念を表現する際に、ほかの概念との共通点や類似性と紐づけながら、ベクトル空間上に表現します。\n\n単語類推: 分散表現では異なる概念を表現するベクトル同士での計算が可能です\n\n\n\n単語分散表現のイメージ\n\n1910年代からのテキストデータで単語分散表現を学習し、単語分散表現で男性と女性はそれぞれどのような単語と関連していることを検証することで、ジェンダーのステレオタイプの実態と変化を定量的に分析する \n\nGarg et al., 2018\n\n単語分散表現の類似性と単語類推特性で、単語分散表現の計算を通じて、文化概念の潜在的意味と関係を定量的に測定できました\n\nKozlowski et al., 2019\n\n","type":"content","url":"/introduction#id-2","position":7},{"hierarchy":{"lvl1":"ガイダンス","lvl3":"テキスト分類","lvl2":"計算社会科学において自然言語処理の応用"},"type":"lvl3","url":"/introduction#id-3","position":8},{"hierarchy":{"lvl1":"ガイダンス","lvl3":"テキスト分類","lvl2":"計算社会科学において自然言語処理の応用"},"content":"テキスト分類とは、事前定義済みカテゴリまたはラベルを非構造化テキスト形式に割り当てる処理のことです。主な使用例として、感情分析、偽情報の検出や内容判定などが挙げられます。\n\n言語は本質的に曖昧で、変化し続け、適切に定義されていないため、テキスト分類は決して簡単なタスクではないが、深層学習による自然言語処理が発展したことにより、高精度化させることが可能になってきています。\n\nとくにBERT \n\nDevlin et al., 2019 はセンチメント分析を含めた多くのタスクに関して、当時の最高性能(SOTA)を達成する画期的な技術でした。\n\nBERTは事前学習モデルの一種で、事前に一定のタスクに基づいて事前学習することで汎用性を獲得することに特徴があります。そのため、特定のタスクについてより少ないデータで性能を発揮することができます。\n\nBERTのような事前学習モデルによるテキスト分類の社会科学における応用可能性について多くの注目を集めています \n\nLaurer et al., 2023。\n\nさらに、近年、ChatGPTをはじめとする生成AIの大規模な言語モデルの進展により、テキスト分類に含めて言語に関わるさまざまなタスクができるようになりました。\n\nこれらのモデルは、学習済みのパラメータを更新することなく、プロンプト中に提示された少数の例や指示から新しい分類規則を即座に獲得し、タスクを遂行できるようになっています。\n\n","type":"content","url":"/introduction#id-3","position":9},{"hierarchy":{"lvl1":"ガイダンス","lvl2":"講義の構成"},"type":"lvl2","url":"/introduction#id-4","position":10},{"hierarchy":{"lvl1":"ガイダンス","lvl2":"講義の構成"},"content":"イントロダクション\n\n基礎知識  💻 🔣\n\n自然言語処理の基本概念\n\n機械学習の基本概念\n\n深層学習による自然言語処理ための数学の復習\n\nニューラルネットワーク 💻 🔣\n\nニューラルネットワークの構造\n\nニューラルネットワークの学習\n\n誤差逆伝播法\n\nPyTorch 💻\n\n単語埋め込みモデル(Word Embedding) 💻 🔣\n\n単語埋め込みアルゴリズム\n\n単語埋め込みモデルの性質\n\nword2vecの原理\n\nword2vecの実装 💻📄\n\ngensimによるword2vecモデルの学習\n\n既存word2vecモデルの利用\n\nword2vecが人文・社会科学研究における応用 📄\n\nシーケンスモデリング(1) 💻 🔣\n\nRNN\n\nLSTM\n\nseq2seq\n\nTransformer(1) 💻 🔣\n\nAttentionモデル\n\nSelf-attention\n\nTransformerの構成要素\n\nBERTによるテキスト分類の実装(1) 💻\n\n事前学習済みモデルと転移学習\n\nHuggingFace\n\nGPUの設定\n\nBERTによるテキスト分類の実装(2) 💻\n\n大規模言語モデルの概要\n\n大規模言語モデルの応用(1) 💻\n\n大規模言語モデルの応用(2) 💻\n\n大規模言語モデルの応用(3) 💻\n\n💻 : プログラミング作業が含む講義、PythonとJupyterの基本の使い方を把握することが前提となります\n\n🔣　: 数学に関わる解説が含む講義、基本的な微積分と線形代数の知識が前提となります\n\n📄　: 英語論文を読む必要がある講義\n\n","type":"content","url":"/introduction#id-4","position":11},{"hierarchy":{"lvl1":"ガイダンス","lvl2":"到達目標"},"type":"lvl2","url":"/introduction#id-5","position":12},{"hierarchy":{"lvl1":"ガイダンス","lvl2":"到達目標"},"content":"自然言語処理と深層学習の基礎概念について学ぶとともに、自然言語処理手法を実装する能力を習得する\n\n自然言語処理を用いる研究論文を理解できるようになることを目指す\n\n","type":"content","url":"/introduction#id-5","position":13},{"hierarchy":{"lvl1":"ガイダンス","lvl2":"授業設計と成績評価"},"type":"lvl2","url":"/introduction#id-6","position":14},{"hierarchy":{"lvl1":"ガイダンス","lvl2":"授業設計と成績評価"},"content":"プログラミング操作が含む講義では、必ずPCをご持参する上で、Python環境を整備してください。また、インターネットとの接続が必要される操作もありますので、PCのインターネット接続も事前に設定してください。\n\n授業後課題提出を求める場合があります。基本的には授業の理解度を確認するためのプログラミング課題と想定しています。\n\n成績評価の分配は以下の通りです\n\n出席: 50\\%\n\n課題: 50\\%\n\n授業の内容に関して不明点あるいはご要望があれば、随時\n\nメールでご連絡ください。また、プログラミングやソフトウェア操作の質問については、Google ClassroomまたはGitHub Issueでも受け付けます。\n\n授業のオフィスアワーは、できれば二日前アポイントを取ってくだい。","type":"content","url":"/introduction#id-6","position":15},{"hierarchy":{"lvl1":"LLMSの応用(1)：Langchainの基礎"},"type":"lvl1","url":"/langchain-basic","position":0},{"hierarchy":{"lvl1":"LLMSの応用(1)：Langchainの基礎"},"content":"\n\nLangChainは大規模言語モデルを活用したアプリケーション開発を支えるフレームワークです。具体的にはプロンプト設計、RAGインデックス構築、会話メモリ、外部ツール連携、自律エージェントなどLLMアプリに必要な一連の機能を統合的に提供します。これにより、実験段階のプロトタイプから実運用を想定したシステムまで、開発を効率化しやすくなります。\n\nllms: 言語モデルを呼び出すためのラッパーを提供します\n\nprompts: プロンプトのテンプレートを作成する機能を提供します\n\nchains: ひとつのワークフロー内で LLM やプロンプトテンプレートを組み合わせて使用するための機能を提供します\n\nagents: エージェントを使用することで、課題の解決順序をも LLM を用いて決定し、実行させることができます\n\nmemory: チェーンとエージェントに状態を持たせるための機能を提供します\n\n!pip install langchain==0.1.12\n!pip install langchain-community\n!pip install langchain-core\n!pip install langsmith\n!pip install openai\n!pip install python-dotenv\n!pip install langchain-openai\n!pip install -U --quiet langchain-google-genai pillow\n!pip install google-generativeai\n!pip install google-search-results numexpr wikipedia langchain-experimental\n!pip install duckduckgo-search\n\n\n\n","type":"content","url":"/langchain-basic","position":1},{"hierarchy":{"lvl1":"LLMSの応用(1)：Langchainの基礎","lvl2":"LangChainでLLMを使う"},"type":"lvl2","url":"/langchain-basic#langchain-llm","position":2},{"hierarchy":{"lvl1":"LLMSの応用(1)：Langchainの基礎","lvl2":"LangChainでLLMを使う"},"content":"LLMは、ほとんどの生成AIアプリケーションを駆動する原動力である。LangChainは、あらゆるLLM APIプロバイダーとやり取りするためのシンプルなインターフェースを2種類提供している。\n\nチャットモデルインターフェース: チャットモデルインターフェースを使用すると、ユーザーとモデルの間で双方向の対話を行える\n\nLLMインターフェース: LLMインターフェースは、文字列プロンプトを入力として受け取り、それをモデルプロバイダーへ送信し、予測結果を出力として返すだけである。\n\nOpenAIと\n\nGoogle(Gemini)が提供するAPIを通じて、多岐にわたるAIモデルへのアクセスを可能になります。\n\nAPIを使うためには、まず「自分がサービスを利用できる証」となる「APIキー」を発行する必要があります。そして、OpenAIのAPIは、このAPIキーによって利用した使用量に応じて、課金される仕組みです。\n\nそのため、APIキーが外部に漏れると、他者によって不正に使用されて料金が発生してしまうため、他の人へ共有しないように注意しましょう。\n\nLangChainは、さまざまな LLM に汎用インターフェースを提供し、ユーザーがAPIを介してさまざまなモデルを操作できるようにします。\n\n","type":"content","url":"/langchain-basic#langchain-llm","position":3},{"hierarchy":{"lvl1":"LLMSの応用(1)：Langchainの基礎","lvl3":"OpenAI API","lvl2":"LangChainでLLMを使う"},"type":"lvl3","url":"/langchain-basic#openai-api","position":4},{"hierarchy":{"lvl1":"LLMSの応用(1)：Langchainの基礎","lvl3":"OpenAI API","lvl2":"LangChainでLLMを使う"},"content":"OpenAIクラスのインスタンスでGPT-3モデルを使用するための設定を行っています。\n\nmodel_nameという引数で、使用する\n\nモデルの名前を指定します。\n\ntemperatureという引数は、生成されるテキストのランダム性を制御します。temperatureが高いほど（1に近いほど）、出力はよりランダムになります。逆に、temperatureが低いほど（0に近いほど）、モデルの出力はより一貫性があり、予測可能になります。\n\nmax_tokensという引数は、生成するテキストの最大トークン数を指定します。この場合、生成されるテキストは最大で256トークンになります。トークンとは、テキストを分割した単位のことで、一般的には単語や句読点などが1トークンとなります。\n\nfrom dotenv import load_dotenv\nimport os\nimport getpass\nfrom langchain_openai import OpenAI, ChatOpenAI     \n\nload_dotenv(dotenv_path='../.env')\nif \"OPENAI_API_KEY\" not in os.environ:\n    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")\n\n\n\nllm = OpenAI(\n    model=\"gpt-3.5-turbo-instruct\",\n    temperature=0.9,\n    max_tokens=512\n)\n\n\n\nllm.invoke(\"東北大学を紹介してください：\")\n\n\n\nチャットモデルインターフェースを使用すると、ユーザーとモデルの間で双方向の対話を行える。これが別のインターフェースとして用意されている理由は、OpenAIのような主要LLMプロバイダーが、送受信されるメッセージを「ユーザー」「アシスタント」「システム」というロールに分けて扱っているためである.\n\nここでのロールとは、メッセージに含まれるコンテンツの種類を示す）。\n\nシステム（system）」ロール: モデルに与える指示を記述する。\n\nユーザー（user）ロール: ユーザーのクエリおよびユーザーが生成したコンテンツを表す。\n\nアシスタント（assistant）ロール: モデルが生成したコンテンツを表す。\n\nfrom langchain_openai.chat_models import ChatOpenAI\nfrom langchain_core.messages import HumanMessage\n\nmodel = ChatOpenAI()\nprompt = [HumanMessage(\"東北大学を紹介してください\")]\n\nmodel.invoke(prompt)\n\n\n\n\nfrom langchain_core.messages import HumanMessage, SystemMessage\nfrom langchain_openai.chat_models import ChatOpenAI\n\nmodel = ChatOpenAI()         \nsystem_msg = SystemMessage(\n    '''You are a helpful assistant that responds to questions with three exclamation marks.'''\n)# あなたは優秀なアシスタントです。質問に対して3つの感嘆符を付けて回答してください。\n\nhuman_msg = HumanMessage(\"東北大学を紹介してください\")\nmodel.invoke([system_msg, human_msg])\n\n\n\n\n","type":"content","url":"/langchain-basic#openai-api","position":5},{"hierarchy":{"lvl1":"LLMSの応用(1)：Langchainの基礎","lvl3":"Gemini API","lvl2":"LangChainでLLMを使う"},"type":"lvl3","url":"/langchain-basic#gemini-api","position":6},{"hierarchy":{"lvl1":"LLMSの応用(1)：Langchainの基礎","lvl3":"Gemini API","lvl2":"LangChainでLLMを使う"},"content":"","type":"content","url":"/langchain-basic#gemini-api","position":7},{"hierarchy":{"lvl1":"LLMSの応用(1)：Langchainの基礎","lvl4":"Gemini APIの設定","lvl3":"Gemini API","lvl2":"LangChainでLLMを使う"},"type":"lvl4","url":"/langchain-basic#gemini-api-1","position":8},{"hierarchy":{"lvl1":"LLMSの応用(1)：Langchainの基礎","lvl4":"Gemini APIの設定","lvl3":"Gemini API","lvl2":"LangChainでLLMを使う"},"content":"\n\nif \"GOOGLE_API_KEY\" not in os.environ:\n    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your GOOGLE API key: \")\n\n\n\nimport google.generativeai as genai\nmodels = [m for m in genai.list_models()]\nmodels\n\n\n\n\n\nimport google.generativeai as genai\n\nllm = genai.GenerativeModel('models/gemini-2.5-flash')\n\n\n\nresponse = llm.generate_content(\"東北大学を紹介してください：\")\n\n\n\nfrom IPython.display import Markdown\nMarkdown(response.text)\n\n\n\n","type":"content","url":"/langchain-basic#gemini-api-1","position":9},{"hierarchy":{"lvl1":"LLMSの応用(1)：Langchainの基礎","lvl2":"LLMプロンプトを再利用する: Prompt Templates"},"type":"lvl2","url":"/langchain-basic#llm-prompt-templates","position":10},{"hierarchy":{"lvl1":"LLMSの応用(1)：Langchainの基礎","lvl2":"LLMプロンプトを再利用する: Prompt Templates"},"content":"プロンプトがモデルの出力に大きく影響することを示した。プロンプトはモデルに文脈を理解させ、クエリに対する適切な回答を生成させる助けとなります。\n\nプロンプトは一見単なる文字列であるが、どのような内容を含めるべきか、またユーザー入力に応じてどのように変化させるべきかを設計するのは容易ではない。\n\nプロンプトテンプレートは、プロンプトを作成する再現可能な方法を指します。これには、エンドユーザーから一連のパラメーターを受け取り、プロンプトを生成するテキスト文字列(テンプレート)が含まれます。\n\nテンプレートは動的パラメータを挿入する位置の定義することで、静的かつ具体的なプロンプトを生成するレシピとして利用できます。\n\nfrom langchain_core.prompts import PromptTemplate\n\nprompt = PromptTemplate(\n    input_variables=[\"language\",\"text\"],\n    template=\"次の日本語のテキストを{language}に翻訳してください：{text}\",\n)\n\n\n\n\nprint(prompt.format(language=\"英語\", text=\"東北大学は日本の東北地方にある大学です。\"))\n\n\n\n","type":"content","url":"/langchain-basic#llm-prompt-templates","position":11},{"hierarchy":{"lvl1":"LLMSの応用(1)：Langchainの基礎","lvl2":"チェーン"},"type":"lvl2","url":"/langchain-basic#id","position":12},{"hierarchy":{"lvl1":"LLMSの応用(1)：Langchainの基礎","lvl2":"チェーン"},"content":"","type":"content","url":"/langchain-basic#id","position":13},{"hierarchy":{"lvl1":"LLMSの応用(1)：Langchainの基礎","lvl3":"チェーンの基本","lvl2":"チェーン"},"type":"lvl3","url":"/langchain-basic#id-1","position":14},{"hierarchy":{"lvl1":"LLMSの応用(1)：Langchainの基礎","lvl3":"チェーンの基本","lvl2":"チェーン"},"content":"LLM は単独でも十分に強力に機能します。 しかし、 LLM 同士を組合わせたり、ある機能に特化した他のモジュールとともに利用することで、より複雑なアプリケーションを構築することができます。 LangChain では、このような他の機能と連結するための汎用的なインターフェースとして、チェーンを提供しています。 チェーンを用いることで、LLM の利用を含む “一連の処理” を一つのまとまりとして扱うことができます。\n\nつまり、平易な言葉でいえば、チェーンは「複数の処理の連なり」です。 この処理の連鎖の部品となるチェーンの構成要素のことを リンク と呼びます。 リンクの一例は、LLM の呼び出しなどの基本的な処理です。さらには、その他のチェーン全体をリンクとして含むチェーンも作成できます。\n\nチェーンの代表例は、LLM とプロンプトテンプレートを組合わせて使用するためのLLMChainです。 このチェーンを用いると、\n\nユーザーの入力を受け取り\n\nそれをPromptTemplateでフォーマットし\n\nフォーマットされたレスポンスを LLM に渡す\n\nという一連の操作を一つのまとまりとして実行できます。\n\n基本的な使い方としては、LLM や プロンプトテンプレートなどの基本要素を組み合わせて使用することが考えられます。\n\nfrom langchain_google_genai import ChatGoogleGenerativeAI\n\nllm = ChatGoogleGenerativeAI(model=\"models/gemini-2.5-flash\", google_api_key=os.getenv(\"GOOGLE_API_KEY\"))\n\n\n\n\nchain = prompt | llm\nchain.invoke(\n    {\"language\":\"英語\", \"text\":\"東北大学は日本の東北地方にある大学です。\"}\n)\n\n\n\nchain.invoke({\"language\":\"中国語\", \"text\":\"東北大学は日本の東北地方にある大学です。\"})\n\n\n\n","type":"content","url":"/langchain-basic#id-1","position":15},{"hierarchy":{"lvl1":"LLMSの応用(1)：Langchainの基礎","lvl3":"実装例:Few Shot Learning","lvl2":"チェーン"},"type":"lvl3","url":"/langchain-basic#id-few-shot-learning","position":16},{"hierarchy":{"lvl1":"LLMSの応用(1)：Langchainの基礎","lvl3":"実装例:Few Shot Learning","lvl2":"チェーン"},"content":"\n\nfrom langchain_core.prompts import PromptTemplate, FewShotPromptTemplate\n\n\n\n\nexamples = [\n    {\"word\": \"楽しい\", \"antonym\": \"悲しい\"},\n    {\"word\": \"高い\", \"antonym\": \"低い\"},\n]\n\n\n\nexample_formatter_template = \"\"\"\nWord: {word}\nAntonym: {antonym}\\n\n\"\"\"\n\nexample_prompt = PromptTemplate(\n    input_variables=[\"word\", \"antonym\"],\n    template=example_formatter_template,\n)\n\n\n\nexamples: モデルに示す例を指定します。\n\nexample_prompt: 例をどのように提示するかを指定します。\n\nprefix: プロンプトの前置詞を指定します。一般的には、モデルにタスクを説明するためのものです。\n\nsuffix: プロンプトの後置詞を指定します。一般的には、モデルに入力と出力の形式を示すためのものです。\n\ninput_variables: 入力の変数名を指定します。\n\nexample_separator: 例を区切るための文字列を指定します。\n\nfew_shot_prompt = FewShotPromptTemplate(\n    examples=examples,\n    example_prompt=example_prompt,\n    prefix=\"Give the antonym of every input\",\n    suffix=\"Word: {input}\\nAntonym:\",\n    input_variables=[\"input\"],\n    example_separator=\"\\n\",\n)\n\n\n\nprint(few_shot_prompt.format(input=\"大きい\"))\n\n\n\nllm = ChatGoogleGenerativeAI(model=\"models/gemini-2.5-flash\", google_api_key=os.getenv(\"GOOGLE_API_KEY\"))\n\n\n\n\nfrom langchain_core.output_parsers import StrOutputParser\n\nchain = few_shot_prompt | llm | StrOutputParser()\n\nvar = few_shot_prompt.input_variables[0] \nprint(chain.invoke({var: \"大きい\"}))\n\n\n\n","type":"content","url":"/langchain-basic#id-few-shot-learning","position":17},{"hierarchy":{"lvl1":"LLMSの応用(1)：Langchainの基礎","lvl4":"課題","lvl3":"実装例:Few Shot Learning","lvl2":"チェーン"},"type":"lvl4","url":"/langchain-basic#id-2","position":18},{"hierarchy":{"lvl1":"LLMSの応用(1)：Langchainの基礎","lvl4":"課題","lvl3":"実装例:Few Shot Learning","lvl2":"チェーン"},"content":"Few Shot Learningでセンチメント分析を実装しなさい。\n\nデータフレームから一部のテキストとラベル(n=5)を抽出し、examplesを作成します\n\nFew Shot Learningためのpromptを作成します\n\nchainを作成し、テストデータから任意のテキストに対するセンチメント予測結果を確認します\n\n#!pip install datasets\nfrom datasets import load_dataset\ndataset = load_dataset(\"imdb\")\n\n\n\ndf_train_sample = dataset[\"train\"].to_pandas().sample(5, random_state = 123)\ndf_test_sample = dataset[\"test\"].to_pandas().sample(5, random_state = 123)\n\n\n\ndf_train_sample\n\n","type":"content","url":"/langchain-basic#id-2","position":19},{"hierarchy":{"lvl1":"大規模言語モデルの基本"},"type":"lvl1","url":"/llm","position":0},{"hierarchy":{"lvl1":"大規模言語モデルの基本"},"content":"近年、GPTをはじめとする大規模言語モデルが自然言語処理分野において顕著な成果を挙げ、文章の生成や論理推論など高度な課題においても高い性能を示している。それに伴う、大規模言語モデルを活用することでより効率的に人間の行動や社会現象に関する理論や仮説の検証と発展し、新たな可能性をもたらすと期待されています。\n\n今まで説明した通り、文脈を考慮した単語埋め込みである文脈化単語埋め込み(contextualized word embedding)を計算するTransformerを大規模コーパスで自己教師あり学習(Masked LMやNext sentence prediction)で事前学習し、そのモデルを下流タスクのテータセットを使って微調整する方法は、自然言語処理でよく用いられる手法です。\n\nこうした事前学習した大規模なニューラルネットワークは、大規模言語モデルのように呼ばれます。\n\n特に、2020年以降にパラメータ数およびテキストデータをさらに大規模することで、大規模言語モデルの性能も飛躍的に向上し、人間と自然にやりとりできるような能力を身につけました。これによって、ファインチューニングは行なわずに、事前学習された大規模言語モデルをプロンプト(prompt)と呼ばれるテキストを通じて制御することで下流タスク方法も一般的になりつつあります。\n\nここでは、大規模言語モデルの近年(2023年まで)の進展について解説します。進展が激しい分野のため、ここでの解説に基づいて適宜最新の情報も各自調べてほしいです。\n\n","type":"content","url":"/llm","position":1},{"hierarchy":{"lvl1":"大規模言語モデルの基本","lvl2":"モデルの大規模化とその効果"},"type":"lvl2","url":"/llm#id","position":2},{"hierarchy":{"lvl1":"大規模言語モデルの基本","lvl2":"モデルの大規模化とその効果"},"content":"大規模言語モデルの開発が進むにつれて、モデルに含まれるパラメータ数が飛躍的に増加してきています。\n\n2018年に発表されたBERTは3.4億個、2019年のGPT-2では15億個だったパラメータ数が2020年のGPT-3では1750億、そしてGPT-4はは100兆個のパラメータを持つと言われており、加速的に増加していることがわかります。\n\n\n\nこうした大規模化が行われている背景には、モデルの規模を大きくすることで性能が比例して改善していくという経験的な法則であるスケール則があります。\n\nさらに、大規模言語モデルが一定の規模を超えると、タスクの性能が飛躍的に向上する現象も報告されています。こうした大規模化することで性能が改善し獲得される能力を創発的能力(emergent abilities)と呼ぶことがあります。\n\n一方、大規模言語モデルが創発的能力を持つことに疑問視している声もあります。\n\n","type":"content","url":"/llm#id","position":3},{"hierarchy":{"lvl1":"大規模言語モデルの基本","lvl2":"Prompting"},"type":"lvl2","url":"/llm#prompting","position":4},{"hierarchy":{"lvl1":"大規模言語モデルの基本","lvl2":"Prompting"},"content":"","type":"content","url":"/llm#prompting","position":5},{"hierarchy":{"lvl1":"大規模言語モデルの基本","lvl3":"文脈内学習","lvl2":"Prompting"},"type":"lvl3","url":"/llm#id-1","position":6},{"hierarchy":{"lvl1":"大規模言語モデルの基本","lvl3":"文脈内学習","lvl2":"Prompting"},"content":"\n\nGPTのようなパラメータが非常に大きいモデルでは、学習時に入力される文章内には、様々なサブタスクが埋め込まれると考えられます。こうした文章の生成を学習することで、内包される様々な言語タスクへの処理能力の獲得が期待できます。\nこのアプローチはGPT-2から用いられており、文脈内学習(in-context learning)と呼ばれます。\n\n詳細はこちらの\n\n論文:Language Models are Few-Shot Learnersを参照してください。\n\nモデルは新しいタスクをこなすための追加のトレーニングデータなしに（zero-shot learning）、または非常に少量のデータで（few-shot learning）多様なタスクを達成することが可能になります。\n\nFew-shot learning: 推論時に少数（10から100）のデモンストレーションを与えます\n\nOne-shot learning: 推論時に一つのデモンストレーションを与えます\n\nZero-shot learning: 推論時にデモンストレーションは与えられず、自然言語によるタスク指示のみが与えられます\n\nこのように、従来ファインチューニングが必要される多くのタスクは、モデルにプロンプト(prompt)と呼ばれるテキストを入力して後続けするテキストを予測するという形で解かせることが知られています。\n\n基本的には、タスクの説明と多くのデモンストレーションを与えることでモデルの性能が向上していく傾向が見られます。\n\nzero, one-shotでも悪くない性能、few-shotでは一部ファインチューニングを用いたSOTAモデルに匹敵する性能を得ることが確認できました。\n\n現在、一般的には、few-shot learningモデルの性能がファインチューニングによる教師あり学習モデルを超えることはまだ少ないが、ラベル付きデータの生成に費やすコストと時間を効率すると、プロンプトはファインチューニングによる教師あり学習の効率的な代替手段と考えられます。\n\n","type":"content","url":"/llm#id-1","position":7},{"hierarchy":{"lvl1":"大規模言語モデルの基本","lvl3":"chain-of-thought推論","lvl2":"Prompting"},"type":"lvl3","url":"/llm#chain-of-thought","position":8},{"hierarchy":{"lvl1":"大規模言語モデルの基本","lvl3":"chain-of-thought推論","lvl2":"Prompting"},"content":"詳細はこちらの\n\n論文:Chain-of-Thought Prompting Elicits Reasoning in Large Language Modelsを参照してください。\n\n大規模言語モデルが苦手とされるタスクの一つに他段階の推論が必要となるマルチステップ推論(multi-step reasoning)があります。\n\n複数の段階の推論が必要な際に、推論過程の例示を与えるchain-of-thought推論(chain-of-thought reasoning)を用いることで性能が改善することが報告されています。\n\n具体的言えば、chain-of-thought推論では、回答を加えて推論過程を示す例示を与えて、モデルが回答を行う際に推論過程を含めて出力テキストを生成するようにします。\n\nさらに、chain-of-thought推論の推論過程を人間が与えるのではなく、推論過程の生成を促す「Let’s think step by step」のような文字列をプロンプトの末尾に追加して、推論過程を生成されてから回答を抽出するzero-shot chain-of-thought reasoningも提案されています。\n\nこの方法を使うと、プロンプトを書き換えるだけで推論の性能を改善することができます。このことから、大規模言語モデルを使うあたってはプロンプトの与え方を工夫することが重要であることがわかります。\n\n詳細はこちらの\n\n論文:Large Language Models are Zero-Shot Reasonersを参照してください。\n\n","type":"content","url":"/llm#chain-of-thought","position":9},{"hierarchy":{"lvl1":"大規模言語モデルの基本","lvl2":"指示チューニング"},"type":"lvl2","url":"/llm#id-2","position":10},{"hierarchy":{"lvl1":"大規模言語モデルの基本","lvl2":"指示チューニング"},"content":"\n\n通常の事前学習では、モデルは膨大なデータセットから次の単語を予測する能力を学習します。しかし、そのままでは具体的なタスクを効率的にこなすことは難しい場合があります。\n\n指示チューニング（instruction tuning）とは、様々なタスクのデータを指示と理想的な回答の組で構成されるデータセットを言語モデルに与え追加学習させることで、言語モデルの性能を向上させる技術です。\n\n指示チューニングデータセットの構築\n\nファインチューニング: 指示チューニングデータセットを用いて、既存のLLMを再トレーニングします。これにより、モデルが指示に応じた出力を生成する能力が向上します。\n\nFlan(Finetuned Lanaguage Net)では、大規模言語モデルをデータセットを集約してフィインチューニングした結果、多数のタスクにおけるzero-shot学習の性能が向上したことが報告されています。\n\n「Flan-T5」は、Google AI の新しいオープンソース言語モデルです。1,800 以上の言語タスクでファインチューニングされており、プロンプトとマルチステップの推論能力が劇的に向上しています。\n\n","type":"content","url":"/llm#id-2","position":11},{"hierarchy":{"lvl1":"大規模言語モデルの基本","lvl2":"人間のフィードバックからの強化学習"},"type":"lvl2","url":"/llm#id-3","position":12},{"hierarchy":{"lvl1":"大規模言語モデルの基本","lvl2":"人間のフィードバックからの強化学習"},"content":"\n\nRLHF（Reinforcement Learning from Human Feedback）とは、「人間のフィードバックからの強化学習」という名前の通り、人間の価値基準に沿うように、人間のフィードバックを使ってAI（言語）モデルを強化学習で微調整（ファインチューニング）する手法である。","type":"content","url":"/llm#id-3","position":13},{"hierarchy":{"lvl1":"大規模言語モデルの基本","lvl3":"強化学習","lvl2":"人間のフィードバックからの強化学習"},"type":"lvl3","url":"/llm#id-4","position":14},{"hierarchy":{"lvl1":"大規模言語モデルの基本","lvl3":"強化学習","lvl2":"人間のフィードバックからの強化学習"},"content":"強化学習 (Reinforcement Learning) とは、機械学習の一種であり、エージェントが動的環境と、繰り返し試行錯誤のやりとりを重ねることによってタスクを実行できるようになる手法です。この学習手法により、エージェントは、タスクの報酬を最大化する一連の意思決定を行うことができます。教師付き学習とよく似た問題設定ですが、与えられた正解の出力をそのまま学習すれば良いわけではなく、もっと広い意味での「価値」を最大化する行動を学習しなければなりません。\n\n例えば、犬を訓練すること例として考えてください。強化学習の用語を使用すると、この場合の学習の目的は、犬 (エージェント) のしつけ (学習) を行い、ある環境の中でタスクを完了させることです。これには、犬の周囲の環境や訓練士が含まれます。\n\nまず、訓練士が命令や合図を出し、それを犬が観察 (観測) します。続いて、犬は行動を起こすことで反応します。犬の行動が目的の行動に近い場合、訓練士は、おやつやおもちゃなどのごほうび (報酬) を与えますが、それ以外の場合、ごほうびは与えません。\n\nしつけ (学習) を始めたばかりの頃は、犬はランダムな行動を取る傾向にあります。犬は観測した特定の状況を行動やごほうび (報酬) と関連付けようとするため、与えられた指示が「おすわり」であっても、ローリングなど別の行動を取る場合があります。\n\n犬の立場から見ると、すべての合図に正しく反応して、おやつをできるだけ多くもらえるような状況が理想的です。\n\n強化学習のしつけ (学習) とは、犬が何らかのごぼうび (報酬) を最大化する理想的な行動を学習するように、犬の方策を「調整」することを指します。学習が完了すると、犬は飼い主を観察し、獲得した方策によって、その場にふさわしい行動 (「おすわり」と命令されたらおすわりをするなど) が取れるようになります。\n\nまとめると、強化学習の目的は、与えられた「環境」における価値（あるいは「利益」と呼びます）を最大化するように「エージェント」を学習させます。\n\n","type":"content","url":"/llm#id-4","position":15},{"hierarchy":{"lvl1":"大規模言語モデルの基本","lvl2":"RLHF（Reinforcement Learning from Human Feedback）"},"type":"lvl2","url":"/llm#rlhf-reinforcement-learning-from-human-feedback","position":16},{"hierarchy":{"lvl1":"大規模言語モデルの基本","lvl2":"RLHF（Reinforcement Learning from Human Feedback）"},"content":"RLHFの役割は、人間の好みや意図といった「人間の価値基準」がAIモデルに反映されることになります。具体的には、あるプロンプトに対してAIが生成した応答文の良し悪しを人間がランク付けし、そのランク付されたデータセットを使って「より望ましい応答文とはどんな感じの文章なのか」を評価できる報酬モデルを作成するわけです。\n\n教師あり学習で既存の言語モデルをfine-tuning\n\n指示チューニング済みモデルが出力したテキストに対して人手で優劣に順位付けします\n\nこのデータセットを使って、報酬(テキストの優劣を反映したスカラー値)を予測する報酬モデルを学習します\n\n報酬モデルをこのように学習させることで、似たようなプロンプトが与えられた時に、より望ましいと評価された応答（＝よりランクが高かった応答）に近い応答文が、より報酬が高くなります。結果として、より好ましいと評価された応答文に近い応答文が生成される確率が高まります。\n\n","type":"content","url":"/llm#rlhf-reinforcement-learning-from-human-feedback","position":17},{"hierarchy":{"lvl1":"大規模言語モデルの基本","lvl2":"Parameter efficient fine-tuning (PEFT)"},"type":"lvl2","url":"/llm#parameter-efficient-fine-tuning-peft","position":18},{"hierarchy":{"lvl1":"大規模言語モデルの基本","lvl2":"Parameter efficient fine-tuning (PEFT)"},"content":"\n\n項目\n\nPEFT\n\nFull Fine-Tuning\n\n概要\n\nモデル全体ではなく、追加的に設定したパラメータや、一部のパラメータを微調整する手法。\n\n事前学習済みモデル全体のパラメータを調整する手法。\n\n計算リソース\n\n大幅に削減可能。学習中に更新するパラメータ数が少ないため、GPUメモリや計算時間が少なくて済む。 (例　GPT-3 LoRA: 350GB)\n\n多大な計算リソースを必要とする。全パラメータを更新するため、GPUメモリや計算負荷が高い (例　GPT-3: 1.2TB)。\n\n保存領域\n\n調整されたパラメータのみを保存するため、小さな保存領域で十分(例　GPT-3 LoRA: 35MB)。\n\n微調整後のモデル全体を保存する必要があるため、元モデルと同じサイズの保存領域は必要(例　GPT-3: 350GB)。\n\nPEFTの手法は主に以下の2つのカテゴリに分類できます：\n\nReparameterization: モデルの一部パラメータを特定の形式に再構成し、その部分のみを微調整します。\n\nLoRA (Low-Rank Adaptation)\n\nAdditive: 元のモデルのパラメータに加算的に新しいパラメータ（追加の層など）を導入して微調整を行います。\n\nSoft Prompt: 入力系列にタスクごとのベクトル(Soft Prompt)を付加し、学習を行います。\n\n\n\n","type":"content","url":"/llm#parameter-efficient-fine-tuning-peft","position":19},{"hierarchy":{"lvl1":"大規模言語モデルの基本","lvl3":"LoRA (Low-Rank Adaptation)","lvl2":"Parameter efficient fine-tuning (PEFT)"},"type":"lvl3","url":"/llm#lora-low-rank-adaptation","position":20},{"hierarchy":{"lvl1":"大規模言語モデルの基本","lvl3":"LoRA (Low-Rank Adaptation)","lvl2":"Parameter efficient fine-tuning (PEFT)"},"content":"LoRA は、モデル内の大きな重み行列（例: 全結合層や注意メカニズムの線形変換）を低ランクの行列に分解し、その一部だけを学習することで、計算コストとストレージを大幅に削減します。\n\nW + \\Delta W = W + AB\n\nA \\in \\mathbb{R}^{d \\times r}, \\, B \\in \\mathbb{R}^{r \\times k} は学習可能な低ランク行列。\n\nr はランクで、通常は r \\ll \\min(d, k)。\n\nこの近似により、学習すべきパラメータの総数は元の W の \\mathcal{O}(d \\cdot k) から、低ランク部分のみの \\mathcal{O}(r \\cdot (d + k)) に大幅に削減されます。\n\nTransformerのパラメータ数 512 \\times 64 = 32768\n\nLoRAのパラメータ数d \\times r = 512 \\times 8 = 4096 (r=8)\n\n86 \\%削減\n\n\n\n\n\n","type":"content","url":"/llm#lora-low-rank-adaptation","position":21},{"hierarchy":{"lvl1":"LSTM"},"type":"lvl1","url":"/lstm","position":0},{"hierarchy":{"lvl1":"LSTM"},"content":"RNNの学習においては勾配消失問題を解決するためには、RNNレイヤのアーキテクチャを根本から変える必要があります。\n\nここで登場するのは、シンプルのRNNを改良した「ゲート付きRNN」です。この「ゲート付きRNN」では多くのアーキテクチャが提案されており、その代表的な仕組みはLSTM（Long Short-Term Memory）になります。\n\n","type":"content","url":"/lstm","position":1},{"hierarchy":{"lvl1":"LSTM","lvl2":"LSTMのインタフェース"},"type":"lvl2","url":"/lstm#lstm","position":2},{"hierarchy":{"lvl1":"LSTM","lvl2":"LSTMのインタフェース"},"content":"","type":"content","url":"/lstm#lstm","position":3},{"hierarchy":{"lvl1":"LSTM","lvl3":"LSTMの全体像","lvl2":"LSTMのインタフェース"},"type":"lvl3","url":"/lstm#lstm-1","position":4},{"hierarchy":{"lvl1":"LSTM","lvl3":"LSTMの全体像","lvl2":"LSTMのインタフェース"},"content":"RNNとLSTMレイヤのインタフェースの違いは、LSTMにはcという経路があることです。このcは記憶セルと呼ばれ、これを通じてネットワークを流れる情報の流れを制御します。セル状態は、ネットワークの一種の「記憶」であり、重要な情報を長期間にわたって保持する能力を持っています。これにより、LSTMは長期間にわたる依存関係を捉えることができます。\n\nc_tには、時刻tにおけるLSTMの記憶が格納されています。具体的に言えば、現在の記憶セルc_tは、三つの入力(c_{t-1},h_{t-1},x_t)から何らかの計算によって求められています。そのため、これに過去から時刻tまでにおいて必要な情報が全て格納されていると考えられます。\n\n必要な情報が詰まった記憶を元に、外部のレイヤへ隠れ状態h_tを出力します。\n\n\n\n","type":"content","url":"/lstm#lstm-1","position":5},{"hierarchy":{"lvl1":"LSTM","lvl3":"ゲート機構","lvl2":"LSTMのインタフェース"},"type":"lvl3","url":"/lstm#id","position":6},{"hierarchy":{"lvl1":"LSTM","lvl3":"ゲート機構","lvl2":"LSTMのインタフェース"},"content":"\n\nゲートは、セル状態に流れる情報を制御するために使用される仕組みです。\n\nゲートの「開き具合」は0.0 - 1.0までの実数で表されます。そしてその数値によって、必要な情報を保持し、不要な情報を排除し、適切な時に適切な情報を出力することができます。\n\nここで重要なのは、ゲートの「開き具合」ということは、データから自動的に学ばせるということです。\n\n","type":"content","url":"/lstm#id","position":7},{"hierarchy":{"lvl1":"LSTM","lvl2":"LSTMの構造"},"type":"lvl2","url":"/lstm#lstm-2","position":8},{"hierarchy":{"lvl1":"LSTM","lvl2":"LSTMの構造"},"content":"\\odotはアダマール積（要素ごとの積）を表します。シグモイド関数によって得られたゲートの値0_tと、tanh関数によって得られたセル状態c_tの正規化された値を要素ごとに掛け合わせることを意味します。\n\n\n\nNote\n\ntanh関数:tanh関数は出力を-1から1の範囲に正規化します。この数値は、何らかのエンコードされた「情報」に対する度合いが表されていると解釈できます。\n\nシグモイド関数: シグモイド関数は出力を0から1の範囲に正規化します。データをどれだけ通すかという割合を表しますので、各ゲート（忘却ゲート、入力ゲート、出力ゲート）で情報の流れを制御するのに適しています。\n\nそのため、ゲートではシグモイド関数、実質的な「情報」を持つデータにはtanh関数が活性化関数として使われます。\n\n","type":"content","url":"/lstm#lstm-2","position":9},{"hierarchy":{"lvl1":"LSTM","lvl3":"forgetゲート","lvl2":"LSTMの構造"},"type":"lvl3","url":"/lstm#forget","position":10},{"hierarchy":{"lvl1":"LSTM","lvl3":"forgetゲート","lvl2":"LSTMの構造"},"content":"LSTMのforgetゲートでは、長期記憶から不要な情報を忘却するための制御を行っている。\n\nここで、forgetゲートで行う一連の計算を\\sigmaで表すことにします。計算は次の式で表されます。\\sigmaはsigmoid関数を表します。f_t = \\sigma(x_tW_x^{(f)}+h_{t-1}W_h^{(f)} + b^{(f)})\n\n入力\n\n現在の入力x_t\n\n前の時点の隠れ層の出力h_{t-1}\n\n重みとバイアス\n\n現在の入力x_tに適用されるゲートの重み行列W_x^{(f)}\n\n前の時点の隠れ層の出力h_{t-1}に適用されるゲートの重み行列W_h^{(f)}\n\nみ付けされた入力とバイアスの合計にシグモイド関数によって計算されるため、0 から 1 の値をとります。\n\n","type":"content","url":"/lstm#forget","position":11},{"hierarchy":{"lvl1":"LSTM","lvl3":"inputゲート","lvl2":"LSTMの構造"},"type":"lvl3","url":"/lstm#input","position":12},{"hierarchy":{"lvl1":"LSTM","lvl3":"inputゲート","lvl2":"LSTMの構造"},"content":"新しい情報を追加する際、何も考えずに追加するのではなく、追加する情報としてどれだけ価値があるかを判断する上で、追加する情報を選択します。これにより、長期間にわたる依存関係をより効果的に管理し、複雑なシーケンスデータを扱うことができるようになります。\n\n具体的には、inputゲートによって重みつけされた情報が新たに追加されることになります。i_t = \\sigma(x_tW_x^{(i)}+h_{t-1}W_h^{(i)} + b^{(i)})\n\n\n\n","type":"content","url":"/lstm#input","position":13},{"hierarchy":{"lvl1":"LSTM","lvl3":"新しい記憶セル","lvl2":"LSTMの構造"},"type":"lvl3","url":"/lstm#id-1","position":14},{"hierarchy":{"lvl1":"LSTM","lvl3":"新しい記憶セル","lvl2":"LSTMの構造"},"content":"LSTMではセルの長期記憶を保つための変数c_tが用意されています。長期記憶c_tに対して、古くなった記憶を削除したり、新しい情報を新規追加したりすることで、適当な長期記憶を可能にしています。\n\n具体的には、\n\n新しい「情報」を記憶セルに追加することを目的としていますので、「ケード」ではなくtanhノードが使われます。\n\n入力ゲートの計算\n\n現在の入力x_tと前の時点の隠れ状態h_{t-1}から、inputゲートi_tがシグモイド関数を用いて計算されます。\n\n同時に、tanh関数を用いて新しい候補セル状態\\tilde{c}_tが生成されます。g = tanh(x_tW_x^{(g)}+h_{t-1}W_h^{(g)} + b^{(g)})\n\nセル状態の更新\n\ninputゲートi_tと新しい候補セル状態\\tilde{c}_tがアダマール積によって組み合わされます。\n\nforgetゲートf_tを用いて、前のセル状態c_{t-1}が更新されます。\n\n最終的なセル状態\n\n更新された前のセル状態と新しく生成されたセル状態が加算され、新しいセル状態c_tが生成されます。\n\n","type":"content","url":"/lstm#id-1","position":15},{"hierarchy":{"lvl1":"LSTM","lvl3":"outputゲート","lvl2":"LSTMの構造"},"type":"lvl3","url":"/lstm#output","position":16},{"hierarchy":{"lvl1":"LSTM","lvl3":"outputゲート","lvl2":"LSTMの構造"},"content":"outputゲートは隠れ状態h_tの形成に使用されます。\n\n現在の記憶セルc_tは、(c_{t-1},h_{t-1},x_t)を入力として求められます。そして、更新されたc_tを使って、隠れ状態のh_t計算されます。ここで、tanh(c_t)の各要素に対して、「それらが次時刻の隠れ状態としてどれだけ重要か」ということを調整します。\n\nなお、outputゲートの開き具合は、入力x_tと前の状態h_{t-1}から求めます。o_t = \\sigma(x_tW_x^{(o)}+h_{t-1}W_h^{(o)} + b^{(o)})\n\nsigmoid関数による出力とtanh関数によるセル状態の出力を掛け合わせ、新しい隠れ状態h_tを生成します。h_t = o_t \\odot \\tanh(c_t)\n\n\n\nNote\n\nLSTMは、勾配消失問題を軽減するように設計されています。\n\nゲート機構\n\nLSTMのゲートは、セル状態に流れる情報を精密に制御します。セル状態のゲート制御された更新は、勾配が長いシーケンスを通じても安定して伝播するのを支援し、勾配消失問題の軽減に貢献します。また、各時点での情報の重要性を評価し、必要な情報を保持しながら不要な情報を取り除くことで、ニューラルネットワークがより効率的に学習し、安定した勾配を維持できるようにします。\n\nforgetゲートが「忘れるべき」と判断した記憶セルの要素に対しては、その勾配の要素は小さくなります。その一方で、forgetゲートが「忘れではいけない」と導いた要素に対しては、その勾配の要素は劣化することなく過去方向へ伝わります。そのため、記憶セルの勾配は、勾配消失は起こさずに伝播することが期待できるのです。\n\n非線形性の制御\n\nRNNの逆伝播で、「行列の積」を繰り返し行っていましたので、勾配消失が起きたのです。\n\nLSTMでは、「行列の積」の計算ではなく「要素ごとの積」で逆伝播行います。特に、シグモイドやtanhのような非線形活性化関数は、ゲートの開閉を制御しますが、セル状態自体の更新は加算によって行われるため、勾配が長いシーケンスを通しても安定して伝播し続けることができます。\n\n","type":"content","url":"/lstm#output","position":17},{"hierarchy":{"lvl1":"LSTM","lvl2":"発展的なLSTM"},"type":"lvl2","url":"/lstm#id-lstm","position":18},{"hierarchy":{"lvl1":"LSTM","lvl2":"発展的なLSTM"},"content":"","type":"content","url":"/lstm#id-lstm","position":19},{"hierarchy":{"lvl1":"LSTM","lvl3":"多層LSTM","lvl2":"発展的なLSTM"},"type":"lvl3","url":"/lstm#id-lstm-1","position":20},{"hierarchy":{"lvl1":"LSTM","lvl3":"多層LSTM","lvl2":"発展的なLSTM"},"content":"多層LSTM(Multi-layer LSTM, Stacked LSTMとも呼ばれます)は、複数のリカレント層を積み重ねたニューラルネットワークの構造です。各層は独自の隠れ状態を持ち、前の層からの出力を次の層の入力として受け取ります。\n\n各層が異なるレベルの特徴を学習できるため、多層RNNは単層RNNよりも複雑なパターンを捉えることができます。\n\n\n\n","type":"content","url":"/lstm#id-lstm-1","position":21},{"hierarchy":{"lvl1":"LSTM","lvl3":"双方向LSTM（Bidirectional LSTM）","lvl2":"発展的なLSTM"},"type":"lvl3","url":"/lstm#id-lstm-bidirectional-lstm","position":22},{"hierarchy":{"lvl1":"LSTM","lvl3":"双方向LSTM（Bidirectional LSTM）","lvl2":"発展的なLSTM"},"content":"通常のLSTMは、時系列データを順方向に学習していますので、後ろにある単語やフレーズの情報を取り込むことができません。そのため、過去の情報を活用して現在の出力を決定するのには有効ですが、「未来」の情報を考慮に入れることができません。\n\nテキスト処理のタスクでは、文の意味を完全に理解するためには、未来の情報が過去の情報と同じくらい、またはそれ以上に重要な場合があります。\n\nApple is something that I like to eat.\n\n双方向LSTMはこれらの欠点を補うために設計されており,シーケンスデータの処理において、時間の前後の両方の方向から情報を捉えるために設計されます。\n\n具体的は、双方向LSTMは、シーケンスデータを順方向（前から後ろへ）と逆方向（後ろから前へ）の両方で処理する二つのLSTM層から構成されます。\n\n順方向のLSTM: 一方のLSTM層がシーケンスを通常の時間の流れに沿って処理し、各時点での隠れ状態を更新します。\n\n逆方向のLSTM: もう一方のLSTM層がシーケンスを逆順に処理し、別の隠れ状態シーケンスを生成します\n\n出力の結合: 二つの層の出力（隠れ状態）は、各時点で結合されて最終的な出力を形成します。\n\nNote\n\n入力シーケンス全体にアクセスできる場合、双方向LSTMはより豊かな特徴抽出と精度の高い予測ができるため、デフォルトでの使用が推奨されます。しかし、全てのシナリオで双方向LSTMが最適というわけではなく、タスクの特性に応じて選択する必要があります。\n\n","type":"content","url":"/lstm#id-lstm-bidirectional-lstm","position":23},{"hierarchy":{"lvl1":"LSTMによる文書分類"},"type":"lvl1","url":"/lstm-classification","position":0},{"hierarchy":{"lvl1":"LSTMによる文書分類"},"content":"import pandas as pd\nimport numpy as np\nimport torch\nimport re\nimport nltk\nfrom nltk.tokenize import word_tokenize\nnltk.download('punkt')\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import DataLoader, TensorDataset\nimport torch.nn as nn\nimport torch.optim as optim\nfrom collections import Counter\nfrom torchtext.vocab import vocab\nfrom sklearn.metrics import accuracy_score, f1_score\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n\n\n","type":"content","url":"/lstm-classification","position":1},{"hierarchy":{"lvl1":"LSTMによる文書分類","lvl2":"データ準備"},"type":"lvl2","url":"/lstm-classification#id","position":2},{"hierarchy":{"lvl1":"LSTMによる文書分類","lvl2":"データ準備"},"content":"","type":"content","url":"/lstm-classification#id","position":3},{"hierarchy":{"lvl1":"LSTMによる文書分類","lvl3":"CSVファイルを読み込む","lvl2":"データ準備"},"type":"lvl3","url":"/lstm-classification#csv","position":4},{"hierarchy":{"lvl1":"LSTMによる文書分類","lvl3":"CSVファイルを読み込む","lvl2":"データ準備"},"content":"\n\ndf= pd.read_csv('./Data/twitter_training.csv',names=['index','brand','sentiment','text'])\ndf.head()\n\n\n\n","type":"content","url":"/lstm-classification#csv","position":5},{"hierarchy":{"lvl1":"LSTMによる文書分類","lvl3":"ラベルデータの処理","lvl2":"データ準備"},"type":"lvl3","url":"/lstm-classification#id-1","position":6},{"hierarchy":{"lvl1":"LSTMによる文書分類","lvl3":"ラベルデータの処理","lvl2":"データ準備"},"content":"\n\ndf[\"label\"]=df[\"sentiment\"].replace({\"Positive\":2,\"Negative\":0,\"Neutral\":1,\"Irrelevant\":np.nan})\ndf.dropna(inplace=True)\ndf.head()\n\n\n\n","type":"content","url":"/lstm-classification#id-1","position":7},{"hierarchy":{"lvl1":"LSTMによる文書分類","lvl3":"テキストデータの前処理","lvl2":"データ準備"},"type":"lvl3","url":"/lstm-classification#id-2","position":8},{"hierarchy":{"lvl1":"LSTMによる文書分類","lvl3":"テキストデータの前処理","lvl2":"データ準備"},"content":"テキストを小文字に変換\n\n句読点を削除\n\nトークン化\n\n単語ID化\n\n","type":"content","url":"/lstm-classification#id-2","position":9},{"hierarchy":{"lvl1":"LSTMによる文書分類","lvl4":"Tokenization","lvl3":"テキストデータの前処理","lvl2":"データ準備"},"type":"lvl4","url":"/lstm-classification#tokenization","position":10},{"hierarchy":{"lvl1":"LSTMによる文書分類","lvl4":"Tokenization","lvl3":"テキストデータの前処理","lvl2":"データ準備"},"content":"\n\ndef preprocess_text(text):\n    text = text.lower()  # Lowercasing\n    text = re.sub(r'\\W+', ' ', text)  # Remove punctuation\n    tokens = word_tokenize(text)  # Tokenization\n    return tokens\n\n\n\ndf[\"processed_text\"]=df[\"text\"].apply(preprocess_text)\n\n\n\ndf.head()\n\n\n\n","type":"content","url":"/lstm-classification#tokenization","position":11},{"hierarchy":{"lvl1":"LSTMによる文書分類","lvl4":"単語辞書","lvl3":"テキストデータの前処理","lvl2":"データ準備"},"type":"lvl4","url":"/lstm-classification#id-3","position":12},{"hierarchy":{"lvl1":"LSTMによる文書分類","lvl4":"単語辞書","lvl3":"テキストデータの前処理","lvl2":"データ準備"},"content":"Vocabは、各単語（トークン）に対して一意のインデックス（またはID）を割り当てます。このマッピングにより、テキストデータを数値データに変換することができます。\n\ncounter = Counter()\nfor line in df[\"processed_text\"]:\n    counter.update(line)\nVocab = vocab(counter, min_freq=1)\n\n\n\n# 単語からインデックスへのマッピング\nword_to_index = Vocab.get_stoi()\n\n# 最初の5つのアイテムを取得して表示\nfor i, (word, index) in enumerate(word_to_index.items()):\n    if i >= 5:  # 最初の5つのアイテムのみ表示\n        break\n    print(f\"'{word}': {index}\")\n\n\n\ndf['numericalized_text'] = df[\"processed_text\"].apply(lambda x: [Vocab[token] for token in x])\n\n\n\ndf.head()\n\n\n\ndef pad_sequences(seq, max_len):\n    padded = np.zeros((max_len,), dtype=np.int64)\n    if len(seq) > max_len: padded[:] = seq[:max_len]\n    else: padded[:len(seq)] = seq\n    return padded\n\n\n\n","type":"content","url":"/lstm-classification#id-3","position":13},{"hierarchy":{"lvl1":"LSTMによる文書分類","lvl4":"Padding","lvl3":"テキストデータの前処理","lvl2":"データ準備"},"type":"lvl4","url":"/lstm-classification#padding","position":14},{"hierarchy":{"lvl1":"LSTMによる文書分類","lvl4":"Padding","lvl3":"テキストデータの前処理","lvl2":"データ準備"},"content":"ニューラルネットワークは、入力データが固定長であることを前提としていますので、テキストシーケンスを特定の最大長にパディング（埋める）する必要があります。\n\ndf[\"text_length\"]=df[\"numericalized_text\"].apply(lambda x: len(x)) \n\n\n\ndf[\"text_length\"].describe()\n\n\n\nmax_len=30\ndf['padded_text'] = df['numericalized_text'].apply(lambda x: pad_sequences(x, max_len))\n\n\n\ndf.head()\n\n\n\n","type":"content","url":"/lstm-classification#padding","position":15},{"hierarchy":{"lvl1":"LSTMによる文書分類","lvl2":"学習用データセットの作成(Batch Datasets)"},"type":"lvl2","url":"/lstm-classification#id-batch-datasets","position":16},{"hierarchy":{"lvl1":"LSTMによる文書分類","lvl2":"学習用データセットの作成(Batch Datasets)"},"content":"\n\n# Split the original dataset into training plus validation and testing sets\ntrain_val_df, test_df = train_test_split(df, test_size=0.2)\n\n# Split the training plus validation set into separate training and validation sets\ntrain_df, val_df = train_test_split(train_val_df, test_size=0.25)\n\n\n\n# Create TensorDatasets\ntrain_data = TensorDataset(torch.LongTensor(train_df['padded_text'].tolist()), torch.LongTensor(train_df['label'].tolist()))\nval_data = TensorDataset(torch.LongTensor(val_df['padded_text'].tolist()), torch.LongTensor(val_df['label'].tolist()))\ntest_data = TensorDataset(torch.LongTensor(test_df['padded_text'].tolist()), torch.LongTensor(test_df['label'].tolist()))\n\n# Create DataLoaders\nbatch_size = 32\ntrain_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\nval_loader = DataLoader(val_data, batch_size=batch_size)\ntest_loader = DataLoader(test_data, batch_size=batch_size)\n\n\n\n","type":"content","url":"/lstm-classification#id-batch-datasets","position":17},{"hierarchy":{"lvl1":"LSTMによる文書分類","lvl2":"モデルの作成"},"type":"lvl2","url":"/lstm-classification#id-4","position":18},{"hierarchy":{"lvl1":"LSTMによる文書分類","lvl2":"モデルの作成"},"content":"","type":"content","url":"/lstm-classification#id-4","position":19},{"hierarchy":{"lvl1":"LSTMによる文書分類","lvl3":"メソッドの説明","lvl2":"モデルの作成"},"type":"lvl3","url":"/lstm-classification#id-5","position":20},{"hierarchy":{"lvl1":"LSTMによる文書分類","lvl3":"メソッドの説明","lvl2":"モデルの作成"},"content":"","type":"content","url":"/lstm-classification#id-5","position":21},{"hierarchy":{"lvl1":"LSTMによる文書分類","lvl4":"nn.Embedding","lvl3":"メソッドの説明","lvl2":"モデルの作成"},"type":"lvl4","url":"/lstm-classification#nn-embedding","position":22},{"hierarchy":{"lvl1":"LSTMによる文書分類","lvl4":"nn.Embedding","lvl3":"メソッドの説明","lvl2":"モデルの作成"},"content":"nn.Embeddingは単語の埋め込みを行うために使用されます。単語の埋め込みとは、単語を固定長のベクトルに変換することを指します。このベクトルは、単語の意味的な特性を捉えることができます。nn.Embeddingの主なパラメータは以下の通りです：\n\nnum_embeddings：埋め込みを行う単語の総数。通常は語彙のサイズに設定します。\n\nembedding_dim：各単語の埋め込みベクトルの次元数。\n\nnn.Embeddingは、整数のインデックスを入力として受け取り、それに対応する埋め込みベクトルを出力します。\n\n下の例では、inputの各インデックスが対応する埋め込みベクトルに置き換えられ、embeddedはサイズ(batch_size, sequence_length, embedding_dim)のテンソルになります。","type":"content","url":"/lstm-classification#nn-embedding","position":23},{"hierarchy":{"lvl1":"LSTMによる文書分類","lvl4":"nn.Dropout","lvl3":"メソッドの説明","lvl2":"モデルの作成"},"type":"lvl4","url":"/lstm-classification#nn-dropout","position":24},{"hierarchy":{"lvl1":"LSTMによる文書分類","lvl4":"nn.Dropout","lvl3":"メソッドの説明","lvl2":"モデルの作成"},"content":"ドロップアウトは、ニューラルネットワークの訓練中にランダムにノードを「ドロップアウト」（つまり無効化）することで、過学習を防ぐための一般的なテクニックですnn.Dropoutの主なパラメータは以下の通りです：\n\np：ノードをドロップアウトする確率。0.0（ノードをドロップアウトしない）から1.0（全てのノードをドロップアウトする）までの値を取ります。デフォルトは0.5です。\n\nnn.Dropoutは、訓練中にのみドロップアウトを適用し、評価（つまりモデルが.eval()モードにあるとき）中にはドロップアウトを適用しません。これは、訓練中にはモデルのロバスト性を向上させるためにランダム性が必要である一方、評価中にはモデルの全ての学習特性を使用して一貫した出力を得る必要があるためです。\n\nembedding = nn.Embedding(num_embeddings=10000, embedding_dim=300)\ninput = torch.LongTensor([[1, 2, 4, 5], [4, 3, 2, 9]])\nembedded = embedding(input)\nembedded.shape\n\n\n\n","type":"content","url":"/lstm-classification#nn-dropout","position":25},{"hierarchy":{"lvl1":"LSTMによる文書分類","lvl3":"モデルの定義","lvl2":"モデルの作成"},"type":"lvl3","url":"/lstm-classification#id-6","position":26},{"hierarchy":{"lvl1":"LSTMによる文書分類","lvl3":"モデルの定義","lvl2":"モデルの作成"},"content":"hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n\nここでは、双方向LSTMの最後の隠れ状態を取り扱っています。\n\n双方向LSTMは、順方向と逆方向の2つのLSTMを使用します。順方向のLSTMはシーケンスを通常の順序で処理し、逆方向のLSTMはシーケンスを逆順で処理します。その結果、各時間ステップで2つの隠れ状態（順方向と逆方向のそれぞれから1つずつ）が得られます。\n\nhidden[-2,:,:]とhidden[-1,:,:]は、それぞれ最後の時間ステップでの順方向と逆方向の隠れ状態を取得しています。\n\ntorch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)は、これら2つの隠れ状態を結合しています。結合はdim=1（つまり、特徴量の次元）に沿って行われます。\n\nその結果、順方向と逆方向の隠れ状態が1つのベクトルに結合され、そのベクトルは次の全結合層に入力されます。\n\nself.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n\nself.fcは全結合層で、LSTMからの出力を最終的な出力次元に変換します。この出力は、分類タスクのクラス数に等しいなります。\n\n全結合層の入力次元は、LSTMの隠れ状態の次元数に依存します。\n\nLSTMが双方向の場合（bidirectional=True）、順方向と逆方向の隠れ状態が結合されるため、隠れ状態の次元数はhidden_dim * 2になります。\n\nLSTMが一方向の場合（bidirectional=False）、隠れ状態の次元数はhidden_dimになります。\n\nしたがって、nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)は、LSTMの方向性に応じて全結合層の入力次元を適切に設定します。\n\n出力次元output_dimは、タスクのクラス数または回帰の出力次元に設定します。\n\nbatch_first=True\n\nbatch_first=Trueを設定すると、\n\n入力テンソルの形状は(batch_size, sequence_length, input_size)と解釈されます。つまり、バッチの次元が最初に来ます。\n\noutputテンソルの形状は(batch_size, seq_len, num_directions * hidden_size)になります。\n\nbatch_first=Trueを使用する主な理由は、多くの場合、バッチの次元を最初に持ってくると、テンソル操作が直感的になり、コードが読みやすくなります。\n\nclass LSTMClassifier(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.lstm = nn.LSTM(embedding_dim, \n                            hidden_dim, \n                            num_layers=n_layers, \n                            bidirectional=bidirectional, \n                            dropout=dropout, \n                            batch_first=True)\n        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n        self.bidirectional = bidirectional\n    \n    def forward(self, text):\n        embedded = self.embedding(text)\n        output, (hidden, cell) = self.lstm(embedded)\n        if self.bidirectional:\n            hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)\n        else:\n            hidden = hidden[-1,:,:]\n        return self.fc(hidden.squeeze(0))\n\n\n\nvocab_size = len(Vocab)\nembedding_dim = 100  \nhidden_dim = 256     \noutput_dim = 3 \nn_layers = 2        \nbidirectional = True \ndropout = 0.2        \n\nmodel = LSTMClassifier(vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout)\n\n\n\nmodel = model.to(device)\n\n\n\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters())\n\n\n\ndef train_model(model, train_loader, val_loader, optimizer, criterion, n_epochs):\n    model.train()\n    for epoch in range(n_epochs):\n        for texts, labels in train_loader:\n            texts, labels = texts.to(device), labels.to(device)\n            optimizer.zero_grad()\n            predictions = model(texts)\n            loss = criterion(predictions, labels)\n            loss.backward()\n            optimizer.step()\n\n        # Validation\n        model.eval()\n        with torch.no_grad():\n            val_labels = []\n            val_preds = []\n            for texts, labels in val_loader:\n                texts, labels = texts.to(device), labels.to(device)\n                predictions = model(texts)\n                val_labels.extend(labels.tolist())\n                val_preds.extend(torch.argmax(predictions, dim=1).tolist())\n\n            accuracy = accuracy_score(val_labels, val_preds)\n            f1 = f1_score(val_labels, val_preds, average='weighted')\n            print(f\"Epoch {epoch+1}, Loss: {loss.item()}, Accuracy: {accuracy}, F1 Score: {f1}\")\n        model.train()\n\n\n\nfrom torch.utils.tensorboard import SummaryWriter\n\ndef train_model(model, train_loader, val_loader, optimizer, criterion, n_epochs, tensorboard=False, tensorboard_path='./runs'):\n    # Initialize TensorBoard writer if tensorboard logging is enabled\n    writer = SummaryWriter(tensorboard_path) if tensorboard else None\n\n    model.train()\n    for epoch in range(n_epochs):\n        for texts, labels in train_loader:\n            texts, labels = texts.to(device), labels.to(device)\n            optimizer.zero_grad()\n            predictions = model(texts)\n            loss = criterion(predictions, labels)\n            loss.backward()\n            optimizer.step()\n\n        # Validation\n        model.eval()\n        with torch.no_grad():\n            val_labels = []\n            val_preds = []\n            for texts, labels in val_loader:\n                texts, labels = texts.to(device), labels.to(device)\n                predictions = model(texts)\n                val_labels.extend(labels.tolist())\n                val_preds.extend(torch.argmax(predictions, dim=1).tolist())\n\n            accuracy = accuracy_score(val_labels, val_preds)\n            f1 = f1_score(val_labels, val_preds, average='weighted')\n\n            # Log metrics to TensorBoard\n            if tensorboard:\n                writer.add_scalar('Loss/train', loss.item(), epoch)\n                writer.add_scalar('Accuracy/val', accuracy, epoch)\n                writer.add_scalar('F1-Score/val', f1, epoch)\n\n            print(f\"Epoch {epoch+1}, Loss: {loss.item()}, Accuracy: {accuracy}, F1 Score: {f1}\")\n\n        model.train()\n\n    # Close the TensorBoard writer\n    if tensorboard:\n        writer.close()\n\n\n\n# Train the model\nn_epochs = 30\n#train_model(model, train_loader, val_loader, optimizer, criterion, n_epochs, tensorboard=True, tensorboard_path='./runs/lstm')\n\n","type":"content","url":"/lstm-classification#id-6","position":27},{"hierarchy":{"lvl1":"数学基礎"},"type":"lvl1","url":"/math-basis2","position":0},{"hierarchy":{"lvl1":"数学基礎"},"content":"\n\n本講義は、深層学習(主にニューラルネットワーク)について解説します。ここでは、ニューラルネットワークの理解に必要な数学の基本知識をおさらいします。\n\n","type":"content","url":"/math-basis2","position":1},{"hierarchy":{"lvl1":"数学基礎","lvl2":"微分"},"type":"lvl2","url":"/math-basis2#id","position":2},{"hierarchy":{"lvl1":"数学基礎","lvl2":"微分"},"content":"\n\nゼノンのパラドックス\n\n速いアキレスが遅い亀を追いかける。亀が少し先からスタートした場合、アキレスがその地点に到達する間に、亀は少しだけ前に進んでしまう。\nアキレスが新しい位置に到達するたびに、亀もわずかに進むため、アキレスは無限に追いつけないように見える。\n\n空を飛ぶ矢を、ある瞬間に観察すると、その矢は特定の位置に「静止」している。すべての瞬間で矢が静止しているなら、「動き」は存在しないのではないか？\n\nパラドックスの原因は 0÷0:\\frac{\\Delta x}{\\Delta t} = \\frac{x(t+\\Delta t) - x(t)}{\\Delta t}\n\n極限（limit）の概念を導入することで、これらのパラドックスを解決できる。極限は、ある変数が特定の値に近づくときの関数の挙動を理解するための数学的手法である。\n\nアキレスと亀の例では、アキレスが亀に追いつく時間を考えるとき、時間の間隔を無限に小さくすることで、アキレスが亀に追いつく瞬間を正確に特定できる。\n\n矢の例では、矢の位置を時間の関数として表現し、時間が非常に小さな値に近づくときの矢の位置の変化を考えることで、矢が動いていることを理解できる。\n\n微分は「極限的に短い時間における変化率」を定義することにより、時間間隔がゼロに近づいても変化率（＝速度）は有限の値として存在することを整合的に表す（極限で無限を有限として扱える）。\n\n","type":"content","url":"/math-basis2#id","position":3},{"hierarchy":{"lvl1":"数学基礎","lvl3":"微分の概念","lvl2":"微分"},"type":"lvl3","url":"/math-basis2#id-1","position":4},{"hierarchy":{"lvl1":"数学基礎","lvl3":"微分の概念","lvl2":"微分"},"content":"微分とは、結論から言うと、変数の微小な変化に対応する、関数の変化量を求めることです。\n\n微分を用いると接線の傾きを計算することができます。このことから、微分が関数の最小化問題に有用なツールであることがわかります。\n\nxからℎだけ離れた点𝑥+ℎを考え, 2点を通る直線の傾きを求めることができます。a= \\frac{f(x + h) - f(x)}{(x+h)-x}\n\n次にhをh \\rightarrow 0のように小さくしていけば、直線の開始点と終了点の2点が1点に収束し、1点での接線として考えることができます。このように、平均変化率の極限値が存在するならば、微分可能であると言います。この式をfの導関数 (derivative)と呼び、f'(x)と書きます。関数f(x)の微分係数f'(a)はその曲線の点(a,f(a))(接点)における接線の傾きです。f'(x)= \\lim_{h \\rightarrow 0} \\frac{f(x + h) - f(x)}{h}\n\n導関数を求めることを微分(differentiation)するといいます。 記号の使い方として、f'(x)を\\frac{df}{dx} (x) または  \\frac{d}{dx}f (x)と書きます。\n\nNote\n\n関数 f(x) について、変数 xをある値 a に限りなく近づけるとき、どのような近づけ方に対しても関数 f(x)　の値が一定値Aに限りなく近づくならば、xがaに近づくときf(x)は値Aに収束といい、Aを極限値と言います。\n\n","type":"content","url":"/math-basis2#id-1","position":5},{"hierarchy":{"lvl1":"数学基礎","lvl3":"Pythonによる微分計算","lvl2":"微分"},"type":"lvl3","url":"/math-basis2#python","position":6},{"hierarchy":{"lvl1":"数学基礎","lvl3":"Pythonによる微分計算","lvl2":"微分"},"content":"Pythonで分の近似計算関数を実装することができます\n\n# 微分の近似計算の精度と計算の安定性のバランスをとるために、中心差分法といった関数の前後の点を使用して計算する方法を用いる\ndef numerical_diff(f,x):\n    h = 1e-4 # 微小な変化量: 0.0001\n    nd = (f(x+h) - f(x-h))/(2 * h)\n    return nd\n\n\n\ndef function_1(x):\n    return x**3 - 3*x**2 + x\n\n\n\nprint(numerical_diff(function_1, 4)) # 25.0 に近い値が期待される\n\n\n\nPyTorchでは、微分を計算するために自動微分機能を利用することができます。\n\nNote\n\nPyTorchはPythonのオープンソースの機械学習・深層学習ライブラリです。\n\nimport torch\n\n# 入力値をテンソルとして作成し、勾配計算を有効にする\nx = torch.tensor(4.0, requires_grad=True)\n\n# 関数の出力を計算\ny = function_1(x)\n\n# 勾配を計算\ny.backward()\n\n# 勾配を表示\nprint(x.grad.item())\n\n\n\n","type":"content","url":"/math-basis2#python","position":7},{"hierarchy":{"lvl1":"数学基礎","lvl3":"微分の公式","lvl2":"微分"},"type":"lvl3","url":"/math-basis2#id-2","position":8},{"hierarchy":{"lvl1":"数学基礎","lvl3":"微分の公式","lvl2":"微分"},"content":"覚えておくと便利な微分の公式がありますので，以下に幾つか紹介していきます。\\begin{split}\\begin{align}\n\\left( c\\right) ^{'}&=0 \\\\\n\\left( x\\right)^{'}&=1\\\\\n\\left( cf(x) \\right)^{'} &= c f'(x) \\\\\n\\left( x^{n} \\right)^{'} &=nx^{n-1} \\\\\n\\left( f(x) + g(x) \\right) ^{'} &=f^{'}(x)+g^{'}(x) \\\\\n\\left( f(x) g(x) \\right) ^{'} &= f^{'}(x)g(x) + f(x)g^{'}(x) \\\\\n\\left( f(g(x)) \\right) ^{'} &= \\frac{df(u)}{du}\\frac{du}{dx} = f^{'}(g(x)) \\cdot g^{'}(x) \\\\\n\\end{align}\\end{split}\n\n","type":"content","url":"/math-basis2#id-2","position":9},{"hierarchy":{"lvl1":"数学基礎","lvl3":"合成関数の微分","lvl2":"微分"},"type":"lvl3","url":"/math-basis2#id-3","position":10},{"hierarchy":{"lvl1":"数学基礎","lvl3":"合成関数の微分","lvl2":"微分"},"content":"𝑦=𝑓(𝑥)と 𝑧=𝑔(𝑦)の合成関数とは、𝑓を適用したあとに𝑔を適用する関数、すなわち 𝑧=𝑔(𝑓(𝑥))のことを指します。\n\n合成関数の導関数がそれぞれの導関数の積で与えられる性質は連鎖律（chain rule）と言います。\\frac{d}{dx} f(g(x)) = \\frac{df(u)}{du}\\frac{du}{dx}\n\n合成関数の計算例\n\n関数\ny = \\sin(3x^2 + 4x)\nの微分を求めます。\n\nここで、f(u) = \\sin(u)g(x) = 3x^2 + 4x\n\nとし、y は f と g の合成関数、すなわち y = f(g(x)) であると考えます。\n\n連鎖律を用いると:\\frac{dy}{dx} = \\frac{df}{du} \\cdot \\frac{du}{dx}\n\nf'(u) = \\cos(u)\n\ng'(x) = 6x + 4\n\nこれらを連鎖律の式に代入すると:\\frac{dy}{dx} = \\cos(3x^2 + 4x) \\cdot (6x + 4)\n\nしたがって、y' =  \\cos(3x^2 + 4x) \\cdot (6x + 4)\n\nが得られます。\n\nこのように、連鎖律を使用することで、合成関数の微分を計算することができます。\n\n","type":"content","url":"/math-basis2#id-3","position":11},{"hierarchy":{"lvl1":"数学基礎","lvl3":"偏微分","lvl2":"微分"},"type":"lvl3","url":"/math-basis2#id-4","position":12},{"hierarchy":{"lvl1":"数学基礎","lvl3":"偏微分","lvl2":"微分"},"content":"機械学習において、多くの場合、複数の入力変数 𝑥_1,𝑥_2,…,𝑥_nを用いて𝑦を予測する多変数関数が扱われます。\n\n偏微分とは、n変数関数のある一つの変数以外のn-1個の変数の値を固定し、残りの1つの変数について関数を微分することです。\n\n例えば、ある入力 𝑥_nにのみ注目する偏微分は以下のように表します。\\frac{\\partial}{\\partial x_{n}} f(x_1, x_2, \\dots, x_n)\n\n微分を意味する記号が、𝑑から\\partialに変わっています。こうすると、\\frac{\\partial}{\\partial x_{n}}は x_n以外を定数と考え、 x_nにのみ着目して微分を行うという意味となります。\n\n偏微分の例\\begin{split}\\begin{aligned}\n\\frac{\\partial}{\\partial x_1}\n\\left( 3x_1+4x_2 \\right)\n&= \\frac{\\partial}{\\partial x_1}\n\\left( 3x_1 \\right) + \\frac{\\partial}{\\partial x_1} \\left( 4x_2 \\right) \\\\\n&= 3 \\times \\frac{\\partial}{\\partial x_1} \\left( x_1 \\right) + 4 \\times \\frac{\\partial}{\\partial x_1} x_2 \\\\\n&= 3 \\times 1 + 4 \\times 0 \\\\\n&= 3\n\\end{aligned}\\end{split}\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sympy import symbols, diff\n#plt.style.use('seaborn-poster') \n\nx, y = symbols('x y')\n\n# 関数の定義\nf = x**2 + y**2\n\n# 偏微分の計算\npartial_x = diff(f, x)\npartial_y = diff(f, y)\n\nprint(\"Partial Derivative with respect to x:\", partial_x)\nprint(\"Partial Derivative with respect to y:\", partial_y)\n\n# 可視化のためのデータ生成\nX, Y = np.meshgrid(np.linspace(-5, 5, 100), np.linspace(-5, 5, 100))\nZ = X**2 + Y**2\nZx = 2*X\nZy = 2*Y\n\nfig = plt.figure(figsize=(20, 6))\n\n# 関数の可視化\nax1 = fig.add_subplot(1, 3, 1, projection='3d')\nax1.plot_surface(X, Y, Z, cmap='jet')\nax1.set_title(\"Function: $f(x, y) = x^2 + y^2$\",size=20)\nax1.set_xlabel('$x$', labelpad=15)\nax1.set_ylabel('$y$', labelpad=15)\nax1.set_zlabel('$f(x, y)$', labelpad=15)\n\n# xに関する偏微分\nax2 = fig.add_subplot(1, 3, 2, projection='3d')\nax2.plot_surface(X, Y, Zx, cmap='jet')\nax2.set_title(r\"Partial Derivative: $\\frac{\\partial}{\\partial x}$\",size=20)\nax2.set_xlabel('$x$', labelpad=15)\nax2.set_ylabel('$y$', labelpad=15)\nax2.set_zlabel(r'$\\frac{\\partial f}{\\partial x}$', labelpad=15)\n\n# yに関する偏微分\nax3 = fig.add_subplot(1, 3, 3, projection='3d')\nax3.plot_surface(X, Y, Zy, cmap='jet')\nax3.set_title(r\"Partial Derivative: $\\frac{\\partial}{\\partial y}$\",size=20)\nax3.set_xlabel('$x$', labelpad=15)\nax3.set_ylabel('$y$', labelpad=15)\nax3.set_zlabel(r'$\\frac{\\partial f}{\\partial y}$', labelpad=15)\n\nplt.tight_layout()\nplt.show()\n\n\n\n","type":"content","url":"/math-basis2#id-4","position":13},{"hierarchy":{"lvl1":"数学基礎","lvl2":"線型代数"},"type":"lvl2","url":"/math-basis2#id-5","position":14},{"hierarchy":{"lvl1":"数学基礎","lvl2":"線型代数"},"content":"\n\n","type":"content","url":"/math-basis2#id-5","position":15},{"hierarchy":{"lvl1":"数学基礎","lvl3":"ベクトル","lvl2":"線型代数"},"type":"lvl3","url":"/math-basis2#id-6","position":16},{"hierarchy":{"lvl1":"数学基礎","lvl3":"ベクトル","lvl2":"線型代数"},"content":"","type":"content","url":"/math-basis2#id-6","position":17},{"hierarchy":{"lvl1":"数学基礎","lvl4":"ベクトルとは","lvl3":"ベクトル","lvl2":"線型代数"},"type":"lvl4","url":"/math-basis2#id-7","position":18},{"hierarchy":{"lvl1":"数学基礎","lvl4":"ベクトルとは","lvl3":"ベクトル","lvl2":"線型代数"},"content":"ベクトル(vector)とは、大きさと向きを持つ量です。ベクトルは、数が一列に並んだ集まりとして表現できます。例えば、\\begin{split}{\\bf x}= \\begin{bmatrix}\nx_{1} \\\\\nx_{2} \\\\\nx_{3}\n\\end{bmatrix}, \\\n{\\bf y}=\\begin{bmatrix}\ny_{1} \\\\\ny_{2} \\\\\n\\vdots \\\\\ny_{N}\n\\end{bmatrix}\\end{split}\n\n上の例のように、その要素を縦方向に並べたものは列ベクトルと呼びます。一方、{\\bf z}=\\begin{bmatrix}\nz_{1} & z_{2} & z_{3}\n\\end{bmatrix}\n\nのように、要素を横方向に並べたものは行ベクトルと呼びます。\n\n一般的には、ベクトルを数式で書く際には, \\mathbf{W}のように太字の記号で表現するか、\\vec{W}のようにベクトルの上に矢印を付けてベクトルを示すことが多いです。\n\nNote\n\n線形代数に対する直感的な理解を深めるために、\n\nImmersive Linear Algebraを参照してください。\n\n","type":"content","url":"/math-basis2#id-7","position":19},{"hierarchy":{"lvl1":"数学基礎","lvl4":"ベクトルの基本演算","lvl3":"ベクトル","lvl2":"線型代数"},"type":"lvl4","url":"/math-basis2#id-8","position":20},{"hierarchy":{"lvl1":"数学基礎","lvl4":"ベクトルの基本演算","lvl3":"ベクトル","lvl2":"線型代数"},"content":"加算（足し算）及び減算（引き算）は同じサイズのベクトル同士の間だけで成立します。\\begin{split}\\begin{bmatrix}\n1 \\\\\n2 \\\\\n3\n\\end{bmatrix}+\\begin{bmatrix}\n4 \\\\\n5 \\\\\n6\n\\end{bmatrix}=\\begin{bmatrix}\n1 + 4 \\\\\n2 + 5 \\\\\n3 + 6\n\\end{bmatrix}=\\begin{bmatrix}\n5 \\\\\n7 \\\\\n9\n\\end{bmatrix}\\end{split}\n\nスカラ倍とはベクトルにスカラを掛ける演算です。\\begin{split}\n10\n\\begin{bmatrix}\n1 \\\\\n2 \\\\\n3\n\\end{bmatrix}=\\begin{bmatrix}\n10 * 1 \\\\\n10 * 2 \\\\\n10 * 3\n\\end{bmatrix}=\\begin{bmatrix}\n10 \\\\\n20 \\\\\n30\n\\end{bmatrix}\\end{split}\n\n複数のベクトル \\vec{a}_1, \\vec{a}_2, \\dots, \\vec{a}_n に対して、c_1\\vec{a}_1 + c_2\\vec{a}_2 + \\dots + c_n\\vec{a}_n\n\nの形で作られるベクトルを線形結合といいます。\n\nそれらのベクトルをスカラーで伸ばしたり、足したりして作れるすべてのベクトルの集合をそれらのベクトルが張る空間（span）と呼びます\n\n「あるベクトル集合のすべての線形結合を集めた集合」は、ベクトル空間になる。\n\nベクトル空間 V において、\nその空間を張る最小限のベクトルの組（基底）が存在します。\n\nこのとき、\\dim(V) = \\text{基底を構成するベクトルの本数}\n\nを次元（dimension）と呼びます。\n\n","type":"content","url":"/math-basis2#id-8","position":21},{"hierarchy":{"lvl1":"数学基礎","lvl4":"ベクトルの内積","lvl3":"ベクトル","lvl2":"線型代数"},"type":"lvl4","url":"/math-basis2#id-9","position":22},{"hierarchy":{"lvl1":"数学基礎","lvl4":"ベクトルの内積","lvl3":"ベクトル","lvl2":"線型代数"},"content":"\n\n内積 (ドット積) とは、同じサイズの2つのベクトルは、それぞれのベクトルの同じ位置に対応する要素同士を掛け、それらを足し合わせる計算です。𝐱と𝐲の内積は𝐱\\cdot𝐲で表されます。\\begin{split}\\begin{aligned}& \\begin{bmatrix}\n1 & 2 & 3\n\\end{bmatrix} \\cdot \\begin{bmatrix}\n4 \\\\\n5 \\\\\n6\n \\end{bmatrix} = 1 \\times 4 + 2 \\times 5  + 3 \\times 6 = 32 \\end{aligned}\\end{split}\n\nドット積は、2つのベクトルの長さと角度に関係しています：\\mathbf{a} \\cdot \\mathbf{b} = |\\mathbf{a}|\\,|\\mathbf{b}|\\,\\cos\\theta\n\nここで：\n\n|\\mathbf{a}| と |\\mathbf{b}| はベクトルの長さ（ノルム）\n\n\\theta はベクトル間の角度\n\n証明\n\nベクトル\\mathbf{a} = (a_1, a_2), \\quad \\mathbf{b} = (b_1, b_2)\n\nを考え、原点 O からそれぞれの終点を A, B とします。\nなす角を \\theta とします（図のように）。\n\n三角形 OAB に余弦定理を使うと，AB^2 = OA^2 + OB^2 - 2\\,OA\\,OB\\,\\cos\\theta\n\nすなわちベクトルの長さで表すと，\\|\\mathbf{a} - \\mathbf{b}\\|^2 = \\|\\mathbf{a}\\|^2 + \\|\\mathbf{b}\\|^2 - 2\\,\\|\\mathbf{a}\\|\\,\\|\\mathbf{b}\\|\\,\\cos\\theta\\|\\mathbf{a} - \\mathbf{b}\\|^2\n= (a_1 - b_1)^2 + (a_2 - b_2)^2\n\n展開して整理すると：\\|\\mathbf{a} - \\mathbf{b}\\|^2\n= (a_1^2 + a_2^2) + (b_1^2 + b_2^2) - 2(a_1b_1 + a_2b_2)\n\nすなわち，\\|\\mathbf{a} - \\mathbf{b}\\|^2 = \\|\\mathbf{a}\\|^2 + \\|\\mathbf{b}\\|^2 - 2(a_1b_1 + a_2b_2)\n\n右辺の形が同じなので，2(a_1b_1 + a_2b_2)\n= 2\\,\\|\\mathbf{a}\\|\\,\\|\\mathbf{b}\\|\\,\\cos\\theta\n\nしたがって，\\cos\\theta = \\dfrac{a_1b_1 + a_2b_2}{\\|\\mathbf{a}\\|\\,\\|\\mathbf{b}\\|}\n\n","type":"content","url":"/math-basis2#id-9","position":23},{"hierarchy":{"lvl1":"数学基礎","lvl3":"行列","lvl2":"線型代数"},"type":"lvl3","url":"/math-basis2#id-10","position":24},{"hierarchy":{"lvl1":"数学基礎","lvl3":"行列","lvl2":"線型代数"},"content":"","type":"content","url":"/math-basis2#id-10","position":25},{"hierarchy":{"lvl1":"数学基礎","lvl4":"行列とは","lvl3":"行列","lvl2":"線型代数"},"type":"lvl4","url":"/math-basis2#id-11","position":26},{"hierarchy":{"lvl1":"数学基礎","lvl4":"行列とは","lvl3":"行列","lvl2":"線型代数"},"content":"行列 (matrix) は同じサイズのベクトルを複数個並べたものです。例えば、\\begin{split}\n{\\bf X} =\n\\begin{bmatrix}\nx_{11} & x_{12} \\\\\nx_{21} & x_{22} \\\\\nx_{31} & x_{32}\n\\end{bmatrix}\n\\end{split}\n\n\\mathbf{X}は「 3 行 2 列の行列」になります。","type":"content","url":"/math-basis2#id-11","position":27},{"hierarchy":{"lvl1":"数学基礎","lvl4":"行列積","lvl3":"行列","lvl2":"線型代数"},"type":"lvl4","url":"/math-basis2#id-12","position":28},{"hierarchy":{"lvl1":"数学基礎","lvl4":"行列積","lvl3":"行列","lvl2":"線型代数"},"content":"行列の乗算には、行列積、外積、要素積（アダマール積）など複数の方法があります。 ここではそのうち、機械学習の多くの問題で登場します行列積について説明します。\n\n行列\\mathbf{A}と行列\\mathbf{B}の行列積は\\mathbf{AB}と書き 、\\mathbf{A}の各行と\\mathbf{B}の各列の内積を並べたものとして定義されます。\n\n例えば、行列\\mathbf{A}の1行目の行ベクトルと、行列\\mathbf{B}の1列目の列ベクトルの内積の結果は、\\mathbf{A}と\\mathbf{B}の行列積の結果を表す行列\\mathbf{C}の1行1列目に対応します。\n\n内積が定義される条件はベクトルのサイズが等しいということでしたが、ここでもそれが成り立つために、\\mathbf{A}の行のサイズと \\mathbf{B} の列のサイズが一致する必要があります。\n\n外積と要素積\n\n外積\n\n2つのベクトル \\mathbf{a}\\in\\mathbb{R}^m，\\mathbf{b}\\in\\mathbb{R}^n について：\\mathbf{a}\\mathbf{b}^T =\n\\begin{bmatrix}\na_1 b_1 & a_1 b_2 & \\dots & a_1 b_n \\\\\na_2 b_1 & a_2 b_2 & \\dots & a_2 b_n \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_m b_1 & a_m b_2 & \\dots & a_m b_n\n\\end{bmatrix}\n\nとなり、行列（m×n）を生成します。\\mathbf{a} =\n\\begin{bmatrix}1\\\\2\\\\3\\end{bmatrix},\n\\quad\n\\mathbf{b} =\n\\begin{bmatrix}4\\\\5\\end{bmatrix}\\mathbf{a}\\mathbf{b}^T =\n\\begin{bmatrix}\n1\\cdot4 & 1\\cdot5\\\\\n2\\cdot4 & 2\\cdot5\\\\\n3\\cdot4 & 3\\cdot5\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n4 & 5\\\\\n8 & 10\\\\\n12 & 15\n\\end{bmatrix}\n\n要素積\n\n同じサイズの2つの行列 A, B に対して，A \\circ B = [A_{ij} B_{ij}]\n\nすなわち、同じ位置の要素同士を掛けるだけの積です。A =\n\\begin{bmatrix}\n1 & 2\\\\\n3 & 4\n\\end{bmatrix}, \\quad\nB =\n\\begin{bmatrix}\n5 & 6\\\\\n7 & 8\n\\end{bmatrix}A \\circ B =\n\\begin{bmatrix}\n1\\cdot5 & 2\\cdot6\\\\\n3\\cdot7 & 4\\cdot8\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n5 & 12\\\\\n21 & 32\n\\end{bmatrix}\n\n","type":"content","url":"/math-basis2#id-12","position":29},{"hierarchy":{"lvl1":"数学基礎","lvl4":"転置","lvl3":"行列","lvl2":"線型代数"},"type":"lvl4","url":"/math-basis2#id-13","position":30},{"hierarchy":{"lvl1":"数学基礎","lvl4":"転置","lvl3":"行列","lvl2":"線型代数"},"content":"転置（transpose）とは、m 行 n 列の行列 \\mathbf{A} に対して、 \\mathbf{A} の (i, j) 要素と (j, i) 要素を入れ替えて、n 行 m 列の行列に変換する操作です。転置は行列の右肩にTと書くことで表します。\\begin{split}\n{\\bf A} =\\begin{bmatrix}\n1 & 4 \\\\\n2 & 5 \\\\\n3 & 6\n\\end{bmatrix}, \\\n{\\bf A}^{\\rm T}=\\begin{bmatrix}\n1 & 2 & 3 \\\\\n4 & 5 & 6\n\\end{bmatrix}\n\\end{split}\n\n転置について、以下の定理を覚えておきましょう。\\begin{split}\\begin{aligned}\n&\\left( 1\\right) \\ \\left( {\\bf A}^{\\rm T} \\right)^{\\rm T} = {\\bf A} \\\\\n&\\left( 2\\right) \\ \\left( {\\bf A}{\\bf B} \\right)^{\\rm T} = {\\bf B}^{\\rm T}{\\bf A}^{\\rm T}\\\\\n&\\left( 3\\right) \\ \\left( {\\bf A}{\\bf B}{\\bf C} \\right)^{\\rm T} = {\\bf C}^{\\rm T}{\\bf B}^{\\rm T}{\\bf A}^{\\rm T}\n\\end{aligned}\\end{split}\n\n","type":"content","url":"/math-basis2#id-13","position":31},{"hierarchy":{"lvl1":"数学基礎","lvl4":"ベクトルによる微分と勾配","lvl3":"行列","lvl2":"線型代数"},"type":"lvl4","url":"/math-basis2#id-14","position":32},{"hierarchy":{"lvl1":"数学基礎","lvl4":"ベクトルによる微分と勾配","lvl3":"行列","lvl2":"線型代数"},"content":"線形代数と深層学習\n\n深層学習は、本質的に「多数のパラメータによる線形結合」と「非線形変換」による成り立つモデルであり、以下のような計算が行われますy_i = w_{i1}x_1 + w_{i2}x_2 + \\dots + w_{in}x_n + b_i\n\nただ、深層学習モデルには、このような計算が多く含まれています\\begin{cases}\ny_1 = w_{11}x_1 + w_{12}x_2 + \\cdots + w_{1n}x_n + b_1 \\\\\ny_2 = w_{21}x_1 + w_{22}x_2 + \\cdots + w_{2n}x_n + b_2 \\\\\n\\vdots \\\\\ny_m = w_{m1}x_1 + w_{m2}x_2 + \\cdots + w_{mn}x_n + b_m\n\\end{cases}\n\nここで、線形代数を使うことでまとめると非常に簡潔になります\\mathbf{y} = \\mathbf{W}\\mathbf{x} + \\mathbf{b}\n\n線形結合とは、スカラー倍したベクトル同士を足し合わせることです。\n\n例えば、\\begin{split}\\begin{aligned}\n{\\bf b}\n&=\\begin{bmatrix}\n3 \\\\\n4\n\\end{bmatrix}, \\\n{\\bf x} =\n\\begin{bmatrix}\nx_{1} \\\\\nx_{2}\n\\end{bmatrix}\\\\\n{\\bf b}^{\\rm T}{\\bf x} &=\n\\begin{bmatrix}\n3 & 4\n\\end{bmatrix}\n\\begin{bmatrix}\nx_1 \\\\\nx_2\n\\end{bmatrix}\n= 3x_1 + 4x_2\n\\end{aligned}\\end{split}\n\nのように\\mathbf{x}の要素であるx_1およびx_2に関して一次式となっています。\n\n\\mathbf{𝐛^T𝐱}をベクトル\\mathbf{x}で微分したものを、\\frac{\\partial}{\\partial {\\bf x}} \\left( {\\bf b}^{\\rm T}{\\bf x} \\right)\n\nと表します。\n\n「ベクトルで微分」とは、ベクトルのそれぞれの要素で対象を微分し、その結果を要素に対応する位置に並べてベクトルを作ることです。つまり、\\begin{split}\n\\begin{aligned}\n\\frac{\\partial}{\\partial {\\bf x}} \\left( {\\bf b}^{\\rm T} {\\bf x} \\right)\n&= \\frac{\\partial}{\\partial {\\bf x}} \\left( 3x_1 + 4x_2 \\right) \\\\\n&=\n\\begin{bmatrix}\n\\frac{\\partial}{\\partial x_1} \\left( 3x_1 + 4x_2 \\right) & \\frac{\\partial}{\\partial x_2} \\left( 3x_1 + 4x_2 \\right)\n\\end{bmatrix}\n\\end{aligned}\n\\end{split}\n=\n\\begin{bmatrix}\n3 & 4\n\\end{bmatrix}\n\n\\mathbf{x}^{\\mathrm{T}} \\mathbf{y} = \\mathbf{x} \\cdot \\mathbf{y}\n\n転置はベクトルに対しても定義できます。転置を用いると、2 つの列ベクトル𝐱,𝐲の内積𝐱\\cdot𝐲は、行列積を用いてx^T 𝐲と書けます。\n\n\\mathbf{x} と \\mathbf{y} がともに n 次元のベクトルであるとき：\\mathbf{x} =\n\\begin{bmatrix}\nx_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n\n\\end{bmatrix},\n\\quad\n\\mathbf{y} =\n\\begin{bmatrix}\ny_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n\n\\end{bmatrix}\n\n列ベクトル \\mathbf{x} の転置は行ベクトルになります：\\mathbf{x}^{\\mathrm{T}} = [x_1, x_2, \\dots, x_n]\n\nしたがって、行列の掛け算として：\\mathbf{x}^{\\mathrm{T}} \\mathbf{y}\n=\n[x_1, x_2, \\dots, x_n]\n\\begin{bmatrix}\ny_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n\n\\end{bmatrix}\n=\nx_1 y_1 + x_2 y_2 + \\cdots + x_n y_n\n\nすなわち、内積は転置を用いた行列積として表すことができる\\boxed{\\mathbf{x}^{\\mathrm{T}} \\mathbf{y} = \\mathbf{x} \\cdot \\mathbf{y}}\n\n入力ベクトルの要素毎に出力に対する偏微分を計算し、それらを並べてベクトルにしたものが 勾配 (gradient) と言います。\n\nつまり、多変数関数における傾きのようなもので、1変数関数のようにスカラーではなく、勾配で最も急な変化の方向とその大きさを示しています。勾配ベクトルは以下のように定義されます：\\nabla f = \\left( \\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y}, \\ldots \\right)\n\nここで、関数f(x,y)=x^2+y^2の値等高線とその勾配を同時にプロットします。赤い矢印が勾配ベクトルを示しており、各点で関数の最も急な上昇方向を指し示しています。\n\nNote\n\n勾配は、関数の局所的な形状や変化の方向を理解する上で非常に役立ちます。特に機械学習や最適化の分野では、勾配降下法というアルゴリズムが関数の最小値を求めるために使用され、この勾配の概念が中心的な役割を果たしています。\n\n\n# 関数とその勾配\ndef function(x, y):\n    return x**2 + y**2\n\ndef gradient(x, y):\n    dfdx = 2*x\n    dfdy = 2*y\n    return dfdx, dfdy\n\n# 座標の生成\nx = np.linspace(-5, 5, 20)\ny = np.linspace(-5, 5, 20)\nX, Y = np.meshgrid(x, y)\nZ = function(X, Y)\nU, V = gradient(X, Y)\n\n# 可視化\nplt.figure(figsize=(10, 8))\nplt.contour(X, Y, Z, levels=50, cmap='jet')  # 値を等高線で表示\nplt.quiver(X, Y, U, V, angles='xy', scale_units='xy', scale=10, color='red', width=0.005)  # 勾配ベクトルを矢印で表示\nplt.axhline(0, color='black', linewidth=0.5)\nplt.axvline(0, color='black', linewidth=0.5)\nplt.title('Gradient of $f(x, y) = x^2 + y^2$')\nplt.xlabel('$x$')\nplt.ylabel('$y$')\nplt.grid(True)\nplt.show()\n\n","type":"content","url":"/math-basis2#id-14","position":33},{"hierarchy":{"lvl1":"機械学習の基本概念"},"type":"lvl1","url":"/ml-basis2","position":0},{"hierarchy":{"lvl1":"機械学習の基本概念"},"content":"","type":"content","url":"/ml-basis2","position":1},{"hierarchy":{"lvl1":"機械学習の基本概念","lvl2":"教師あり学習"},"type":"lvl2","url":"/ml-basis2#id","position":2},{"hierarchy":{"lvl1":"機械学習の基本概念","lvl2":"教師あり学習"},"content":"機械学習は、コンピュータが大量のデータを学習することで、データの中に潜むパターンと規則性を抽出する技術です。ここで、「学習」は、観察されたデータをモデルに適合させるための調整可能な「パラメータ」を与えるために行われます。\n\n機械学習は「教師あり学習」や「教師なし学習」、「強化学習」などの枠組が存在します。ここでは、教師あり学習に注目します。\n\n教師あり学習（Supervised Learning）では、入力データ（特徴量）とそれに対応する正解ラベル（目標値）のペアを使用してモデルを訓練します。\n\nモデルは、入力データと正解ラベルの間の関係やパターンを学習し、未知の入力データに対して正しい予測や分類を行うことが期待されます。テキスト分類、構文解析、機械通訳などの様々な自然言語処理タスクは、教師あり学習の問題として定式化できます。\n\n教師データで学習した入出力の関係性がラベルを持たない未知の入力データにも使えるような関係性である必要があります。このように未知のデータにも対応できる性質を 汎用性 と言います。\n\n高い推定精度を持ちかつ汎用性の高い関係性を見つけるのは教師あり学習の目的になります。\n\n","type":"content","url":"/ml-basis2#id","position":3},{"hierarchy":{"lvl1":"機械学習の基本概念","lvl2":"機械学習のデータ構造"},"type":"lvl2","url":"/ml-basis2#id-1","position":4},{"hierarchy":{"lvl1":"機械学習の基本概念","lvl2":"機械学習のデータ構造"},"content":"","type":"content","url":"/ml-basis2#id-1","position":5},{"hierarchy":{"lvl1":"機械学習の基本概念","lvl3":"学習データ、検証データとテストデータ","lvl2":"機械学習のデータ構造"},"type":"lvl3","url":"/ml-basis2#id-2","position":6},{"hierarchy":{"lvl1":"機械学習の基本概念","lvl3":"学習データ、検証データとテストデータ","lvl2":"機械学習のデータ構造"},"content":"教師あり学習ではデータセットを「学習データ」、「検証データ」、「テストデータ」の3つに分けて使うのが一般的です。\n\n学習データ（Training Data）:モデルのパラメータを調整するために使用されます。学習データから、モデルがデータのパターンや関連性を「学び」ます。\n\n検証データ（Validation Data）: 検証データを使用してモデルの性能を評価し、その結果に基づいてハイパーパラメータを調整することができます。\n\nテストデータ（Test Data）: このデータセットは、学習や検証のプロセスには一切使用されず、モデルの最終的な性能を評価するためだけに使用されます。テストデータを使用してモデルの性能を評価することで、実際の未知のデータに対するモデルの予測性能を推定することができます。\n\n","type":"content","url":"/ml-basis2#id-2","position":7},{"hierarchy":{"lvl1":"機械学習の基本概念","lvl2":"機械学習モデルの評価指標"},"type":"lvl2","url":"/ml-basis2#id-3","position":8},{"hierarchy":{"lvl1":"機械学習の基本概念","lvl2":"機械学習モデルの評価指標"},"content":"機械学習のモデルが良いか悪いかを評価するための評価基準は「評価指標」と言います。","type":"content","url":"/ml-basis2#id-3","position":9},{"hierarchy":{"lvl1":"機械学習の基本概念","lvl2":"回帰タスク評価指標"},"type":"lvl2","url":"/ml-basis2#id-4","position":10},{"hierarchy":{"lvl1":"機械学習の基本概念","lvl2":"回帰タスク評価指標"},"content":"y_iはi個目サンプルの真の値、p_iはi個目サンプルの予測値とすると、\n\nMAE（Mean Absolute Error）:平均絶対誤差MAE = \\frac{1}{N} \\cdot \\sum_{i=1}^{N} |y_i - p_i|\n\nRMSE（Root Mean Squared Error）: 平均二乗誤差平方根RMSE = \\sqrt{\\frac{1}{N} \\cdot \\sum_{i=1}^{N} (y_i - p_i)^2}","type":"content","url":"/ml-basis2#id-4","position":11},{"hierarchy":{"lvl1":"機械学習の基本概念","lvl2":"分類タスク評価指標"},"type":"lvl2","url":"/ml-basis2#id-5","position":12},{"hierarchy":{"lvl1":"機械学習の基本概念","lvl2":"分類タスク評価指標"},"content":"分類タスクの評価指標は、よくある二分類タスクで説明します。分類の実際値と予測値は下記の４種類があります。\n\nTP：True Positive 真陽性\n\nFN：False Negative 偽陰性\n\nFP：False Positive 偽陽性\n\nTN：True Negative 真陰性","type":"content","url":"/ml-basis2#id-5","position":13},{"hierarchy":{"lvl1":"機械学習の基本概念","lvl3":"正解率（Accuracy）","lvl2":"分類タスク評価指標"},"type":"lvl3","url":"/ml-basis2#id-accuracy","position":14},{"hierarchy":{"lvl1":"機械学習の基本概念","lvl3":"正解率（Accuracy）","lvl2":"分類タスク評価指標"},"content":"正解率は、正や負と予測したデータのうち、実際にそうであるものの割合です。{Accuracy= \\frac{TP+TN}{TP+FP+TN+FN}\n}","type":"content","url":"/ml-basis2#id-accuracy","position":15},{"hierarchy":{"lvl1":"機械学習の基本概念","lvl3":"適合率（Precision）","lvl2":"分類タスク評価指標"},"type":"lvl3","url":"/ml-basis2#id-precision","position":16},{"hierarchy":{"lvl1":"機械学習の基本概念","lvl3":"適合率（Precision）","lvl2":"分類タスク評価指標"},"content":"適合率は、正と予測したデータのうち，実際に正であるものの割合です。{Precision= \\frac{TP}{TP+FP}\n}","type":"content","url":"/ml-basis2#id-precision","position":17},{"hierarchy":{"lvl1":"機械学習の基本概念","lvl3":"再現率（Recall）","lvl2":"分類タスク評価指標"},"type":"lvl3","url":"/ml-basis2#id-recall","position":18},{"hierarchy":{"lvl1":"機械学習の基本概念","lvl3":"再現率（Recall）","lvl2":"分類タスク評価指標"},"content":"再現率は、実際に正であるもののうち，正であると予測されたものの割合です。{Recall = \\frac{TP}{TP+FN}\n}","type":"content","url":"/ml-basis2#id-recall","position":19},{"hierarchy":{"lvl1":"機械学習の基本概念","lvl3":"F-1値（F-1 score）","lvl2":"分類タスク評価指標"},"type":"lvl3","url":"/ml-basis2#f-1-f-1-score","position":20},{"hierarchy":{"lvl1":"機械学習の基本概念","lvl3":"F-1値（F-1 score）","lvl2":"分類タスク評価指標"},"content":"F値は、再現率と適合率の調和平均です。{F = \\frac{2\\cdot Recall \\cdot Precision}{Recall+Precision}\n}\n\n","type":"content","url":"/ml-basis2#f-1-f-1-score","position":21},{"hierarchy":{"lvl1":"機械学習の基本概念","lvl2":"過学習"},"type":"lvl2","url":"/ml-basis2#id-6","position":22},{"hierarchy":{"lvl1":"機械学習の基本概念","lvl2":"過学習"},"content":"過学習(Overfitting)とは、データの傾向に沿うようにモデルを学習させた結果、学習時のデータに対してはよい精度を出すが、未知データに対しては同様の精度を出せないモデルが構築されてしまうことです。\n\n過学習を防ぐための一つの方法としては、交差検証法を使うことです。\n\n交差検証とは、1つのデータを訓練データと検証データに分けるときに複数の分け方をして平均をとるという方法です。　データの分け方を複数作ることでリスクを分散し、訓練データと検証データの傾向の違いにより生じる過学習を最小化します。\n\n最もよく使われるK-交差検証では、\n\n全体データをK個にデータを分割します。\n\nA～Kまであるうち、最初にAを検証データにしてB～Kのデータから予測モデルを作成。\n\nBを検証データにしてAとC～Kのデータから予測モデルを作成という流れで順番にK回検証していきます.\n\n\n","type":"content","url":"/ml-basis2#id-6","position":23},{"hierarchy":{"lvl1":"機械学習の基本概念","lvl2":"機械学習のハイパーパラメータ"},"type":"lvl2","url":"/ml-basis2#id-7","position":24},{"hierarchy":{"lvl1":"機械学習の基本概念","lvl2":"機械学習のハイパーパラメータ"},"content":"機械学習のハイパーパラメータは、学習アルゴリズムの動作を制御するためのパラメータです。\n\nこれらのハイパーパラメータは、モデルの学習プロセスの前に設定され、学習中には通常変わりません。ハイパーパラメータの設定に応じてモデルの精度やパフォーマンスが大きく変わることがあります。\n\n対照的に、パラメータとは、おもに機械学習モデルが学習過程において最適化を行う重みを指します。\n\nここでは、多くの機械学習・深層学習モデルに共通しているくつかのハイパーパラメータを紹介します。","type":"content","url":"/ml-basis2#id-7","position":25},{"hierarchy":{"lvl1":"機械学習の基本概念","lvl3":"バッチサイズ","lvl2":"機械学習のハイパーパラメータ"},"type":"lvl3","url":"/ml-basis2#id-8","position":26},{"hierarchy":{"lvl1":"機械学習の基本概念","lvl3":"バッチサイズ","lvl2":"機械学習のハイパーパラメータ"},"content":"機械学習のにおける「バッチ」は、学習アルゴリズムに一度に供給されるデータのサブセットを指します。「バッチサイズ」とは、学習プロセスにおいて、一度にモデルに供給されるサンプルの数を指します。\n\nバッチ学習: 全ての学習データを利用して学習する手法です。つまり、バッチサイズはトレーニングデータの全サンプル数と等しくなります。この方法の利点は、計算が効率的であることですが、大量のメモリが必要になることや局所的な最適解にトラップされやすいという欠点もあります。\n\nオンライン学習:ひとつひとつの学習データごとに学習処理を行います。具体的には、n個の学習データx_1, x_2, …, x_nからランダムに1つの学習データx_iを抽出し、その1つのデータをモデルに投入します。オンライン学習は一件ずつの計算処理を採用しているため、常に更新される最新データ情報も柔軟に取り入れることができます。一方で、パラメータの更新をデータ一件ごとに行うので、学習を安定させることが難しいのもオンライン学習の懸念点です。\n\nミニバッチ学習: バッチ学習とオンライン学習の中間のような学習手法であり、データをミニバッチという小さなグループに分割してモデルを学習します。バッチ学習は、計算効率とメモリの使用のバランスが良いこと、および学習の収束速度が適切であるため、一般的に最も使用される方法です。","type":"content","url":"/ml-basis2#id-8","position":27},{"hierarchy":{"lvl1":"機械学習の基本概念","lvl3":"エポック(epoch)数","lvl2":"機械学習のハイパーパラメータ"},"type":"lvl3","url":"/ml-basis2#id-epoch","position":28},{"hierarchy":{"lvl1":"機械学習の基本概念","lvl3":"エポック(epoch)数","lvl2":"機械学習のハイパーパラメータ"},"content":"機械学習・深層学習ようにパラメータの数が多いものになると、訓練データを何回も繰り返する必要があります。エポック数とは、学習において、データセットを何週繰り返してパラメータを調整するかを表す数を指します。1エポックは、トレーニングデータセット全体が一度、モデルを通過することを意味します。\n\n例えば、データ件数が1000件で、バッチサイズが100なら、10回繰り返すと、1000件のデータに相当する件数分処理したことになります。この１単位のことを「エポック」と呼びます。\n\n一般的には、エポック数が増えるほどモデルは訓練データに適応しやすくなります。しかし、エポック数が大きすぎると過学習のリスクが高まります。適切なエポック数を選ぶことで、モデルの性能を最大限引き出すことができます。","type":"content","url":"/ml-basis2#id-epoch","position":29},{"hierarchy":{"lvl1":"機械学習の基本概念","lvl3":"イテレーション (Iteration)","lvl2":"機械学習のハイパーパラメータ"},"type":"lvl3","url":"/ml-basis2#id-iteration","position":30},{"hierarchy":{"lvl1":"機械学習の基本概念","lvl3":"イテレーション (Iteration)","lvl2":"機械学習のハイパーパラメータ"},"content":"イテレーション数はデータセットに含まれるデータが少なくとも1回は学習に用いられるのに必要な学習回数であり、バッチサイズが決まれば自動的に決まる数値です。先程の1000件のデータセットを100件ずつのサブセットに分ける場合では、イテレーション数は10となります。\n\n","type":"content","url":"/ml-basis2#id-iteration","position":31},{"hierarchy":{"lvl1":"機械学習の基本概念","lvl3":"学習率(Learning Rate)","lvl2":"機械学習のハイパーパラメータ"},"type":"lvl3","url":"/ml-basis2#id-learning-rate","position":32},{"hierarchy":{"lvl1":"機械学習の基本概念","lvl3":"学習率(Learning Rate)","lvl2":"機械学習のハイパーパラメータ"},"content":"学習率とは、機械学習の最適化において、重みパラメータを一度にどの程度変化させるかを表すハイパーパラメータのことです。\n\n機械学習とは、反復的に重みパラメータを変更していきますが、学習率の値が高いほど一度に変更する重みパラメータの大きさが大きくなるので学習のスピードは上がり、反対に低ければ学習のスピードは下がります。\n\nNote\n\nLinear Regressionの例\n\nモデル：入力データから出力を予測するための数学的表現やアルゴリズムです。例として、線形回帰、決定木、ニューラルネットワークなどが挙げられます。\n\nハイパーパラメータ：モデルの学習プロセスを制御するための外部から設定されるパラメータです。ハイパーパラメータは学習データから自動的に学習されるものではなく、手動で設定されるか、ハイパーパラメータチューニングの技法を用いて最適な値を探索します。例えば、学習率、バッチサイズ、エポック数、ドロップアウト率のこと．この数値で決められたモデルの構造に従って学習は進められます。\n\n重み：重みは、モデルの学習プロセス中にデータから自動的に調整・学習される内部パラメータです。重みは、入力特徴との関係性を学習し、最終的な予測を形成するのに役立ちます。一般的には、重みは、モデルの出力を調整して、実際のターゲットとの差を最小化することを目指します。学習はこの重みを試行錯誤して決めていく作業になります．\n\nimport numpy as np\nimport math\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\n# ===== ハイパーパラメータ（hyperparameter） =====\nsample_size = 200     # トレーニングデータ数\nbatch_size = 20     # バッチサイズ\nlr = 0.01            # 学習率\nepochs = 1           # 1エポックのみ（全データを1回学習）\n\n# ===== データ生成 =====\nnp.random.seed(42)\nX = np.linspace(0, 10, sample_size)\ny = 2 * X + 1 + np.random.randn(sample_size) * 1.0  # 真の関係: y = 2x + 1 + ノイズ\n\n# ===== モデルのパラメータ（parameter） =====\nw, b = 0.0, 0.0\n\n# ===== 自動調整設定 =====\nnum_iters = len(X) // batch_size\ncols = min(5, num_iters)\nrows = math.ceil(num_iters / cols)\n\n# ===== Plotly版 plt.subplots 相当 =====\nfig = make_subplots(\n    rows=rows, cols=cols,\n    subplot_titles=[f\"Iteration {i+1}\" for i in range(num_iters)],\n    horizontal_spacing=0.05, vertical_spacing=0.12\n)\n\n# ===== 学習ループ =====\nfor t in range(num_iters):\n    # --- ランダムにbatchを選択 ---\n    batch_idx = np.random.choice(len(X), batch_size, replace=False)\n    X_batch, y_batch = X[batch_idx], y[batch_idx]\n\n    # --- 学習前の予測線 ---\n    y_pred_before = w * X + b\n\n    # --- 勾配計算 ---\n    y_pred_batch = w * X_batch + b\n    dw = -2 * np.mean(X_batch * (y_batch - y_pred_batch))\n    db = -2 * np.mean(y_batch - y_pred_batch)\n\n    # --- パラメータ更新 ---\n    w_new = w - lr * dw\n    b_new = b - lr * db\n\n    # --- 学習後の予測線 ---\n    y_pred_after = w_new * X + b_new\n\n    # --- subplotの位置 ---\n    row = t // cols + 1\n    col = t % cols + 1\n\n    # ===== データ描画 =====\n    # 全データ（灰色）\n    fig.add_trace(\n        go.Scatter(\n            x=X, y=y, mode='markers',\n            marker=dict(color='lightgray', size=5),\n            name='全データ',\n            showlegend=(t == 0)\n        ),\n        row=row, col=col\n    )\n    # 現在のバッチ（赤）\n    fig.add_trace(\n        go.Scatter(\n            x=X_batch, y=y_batch, mode='markers',\n            marker=dict(color='red', size=8),\n            name='バッチデータ',\n            showlegend=(t == 0)\n        ),\n        row=row, col=col\n    )\n    # 更新前（青）\n    fig.add_trace(\n        go.Scatter(\n            x=X, y=y_pred_before,\n            mode='lines',\n            line=dict(color='blue', dash='dash'),\n            name='更新前',\n            showlegend=(t == 0)\n        ),\n        row=row, col=col\n    )\n    # 更新後（赤）\n    fig.add_trace(\n        go.Scatter(\n            x=X, y=y_pred_after,\n            mode='lines',\n            line=dict(color='red'),\n            name='更新後',\n            showlegend=(t == 0)\n        ),\n        row=row, col=col\n    )\n\n    # 回帰式（テキスト表示）\n    xref = \"x\" if t == 0 else f\"x{t+1}\"\n    yref = \"y\" if t == 0 else f\"y{t+1}\"\n    fig.add_annotation(\n        x=0.05, y=0.95,\n        xref=f\"{xref} domain\", yref=f\"{yref} domain\",\n        text=f\"y = {w_new:.2f}x + {b_new:.2f}\",\n        showarrow=False,\n        font=dict(size=12, color=\"black\")\n    )\n\n    # パラメータ更新\n    w, b = w_new, b_new\n\n# ===== レイアウト =====\nfig.update_layout(\n    title=f\"線形回帰の学習過程（sample_size={sample_size}, batch_size={batch_size}）\",\n    height=400 * rows,\n    width=1500,\n    showlegend=True,\n)\n\nfig.show()\n\n\n","type":"content","url":"/ml-basis2#id-learning-rate","position":33},{"hierarchy":{"lvl1":"自然言語処理の基礎"},"type":"lvl1","url":"/nlp-basis2","position":0},{"hierarchy":{"lvl1":"自然言語処理の基礎"},"content":"\n\n","type":"content","url":"/nlp-basis2","position":1},{"hierarchy":{"lvl1":"自然言語処理の基礎","lvl2":"テキストの前処理"},"type":"lvl2","url":"/nlp-basis2#id","position":2},{"hierarchy":{"lvl1":"自然言語処理の基礎","lvl2":"テキストの前処理"},"content":"テキストは文字の羅列であり構造化されていないため、そのままでは処理するのが難しいです。そこで、前処理を通じてテキストを整理し、分析やモデルに適した形式に変換する必要があります。\n\n","type":"content","url":"/nlp-basis2#id","position":3},{"hierarchy":{"lvl1":"自然言語処理の基礎","lvl3":"正規表現によるノイズの除去","lvl2":"テキストの前処理"},"type":"lvl3","url":"/nlp-basis2#id-1","position":4},{"hierarchy":{"lvl1":"自然言語処理の基礎","lvl3":"正規表現によるノイズの除去","lvl2":"テキストの前処理"},"content":"テキスト分析における、テキストから不要な情報やノイズを取り除き、解析や処理を行いやすくするための前処理が不可欠になります。\n\nテキストクリーニングを行う際に、正規表現を利用すると非常に柔軟にノイズの除去が可能です。以下、いくつかの一般的なケースを例に、テキストからノイズを除去する方法を紹介します。\n\n正規表現とは\n\n正規表現とは、文字列内で文字の組み合わせを照合するために用いられるパターンです。\n簡単に言えば、文字や記号を組み合わせて「こういう形の文字列を探す」というルールを表現できます。\n\nre.subは、正規表現にマッチした文字列を別の文字列に置換するための関数です。対象となる文字列の中で正規表現パターンにマッチする部分を、置換後の文字列に置換します。\n\n第1引数には正規表現パターン、第2引数には置換後の文字列、第3引数には対象となる文字列を指定します。\n\nimport re\n\n#ウェブページやSNSのテキストからURLを取り除く場合:\n# `http` から始まり、その後に1つ以上の非空白文字 (`\\S+`) が続く文字列にマッチする正規表現パターン `r'http\\S+'` を指定しています。\n\ntext = \"この店は美味しいですhttp://example.com \"\ncleaned_text = re.sub(r'http\\S+', '', text)\nprint(cleaned_text)\n\n\n\n# HTMLタグの除去: HTMLコンテンツからタグを取り除く場合:\n# `\\d` は、0から9までの数字にマッチする特殊文字です。この正規表現パターンにマッチする部分は、空文字列 `''` に置換されます。\n\ntext = \"今日は2023年9月16日です。\"\ncleaned_text = re.sub(r'\\d', '', text)\nprint(cleaned_text)\n\n\n\n# テキストからアルファベットのみを取り除く場合:\n# `[a-zA-Z]` は、英字の大文字と小文字にマッチする文字クラスです。\ntext = \"これはテストtextです。\"\ncleaned_text = re.sub(r'[a-zA-Z]', '', text)\nprint(cleaned_text)\n\n\n\n","type":"content","url":"/nlp-basis2#id-1","position":5},{"hierarchy":{"lvl1":"自然言語処理の基礎","lvl3":"トークン化","lvl2":"テキストの前処理"},"type":"lvl3","url":"/nlp-basis2#id-2","position":6},{"hierarchy":{"lvl1":"自然言語処理の基礎","lvl3":"トークン化","lvl2":"テキストの前処理"},"content":"トークン化は、文字列をモデルで使用される最小単位に分解するステップです。トークンは通常、言語の基本的な構成要素である単語や部分文字列を指します。\n\n日本語は英語などとは異なり、単語の間に（スペースなどの）区切りをつけないので、文から単語を取り出すこと自体が一つのタスクとなります。文から単語（正確には形態素と呼ばれる意味の最小単位）を取り出す解析を形態素解析といいます。\n\nイメージとしては以下のように分割します。\n\n分かち書き（文章を形態素で分ける）\n\n品詞わけ（名詞や動詞などに分類する）\n\n原型付与（単語の基本形をだす） 例：食べた ⇒ 食べる、た\n\n日本語の形態素解析ツールとしては、MeCabやJUMAN++、Janomeなどが挙げられます。ここでは、MeCabを使って形態素解析をしてみましょう。\n\n初めてMeCabを使う場合、!が付いたコードをJupyter Notebookで実行するか、!を除去したコードをターミナルで実行し、MeCabを導入してください。!pip install mecab-python3 # mecab-python3のインストール\n!pip install unidic\n!python -m unidic download # 辞書のダウンロード\n\n結果を確認すると、単語に分割しただけでなく、品詞などの情報も得られました。\n\nただ、デフォルトの設定では新語に対する解析は強くないことがわかります。\n\n# !pip install mecab-python3 # mecab-python3のインストール\n# !pip install unidic\n# !python -m unidic download # 辞書のダウンロード\nimport MeCab\nimport unidic\ntagger = MeCab.Tagger() \nprint(tagger.parse(\"友たちと国立新美術館に行った。\"))\n\n\n\nこの問題は形態素解析器に辞書を追加することである程度解決することが出来ます。特に、NEologdという辞書には、通常の辞書と比べて多くの新語が含まれています。\n\nNEologdのインストールについては、以下のベージに参照してください。\n\nWindows\n\nMac\n\n辞書を指定して、タガーを生成します。\n\nsample_txt = \"友たちと国立新美術館に行った。\"\npath = \"-d /opt/homebrew/lib/mecab/dic/mecab-ipadic-neologd\"\nm = MeCab.Tagger(path)\nprint(\"Mecab ipadic NEologd:\\n\",m.parse(sample_txt))\n\n\n\n上の例から分かるように、NEologdは「国立新美術館」のような固有表現にも対応し、ソーシャルメディアのテキストなど新語が多数含まれている場合こちらの方が適切です。\n\nparseToNode()メソッドは、形態素（ノード）ごとに出力をしていきます。\n\nn=m.parseToNode(sample_txt)\n\n\n\n最初は、BOS/EOS(文頭／文末)を示す符号となります。\n\nn.feature\n\n\n\n.nextで次のノードに進みます。\n\nn = n.next\n\n\n\nn.feature\n\n\n\nところでfeatureは形態素の特徴を表す要素を格納しています。\n\n0番目が品詞、6番目が基本形であり、今回はこの2つを用います。\n\nprint(\"品詞: \"+n.feature.split(',')[0])\nprint(\"基本形: \"+n.feature.split(',')[6])\n\n\n\n文章を解析するためには、whileループを使います。その際、品詞を指定して特定の品詞の基本形だけを取り出すなどという操作が可能です。\n\nn=m.parseToNode(sample_txt)\nwhile n:\n  if n.feature.split(',')[0] in [\"名詞\",\"形容詞\",\"動詞\"]:\n    print(n.feature.split(',')[6])\n  n = n.next\n\n\n\n","type":"content","url":"/nlp-basis2#id-2","position":7},{"hierarchy":{"lvl1":"自然言語処理の基礎","lvl3":"ストップワードの除去","lvl2":"テキストの前処理"},"type":"lvl3","url":"/nlp-basis2#id-3","position":8},{"hierarchy":{"lvl1":"自然言語処理の基礎","lvl3":"ストップワードの除去","lvl2":"テキストの前処理"},"content":"\n\nストップワードとは、助詞など単独で用いられなかったり、一般的に使用されすぎていて文の意味の分析に寄与しない、あるいや逆に使用頻度が少ないため、含めると分析結果が歪んでしまうなどの理由で分析からあらかじめ除外しておく語のことをいいます。\n\n一般的には、あらかじめストップワードを辞書に定義しておき、辞書内に含まれる単語をテキスとから除外します。\n\n一般的な用語のストップリストは例えばSlothLibプロジェクトから取得することができます。\n\n#import urllib\n#slothlib_path = 'http://svn.sourceforge.jp/svnroot/slothlib/CSharp/Version1/SlothLib/NLP/Filter/StopWord/word/Japanese.txt'\n#slothlib_file = urllib.request.urlopen(slothlib_path)\n#slothlib_stopwords = [line.decode(\"utf-8\").strip() for line in slothlib_file]\n#slothlib_stopwords = [ss for ss in slothlib_stopwords if not ss==u'']\n\n\n\n","type":"content","url":"/nlp-basis2#id-3","position":9},{"hierarchy":{"lvl1":"自然言語処理の基礎","lvl3":"テキストの特徴量表現","lvl2":"テキストの前処理"},"type":"lvl3","url":"/nlp-basis2#id-4","position":10},{"hierarchy":{"lvl1":"自然言語処理の基礎","lvl3":"テキストの特徴量表現","lvl2":"テキストの前処理"},"content":"機械学習では、実数値を要素とするベクトルで入力を表現することが多いです。適切な特徴(feature)を生データから作成すること、機械学習モデル性能も向上につながります。\n\nテキストデータもベクトク化する必要があります。","type":"content","url":"/nlp-basis2#id-4","position":11},{"hierarchy":{"lvl1":"自然言語処理の基礎","lvl3":"n-gramベクトル","lvl2":"テキストの前処理"},"type":"lvl3","url":"/nlp-basis2#n-gram","position":12},{"hierarchy":{"lvl1":"自然言語処理の基礎","lvl3":"n-gramベクトル","lvl2":"テキストの前処理"},"content":"n-gramは、テキストデータやシーケンスデータの連続するN個のアイテム（文字、単語など）を指す言葉です。特にn=1の場合をuni-gram, n=2の場合をbi-gramと呼びます。\n\n例えば、I love machine learningという文をn-gramで表現してみます。n=1の場合は[\"I\", \"love\", \"machine\", \"learning\"]、n=2の場合は[\"I love\", \"love machine\", \"machine learning\"]、n=3の場合は[\"I love machine\", \"love machine learning\"]のようにテキストを表現できます。\n\n抽出されたN-gramを一意なものとしてリストアップし、各n-gramに対して重複のないように数値を割り当てます。これを語彙(vocabulary)と呼びます。\n\n次に、 テキストごとに、語彙に含まれるN-gramの出現頻度や存在をベクトルとして表現します。\n\nNote\n\nn-gramでは、ある程度にローカルな情報、文の構造や単語の順序を考慮することができます。一方、語彙のサイズが大きくなると、スパースなベクトルが生成され、計算コストが高くになるなどの欠点もあります。そのため、N-gramベクトルはある意味で「古典的な」特徴量表現になります。現在のNLPの分野では、埋め込みベクトルや事前学習済みモデルを使用する手法が主流になっています。","type":"content","url":"/nlp-basis2#n-gram","position":13},{"hierarchy":{"lvl1":"自然言語処理の基礎","lvl3":"one-hotエンコーディング","lvl2":"テキストの前処理"},"type":"lvl3","url":"/nlp-basis2#one-hot","position":14},{"hierarchy":{"lvl1":"自然言語処理の基礎","lvl3":"one-hotエンコーディング","lvl2":"テキストの前処理"},"content":"one-hotエンコーディングでは、ある単語がテキストに存在するかどうかでベクトルを作成します。具体的には、\n\n語彙（ユニークな単語のリスト）を作成する。\n\nこの語彙のサイズをベクトルの長さとし、各単語が語彙のどの位置に存在するかに応じて1の値を持つベクトルを生成する。\n\n例えば、以下の語彙に基づいて、“like a banana”のone-hotエンコーディング結果は[0, 0, 0, 1, 1, 0, 0, 1]","type":"content","url":"/nlp-basis2#one-hot","position":15},{"hierarchy":{"lvl1":"自然言語処理の基礎","lvl3":"tf-idf","lvl2":"テキストの前処理"},"type":"lvl3","url":"/nlp-basis2#tf-idf","position":16},{"hierarchy":{"lvl1":"自然言語処理の基礎","lvl3":"tf-idf","lvl2":"テキストの前処理"},"content":"tf-idfとは、「ある文書内」で「ある単語」が「どれくらい多い頻度で出現するか」を表すtf（term frequency：単語頻度）値と、「全文書中」で「ある単語を含む文書」が「（逆に）どれくらい少ない頻度で存在するか」を表すidf（inverse document frequency：逆文書頻度）値を掛け合わせた値のことです。\n\nTerm Frequency (TF): 特定の文書内の単語の出現頻度を表します。\n\nInverse Document Frequency (IDF): コーパス全体において、特定の単語がどれほど珍しいかを評価する指標です。td-idf(t,d)=tf(t,d) \\times idf(t)idf(t)=log (\\frac{Total\\ number\\ of\\ documents}{Number\\ of\\ documents\\ containing\\ the\\ term})\n\n要するには、tf-idfの基本的な考え方は、ある単語が多くの文書に出現するなら、その単語は一般的に重要でないと考えられます。例えば、ある文書で「の」の出現頻度は高いが、同時に多くの文書に出現すると重要性が小さくなります。\n\nNote\n\nここまでテキストベクトル化の手法を説明しましたが、実に、これらの手法では単語の意味や関係をうまく捉えない、大規模のテキストデータに対応できない、汎用性は低いなどの欠点が挙げられます。これらの制約を克服するため、現在のNLPの分野では、埋め込みベクトルや事前学習済みモデルを使用する手法が主流になっています。\n\n","type":"content","url":"/nlp-basis2#tf-idf","position":17},{"hierarchy":{"lvl1":"自然言語処理の基礎","lvl3":"埋め込みベクトル","lvl2":"テキストの前処理"},"type":"lvl3","url":"/nlp-basis2#id-5","position":18},{"hierarchy":{"lvl1":"自然言語処理の基礎","lvl3":"埋め込みベクトル","lvl2":"テキストの前処理"},"content":"単語埋め込みベクトル（Word Embedding Vector）とは、単語を数値ベクトルの形で表現したものです。\n\nこの表現は、単語の意味や文脈情報を数値で表現することにより、機械学習モデルがテキストデータを効果的に処理できるようにするために使用されます。\n\n類似性: ある概念を表現する際に、ほかの概念との共通点や類似性と紐づけながら、ベクトル空間上に表現します。\n\n単語類推: 分散表現では異なる概念を表現するベクトル同士での計算が可能です\n\nfrom gensim.models import KeyedVectors\nimport gensim.downloader as api\n\nmodel = api.load(\"glove-wiki-gigaword-50\")  # 50-dim GloVe vectors\n\n\n\nprint(\"類似性 (word similarity):\")\nprint(model.similarity('cat', 'dog'))\nprint(model.similarity('cat', 'house'))\n\n\n\nprint(\"\\n単語類推 (word analogy):\")\n# analogy: king - man + woman = ?\nprint(model.most_similar(positive=['king', 'woman'], negative=['man'], topn=5))\n\n","type":"content","url":"/nlp-basis2#id-5","position":19},{"hierarchy":{"lvl1":"Pytorch"},"type":"lvl1","url":"/pytorch2","position":0},{"hierarchy":{"lvl1":"Pytorch"},"content":"\n\nPyTorchはPythonのオープンソースの機械学習・深層学習ライブラリです。\n\n柔軟性を重視した設計であり、さらに、機械学習・深層学習モデルをPythonの慣用的なクラスや関数の取り扱い方で実装できるようになっています。\n\nGPUを使用した計算をサポートしますので、CPU上で同じ計算を行う場合に比べて、数十倍の高速化を実現します。\n\n#pip install torch torchvision torchaudio\nimport torch\nimport numpy as np\n\n\n\n","type":"content","url":"/pytorch2","position":1},{"hierarchy":{"lvl1":"Pytorch","lvl2":"テンソル"},"type":"lvl2","url":"/pytorch2#id","position":2},{"hierarchy":{"lvl1":"Pytorch","lvl2":"テンソル"},"content":"深層学習モデルは通常、入力から出力にどのようにマッピングされるのかを対応つけるデータ構造を表します。一般的に、このようなある形式のデータから別の形式への変換は膨大な浮動小数点数の計算を通じて実現されています。\n\nデータを浮動小数点数を扱うためには、Pytorchは基本的なデータ構造として「テンソル」を導入しています。\n\n深層学習の文脈でのテンソルとは、ベクトルや行列を任意の次元数に一般化したものを指します。つまり、多次元配列を扱います。\n\nNote\n\nTensorとの同じように、NumPyも多次元配列を扱えます。ただ、PyTorchにおいてテンソルはGPU上でも使用できるため、処理速度の向上させることも可能です。\n\n","type":"content","url":"/pytorch2#id","position":3},{"hierarchy":{"lvl1":"Pytorch","lvl3":"テンソルの操作","lvl2":"テンソル"},"type":"lvl3","url":"/pytorch2#id-1","position":4},{"hierarchy":{"lvl1":"Pytorch","lvl3":"テンソルの操作","lvl2":"テンソル"},"content":"","type":"content","url":"/pytorch2#id-1","position":5},{"hierarchy":{"lvl1":"Pytorch","lvl4":"テンソルの作成","lvl3":"テンソルの操作","lvl2":"テンソル"},"type":"lvl4","url":"/pytorch2#id-2","position":6},{"hierarchy":{"lvl1":"Pytorch","lvl4":"テンソルの作成","lvl3":"テンソルの操作","lvl2":"テンソル"},"content":"\n\nx = torch.ones(5, 3)\nprint(x)\n\n\n\nx = torch.rand(5, 3)\nprint(x)\n\n\n\nx = torch.tensor([5.5, 3])\nprint(x)\n\n\n\nNote\n\nPyTorchのテンソルは、クラス torch.Tensor のインスタンスであり、\nValue（値・データ部分） と Method（メソッド・操作部分） を持つ**オブジェクト（object）**として実装されています。\n\nValue には、実際の数値データ（多次元配列）が格納されています\n\nMethod には多くの演算関数が定義されており、これらを呼び出すことで、行列演算・転置・要素ごとの計算などの操作ができます\n\n","type":"content","url":"/pytorch2#id-2","position":7},{"hierarchy":{"lvl1":"Pytorch","lvl4":"テンソル要素の型","lvl3":"テンソルの操作","lvl2":"テンソル"},"type":"lvl4","url":"/pytorch2#id-3","position":8},{"hierarchy":{"lvl1":"Pytorch","lvl4":"テンソル要素の型","lvl3":"テンソルの操作","lvl2":"テンソル"},"content":"テンソル要素の型は、引数に適切なdtypeを渡すことで指定します。\n\ndouble_points = torch.ones(10, 2, dtype=torch.double)\nshort_points = torch.tensor([[1, 2], [3, 4]], dtype=torch.short)\n\n\n\n","type":"content","url":"/pytorch2#id-3","position":9},{"hierarchy":{"lvl1":"Pytorch","lvl4":"テンソルの操作（変形・変換等）","lvl3":"テンソルの操作","lvl2":"テンソル"},"type":"lvl4","url":"/pytorch2#id-4","position":10},{"hierarchy":{"lvl1":"Pytorch","lvl4":"テンソルの操作（変形・変換等）","lvl3":"テンソルの操作","lvl2":"テンソル"},"content":"PyTorchにはテンソルに対する\n\n操作（変形・演算など）が多く用意されています。\n\nx = torch.rand(5, 3)\ny = torch.rand(5, 3)\nprint(x + y)\n\n\n\nprint(torch.add(x, y))\n\n\n\n","type":"content","url":"/pytorch2#id-4","position":11},{"hierarchy":{"lvl1":"Pytorch","lvl4":"テンソルの一部指定や取り出し(Indexing)","lvl3":"テンソルの操作","lvl2":"テンソル"},"type":"lvl4","url":"/pytorch2#id-indexing","position":12},{"hierarchy":{"lvl1":"Pytorch","lvl4":"テンソルの一部指定や取り出し(Indexing)","lvl3":"テンソルの操作","lvl2":"テンソル"},"content":"Pytorchテンソルは、Numpyや他のPythonの科学計算ライブラリーと同じく、テンソルの次元ごとのレンジインデックス記法で一部指定や取り出しを行えます。\n\nx[3:,:]\n\n\n\nx[1:,0]\n\n\n\n","type":"content","url":"/pytorch2#id-indexing","position":13},{"hierarchy":{"lvl1":"Pytorch","lvl4":"テンソルの微分機能","lvl3":"テンソルの操作","lvl2":"テンソル"},"type":"lvl4","url":"/pytorch2#id-5","position":14},{"hierarchy":{"lvl1":"Pytorch","lvl4":"テンソルの微分機能","lvl3":"テンソルの操作","lvl2":"テンソル"},"content":"PyTorchテンソルは、テンソルに対して実行された計算を追跡し、計算結果の出力テンソルの微分を、各テンソルの要素に対して解析的に計算することができます。\n\n具体的には、requires_grad=True を設定したテンソルに対して行われたすべての演算が計算グラフとして記録され、.backward()メソッドを呼び出すことで、損失関数などの出力に対する各パラメータの勾配が自動的に計算されます。\n\n# 微分を計算するためのテンソルを作成\nx = torch.tensor(2.0, requires_grad=True)\n# 関数の定義\ny = x ** 2\n# 勾配を計算\ny.backward()\n\n# 勾配の値を表示\nprint(x.grad)  # 4.0 (これは2*xの値、x=2のとき)\n\n\n\n次の関数について、各変数に対する勾配を求めなさいz = (x_1^2 + x_2^2) \\times y\n\nただし、x_1=1.0, x_2=2.0, y=3.0 とする。\n\n","type":"content","url":"/pytorch2#id-5","position":15},{"hierarchy":{"lvl1":"Pytorch","lvl4":"CUDA Tensors（CUDA テンソル）","lvl3":"テンソルの操作","lvl2":"テンソル"},"type":"lvl4","url":"/pytorch2#cuda-tensors-cuda","position":16},{"hierarchy":{"lvl1":"Pytorch","lvl4":"CUDA Tensors（CUDA テンソル）","lvl3":"テンソルの操作","lvl2":"テンソル"},"content":"tensorは .to メソッドを使用することであらゆるデバイス上のメモリへと移動させることができます。\n\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")          # a CUDA device object\n    y = torch.ones_like(x, device=device)  # directly create a tensor on GPU\n    x = x.to(device)                       # or just use strings ``.to(\"cuda\")``\n    z = x + y\n    print(z)\n    print(z.to(\"cpu\", torch.double))       # ``.to`` can also change dtype together!\n\n\n\n","type":"content","url":"/pytorch2#cuda-tensors-cuda","position":17},{"hierarchy":{"lvl1":"Pytorch","lvl2":"学習データセットの作成"},"type":"lvl2","url":"/pytorch2#id-6","position":18},{"hierarchy":{"lvl1":"Pytorch","lvl2":"学習データセットの作成"},"content":"深層学習を実装する際には、大量のデータを扱う必要があります。しかし、その際には次のような課題が生じます：\n\nデータの読み込みや前処理のコードが複雑になる(for文で個別処理する必要がある)\n\n学習時にデータをランダムにシャッフルしたり、ミニバッチに分割したりする必要がある","type":"content","url":"/pytorch2#id-6","position":19},{"hierarchy":{"lvl1":"Pytorch","lvl3":"Datasetクラス","lvl2":"学習データセットの作成"},"type":"lvl3","url":"/pytorch2#dataset","position":20},{"hierarchy":{"lvl1":"Pytorch","lvl3":"Datasetクラス","lvl2":"学習データセットの作成"},"content":"PyTorchのtorch.utils.data.Datasetは、これらの課題を解決するために用意されたデータ管理の基本インターフェースです\n\nPytorchで深層学習を実装する際には、特徴量行列とラベルをDatasetというクラスに渡して、特徴量行列とラベルを一つのデータベース的なものにまとめる働きをします。\n\n一般的には、PyTorchのtorch.utils.data.Datasetクラスを継承して定義します。以下に示すメソッドを定義するように指定されています。\n\n__init__(self): 初期実行関数です。Datasetを定義する際に必要な情報を受け取ります。\n\n__len__(self): データ全体の数を返す関数です。\n\n__getitem__(self, index): 指定されたindexに対応するデータと正解ラベル(ターゲット)を返します。これにより、データの取得方法を統一化します。\n\nfrom torch.utils.data import Dataset\n\n\nclass CustomDataset(Dataset):\n    def __init__(self, data, labels):\n        self.data = data\n        self.labels = labels\n\n    def __len__(self):\n        # データセットのサイズを返す\n        return len(self.data)\n\n    def __getitem__(self, index):\n        # 指定されたインデックスのデータとラベルを返す\n        return self.data[index], self.labels[index]\n\n\n\n# サンプルデータ\ndata = torch.tensor([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\nlabels = torch.tensor([0, 1, 0])\n\n# カスタムデータセットのインスタンスを作成\ndataset = CustomDataset(data, labels)\n\n\n\ndataset[0]\n\n\n\nNote\n\nデータをPyTorchテンソルに変換する前に、なんらかの加工を加えたい場合もあります。特に画像データに足して、transformsは色々な前処理をサポートします。\n\n","type":"content","url":"/pytorch2#dataset","position":21},{"hierarchy":{"lvl1":"Pytorch","lvl3":"DataLoaderクラス","lvl2":"学習データセットの作成"},"type":"lvl3","url":"/pytorch2#dataloader","position":22},{"hierarchy":{"lvl1":"Pytorch","lvl3":"DataLoaderクラス","lvl2":"学習データセットの作成"},"content":"機械学習のトレーニングには、データセットのサンプリング、シャッフル、バッチ分割などの操作が必要されます。これらの操作を効率化にするためにDataLoaderクラスが用意されます。\n\nバッチ処理: 指定したバッチサイズでデータを分割します\n\nシャッフル: データの順序をランダムに並べ替えられます\n\n# サンプルデータ\ndata = torch.randn(100, 3)  # 100個のデータ、3つの特徴\nlabels = torch.randint(0, 2, (100,))  # 100個のラベル (0または1)\ndataset = CustomDataset(data, labels)\n\n\n\nfrom torch.utils.data import DataLoader\n\ndataloader = DataLoader(dataset, batch_size=10, shuffle=True)\n\n\n\ndataloader\n\n\n\nfor batch_data, batch_labels in dataloader:\n    print(\"Batch data shape:\", batch_data.shape)\n    print(\"Batch labels shape:\", batch_labels.shape)\n\n\n\n# 最初のバッチを取得\ndata_iter = iter(dataloader)\nsample_data, sample_label = next(data_iter)\n\n# 1つ目のサンプルのデータとラベルを確認\nprint(\"Sample data:\", sample_data[0])\nprint(\"Sample label:\", sample_label[0])\n\n\n\nDataLoaderは反復処理が可能なので、トレーニング中のループで直接に使用することができます。\n\nイテレーターとは\n\niter(dataloader)は、DataLoaderオブジェクトからイテレーターを作成します。 イテレーターは順次データを取り出すためのオブジェクトで、forループやnext()を使用して1つずつデータを取得できます。\n\n","type":"content","url":"/pytorch2#dataloader","position":23},{"hierarchy":{"lvl1":"Pytorch","lvl2":"深層学習モデルの構築"},"type":"lvl2","url":"/pytorch2#id-7","position":24},{"hierarchy":{"lvl1":"Pytorch","lvl2":"深層学習モデルの構築"},"content":"\n\ntorch.nnで用意されているクラス、関数は、独自のニューラルネットワークを構築するために必要な要素を網羅しています。\n\nPyTorchの全てのモジュールは、\n\nnn.Moduleを継承しています。\n\nそしてニューラルネットワークは、モジュール自体が他のモジュール（レイヤー）から構成されています。\n\nこの入れ子構造により、複雑なアーキテクチャを容易に構築・管理することができます。\n\n","type":"content","url":"/pytorch2#id-7","position":25},{"hierarchy":{"lvl1":"Pytorch","lvl3":"クラスの定義","lvl2":"深層学習モデルの構築"},"type":"lvl3","url":"/pytorch2#id-8","position":26},{"hierarchy":{"lvl1":"Pytorch","lvl3":"クラスの定義","lvl2":"深層学習モデルの構築"},"content":"nn.Moduleを継承し、独自のネットワークモデルを定義し、その後ネットワークのレイヤーを __init__で初期化します。\n\nnn.Module を継承した全モジュールは、入力データの順伝搬関数であるforward関数を持ちます。\n\nfrom torch import nn\n\n\n\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super(NeuralNetwork, self).__init__()\n        self.linear_relu_stack = nn.Sequential(\n            nn.Linear(512, 128),\n            nn.ReLU(),\n            nn.Linear(128, 128),\n            nn.ReLU(),\n            nn.Linear(128, 3),\n            nn.ReLU()\n        )\n\n    def forward(self, x):\n        logits = self.linear_relu_stack(x)\n        return logits\n\n\n\nこのクラスは、PyTorchのnn.Moduleを継承した単純なニューラルネットワークの実装を示しています。入力は固定長の512とされており、出力は3の次元を持つベクトルです。\n\n最大長512であるテキストに対して、センチメント(ポジティブ、中立、ネガティブ)を予測するタスクをイメージしてください。\n\nself.linear_relu_stack: このシーケンシャルな層は、3つの線形層とそれぞれの後に続くReLU活性化関数から構成されています。\n\nnn.Sequentialにレイヤーを順に渡すだけで、数のレイヤーを順に積み重ねたモデルを簡単に定義できます。\n\nlinear layerは、線形変換を施します。linear layerは重みとバイアスのパラメータを保持しています。\n\nnn.ReLUという活性化関数を設置することで、ニューラルネットワークの表現力を向上させます。\n\n順伝播メソッド (forward): 入力テンソルxを受け取り、ネットワークを通して出力を生成する機能を持ちます。\n\n","type":"content","url":"/pytorch2#id-8","position":27},{"hierarchy":{"lvl1":"Pytorch","lvl3":"GPUの利用","lvl2":"深層学習モデルの構築"},"type":"lvl3","url":"/pytorch2#gpu","position":28},{"hierarchy":{"lvl1":"Pytorch","lvl3":"GPUの利用","lvl2":"深層学習モデルの構築"},"content":"\n\nNeuralNetworkクラスのインスタンスを作成し、変数device上に移動させます。\n\n以下でネットワークの構造を出力し確認します。\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint('Using {} device'.format(device))\n\n\n\nmodel = NeuralNetwork().to(device)\nprint(model)\n\n\n\n","type":"content","url":"/pytorch2#gpu","position":29},{"hierarchy":{"lvl1":"Pytorch","lvl3":"モデルによる計算","lvl2":"深層学習モデルの構築"},"type":"lvl3","url":"/pytorch2#id-9","position":30},{"hierarchy":{"lvl1":"Pytorch","lvl3":"モデルによる計算","lvl2":"深層学習モデルの構築"},"content":"\n\nニューラルネットワークの最後のlinear layerはlogitsを出力します。このlogitsは\n\nnn.Softmaxモジュールへと渡されます。出力ベクトルの要素の値は[0, 1]の範囲となり、これは各クラスである確率を示します。\n\nX = torch.rand(3, 512, device=device)\nlogits = model(X) \nprint(logits)\n\n\n\npred_probab = nn.Softmax(dim=1)(logits)\ny_pred = pred_probab.argmax(1)\nprint(f\"Predicted class: {y_pred}\")\n\n\n\nおまけ：tensorboard\n\ntensorboardでニューラルネットワークの構造を確認する。from torch.utils.tensorboard import SummaryWriter\nX = torch.rand(3, 28, 28)\nwriter = SummaryWriter(\"torchlogs/\")\nwriter.add_graph(model, X)\nwriter.close()\n\n","type":"content","url":"/pytorch2#id-9","position":31},{"hierarchy":{"lvl1":"Pytorch","lvl2":"自動微分"},"type":"lvl2","url":"/pytorch2#id-10","position":32},{"hierarchy":{"lvl1":"Pytorch","lvl2":"自動微分"},"content":"ニューラルネットワークを訓練する際、その学習アルゴリズムとして、バックプロパゲーション（back propagation） がよく使用されます。\n\nバックプロパゲーションでは、モデルの重みなどの各パラメータは、損失関数に対するその変数の微分値（勾配）に応じて調整されます。\n\nこれらの勾配の値を計算するために、PyTorchにはtorch.autograd という微分エンジンが組み込まれています。\n\nautogradはPyTorchの計算グラフに対する勾配の自動計算を支援します。\n\nシンプルな1レイヤーのネットワークを想定しましょう。\n\n入力をx、パラメータをw と b、そして適切な損失関数を決めます。\n\nPyTorchでは例えば以下のように実装します。\n\nimport torch\n\nx = torch.rand(5)  # input tensor\ny = torch.zeros(3)  # expected output\nw = torch.randn(5, 3, requires_grad=True)\nb = torch.randn(3, requires_grad=True)\nz = torch.matmul(x, w)+b\nloss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)\n\n\n\n","type":"content","url":"/pytorch2#id-10","position":33},{"hierarchy":{"lvl1":"Pytorch","lvl3":"勾配情報の保存","lvl2":"自動微分"},"type":"lvl3","url":"/pytorch2#id-11","position":34},{"hierarchy":{"lvl1":"Pytorch","lvl3":"勾配情報の保存","lvl2":"自動微分"},"content":"\n\nこののニューラルネットワークでは、wとbが最適したいパラメータです。\n\nそのため、これらの変数に対する損失関数の微分値を計算する必要があります。\n\nこれらのパラメータで微分を可能にするために、requires_grad属性をこれらのテンソルに追記します。\n\nそうすると、勾配は、テンソルの grad_fn プロパティに格納されます。\n\nprint('Gradient function for z =',z.grad_fn)\nprint('Gradient function for loss =', loss.grad_fn)\n\n\n\n","type":"content","url":"/pytorch2#id-11","position":35},{"hierarchy":{"lvl1":"Pytorch","lvl3":"勾配計算","lvl2":"自動微分"},"type":"lvl3","url":"/pytorch2#id-12","position":36},{"hierarchy":{"lvl1":"Pytorch","lvl3":"勾配計算","lvl2":"自動微分"},"content":"ニューラルネットワークの各パラメータを最適化するために、入力xと出力yが与えられたもとで、損失関数の各変数の偏微分値、\n\nすなわち\n\n\\frac{\\partial loss}{\\partial w} 、\\frac{\\partial loss}{\\partial b}\n\nを求める必要があります。\n\nこれらの偏微分値を求めるためにloss.backward()を実行し、w.gradとb.gradの値を導出します。\n\n逆伝搬では、.backward()がテンソルに対して実行されると、autogradは、\n\n各変数の .grad_fnを計算する\n\n各変数の.grad属性に微分値を代入する\n\n微分の連鎖律を使用して、各leafのテンソルの微分値を求める\n\nを行います。\n\nloss.backward()\nprint(w.grad)\nprint(b.grad)\n\n\n\n最適化ループを構築し、Pytorchより自動的に逆伝播\n\nimport torch.nn.functional as F\n\ndef training_loop(n_epochs, learning_rate, model, input, target):\n    for epoch in range(1, n_epochs + 1):\n        # Forward pass\n        outputs = model(input)\n        \n        # Compute the loss using Binary Cross Entropy with Logits\n        loss = F.binary_cross_entropy_with_logits(outputs, target)\n        \n        # Backward pass\n        loss.backward()\n        \n        # Update the parameters\n        with torch.no_grad():\n            for param in model.parameters():\n                param -= learning_rate * param.grad\n        model.zero_grad()\n        # Zero the parameter gradients after updating \n        if epoch % 100 == 0:\n            print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n\n    return model\n\n\n\n# Example usage (with dummy data)\ninput = torch.rand(10, 512)  # 10 samples with 512 features each\ntarget = torch.rand(10, 3)  # 10 samples with 3 target values each\n\nn_epochs = 500\nlearning_rate = 0.01\nmodel = NeuralNetwork()\n\ntrained_model = training_loop(n_epochs, learning_rate, model, input, target)\n\n\n\n\nPyTorchにおける勾配計算の初期化\n\nPyTorchの勾配計算メカニズムでは、.backwardを呼び出すと、リーフノードで導関数の計算結果が累積されます。つまり、もし.backwardが以前にも呼び出されていた場合、損失関数が再び計算され、.backwardも再び呼び出され、各リーフの勾配が前の反復で計算された結果の上に累積されます。その結果、勾配の値は誤ったものになります。\n\nこのようなことが起こらないようにするためには、反復のルーブのたびにmodel.zero_grad()を用いて明示的に勾配をゼロに設定する必要があります。\n\n","type":"content","url":"/pytorch2#id-12","position":37},{"hierarchy":{"lvl1":"Pytorch","lvl3":"最適化関数","lvl2":"自動微分"},"type":"lvl3","url":"/pytorch2#id-13","position":38},{"hierarchy":{"lvl1":"Pytorch","lvl3":"最適化関数","lvl2":"自動微分"},"content":"最適化は各訓練ステップにおいてモデルの誤差を小さくなるように、モデルパラメータを調整するプロセスです。\n\nここまでの説明は、単純な勾配下降法を最適化に使用しました。これは、シンプルなケースでは問題なく機能しますが、モデルが複雑になったときのために、パラメータ学習の収束を助ける最適化の工夫が必要されます。","type":"content","url":"/pytorch2#id-13","position":39},{"hierarchy":{"lvl1":"Pytorch","lvl4":"Optimizer","lvl3":"最適化関数","lvl2":"自動微分"},"type":"lvl4","url":"/pytorch2#optimizer","position":40},{"hierarchy":{"lvl1":"Pytorch","lvl4":"Optimizer","lvl3":"最適化関数","lvl2":"自動微分"},"content":"optimというモジュールには、様々な最適化アルゴリズムが実装されています。\n\nここでは、確率的勾配降下法（Stochastic Gradient Descent）を例として使い方を説明します。\n\n確率的勾配降下法は、ランダムに選んだ１つのデータのみで勾配を計算してパラメータを更新し、データの数だけ繰り返す方法です。\n\n訓練したいモデルパラメータをoptimizerに登録し、合わせて学習率をハイパーパラメータとして渡すことで初期化を行います。訓練ループ内で、最適化（optimization）は3つのステップから構成されます。\n\noptimizer.zero_grad()を実行し、モデルパラメータの勾配をリセットします。勾配の計算は蓄積されていくので、毎イテレーション、明示的にリセットします。\n\n続いて、loss.backwards()を実行し、バックプロパゲーションを実行します。PyTorchは損失に対する各パラメータの偏微分の値（勾配）を求めます。\n\n最後に、optimizer.step()を実行し、各パラメータの勾配を使用してパラメータの値を調整します。\n\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n\n\n\ndef training_loop(n_epochs, learning_rate, model, input, target):\n    # Use Binary Cross Entropy with Logits as the loss function\n    \n    # Use Adam as the optimizer\n    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n    \n    for epoch in range(1, n_epochs + 1):\n        # Zero the parameter gradients\n        optimizer.zero_grad()\n        \n        # Forward pass\n        outputs = model(input)\n        loss = F.binary_cross_entropy_with_logits(outputs, target)\n        \n        # Backward pass and optimize\n        loss.backward()\n        optimizer.step()\n\n        # Print loss every 100 epochs\n        if epoch % 100 == 0:\n            print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n\n    return model\n\n\n\ninput = torch.rand(10, 512)  # 10 samples with 512 features each\ntarget = torch.rand(10, 3)  # 10 samples with 3 target values each\n\nn_epochs = 1000\nlearning_rate = 0.001\nmodel = NeuralNetwork()\n\ntrained_model = training_loop(n_epochs, learning_rate, model, input, target)\n\n\n\n","type":"content","url":"/pytorch2#optimizer","position":41},{"hierarchy":{"lvl1":"Pytorch","lvl2":"Pytorchによる深層学習の実装例"},"type":"lvl2","url":"/pytorch2#pytorch","position":42},{"hierarchy":{"lvl1":"Pytorch","lvl2":"Pytorchによる深層学習の実装例"},"content":"","type":"content","url":"/pytorch2#pytorch","position":43},{"hierarchy":{"lvl1":"Pytorch","lvl3":"データの読み込み","lvl2":"Pytorchによる深層学習の実装例"},"type":"lvl3","url":"/pytorch2#id-14","position":44},{"hierarchy":{"lvl1":"Pytorch","lvl3":"データの読み込み","lvl2":"Pytorchによる深層学習の実装例"},"content":"\n\nimport pandas as pd\ntrain_df = pd.read_csv('./Data/Titanic/train.csv')\ntrain_df.head()\n\n\n\ntest_df = pd.read_csv('./Data/Titanic/test.csv')\ntest_df.head()\n\n\n\nprint(train_df.dtypes)\n\n\n\n\n","type":"content","url":"/pytorch2#id-14","position":45},{"hierarchy":{"lvl1":"Pytorch","lvl3":"データの前処理","lvl2":"Pytorchによる深層学習の実装例"},"type":"lvl3","url":"/pytorch2#id-15","position":46},{"hierarchy":{"lvl1":"Pytorch","lvl3":"データの前処理","lvl2":"Pytorchによる深層学習の実装例"},"content":"列の削除: 要らない列を消していきます．乗客IDや名前，チケット，港(Embarked)や部屋番号(Cabin)はは生死にあまり関係がなさそうので、削除します。\n\n欠損の補完: 平均値で欠損しているデータを補完します。\n\n文字列を数字に置換\n\ndef process_df(df):\n    df = df.drop([\"PassengerId\", \"Name\", \"Ticket\", \"Cabin\", \"Embarked\"], axis=1)\n    df[\"Age\"] = df[\"Age\"].fillna(df[\"Age\"].mean())\n    df = df.replace(\"male\", 0)\n    df = df.replace(\"female\", 1)\n    return df\n\ntrain_df = process_df(train_df)\ntest_df = process_df(test_df)\ntrain_df.head()\n\n\n\n","type":"content","url":"/pytorch2#id-15","position":47},{"hierarchy":{"lvl1":"Pytorch","lvl3":"データセットの作成","lvl2":"Pytorchによる深層学習の実装例"},"type":"lvl3","url":"/pytorch2#id-16","position":48},{"hierarchy":{"lvl1":"Pytorch","lvl3":"データセットの作成","lvl2":"Pytorchによる深層学習の実装例"},"content":"\n\nデータセットとデータローダーでミニバッチ学習ためのデータセットを作成します。\n\nfeatures = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\"]\nX = train_df[features]\ny = train_df[\"Survived\"]\n\n\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n\n\n\nclass TitanicDataset(Dataset):\n    def __init__(self, X, y):\n        # DataFrameをNumPy配列に変換\n        self.X = torch.tensor(X.values, dtype=torch.float32)  # XをNumPy配列に変換してからテンソル化\n        self.y = torch.tensor(y.values, dtype=torch.float32)  # yも同様にテンソル化\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, idx):\n        return self.X[idx], self.y[idx]\n\n\n\n\n\n# データローダーの作成\ntrain_dataset = TitanicDataset(X_train, y_train)\nval_dataset = TitanicDataset(X_val, y_val)\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n\n\n\ntrain_dataset[0]\n\n\n\n","type":"content","url":"/pytorch2#id-16","position":49},{"hierarchy":{"lvl1":"Pytorch","lvl3":"モデルの定義","lvl2":"Pytorchによる深層学習の実装例"},"type":"lvl3","url":"/pytorch2#id-17","position":50},{"hierarchy":{"lvl1":"Pytorch","lvl3":"モデルの定義","lvl2":"Pytorchによる深層学習の実装例"},"content":"__init__で行列の掛け算の定義をして，forwardでそれをどの順番で行うかを指定する感じです．\n\nclass TitanicModel(nn.Module):\n    def __init__(self, input_size):\n        super(TitanicModel, self).__init__()\n        self.fc1 = nn.Linear(input_size, 64)\n        self.fc2 = nn.Linear(64, 32)\n        self.fc3 = nn.Linear(32, 1)\n\n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = torch.relu(self.fc2(x))\n        x = torch.sigmoid(self.fc3(x))\n        return x\n\n\n\n","type":"content","url":"/pytorch2#id-17","position":51},{"hierarchy":{"lvl1":"Pytorch","lvl3":"モデルの学習","lvl2":"Pytorchによる深層学習の実装例"},"type":"lvl3","url":"/pytorch2#id-18","position":52},{"hierarchy":{"lvl1":"Pytorch","lvl3":"モデルの学習","lvl2":"Pytorchによる深層学習の実装例"},"content":"\n\n損失関数は2乗誤差、最適化関数はAdamを使用します．\n\nimport torch.optim as optim\n\n\ndef train_model(model, train_loader, val_loader, n_epochs, learning_rate):\n    criterion = nn.BCELoss()  # 分類の損失関数\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n    for epoch in range(n_epochs):\n        model.train()\n        train_loss = 0.0\n        for X_batch, y_batch in train_loader:\n            optimizer.zero_grad()\n            outputs = model(X_batch).squeeze()\n            loss = criterion(outputs, y_batch)\n            loss.backward()\n            optimizer.step()\n            train_loss += loss.item() * X_batch.size(0)\n\n        # 検証データでの評価\n        model.eval()\n        val_loss = 0.0\n        with torch.no_grad():\n            for X_batch, y_batch in val_loader:\n                outputs = model(X_batch).squeeze()\n                loss = criterion(outputs, y_batch)\n                val_loss += loss.item() * X_batch.size(0)\n        # 訓練と検証の損失をデータセットのサイズで割り、平均損失を計算します\n        train_loss /= len(train_loader.dataset) \n        val_loss /= len(val_loader.dataset)\n        if epoch % 10 == 0:\n            print(f\"Epoch {epoch+1}/{n_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n\n\n\n\nmodel = TitanicModel(input_size=X_train.shape[1])\ntrain_model(model, train_loader, val_loader, n_epochs=100, learning_rate=0.001)\n\n\n\n","type":"content","url":"/pytorch2#id-18","position":53},{"hierarchy":{"lvl1":"Pytorch","lvl3":"モデルの評価","lvl2":"Pytorchによる深層学習の実装例"},"type":"lvl3","url":"/pytorch2#id-19","position":54},{"hierarchy":{"lvl1":"Pytorch","lvl3":"モデルの評価","lvl2":"Pytorchによる深層学習の実装例"},"content":"\n\nfrom sklearn.metrics import classification_report\n\ndef evaluate_model(model, loader):\n    model.eval()\n    predictions = []\n    true_labels = []\n    \n    with torch.no_grad():\n        for X_batch, y_batch in loader:\n            outputs = model(X_batch).squeeze()\n            # 出力を0または1に丸める\n            predicted_classes = outputs.round().numpy()\n            predictions.extend(predicted_classes)\n            true_labels.extend(y_batch.numpy())\n\n    # レポートの表示\n    print(classification_report(true_labels, predictions, target_names=[\"Did not survive\", \"Survived\"],digits=4))\n\n\n\n\n# モデルの評価を実行\nevaluate_model(model, val_loader)\n\n\n\n深層学習の実装を以下の指示に従って、改めて学習を行なってください。\n\nバッチサイズを64に変更しなさい\n\nモデルに一つ隠れ層を追加しなさい\n\nドロップアウト層を追加しなさい\n\nオプティマイザはAdamWを設定しなさい\n\nepochを増やしなさい","type":"content","url":"/pytorch2#id-19","position":55},{"hierarchy":{"lvl1":"LSTMの実装"},"type":"lvl1","url":"/pytorch-lstm","position":0},{"hierarchy":{"lvl1":"LSTMの実装"},"content":"","type":"content","url":"/pytorch-lstm","position":1},{"hierarchy":{"lvl1":"LSTMの実装","lvl2":"torch.nn.LSTM"},"type":"lvl2","url":"/pytorch-lstm#torch-nn-lstm","position":2},{"hierarchy":{"lvl1":"LSTMの実装","lvl2":"torch.nn.LSTM"},"content":"\n\nPyTorchの\n\ntorch.nn.LSTMクラスを使用して、LSTMをモデルに簡単にレイヤーとして追加できます。\n\ntorch.nn.LSTMの主なパラメータは以下の通りです：\n\ninput_size：入力xの特徴量の数\n\nhidden_size：隠れ状態の特徴量の数\n\nnum_layers：LSTMを重ねる層の数（デフォルトは1）\n\nbias：バイアスを使用するかどうか（デフォルトはTrue）\n\nbatch_first：入力と出力のテンソルの形状が(batch, seq, feature)になるようにするかどうか（デフォルトはFalse）\n\ndropout：ドロップアウトを適用する確率（デフォルトは0、つまりドロップアウトなし）\n\nbidirectional：双方向LSTMを使用するかどうか（デフォルトはFalse）\n\n","type":"content","url":"/pytorch-lstm#torch-nn-lstm","position":3},{"hierarchy":{"lvl1":"LSTMの実装","lvl2":"入力"},"type":"lvl2","url":"/pytorch-lstm#id","position":4},{"hierarchy":{"lvl1":"LSTMの実装","lvl2":"入力"},"content":"\n\ninput_size=10：各入力要素の特徴の数（入力ベクトルの次元数）は10です\n\nhidden_size=20：隠れ状態とセル状態の各ベクトルのサイズは20です\n\nnum_layers=2： LSTMの層の数は2です\n\n最初のLSTM層は入力シーケンスを受け取り、それを処理して一連の隠れ状態を生成します。\n\n最初の層の出力（隠れ状態）は、次のLSTM層の入力となります。これが複数層にわたって繰り返されます。\n\nimport torch\nimport torch.nn as nn\n# LSTMのインスタンス化\nlstm = nn.LSTM(input_size=10, hidden_size=20,num_layers=2)\n\n\n\n入力データの生成（例：バッチサイズ=3, シーケンス長=5）\n\ninput = torch.randn(5, 3, 10)\n\n\n\nh0:隠れ状態の初期値\n\nh0 のサイズは(num_layers * num_directions, batch_size, hidden_size)になります\n\nnum_directions:LSTMが単方向か双方向かを示し（単方向の場合は1、双方向の場合は2）\n\nc0:セル状態の初期値\n\nc0 のサイズは同様に(num_layers * num_directions, batch_size, hidden_size)になります\n\n# 隠れ状態とセル状態の初期化\nh0 = torch.randn(2, 3, 20)\nc0 = torch.randn(2, 3, 20)\n\n\n\n","type":"content","url":"/pytorch-lstm#id","position":5},{"hierarchy":{"lvl1":"LSTMの実装","lvl2":"出力"},"type":"lvl2","url":"/pytorch-lstm#id-1","position":6},{"hierarchy":{"lvl1":"LSTMの実装","lvl2":"出力"},"content":"torch.nn.LSTMの出力は、出力テンソル（通常は output と呼ばれます）と隠れ状態（h_n と c_n）から構成されています\n\n# 順伝播\noutput, (hn, cn) = lstm(input, (h0, c0))\n\n\n\nprint(f\"output: {output.shape}\")\nprint(f\"hn: {hn.shape}\")\nprint(f\"cn: {cn.shape}\")\n\n\n\n出力テンソル（output）\n\nシーケンス内の各時点におけるLSTMの隠れ状態を含んでいます。\n\nサイズは (seq_len, batch, num_directions * hidden_size) になります。\n\n最終隠れ状態(h_n)\n\nLSTMの最終的な隠れ状態です。\n\nサイズは(num_layers * num_directions, batch, hidden_size) になります。\n\nNote\n\noutputの最終時点の隠れ状態（つまり output[-1] ）は、単層、単方向LSTMの場合、hn と同じです。\n\n多層LSTMの場合、hnは各層の最終隠れ状態を含むため、outputの最終時点の隠れ状態とは異なることがあります。この場合、outputの最後の要素は最終層の最終隠れ状態に対応し、hn にはそれぞれの層の最終隠れ状態が格納されます。\n\n最終セル状態(c_n)\n\nLSTMの最終的なセル状態です。長期的な依存関係をどのように「記憶」しているかを示します。\n\nサイズは (num_layers * num_directions, batch, hidden_size) です\n\nタスクに応じて、torch.nn.LSTMの出力を扱う必要があります。例えば、テキスト生成、機械通訳、文書分類はそれぞれどの出力を使うべきですか？","type":"content","url":"/pytorch-lstm#id-1","position":7},{"hierarchy":{"lvl1":"RNNの基礎"},"type":"lvl1","url":"/rnn","position":0},{"hierarchy":{"lvl1":"RNNの基礎"},"content":"これまで見てきたニューラルネットワークは、フィードフォワードと呼ばれるタイプのネットワークです。具体的には、入力信号が次の層へ信号を伝達し、信号を受け取った層その次の層へ伝達しといったように一方向だけの信号伝達を行います。\n\nフィードフォワードが複雑なパターンや関係を学習でき、様々なタスクに応用できます。しかし、フィードフォワードネットワークは時間的なシーケンスやデータの順序を考慮する能力に欠けています。\n\nフィードフォワードネットワークでは、データは入力層から隠れ層を経て出力層へと一方向に流れます。この構造では、前の層の出力が次の層の入力となるだけで、過去の入力に関する情報は保存されません。\n\nフィードフォワードネットワークは、各入力を独立したものとして扱い、入力間の時間的な連続性や順序を認識しません。\n\n時系列データや言語のように、順序が重要なタスクに対しては、フィードフォワードネットワークはこれらの関連性を捉えることができず、効果的な処理が難しいです。そごで、RNN(Recurrent Neural Network)の出番です。\n\n","type":"content","url":"/rnn","position":1},{"hierarchy":{"lvl1":"RNNの基礎","lvl2":"基本的なRNN"},"type":"lvl2","url":"/rnn#id-rnn","position":2},{"hierarchy":{"lvl1":"RNNの基礎","lvl2":"基本的なRNN"},"content":"RNNは名前通り、「循環する」構造が設計されています。つまり、ある地点をスタートしたものが、時間を経て再び元の場所へと戻って来ること、そして、それを繰り返すこと。\n\n循環するためには、RNNではループする経路を持っています。このループする経路によって、データは絶えず循環することができます。そしてデータが循環することにより、過去の情報を記憶しながら、最新のデータへと更新されます。","type":"content","url":"/rnn#id-rnn","position":3},{"hierarchy":{"lvl1":"RNNの基礎","lvl3":"RNNの主要な要素","lvl2":"基本的なRNN"},"type":"lvl3","url":"/rnn#rnn","position":4},{"hierarchy":{"lvl1":"RNNの基礎","lvl3":"RNNの主要な要素","lvl2":"基本的なRNN"},"content":"入力x_t:時刻をtとして、x_tを入力としています。これは時系列データとして、x_0,x_1,...,x_tというデータがレイヤへ入力されることを示しています。\n\nx_tは何らかのベクトルを想定します。文章(単語の並び)を扱う場合、各単語の分散表現をx_tになります。\n\n隠れ状態h_t:入力に対応する形で、(h_0,h_1,...,h_t)が出力されます。","type":"content","url":"/rnn#rnn","position":5},{"hierarchy":{"lvl1":"RNNの基礎","lvl3":"RNNの動作プロセス","lvl2":"基本的なRNN"},"type":"lvl3","url":"/rnn#rnn-1","position":6},{"hierarchy":{"lvl1":"RNNの基礎","lvl3":"RNNの動作プロセス","lvl2":"基本的なRNN"},"content":"各時刻tにおいて、RNNは現在の入力x_tと前の時刻の隠れ状態h_{t-1}を受け取ります。\n\nこれらの情報は、隠れ層のニューロンを通じて処理され、新しい隠れ状態h_tが生成されます。\n\nこの新しい隠れ状態は、次の時刻t+1の計算における「記憶」の一部として使用され、また必要に応じて出力も生成されます。\n\n隠れ状態h_tは通常、以下のような形で計算されます：\n\nh_t = f(W_{h} h_{t-1} + W_{x} x_t + b)\n\nここで、\n\nf は非線形活性化関数（例：tanh、ReLU）。\n\nW_{h} は隠れ状態間の接続を表す重み行列。\n\nW_{x} は入力と隠れ状態の接続を表す重み行列。\n\nb はバイアスベクトル。\n\nh_tは、別のレイヤへ出力されると同時に、次時刻のRNNレイヤへも出力されます。これで、現在の出力h_tは、ひとつ前の出力h_{t-1}によって計算されることがわかります。\n\n\n\nRNNレイヤの順伝播と逆伝播は以下の計算グラフで示されます。\n\nここで行う計算は、行列の積「MatMul」、足し算「＋」、そして「tanh」を示しています。\n\n順伝播\n\n逆伝播\n\n\n\n\n\n","type":"content","url":"/rnn#rnn-1","position":7},{"hierarchy":{"lvl1":"RNNの基礎","lvl2":"RNNLM"},"type":"lvl2","url":"/rnn#rnnlm","position":8},{"hierarchy":{"lvl1":"RNNの基礎","lvl2":"RNNLM"},"content":"言語モデル(Language Model)は、単語の並びに対して確率を与えることで、自然言語の文や文書を生成または理解します。\n\n言語モデルの基本的な機能は、与えられたトークン列に対して、次のトークンの生成される確率を予測することです。P(w_n | w_1, w_2, \\ldots, w_{n-1})\n\nRNNでは時系列データを扱えますので、言語モデルの実装に用いられます。RNNによる言語モデルは、RNN Language Model(RNNLM)と呼びます。\n\nCBOWモデルでも言語モデルとして適用することは可能ですか？RNNLMと比較することで考えなさい。\n\n\n\nEmbeddingレイヤ：　Embeddingレイヤは、単語を単語の分散表現へ変換します\n\nRNNレイヤ：単語の分散表現がRNNレイヤへと入力されます\n\n隠れ状態を次の層へ出力されます\n\n隠れ状態を次時刻のRNNレイヤへ出力します\n\n次の層へ出力されます隠れ状態は、Affineレイヤを経てSoftmaxレイヤへと伝わっていきます。\n\nSoftmaxレイヤが出力するのは確率分布です\n\n","type":"content","url":"/rnn#rnnlm","position":9},{"hierarchy":{"lvl1":"RNNの基礎","lvl2":"RNNの問題点"},"type":"lvl2","url":"/rnn#rnn-2","position":10},{"hierarchy":{"lvl1":"RNNの基礎","lvl2":"RNNの問題点"},"content":"RNNは過去の情報を継続的に更新しながら新しい入力を処理する機能を持っていますが、長期間の依存関係を捉えるのが難しいという「長期依存問題」が存在します。例えば、次で示されたタスクを考えてみたいと思います。\n\nTom was watching TV in his home. Mary came into the room. Mary said hi to 「？」 \n\n提示されたタスクは、文脈に基づいた適切な単語を特定することを要求しています。\n\nこの例では、「Tom」が以前の文で言及されており、Maryが部屋に入って「hi」と言っているため、空白に当てはまる最も適切な単語は「Tom」です。\n\nRNNで処理する際、正解ラベルとして「Tom」という単語が与えられた場所から、過去の方向に向かって「意味のある勾配」を伝達することによって、時間方向の依存関係を学習することができます。しかし、勾配が途中で弱まったらーほとんど何も情報を持たなくなってしまったらー、重みパラメタは更新されなくなります。特に、期依存問題を処理する場合このような状況が生じる可能性が高い、これが「勾配消失」と呼ばれる現象です。\n\nつまり、RNNは、時系列の問題に取り組むために設計されていますが、「長期依存問題」のために、長い文脈を持つタスクでは性能が低下することがあります。この問題は、RNNが過去の情報を十分に保持し続けることが困難であることに起因します。","type":"content","url":"/rnn#rnn-2","position":11},{"hierarchy":{"lvl1":"Self-Attention"},"type":"lvl1","url":"/self-attention","position":0},{"hierarchy":{"lvl1":"Self-Attention"},"content":"\n\n実に、Attention機構というアイデア自体は汎用的であり、様々な場面で活用されています。Attention機構に基づいたテクニックとして、Transformerの基盤となっているSelf-Attentionというテクニックが挙げられます。","type":"content","url":"/self-attention","position":1},{"hierarchy":{"lvl1":"Self-Attention","lvl2":"RNNの問題点"},"type":"lvl2","url":"/self-attention#rnn","position":2},{"hierarchy":{"lvl1":"Self-Attention","lvl2":"RNNの問題点"},"content":"今まで、seq2seqやAttention付きseq2seqなどRNNに基づくモデルを説明しました。これらの方法は色々なタスクで広く応用されましたが、RNNの構造による本質的な欠点があります。\n\nそれは、RNNは前時点に計算した結果を用いて順番的に計算を行うたま、時間方向で並列的に計算することはできません。この点は、大規模な計算が行われる際、大きなボトルニックになります。そこで、RNNを避けたいモチベーションが生まれます。\n\n並列計算(Parallel Computing)\n\n並列計算（parallel computing）は、コンピュータにおいて特定の処理をいくつかの独立した小さな処理に細分化し、複数の処理装置（プロセッサ）上でそれぞれの処理を同時に実行させることです。\n\n","type":"content","url":"/self-attention#rnn","position":3},{"hierarchy":{"lvl1":"Self-Attention","lvl2":"Self-Attentionの仕組み"},"type":"lvl2","url":"/self-attention#self-attention","position":4},{"hierarchy":{"lvl1":"Self-Attention","lvl2":"Self-Attentionの仕組み"},"content":"Self-Attentionは埋め込みを入力として受け取り、それらを相互に参照して(Attntionの計算)、新しい埋め込みを生成します。","type":"content","url":"/self-attention#self-attention","position":5},{"hierarchy":{"lvl1":"Self-Attention","lvl3":"Key、Query、Valueの生成","lvl2":"Self-Attentionの仕組み"},"type":"lvl3","url":"/self-attention#key-query-value","position":6},{"hierarchy":{"lvl1":"Self-Attention","lvl3":"Key、Query、Valueの生成","lvl2":"Self-Attentionの仕組み"},"content":"\\mathbf{W}^Q, \\mathbf{W}^K, \\mathbf{W}^V \\in R^{d_{model}\\times d_k}\n\nd_{model} 入力埋め込みのサイズ\n\nd_k self-attentionライヤの次元数","type":"content","url":"/self-attention#key-query-value","position":7},{"hierarchy":{"lvl1":"Self-Attention","lvl3":"Attention Weightの計算","lvl2":"Self-Attentionの仕組み"},"type":"lvl3","url":"/self-attention#attention-weight","position":8},{"hierarchy":{"lvl1":"Self-Attention","lvl3":"Attention Weightの計算","lvl2":"Self-Attentionの仕組み"},"content":"Attention(\\mathbf{Q},\\mathbf{K},\\mathbf{V})=softmax(\\frac{\\mathbf{Q}\\mathbf{K}^T}{\\sqrt{d_k}})V\n\nNote\n\n\\sqrt{d_k}は、次元d_kが無えるのにともなって、内積の絶対値が大きな値になりすぎるを防ぐことで、学習の安定化させるために導入します。","type":"content","url":"/self-attention#attention-weight","position":9},{"hierarchy":{"lvl1":"Self-Attention","lvl3":"Attention出力","lvl2":"Self-Attentionの仕組み"},"type":"lvl3","url":"/self-attention#attention","position":10},{"hierarchy":{"lvl1":"Self-Attention","lvl3":"Attention出力","lvl2":"Self-Attentionの仕組み"},"content":"Self-Attentionの出力は、入力の各要素の埋め込みが他の要素の埋め込みとの関連性に基づいて、つまり重要度を加味しつつ生成した新たな埋め込みになります。\n\n\n\n","type":"content","url":"/self-attention#attention","position":11},{"hierarchy":{"lvl1":"Self-Attention","lvl2":"Multi-head Attention"},"type":"lvl2","url":"/self-attention#multi-head-attention","position":12},{"hierarchy":{"lvl1":"Self-Attention","lvl2":"Multi-head Attention"},"content":"Attention機構の表現力をさらに高めるために、Attention機構を同時に複数適用するMulti-head Attentionが開発されました。\n\nモチベーションとしては、テキストを「理解」するためには、単語の意味や係り受けなどの文法的な情報が重要である場合がありますので、複数のAttention機構を同時に適用することで、複数な観点から文脈化を行うことができます。\n\n具体的には、\n\nMulti-head Attentionでは、D次元の入力埋め込みh_iに対して、M個のAttention機構を同時に適用します。q_i^{(m)} = \\mathbf{W}_q^{(m)}h_ik_i^{(m)} = \\mathbf{W}_k^{(m)}h_iv_i^{(m)} = \\mathbf{W}_v^{(m)}h_i\n\nここで、\\mathbf{W}_q^{(m)}、\\mathbf{W}_k^{(m)}、\\mathbf{W}_v^{(m)}は、m番目のヘッド(head)に対応する行列になります。\n\n各ヘッドでAttentionの計算を行い、各ヘッドの出力埋め込みo_i^{m}が得られます。o_i^{m}=Attention(\\mathbf{Q_m},\\mathbf{K_m},\\mathbf{V_m})\n\nMulti-head Attentionの出力は、M個の出力埋め込みを連結して計算されます。\n\n\n\nランダムに初期化されたQ、K、Vを使って、attention weightを計算しなさい\n\npytorchが提供しているメソッドを用いてattention weightを計算しなさい\n\npytorch提供しているメソッドを使わず、数式に従って、pytorchの行列計算でattention weightを計算しなさい\n\nimport torch\nbatch_size = 1\nsequence_length = 3\nembedding_dim = 4\nseed=1234\nQ = torch.rand(sequence_length, embedding_dim)\nK = torch.rand(sequence_length, embedding_dim)\nV = torch.rand(sequence_length, embedding_dim)\n\n\n\n","type":"content","url":"/self-attention#multi-head-attention","position":13},{"hierarchy":{"lvl1":"Self-Attention","lvl3":"補足資料","lvl2":"Multi-head Attention"},"type":"lvl3","url":"/self-attention#id","position":14},{"hierarchy":{"lvl1":"Self-Attention","lvl3":"補足資料","lvl2":"Multi-head Attention"},"content":"Self-Attention by Shusen Wang\n\nSelf-Attention by Hung-yi Lee\n\nAttention Visualization","type":"content","url":"/self-attention#id","position":15},{"hierarchy":{"lvl1":"Seq2seq"},"type":"lvl1","url":"/seq2seq","position":0},{"hierarchy":{"lvl1":"Seq2seq"},"content":"\n\n自然言語処理に関するタスクは、言語の時系列データを別の時系列データに変換するような問題として定式化することができます。\n\n機械通訳のタスクでは、ある言語の文章（入力時系列データ）を別の言語の文章（出力時系列データ）に変換します。\n\n要約のタスクでは、長いテキスト（入力時系列データ）を短い要約文（出力時系列データ）に変換します。\n\nこのように、自然言語処理の多くのタスクは、時間的に順序付けられたデータを取り扱い、それを新しい形式の順序付けられたデータに変換するプロセスとして捉えることができます。このプロセスは、データの時系列的な性質と言語の構造的な特徴を考慮に入れる必要があります。そのためのモデルは、seq2seqと呼ばれます。\n\n","type":"content","url":"/seq2seq","position":1},{"hierarchy":{"lvl1":"Seq2seq","lvl2":"seq2seqの原理"},"type":"lvl2","url":"/seq2seq#seq2seq","position":2},{"hierarchy":{"lvl1":"Seq2seq","lvl2":"seq2seqの原理"},"content":"seq2seq(Sequence-to-Sequence)はEncoder-Decoderモデルとも呼ばれます。その名前が示す通り、そこには2つのモジュールーEncoderとDecoderーから構成されています。\n\nエンコードとは、情報をある規則に基づいて変換することを指しています。具体的には、入力シーケンスを取り込み、それを固定長の内部表現（通常はベクトル）に変換することです。この内部表現には、入力シーケンスの重要な情報がコンパクトにまとめられています。\n\nデコードは、エンコーダーから受け取った特徴ベクトルを基に、目的の出力シーケンスを生成します。この過程で、デコーダーはエンコーダーが提供した情報を「解釈」し、それに基づいて適切な出力を生成するための処理を行います。\n\n\n\n機械翻訳の歴史\n\n機械翻訳（Machine Translation：MT）は計算機科学の黎明期から長きにわたって取り組まれてきた伝統的な課題であり，自然言語処理研究の発展を牽引しています。\n\nルールベース\n\n1950年代から本格的な研究が始められました。当時は、翻訳先の言語の文法規則にもとづくルールベース機械翻訳が使われていました。膨大なルールを作り込む必要があり、開発にも新語のアップデートにも手間がかかりました。精度は高くなく、主に定型的な文しか翻訳できませんでした。\n\n統計的機械翻訳（Statistical Machine Translation、SMT）\n\n1980年代になると、さらに統計的機械翻訳が生み出されました。統計的機械翻訳では、人の代わりにコンピュータがルールを学びます。原文と訳文のペアを大量に読み込み、原文の単語やフレーズが訳文の特定の単語やフレーズに変換される確率を計算することで、特定の言葉や表現がどのように翻訳されるかを学習します。\n\nただし、この手法は、文法構造や言語の深い理解に基づく翻訳を生成することが難しいことがあります。そのため、十分な並列コーパスがない場合や、文法構造が大きく異なる言語ペアに対しては性能が低下することがあります。結果として、翻訳されたテキストには文法的な不正確さや不自然さが生じることがあります。\n\nSMTでは原言語の部分単語列を目的言語の句に翻訳し，それを適当な順序に並べ替えるという構成になっています。そのため、句の翻訳モデル，句の並べ替えモデル，出力文の言語モデルを用いて、膨大な翻訳結果候補の中からスコアが最大になるものを探索することになりますargmax_{y} P(y|x) = argmax_{y} \\underset{\\text{Translation Model  }}{P(x|y)}  \\underset{\\text{  Language Model}}{P(y)}\n\nニューラル翻訳(Neural Machine Translation, NMT)\n\n機械翻訳が大きな進展を迎えたのは2014年のseq2seqモデルの登場でした。seq2seqでは単語列をRNNに入力した後RNNに単語列を出力させる，という極めて単純な仕組みで機械翻訳が実現できるという可能性を示しました。\n\nSMTが単語や句の単位で翻訳したものを組み合わせて翻訳文を構成しようとするのに対し，NMT はそのように明示的な要素の組合せを考慮する必要がなく、正しい翻訳結果が出力されるようにニューラルネットワークを学習します。\n\nNMTにおいては単語や文はベクトルとして多次元連続空間に埋め込まれて処理され，ある出力単語はそれ以前の処理すべての結果を踏まえて選択されるため，具体的にどの入力単語と対応することを考慮する必要がありません。\n\n","type":"content","url":"/seq2seq#seq2seq","position":3},{"hierarchy":{"lvl1":"Seq2seq","lvl2":"RNNによるseq2seq"},"type":"lvl2","url":"/seq2seq#rnn-seq2seq","position":4},{"hierarchy":{"lvl1":"Seq2seq","lvl2":"RNNによるseq2seq"},"content":"\n\nEncoderとDecoderにはRNNを使うことができます。\n\nEncoderはRNNを利用して、時系列データをhという隠れ状態ベクトルに変換します。つまり、Encoderの出力は、RNNレイヤの最後の隠れ状態です。\n\n情報の集約：最後の隠れ状態hに、入力シーケンスの全ての要素から抽出された情報を集約しています。これにより、タスク（ここは機械通訳）を完成するために必要な情報がエンコードされます。\n\n固定長の表現：RNNによって生成されるhは固定長のベクトルです。エンコードするとは、任意の長さの文章を固定長のベクトルに変換することなのです。h_t = EncoderRNN(e(x_t),h_{t-1})\n\nNote\n\n固定長のベクトルは、長いシーケンスの情報を効率的に圧縮することで、入力データの重要な特徴を抽出し、無関係または冗長な情報を無視することができます。\nまた、固定長の表現を使用することで、モデルは異なる長さの入力データを処理する際にも、一貫した方法で情報を処理・伝達することができます。\n\nDecoderもRNN利用して、Decoderの役割は、単純にvector-to-sequence RNNであり、Encoderから受け取った固定長の隠れ状態hを使用して、目的の出力シーケンスを生成することです。\n\n出力シーケンスの生成: Decoderは、Encoderから受け取った隠れ状態を初期状態として利用し、出力シーケンス（例えば翻訳されたテキスト）を一つずつ生成します。\n\ns_t = DecoderRNN(d(y_t),s_{t-1})\n\nただし、s_0=h_t\n\n特殊トークン(Special Tokens)\n\nここで、<eos>という区切り文字(特殊文字)が利用されています。<eos>を用いて、Decoderに文字生成の開始をお知らせる意図で利用します。一方、Decoderが<eos>を出力するまで単語のランプリングを行うようにするため利用されています。\n\nこのような特定の意味を持つために設計された特別な単語またはシンボルは、特殊トークンといいます。これらは通常、モデルがテキストデータを理解し、適切に処理するのを助けるために使用されます。\n\n<PAD>: パディングトークン。シーケンスの長さを揃えるために使用されます。一般的には、最も長いシーケンスに合わせて他のシーケンスをパディングします。\n\n<SOS> または <BOS>: シーケンスの開始トークン。シーケンスの開始を示します。これは、特にシーケンス生成タスク（例：機械翻訳）で重要です。\n\n<EOS> または <EOT>: シーケンスの終了トークン。シーケンスの終了を示します。これも、シーケンス生成タスクで重要です。\n\n<UNK>: 未知トークン。語彙に存在しない単語を表します。これは、モデルが未知の単語に遭遇したときに使用されます。\n\n<CLS>: 主に分類タスクで使用され、入力の最初に配置されます。このトークンの隠れ状態は、しばしば文全体の表現として使用されます。\n\nこれらの特殊トークンは、モデルがテキストの構造を理解するのを助け、特定のタスク（例えば、シーケンス生成やテキスト分類）を適切に実行するために必要です。\n\n一般的には、テキストデータを前処理し、トークン化する際にこれらの特殊トークンを適切に挿入または使用します。\n\nまとめると、RNNによるseq2seqは二つのRNN ー EncoderのRNNとDecoderのRNN　ー　によって構成されます。この時、RNNレイヤの隠れ状態がEncoderとDecoderの「架け橋」となります。\n\nseq2seqの革新的な点は、入力と出力系列の長さn_xとn_yについてn_x=n_yという制約に対して、seq2seqにおいてn_xとn_yが違っても構わないという点です。\n\n順伝播では、EncoderからDecoderへンコードされた情報がRNNレイヤの隠れ状態を通じて伝わります。そして、seq2seqの逆伝播では、その「架け橋」を通って、勾配がDecoderからEncoderへ伝わります。\n\n","type":"content","url":"/seq2seq#rnn-seq2seq","position":5},{"hierarchy":{"lvl1":"Seq2seq","lvl2":"seq2seqの汎用性"},"type":"lvl2","url":"/seq2seq#seq2seq-1","position":6},{"hierarchy":{"lvl1":"Seq2seq","lvl2":"seq2seqの汎用性"},"content":"seq2seqの構造によって、任意の系列データを扱うことが可能になります。系列にモデル化できるのであれば、文章、音声、ビデオなどを含めて、異なる種類のデータやタスクに適応することができます。\n\nチャットボット: ユーザーの質問やコメント（入力系列）に対して適切な応答（出力系列）を生成するために使用されます。\n\n音声認識: 音声の波形（入力系列）をテキストの形式（出力系列）に変換します。\n\n要約システム: 長いテキスト（入力系列）を短い要約文（出力系列）に変換するために使用されます。\n\n画像キャプション: 画像（入力系列）を解説するテキスト（出力系列）に変換します。\n\n","type":"content","url":"/seq2seq#seq2seq-1","position":7},{"hierarchy":{"lvl1":"Seq2seq","lvl2":"Pytorchによるseq2seqの実装"},"type":"lvl2","url":"/seq2seq#pytorch-seq2seq","position":8},{"hierarchy":{"lvl1":"Seq2seq","lvl2":"Pytorchによるseq2seqの実装"},"content":"\n\nimport torch\nimport torch.nn as nn\n\n\n\n","type":"content","url":"/seq2seq#pytorch-seq2seq","position":9},{"hierarchy":{"lvl1":"Seq2seq","lvl3":"Encoderの実装","lvl2":"Pytorchによるseq2seqの実装"},"type":"lvl3","url":"/seq2seq#encoder","position":10},{"hierarchy":{"lvl1":"Seq2seq","lvl3":"Encoderの実装","lvl2":"Pytorchによるseq2seqの実装"},"content":"\n\nseq2seqモデルのEncoder部分を定義します。\n\n初期化\n\nself.hidden_dimとself.n_layersは、LSTMレイヤーの隠れ層の次元数とレイヤー数を定義します。\n\nself.embeddingは、入力データを埋め込むための埋め込みレイヤーを定義します。このレイヤーは、入力データを低次元の密なベクトルに変換します。この埋め込みレイヤーは、入力次元数と埋め込み次元数を引数に取ります。\n\nself.rnnは、LSTMレイヤーを定義します。このレイヤーは、埋め込みレイヤーからの出力を受け取り、隠れ状態とセル状態を更新します。このLSTMレイヤーは、埋め込み次元数、隠れ層の次元数、レイヤー数、そしてドロップアウト率を引数に取ります。\n\nself.dropoutは、ドロップアウトレイヤーを定義します。このレイヤーは、過学習を防ぐために訓練中にランダムにノードを無効にします。\n\nforward\n\n入力テキスト(src)は埋め込みレイヤーself.embeddingを通過し、その結果得られる埋め込みはself.dropoutで調整されます。\n\nDropoutを適用した埋め込みがLSTMレイヤーを通過し、出力として隠れ状態とセル状態が生成されます。\n\nこのエンコーダークラスは、ソーステキストを入力として受け取り、それを一連の隠れ状態とセル状態に変換します。これらの状態は、次にデコーダーに渡され、ターゲットテキストの生成に使用されます。\n\nclass Encoder(nn.Module):\n    def __init__(self, input_dim, embedding_dim, hidden_dim, n_layers, dropout):\n        super().__init__()\n        self.hidden_dim = hidden_dim\n        self.n_layers = n_layers\n        self.embedding = nn.Embedding(input_dim, embedding_dim)\n        self.rnn = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=dropout)\n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, src):\n        embedded = self.dropout(self.embedding(src))\n        outputs, (hidden, cell) = self.rnn(embedded)\n        return hidden, cell\n\n\n\n","type":"content","url":"/seq2seq#encoder","position":11},{"hierarchy":{"lvl1":"Seq2seq","lvl3":"Decoderの実装","lvl2":"Pytorchによるseq2seqの実装"},"type":"lvl3","url":"/seq2seq#decoder","position":12},{"hierarchy":{"lvl1":"Seq2seq","lvl3":"Decoderの実装","lvl2":"Pytorchによるseq2seqの実装"},"content":"\n\nseq2seqモデルのDecoder部分を定義しています。\n\n初期化\n\nself.output_dimは、出力の次元数を保存します。これは通常、ターゲット語彙のサイズに対応します。\n\nself.hidden_dimとself.n_layersは、LSTMレイヤーの隠れ層の次元数とレイヤー数を保存します。\n\nself.embeddingは、入力データを埋め込むための埋め込みレイヤーを定義します。このレイヤーは、入力データを低次元の密なベクトルに変換します。この埋め込みレイヤーは、出力次元数と埋め込み次元数を引数に取ります。\n\nself.rnnは、LSTMレイヤーを定義します。このレイヤーは、埋め込みレイヤーからの出力を受け取り、隠れ状態とセル状態を更新します。このLSTMレイヤーは、埋め込み次元数、隠れ層の次元数、レイヤー数、そしてドロップアウト率を引数に取ります。\n\nself.dropoutは、ドロップアウトレイヤーを定義します。このレイヤーは、過学習を防ぐために訓練中にランダムにノードを無効にします。\n\nself.fc_outは、全結合レイヤーを定義します。このレイヤーは、LSTMレイヤーからの出力を受け取り、最終的な出力を生成します。この全結合レイヤーは、隠れ層の次元数と出力次元数を引数に取ります。\n\nforward：入力トークンのバッチ、前の隠れ状態、前のセル状態を受け入れます。\n\nDecoderは一度に一つのトークンしか処理しないため、入力トークンは、次元を追加するためにunsqueezeメソッドを使用して変形されます。これにより、入力の形状は[1, バッチサイズ]になります。\n\n変形した入力は、埋め込みレイヤーを通過し、その結果得られる埋め込みはドロップアウトレイヤーを通過します。\n\nドロップアウトを適用した埋め込みは、前の隠れ状態とセル状態と共に、LSTMレイヤーを通過し、新しい隠れ状態とセル状態を生成します。その中に、outputは各タイムステップの隠れ状態になります。\n\noutputを線形層に通してpredictionを受け取ります。\n\nこのデコーダークラスは、入力トークン、隠れ状態、およびセル状態を入力として受け取り、次のトークンの予測、新しい隠れ状態、および新しいセル状態を出力します。\n\nclass Decoder(nn.Module):\n    def __init__(self, output_dim, embedding_dim, hidden_dim, n_layers, dropout):\n        super().__init__()\n        self.output_dim = output_dim\n        self.hidden_dim = hidden_dim\n        self.n_layers = n_layers\n        self.embedding = nn.Embedding(output_dim, embedding_dim)\n        self.rnn = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=dropout)\n        self.fc_out = nn.Linear(hidden_dim, output_dim)\n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, input, hidden, cell):\n  \n        input = input.unsqueeze(0)\n        embedded = self.dropout(self.embedding(input))\n        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n   \n        prediction = self.fc_out(output.squeeze(0))\n        return prediction, hidden, cell\n\n\n\n","type":"content","url":"/seq2seq#decoder","position":13},{"hierarchy":{"lvl1":"Seq2seq","lvl3":"seq2seqの実装","lvl2":"Pytorchによるseq2seqの実装"},"type":"lvl3","url":"/seq2seq#seq2seq-2","position":14},{"hierarchy":{"lvl1":"Seq2seq","lvl3":"seq2seqの実装","lvl2":"Pytorchによるseq2seqの実装"},"content":"\n\nSeq2Seqモデルは、Encoder、Decoder、およびdevice（GPU上でテンソルを配置するために使用）を引数として取ります。\n\n入力（src）とターゲット（trg）のシーケンスを引数として受け取ります。\n\nsrc: 入力シーケンス。これはエンコーダに供給され、情報を抽出し、その情報をデコーダに渡すために使用されます。\n\ntrg: ターゲットシーケンス。これはデコーダに供給され、訓練中は正解の出力シーケンスとして使用されます。\n\nデコーダの出力を格納するためのテンソル（outputs）を作成します。このテンソルは、ターゲットシーケンスの長さ、バッチサイズ、およびターゲット語彙のサイズ（デコーダの出力次元）と同じ形状を持ちます。\n\nエンコーダは入力シーケンスを受け取り、最後の隠れ状態とセル状態を出力します。これらの状態は、デコーダの初期状態として使用されます。\n\nデコーダは、最初に<sos>（シーケンスの開始）トークンを入力として受け取ります(ここで、前処理の際既に<sos>トークンが追加されていると想定しています)。その後、ターゲットシーケンスの各トークンに対して、前の隠れ状態とセル状態を入力として受け取り、新しい出力（予測）、隠れ状態、およびセル状態を生成します。\n\n各タイムステップで、モデルは前のタイムステップで予測したトークンを次の入力として使用します。これは、モデルが実際にデプロイされている場合の動作により近いです。\n\n最後に、デコーダのすべての出力を含むテンソルが返されます。これは、ターゲットシーケンスの各トークンに対するモデルの予測を表します。\n\nclass Seq2Seq(nn.Module):\n    def __init__(self, encoder, decoder, device):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.device = device\n        assert encoder.hidden_dim == decoder.hidden_dim\n        assert encoder.n_layers == decoder.n_layers\n        \n    def forward(self, src, trg):\n\n        batch_size = trg.shape[1]\n        trg_length = trg.shape[0]\n        trg_vocab_size = self.decoder.output_dim\n        outputs = torch.zeros(trg_length, batch_size, trg_vocab_size).to(self.device)\n        hidden, cell = self.encoder(src)\n\n        input = trg[0,:]\n        for t in range(1, trg_length):\n            output, hidden, cell = self.decoder(input, hidden, cell)\n            outputs[t] = output\n            top1 = output.argmax(1)\n            input = top1\n        return outputs\n\n","type":"content","url":"/seq2seq#seq2seq-2","position":15},{"hierarchy":{"lvl1":"Transformerアーキテクチャ"},"type":"lvl1","url":"/transformer","position":0},{"hierarchy":{"lvl1":"Transformerアーキテクチャ"},"content":"OpenAIのGPTなど、現在主流となっている言語モデルには、基本的にTransformerというアーキテクチャが使われています。\n\nTransformerは、自然言語処理の分野で大きな進歩をもたらした重要な技術です。このアーキテクチャは、特に大量のテキストデータからパターンを学習し、文脈に基づいた情報を処理するのに非常に効果的です。\n\n(オリジナルの)Transformerはエンコーダ・デコーダーアーキテクチャをベースにしています。\n\nエンコーダ：入力されたトークン列を、埋め込みベクトル(隠れ状態)に変換します。\n\nデコーダ：エンコーダの隠れ状態を利用して、トークンの出力系列を生成します。\n\n","type":"content","url":"/transformer","position":1},{"hierarchy":{"lvl1":"Transformerアーキテクチャ","lvl2":"Transformerの構成要素"},"type":"lvl2","url":"/transformer#transformer","position":2},{"hierarchy":{"lvl1":"Transformerアーキテクチャ","lvl2":"Transformerの構成要素"},"content":"\n\n","type":"content","url":"/transformer#transformer","position":3},{"hierarchy":{"lvl1":"Transformerアーキテクチャ","lvl3":"Multi-Head Attention","lvl2":"Transformerの構成要素"},"type":"lvl3","url":"/transformer#multi-head-attention","position":4},{"hierarchy":{"lvl1":"Transformerアーキテクチャ","lvl3":"Multi-Head Attention","lvl2":"Transformerの構成要素"},"content":"Attentionは、各トークンに対して固定的な埋め込みを使う代わりに、系列全体を使って各埋め込みの加重平均を計算しています。\n\nつまり、トークン埋め込みの系列x_1,...,x_nが与えるとき、Self-Attentionは新しい埋め込みの系列x'_1,...x'_nを生成します。ここで、x'_iはすべでのx_jの線形結合になります。x'_i=\\sum_{j=1}^n w_{ji}x_j\n\n係数w_{ji}はAteention weightと呼ばれます。各要素をクエリ（Query）、キー（Key）、バリュー（Value）として表現し、これらを用いて他の要素との関連を計算することでAteention weightを求めます。\n\nAttention機構の表現力をさらに高めるために、Attention機構を同時に複数適用するのはMulti-Head Attentionになります。\n\n","type":"content","url":"/transformer#multi-head-attention","position":5},{"hierarchy":{"lvl1":"Transformerアーキテクチャ","lvl3":"位置埋め込み(Positional Encoding)","lvl2":"Transformerの構成要素"},"type":"lvl3","url":"/transformer#id-positional-encoding","position":6},{"hierarchy":{"lvl1":"Transformerアーキテクチャ","lvl3":"位置埋め込み(Positional Encoding)","lvl2":"Transformerの構成要素"},"content":"同じ単語でも、文中の位置によって意味が変わることがあります。位置埋め込み(Positional Encoding)は名前通り、入力トークンの順序や位置情報をモデルに提供する役割を果たします。位置埋め込みにより、モデルは各単語の文中での相対的な位置を理解し、より正確な文脈解析が可能になります。\n\nRNNようなモデルは、、入力データを順序付けて処理しますので、入力トークンの順序情報が自然に考慮されます。しかし、Transformerは入力を並列に処理し、基本的には単語の順序情報を無視します。そのため、文の意味を正確に理解するためには、単語の位置情報が別途必要となります。\n\n埋め込み層は各トークンに対して1つの位置埋め込みが追加され、トークンの相対位置が反映された新しい表現が生成されています。\n\nNote\n\n位置埋め込みを実現する方法はいくつかあります。\n\n絶対位置表現:事前に定義された関数を用いて、各位置に対してユニークな埋め込みを生成します。オリジナルのTransformerモデルは、変調されたサインとコサイン信号から静的なパターンを使用します。\n\n相対位置表現:Transformerベースの後続モデル（例えば、BERTやGPTなど）では、位置埋め込みがモデルのトレーニングプロセスの一部として学習され、より文脈に適応した表現を提供します。\n\n","type":"content","url":"/transformer#id-positional-encoding","position":7},{"hierarchy":{"lvl1":"Transformerアーキテクチャ","lvl3":"Add&Norm","lvl2":"Transformerの構成要素"},"type":"lvl3","url":"/transformer#add-norm","position":8},{"hierarchy":{"lvl1":"Transformerアーキテクチャ","lvl3":"Add&Norm","lvl2":"Transformerの構成要素"},"content":"Add&Normは、Self-AttentionやFeed-Forwardネットワーク出力に対して適用されます。具体的には、「残差結合（Residual Connection）」と「レイヤー正規化（Layer Normalization）」の組み合わせから成り立っています。\n\n残差結合: ある層の出力にその層の入力を加算し、最終的な出力にします。\n\nレイヤー正規化: 過剰に大きい値によって学習が不安定になることを防ぐために、残差接続の結果を正規化します。","type":"content","url":"/transformer#add-norm","position":9},{"hierarchy":{"lvl1":"Transformerアーキテクチャ","lvl3":"Feed-forward層","lvl2":"Transformerの構成要素"},"type":"lvl3","url":"/transformer#feed-forward","position":10},{"hierarchy":{"lvl1":"Transformerアーキテクチャ","lvl3":"Feed-forward層","lvl2":"Transformerの構成要素"},"content":"線形層と中間の非線形活性化関数で構成されています。ただ、注意してほしいのは、ここでは一連の埋め込み全体を一つのベクトルとして処理するのではなく、各埋め込みを独立に処理するように工夫しています。\n\nこのため、この層はposition-wise feed forward netwrokと呼ばれることもあります。z_i=W_2f(W_1 u_i+b_1)+b_2\n\nFeed-forward層は、文脈に関連する情報をその大規模なパラメータの中に記憶しており、入力された文脈に対して関連する情報を付加する役割を果たしてると考えられています。\n\n\n\n","type":"content","url":"/transformer#feed-forward","position":11},{"hierarchy":{"lvl1":"Transformerアーキテクチャ","lvl2":"エンコーダ・デコーダ"},"type":"lvl2","url":"/transformer#id","position":12},{"hierarchy":{"lvl1":"Transformerアーキテクチャ","lvl2":"エンコーダ・デコーダ"},"content":"","type":"content","url":"/transformer#id","position":13},{"hierarchy":{"lvl1":"Transformerアーキテクチャ","lvl3":"エンコーダ","lvl2":"エンコーダ・デコーダ"},"type":"lvl3","url":"/transformer#id-1","position":14},{"hierarchy":{"lvl1":"Transformerアーキテクチャ","lvl3":"エンコーダ","lvl2":"エンコーダ・デコーダ"},"content":"入力テキストをトークン化し、トークン埋め込むに変換します。特に、トークンの位置に関する情報を含む位置埋め込みも生成します。\n\n複数のエンコード層を積み重ねることでエンコーダを構成します。各エンコード層の出力埋め込みと入力とは同じサイズはずです。エンコード層の主な役割は、入力埋め込みを「更新」して、系列中の何らかの文脈情報をエンコード表現を生成することになります。\n\nMulti-Head Attention\n\n各入力埋め込みに適用される全結合の順伝播層\n\n\n\n","type":"content","url":"/transformer#id-1","position":15},{"hierarchy":{"lvl1":"Transformerアーキテクチャ","lvl3":"デコーダ","lvl2":"エンコーダ・デコーダ"},"type":"lvl3","url":"/transformer#id-2","position":16},{"hierarchy":{"lvl1":"Transformerアーキテクチャ","lvl3":"デコーダ","lvl2":"エンコーダ・デコーダ"},"content":"エンコーダの出力はデコーダに送られ、デコーダは系列中に最も可能性の高いトークンとして次のトークンを予測します。\n\n一般的には、デコーダは、<BOS>のようなテキストの先頭であることを表すトークンを入力として受け取り、文章の生成を開始し、最大長に達するまでか、系列の終了を表す<EOS>のような特別トークンに到達するまで続けられます(shifted right)。\n\nエンコーダも複数のエンコード層より構成されています。","type":"content","url":"/transformer#id-2","position":17},{"hierarchy":{"lvl1":"Transformerアーキテクチャ","lvl4":"Masked Multi-Head Attention","lvl3":"デコーダ","lvl2":"エンコーダ・デコーダ"},"type":"lvl4","url":"/transformer#masked-multi-head-attention","position":18},{"hierarchy":{"lvl1":"Transformerアーキテクチャ","lvl4":"Masked Multi-Head Attention","lvl3":"デコーダ","lvl2":"エンコーダ・デコーダ"},"content":"デコーダ側にMulti-Head Attentionには、マスク処理(masking)が行われます。\n\nエンコーダ・デコーダでは、エンコーダに入力されるトークン列u_1,u_2,...,u_Mからw_1,w_2,...w_iを順に予測されるように学習します。ここで、w_iまでを予測した状態のとき、学習はu_1,u_2,...,u_Mとw_1,w_2,...w_iからトークンw_{i+1}を予測できるようにモデルを更新していくことで行われます。\n\nしかし、Self-attentionはトークン全体から情報を取得するため、モデルはw_{i+1}を予測際はw_{i+1},...w_Nの情報も利用できることになります。\n\nマスク処理は、注意機構において位置iのトークンについて処理する際i+1以降のトークンのAttnetionスコアを-\\inftyに設定することで、各時点で生成するトークンが、過去の出力と現在予測されているトークンだけに基づいていることを保証します。\n\n\n\n","type":"content","url":"/transformer#masked-multi-head-attention","position":19},{"hierarchy":{"lvl1":"Transformerアーキテクチャ","lvl4":"交差注意機構(Cross Attention)","lvl3":"デコーダ","lvl2":"エンコーダ・デコーダ"},"type":"lvl4","url":"/transformer#id-cross-attention","position":20},{"hierarchy":{"lvl1":"Transformerアーキテクチャ","lvl4":"交差注意機構(Cross Attention)","lvl3":"デコーダ","lvl2":"エンコーダ・デコーダ"},"content":"デコーダには、交差注意機構が使われています。具体的には、注意機構において、queryはデコーダの埋め込み列、keyとvalueにはエンコーダの出力埋め込み列が使われます。\n\nこのようにして、デコーダが次のトークンを生成する際に、入力シーケンス全体を考慮できるようにします。\n\n\n\n","type":"content","url":"/transformer#id-cross-attention","position":21},{"hierarchy":{"lvl1":"Transformerアーキテクチャ","lvl3":"最終線形層とソフトマックス層","lvl2":"エンコーダ・デコーダ"},"type":"lvl3","url":"/transformer#id-3","position":22},{"hierarchy":{"lvl1":"Transformerアーキテクチャ","lvl3":"最終線形層とソフトマックス層","lvl2":"エンコーダ・デコーダ"},"content":"\n\nデコーダスタックで出力されるベクトルは最終線形層とソフトマックス層で単語に変換します。\n\n線形層は、完全結合型ニューラルネットワークで、デコーダのスタックが出力したベクトルをより大きなベクトル、すなわち「ロジットベクトル（logits vector）」に変換します。\n\n例えば、モデルが10,000語の英単語を学習していると仮定すると、このロジットベクトルは10,000個のセルから成り立っています。それぞれのセルは、モデルが予測する単語のスコアを表しています。この時点では、ロジットベクトル内の各セルは、単語に関連する「スコア」になります。\n\nソフトマックス層がこのロジットベクトルに適用されます。ソフトマックス層は、各スコアを確率に変換します。この確率はすべてが正の値となり、すべての確率の合計が1.0になります。これにより、各単語が選ばれる確率を得ることができます。最終的に、確率が最も高いセルに対応する単語が選ばれ、その単語が出力として生成されます。このようにして、モデルは次に出力すべき単語を予測することができます。\n\n\n\n","type":"content","url":"/transformer#id-3","position":23},{"hierarchy":{"lvl1":"Transformerアーキテクチャ","lvl2":"Transformerの学習"},"type":"lvl2","url":"/transformer#transformer-1","position":24},{"hierarchy":{"lvl1":"Transformerアーキテクチャ","lvl2":"Transformerの学習"},"content":"\n\n\n\nモデルから得られた単語の確率分布と「正解」の確率分布と比較することで、損失関数を算出します。損失関数の値が算出された後、バックプロパゲーションを使って損失を最小化する方向でモデルのパラメータを更新します。このプロセスを繰り返すことで、モデルは学習を進め、次第に予測精度が向上します。\n\n","type":"content","url":"/transformer#transformer-1","position":25},{"hierarchy":{"lvl1":"Transformerアーキテクチャ","lvl2":"まとめ"},"type":"lvl2","url":"/transformer#id-4","position":26},{"hierarchy":{"lvl1":"Transformerアーキテクチャ","lvl2":"まとめ"},"content":"Transformerは、主にエンコーダ、デコーダ、エンコーダ・デコーダーというアーキテクチャに大別されます。Transformerの成功は、Transformerベースの後続モデルの開発を引き起こしました。BERT、GPT、T5（Text-to-Text Transfer Transformer）などは代表的なバリエーションにであり、NLPの分野において画期的な進歩をもたらしました。\n\nTransformer Explainer\n\n","type":"content","url":"/transformer#id-4","position":27},{"hierarchy":{"lvl1":"単語分散表現"},"type":"lvl1","url":"/word2vec-1","position":0},{"hierarchy":{"lvl1":"単語分散表現"},"content":"\n\n単語分散表現とは、単語の意味を低次元の実数値ベクトルで表現することです。\n\n機械学習・深層学習モデルは、ベクトル（数値の配列）を入力として受け取ります。テキストを扱う際、最初に決めなければならないのは、文字列を機械学習モデルに入力する前に、数値に変換する（あるいはテキストを「ベクトル化」する）ための戦略です。\n\n単語の持つ性質や意味をよく反映するベクトル表現を獲得することは、機械学習・深層学習を自然言語処理で活用するために重要なプロセスです。\n\n類似性: ある概念を表現する際に、ほかの概念との共通点や類似性と紐づけながら、ベクトル空間上に表現します。\n\n単語類推: 分散表現では異なる概念を表現するベクトル同士での計算が可能です\n\n\n\n単語分散表現による単語の意味の類似性を表現することができます: 意味的に近い単語（例：cat と kitten）は近い位置に配置されます\n\n単語分散表現による単語類推を行うことができます: man と woman、king と queen のように意味的に対応関係を持つ単語はベクトル差が似た方向を持つため、アナロジー推論（king - man + woman ≈ queen）が可能になります\n\n\n\n単語分散表現による多次元ベクトル空間において意味や関係を捉えます:\n\n単語ベクトルへの変換には様々なアプローチが存在します。最初は、(1)統計情報から単語を表現する手法ーカウントベースの手法ーについて説明します。この方法は、言語のモデル化を理解することに役に立つと考られます。そして、(2)ニューラルネットワークを用いた手法(具体的には、word2vecと呼ばれる手法)を扱います。\n\nカウントベースの手法\n\n統計情報から単語を表現する手法\n\n推論ベースの手法\n\nニューラルネットワークを用いた手法\n\n","type":"content","url":"/word2vec-1","position":1},{"hierarchy":{"lvl1":"単語分散表現","lvl2":"シソーラスによる手法"},"type":"lvl2","url":"/word2vec-1#id","position":2},{"hierarchy":{"lvl1":"単語分散表現","lvl2":"シソーラスによる手法"},"content":"\n\n「単語の意味」を表すためには、人の手によって単語の意味を定義することが考えられます。\n\nシソーラス(thesaurus)と呼ばれるタイプの辞書は、単語間の関係を異表記・類義語・上位下位といった関係性を用いて、単語間の関連を定義できます。car = auto \\ automobile \\ machine \\ motorcar\n\nこの「単語ネットーワーク」を利用することで、コンピュータに単語間の関連性を伝えることができます。しかし、この手法には大きな欠点が存在します。\n\n人の作業コストが高い\n\n時代の変化に対応するのが困難\n\n言語は常に進化しており、新しい単語や意味が生まれては消えていくので、シソーラスを最新の状態に保つのは難しいです。\n\n単語の些細なかニュアンスを表現できない\n\n単語が持つ複数の意味を区別することは難しい\n\n単語間の関連性はシソーラスでは静的なものであり、動的な文脈や知識の流れを反映しきれないことがあります\n\n","type":"content","url":"/word2vec-1#id","position":3},{"hierarchy":{"lvl1":"単語分散表現","lvl2":"カウントベースの手法"},"type":"lvl2","url":"/word2vec-1#id-1","position":4},{"hierarchy":{"lvl1":"単語分散表現","lvl2":"カウントベースの手法"},"content":"\n\nコーパスには、自然言語に対する人の「知識」ー文章の書き方、単語の選び方、単語の意味ーがふんだんに含まれています。カウントベースの手法の目標は、そのような人の知識が詰まったコーパスから自動的に抽出することにあります。\n\n","type":"content","url":"/word2vec-1#id-1","position":5},{"hierarchy":{"lvl1":"単語分散表現","lvl3":"コーパスの前処理","lvl2":"カウントベースの手法"},"type":"lvl3","url":"/word2vec-1#id-2","position":6},{"hierarchy":{"lvl1":"単語分散表現","lvl3":"コーパスの前処理","lvl2":"カウントベースの手法"},"content":"コーパスに対して、テキストデータを単語に分割し、その分割した単語をID化にすることが必要されます。\n\n単語のID化とは、テキストデータを機械学習モデルなどで処理する際に、単語を一意の整数値（ID）に変換するプロセスを指します。これは、テキストデータをベクトルや行列の形でモデルに入力するための前処理として行われます。\n\n例として、簡単なテキストを用意します。\n\ntext = 'You say goodbye and I say hello.'\n\n\n\n# 小文字に変換\ntext = text.lower()\nprint(text)\n\n# ピリオドの前にスペースを挿入\ntext = text.replace('.', ' .')\nprint(text)\n\n# 単語ごとに分割\nwords = text.split(' ')\nprint(words)\n\n\n\nこれで、元の文章を単語リストとして利用できるようになりました。これに基づいて、分割した単語と、単語ごとに通し番号を割り振ったIDを2つのディクショナリに格納します。\n\n# ディクショナリを初期化\nword_to_id = {}\nid_to_word = {}\n\n# 未収録の単語をディクショナリに格納\nfor word in words:\n    if word not in word_to_id: # 未収録の単語のとき\n        # 次の単語のidを取得\n        new_id = len(word_to_id)\n        \n        # 単語IDを格納\n        word_to_id[word] = new_id\n        \n        # 単語を格納\n        id_to_word[new_id] = word\n\n\n\n# 単語IDを指定すると単語を返す\nprint(id_to_word)\nprint(id_to_word[5])\n\n# 単語を指定すると単語IDを返す\nprint(word_to_id)\nprint(word_to_id['hello'])\n\n\n\n最後に、単語リストから単語IDリストに変換します。\n\nimport numpy as np\n# リストに変換\ncorpus = [word_to_id[word] for word in words]\n\n# NumPy配列に変換\ncorpus = np.array(corpus)\nprint(corpus)\n\n\n\n以上の処理をpreprocess()という関数として、まとめて実装することにします。\n\n# 前処理関数の実装\ndef preprocess(text):\n    # 前処理\n    text = text.lower() # 小文字に変換\n    text = text.replace('.', ' .') # ピリオドの前にスペースを挿入\n    words = text.split(' ') # 単語ごとに分割\n    \n    # ディクショナリを初期化\n    word_to_id = {}\n    id_to_word = {}\n    \n    # 未収録の単語をディクショナリに格納\n    for word in words:\n        if word not in word_to_id: # 未収録の単語のとき\n            # 次の単語のidを取得\n            new_id = len(word_to_id)\n            \n            # 単語をキーとして単語IDを格納\n            word_to_id[word] = new_id\n            \n            # 単語IDをキーとして単語を格納\n            id_to_word[new_id] = word\n    \n    # 単語IDリストを作成\n    corpus = [word_to_id[w] for w in words]\n    \n    return corpus, word_to_id, id_to_word\n\n\n\n# テキストを設定\ntext = 'You say goodbye and I say hello.'\n\n# 単語と単語IDに関する変数を取得\ncorpus, word_to_id, id_to_word = preprocess(text)\nprint(id_to_word)\nprint(word_to_id)\nprint(corpus)\n\n\n\n","type":"content","url":"/word2vec-1#id-2","position":7},{"hierarchy":{"lvl1":"単語分散表現","lvl3":"分布仮説","lvl2":"カウントベースの手法"},"type":"lvl3","url":"/word2vec-1#id-3","position":8},{"hierarchy":{"lvl1":"単語分散表現","lvl3":"分布仮説","lvl2":"カウントベースの手法"},"content":"分布仮説（Distributional Hypothesis）は、言語学や自然言語処理の分野で重要な考え方で、単語の意味は、周囲の単語(コンテキスト)によって形成されるというものです。\n\n単語は、その単語が出現する文脈の集合によって意味が形成されるとされます。同じ文脈で出現する単語は、意味が似ていると考えられます。\n\n単語Aと単語Bが多くの共通の文脈で使用される場合、これらの単語は意味的に関連があると見なされます。\n\nこの仮説は、単語の意味を捉えるためのモデルを作成する際に基本的な原則となっています。\n\n周囲の単語による単語の意味を捉える？\n\n   \n\n","type":"content","url":"/word2vec-1#id-3","position":9},{"hierarchy":{"lvl1":"単語分散表現","lvl3":"共起行列","lvl2":"カウントベースの手法"},"type":"lvl3","url":"/word2vec-1#id-4","position":10},{"hierarchy":{"lvl1":"単語分散表現","lvl3":"共起行列","lvl2":"カウントベースの手法"},"content":"分布仮説に基づいた単語ベクトル化の方法を考える際、一番素直な方法は、周囲の単語を\"カウント\"することです。つまり、ある単語に着目した場合、その周囲どのような単語がどれだけ現れるのかをカウントし、それを集計するのです。\n\nここでは、\"You say goodbye and I say hello.\"という文章について、ウィンドウサイズを1とする場合、そのコンテキストに含まれる単語の頻度をカウントしてみます。\n\n\n\n各単語について、そのコンテキストに含まれす単語の頻度\n\n「You」の周辺単語は「say」のみであり、「say」にのみコンテキストの目印として共起した回数の1をカウントします\n\n「say」については文字列中に2回現れていることに注意すると、[1, 0, 1, 0, 1, 1, 0]とベクトル表記できます\n\n全ての単語に対して、共起する単語をまとめたものを共起行列と呼ばれます。\n\n# ウィンドウサイズを指定\nwndow_size = 1\n\n# 単語の種類数を取得\nvocab_size = len(word_to_id)\nprint(f\"単語の種類数: {vocab_size}\")\n\n# 総単語数を取得\ncorpus_size = len(corpus)\nprint(f\"総単語数: {corpus_size}\")\n\n\n\n# 共起行列を初期化\nco_matrix = np.zeros((vocab_size, vocab_size), dtype=np.int32)\nprint(co_matrix)\nprint(co_matrix.shape)\n\n\n\nコーパスの7語目の単語「hello」に注目してみます。\n\n# 単語インデックスを指定\nidx = 6\n\n# 指定した単語のIDを取得\nword_id = corpus[idx]\nprint(word_id)\nprint(id_to_word[word_id])\n\n\n\n# 左隣のインデックス\nleft_idx = idx - 1\nprint(left_idx)\n\n# 右隣のインデックス\nright_idx = idx + 1\nprint(right_idx)\n\n\n\n# 左隣の単語IDを取得\nleft_word_id = corpus[left_idx]\nprint(left_word_id)\nprint(id_to_word[left_word_id])\n\n# 共起行列に記録(加算)\nco_matrix[word_id, left_word_id] += 1\nprint(co_matrix)\n\n\n\n# 右隣の単語IDを取得\nright_word_id = corpus[right_idx]\nprint(right_word_id)\nprint(id_to_word[right_word_id])\n\n# 共起行列に記録(加算)\nco_matrix[word_id, right_word_id] += 1\nprint(co_matrix)\n\n# 対象の単語ベクトル\nprint(co_matrix[word_id])\n\n\n\n処理を共起行列を作成する関数として実装します。\n\n# 共起行列作成関数の実装\ndef create_co_matrix(corpus, vocab_size, window_size=1):\n    \n    # 総単語数を取得\n    corpus_size = len(corpus)\n    \n    # 共起行列を初期化\n    co_matrix = np.zeros((vocab_size, vocab_size), dtype=np.int32)\n    \n    # 1語ずつ処理\n    for idx, word_id in enumerate(corpus):\n        \n        # ウィンドウサイズまでの要素を順番に処理\n        for i in range(1, window_size + 1):\n            # 範囲内のインデックスを計算\n            left_idx = idx - i\n            right_idx = idx + i\n            \n            # 左側の単語の処理\n            if left_idx >= 0: # 対象の単語が最初の単語でないとき\n                # 単語IDを取得\n                left_word_id = corpus[left_idx]\n                \n                # 共起行列にカウント\n                co_matrix[word_id, left_word_id] += 1\n            \n            # 右側の単語の処理\n            if right_idx < corpus_size: # 対象の単語が最後の単語でないとき\n                # 単語IDを取得\n                right_word_id = corpus[right_idx]\n                \n                # 共起行列にカウント\n                co_matrix[word_id, right_word_id] += 1\n    \n    return co_matrix\n\n\n\n","type":"content","url":"/word2vec-1#id-4","position":11},{"hierarchy":{"lvl1":"単語分散表現","lvl3":"ベクトル間の類似度","lvl2":"カウントベースの手法"},"type":"lvl3","url":"/word2vec-1#id-5","position":12},{"hierarchy":{"lvl1":"単語分散表現","lvl3":"ベクトル間の類似度","lvl2":"カウントベースの手法"},"content":"","type":"content","url":"/word2vec-1#id-5","position":13},{"hierarchy":{"lvl1":"単語分散表現","lvl4":"コサイン類似度","lvl3":"ベクトル間の類似度","lvl2":"カウントベースの手法"},"type":"lvl4","url":"/word2vec-1#id-6","position":14},{"hierarchy":{"lvl1":"単語分散表現","lvl4":"コサイン類似度","lvl3":"ベクトル間の類似度","lvl2":"カウントベースの手法"},"content":"\n\n共起行列によって単語をベクトルで表すことができました。単語の意味を「計算」する手法として、ベクトル間の類似度の計測する方法について見ていきます。\n\n様々な方法がありますが、単語のベクトル表現の類似度に関して、コサイン類似度がよく用いられます。\n\nコサイン類似度とは、2つのベクトルを\\mathbf{x} = (x_1, x_2, \\cdots, x_n), \\mathbf{y} = (y_1, y_2, \\cdots, y_n)として、次の式で定義されます。\\begin{align}\n\\mathrm{similarity}(\\mathbf{x}, \\mathbf{y})\n   &= \\frac{\n          \\mathbf{x} \\cdot \\mathbf{y}\n      }{\n          \\|\\mathbf{x}\\| \\|\\mathbf{y}\\|\n      }\n\\\\\n   &= \\frac{\n          x_1 y_1 + x_2 y_2 + \\cdots + x_n y_n\n      }{\n          \\sqrt{x_1^2 + x_2^2 + \\cdots + x_n^2}\n          \\sqrt{y_1^2 + y_2^2 + \\cdots + y_n^2}\n      }\n\\\\\n   &= \\frac{\n          \\sum_{n} x_n y_n\n      }{\n          \\sqrt{\\sum_{n} x_n^2}\n          \\sqrt{\\sum_{n} y_n^2}\n      }\n\\end{align}\n\n分子はベクトルの内積\n\n分母は各ベクトルの「ノルム」(ベクトルの大きさ)があります。\n\nepsは、0除算とならないための微小な値です。通常、このような小さな値は浮動小数点の「丸の誤差」により、他の値に\"吸収\"されますので、最終の計算結果に影響を与えません。\n\n# コサイン類似度の実装\ndef cos_similarity(x, y, eps=1e-8):\n    # コサイン類似度を計算:式(2.1)\n    nx = x / (np.sqrt(np.sum(x**2)) + eps)\n    ny = y / (np.sqrt(np.sum(y**2)) + eps)\n    return np.dot(nx, ny)\n\n\n\n実装した関数を使って、ベクトルの値とコサイン類似度の値との関係を見ましょう。\n\na_vec = np.array([5.0, 5.0])\nb_vec = np.array([3.0, 9.0])\n\n\n\nimport matplotlib.pyplot as plt\n#import seaborn as sns\n\n# Set Seaborn theme\n#sns.set_context(\"paper\") # or \"talk\"\n#sns.set_style(\"whitegrid\")\n\n# コサイン類似度を計算\nsim_val = cos_similarity(a_vec, b_vec)\n\n# 作図\nplt.quiver(0, 0, a_vec[0], a_vec[1], angles='xy', scale_units='xy', scale=1, color='b', label='vector a') # 有効グラフ\nplt.quiver(0, 0, b_vec[0], b_vec[1], angles='xy', scale_units='xy', scale=1, color='r', label='vector b') # 有効グラフ\nplt.xlim(min(0, a_vec[0], b_vec[0]) - 1, max(0, a_vec[0], b_vec[0]) + 1)\nplt.ylim(min(0, a_vec[1], b_vec[1]) - 1, max(0, a_vec[1], b_vec[1]) + 1)\nplt.legend() \nplt.grid() \nplt.title('Similarity:' + str(np.round(sim_val, 3)), fontsize=20)\nplt.show()\n\n\n\n# Function to create a subplot for vectors with a given cosine similarity\ndef plot_vector_similarity(ax, similarity, vector_a):\n    # Generate vector b based on desired cosine similarity and vector a\n    angle = np.arccos(similarity)\n    vector_b = np.array([np.cos(angle), np.sin(angle)]) * np.linalg.norm(vector_a)\n    \n    # Plotting the vectors\n    ax.quiver(0, 0, vector_a[0], vector_a[1], angles='xy', scale_units='xy', scale=1, color='b', label='vector a')\n    ax.quiver(0, 0, vector_b[0], vector_b[1], angles='xy', scale_units='xy', scale=1, color='r', label='vector b')\n    \n    # Setting the limits of the plot\n    lim = np.max(np.abs(np.array([vector_a, vector_b]))) + 0.1\n    ax.set_xlim(-lim, lim)\n    ax.set_ylim(-lim, lim)\n    \n    # Adding grid, legend, and title to the subplot\n    ax.legend()\n    ax.grid(True)\n    ax.set_title('Similarity: ' + str(similarity))\n\n# Initial vector a\na_vec = np.array([1, 0])\n\n# Similarity values to plot\nsimilarities = [1, 0.8, 0.5, 0, -0.5, -1]\n\n# Create subplots\nfig, axes = plt.subplots(2, 3, figsize=(12, 8))\n\n# Flatten axes array for easy iteration\naxes_flat = axes.flatten()\n\n# Plot each similarity in a subplot\nfor ax, sim in zip(axes_flat, similarities):\n    plot_vector_similarity(ax, sim, a_vec)\n\nplt.tight_layout()\nplt.show()\n\n\n\n","type":"content","url":"/word2vec-1#id-6","position":15},{"hierarchy":{"lvl1":"単語分散表現","lvl4":"単語間の類似度","lvl3":"ベクトル間の類似度","lvl2":"カウントベースの手法"},"type":"lvl4","url":"/word2vec-1#id-7","position":16},{"hierarchy":{"lvl1":"単語分散表現","lvl4":"単語間の類似度","lvl3":"ベクトル間の類似度","lvl2":"カウントベースの手法"},"content":"前項で作成した各単語ベクトルを用いて、2つの単語の類似度を測りましょう。\n\n比較したい2つの単語を指定して、単語ベクトルからコサイン類似度を計算します。\n\n# テキストを設定\ntext = 'You say goodbye and I say hello.'\n\n# 単語と単語IDに関する変数を取得\ncorpus, word_to_id, id_to_word = preprocess(text)\n\n# 単語の種類数を取得\nvocab_size = len(word_to_id)\n\n# 共起行列を作成\nword_matrix = create_co_matrix(corpus, vocab_size, window_size=1)\n\n# 単語を指定して単語ベクトルを取得\nc0 = word_matrix[word_to_id['you']]\nc1 = word_matrix[word_to_id['i']]\nprint(c0)\nprint(c1)\n\n# コサイン類似度を計算\nsim_val = cos_similarity(c0, c1)\nprint(sim_val)\n\n\n\n","type":"content","url":"/word2vec-1#id-7","position":17},{"hierarchy":{"lvl1":"単語分散表現","lvl4":"類似度のランキングを表示","lvl3":"ベクトル間の類似度","lvl2":"カウントベースの手法"},"type":"lvl4","url":"/word2vec-1#id-8","position":18},{"hierarchy":{"lvl1":"単語分散表現","lvl4":"類似度のランキングを表示","lvl3":"ベクトル間の類似度","lvl2":"カウントベースの手法"},"content":"単語の「意味」を分析する際には、ある単語に対して類似した単語を探すことがよく挙げられます。ここでは、指定した単語との類似度が高い単語を調べる関数を実装します。\n\n# 対象とする単語を指定\nquery = 'you'\n\n# 指定した単語のIDを取得\nquery_id = word_to_id[query]\nprint(f\"指定した単語のID:{query_id}\")\n\n# 指定した単語のベクトルを取得\nquery_vec = word_matrix[query_id]\nprint(f\"指定した単語のベクトル: {query_vec}\")\n\n# コサイン類似度の記録リストを初期化\nvocab_size = len(id_to_word)\nsimilarity = np.zeros(vocab_size)\n\n# 各単語コサイン類似度を計算\nfor i in range(vocab_size):\n    similarity[i] = cos_similarity(word_matrix[i], query_vec)\n\n# 値を表示\nprint(\"類似度の結果：\")\nfor i,j in zip(list(word_to_id.keys()),np.round(similarity, 5)):\n    print(f\"{i}:{j}\")\n\n\n\nargsortメソッド\n\n.argsort()メソッドは、配列の要素の値が小さい順にインデックスを返します。ここで知りたいのは上位のインデックスのため、similarityに-1を掛けて符号を反転させることで、大小関係を逆転させます。\n\n# 配列を作成\narr = np.array([0, 20, 10, 40, 30])\nprint(arr)\n\n# 低い順のインデックス\nprint(arr.argsort())\n\n# 大小関係を逆転\nprint(-1 * arr)\n\n# 高い順のインデックス\nprint((-1 * arr).argsort())\n\n\n\n# 表示する順位を指定\ntop = 5\n\n# 類似度上位の単語と値を表示\ncount = 0 # 表示回数を初期化\nfor i in (-1 * similarity).argsort():\n    \n    # 指定した単語のときは次の単語に移る\n    if id_to_word[i] == query:\n        continue\n    \n    # 単語と値を表示\n    print(' %s: %s' % (id_to_word[i], similarity[i]))\n    \n    # 指定した回数に達したら処理を終了\n    count += 1 # 表示回数を加算\n    if count >= top:\n        break\n\n\n\n# 類似度の上位単語を検索関数の実装\ndef most_similar(query, word_to_id, id_to_word, word_matrix, top=5):\n    \n    # 指定した単語がテキストに含まれないとき\n    if query not in word_to_id:\n        print('%s is not found' % query)\n        return\n    \n    # 対象の単語を表示\n    print('\\n[query] ' + query)\n    \n    # 指定した単語のIDを取得\n    query_id = word_to_id[query]\n    \n    # 指定した単語のベクトルを取得\n    query_vec = word_matrix[query_id]\n    \n    # コサイン類似度を計算\n    vocab_size = len(id_to_word)\n    similarity = np.zeros(vocab_size)\n    for i in range(vocab_size):\n        similarity[i] = cos_similarity(word_matrix[i], query_vec)\n    \n    # 類似度上位の単語と値を表示\n    count = 0 # 表示回数を初期化\n    for i in (-1 * similarity).argsort():\n        \n        # 指定した単語のときは次の単語に移る\n        if id_to_word[i] == query:\n            continue\n        \n        # 単語と値を表示\n        print(' %s: %s' % (id_to_word[i], similarity[i]))\n        \n        # 指定した回数に達したら処理を終了\n        count += 1 # 表示回数を加算\n        if count >= top:\n            return\n\n\n\n# クエリを指定\nquery = 'you'\n\n# 類似の単語を表示\nmost_similar(query, word_to_id, id_to_word, word_matrix, top=5)\n\n\n\n","type":"content","url":"/word2vec-1#id-8","position":19},{"hierarchy":{"lvl1":"単語分散表現","lvl2":"カウントベースの手法の改善"},"type":"lvl2","url":"/word2vec-1#id-9","position":20},{"hierarchy":{"lvl1":"単語分散表現","lvl2":"カウントベースの手法の改善"},"content":"\n\n情報量の定義\n\n事象xの生起確率p(x)を基にして情報量I(x)はI(x) = - \\log_2 p(x)になります。0 \\leq p(x) \\leq 1なので、\\log p(x)は常に負の値になる。よって符号を反転した値を情報量とすることで、常に正の値をとるようにする。logの底が2なのは、情報学分野において0と1からなるbitとの相性からよく使われるためである。\n\n","type":"content","url":"/word2vec-1#id-9","position":21},{"hierarchy":{"lvl1":"単語分散表現","lvl3":"相互情報量","lvl2":"カウントベースの手法の改善"},"type":"lvl3","url":"/word2vec-1#id-10","position":22},{"hierarchy":{"lvl1":"単語分散表現","lvl3":"相互情報量","lvl2":"カウントベースの手法の改善"},"content":"ここまでは単語がテキストに出現する頻度そのままを扱いましたが、このやり方によって単語分散を計測する際、高頻度単語によるバイアスは生じる可能性があります。\n\n例えば、日本語において、「の」は非常に一般的な助詞で、さまざまな文脈で使用されます。単語の共起行列を作成するとき、「の」は非常に頻繁に出現し、多くの異なる単語とペアを形成するため、単純な共起頻度はその単語の意味のある関連性を捉えるのには不十分です。\n\nそのような問題を解決するために、相互情報量(Pointwise Mutual Information)と呼ばれる指標が使われます。\n\n2つの単語x,yの出現確率をそれぞれP(x),P(y)とします。xの出現確率はP(x)\n    = \\frac{C(x)}{N}\n\nで計算します。ここでC(x)は単語xが(テキストではなく)コーパス(共起行列)にカウントされた回数、Nはコーパスの総単語数とします。\n\nまた単語x,yが共起(ウィンドウサイズ内で続けて出現)した回数をC(x,y)とします。P(x, y)\n    = \\frac{C(x, y)}{N}\n\nこれを用いて、単語x,yの相互情報量(PMI)は次のように定義されます。\\mathrm{PMI}(x, y)\n    = \\log_2 \\frac{\n          P(x, y)\n      }{\n          P(x) P(y)\n      }\n\nまたこの式は、次のように変形することで\\begin{align}\n\\mathrm{PMI}(x, y)\n   &= \\log_2 \\left(\n          P(x, y)\n          \\frac{1}{P(x)}\n          \\frac{1}{P(y)}\n      \\right)\n\\\\\n   &= \\log_2 \\left( \n          \\frac{C(x, y)}{N}\n          \\frac{N}{C(x)}\n          \\frac{N}{C(y)}\n      \\right)\n\\\\\n   &= \\log_2 \\frac{\n          C(x, y) N\n      }{\n          C(x) C(y)\n      }\n\\end{align}\n\n出現回数と総単語数から直接で計算できることが分かります。\n\nただし出現回数が0のときに不都合が生じるため、次の正の相互情報量(PPMI)を用います。\\mathrm{PPMI}(x, y)\n    = \\max(0, \\mathrm{PMI}(x, y))\n\nfrom tqdm.notebook import tqdm\n# 相互情報量行列の作成関数の実装\ndef ppmi(C, verbose=False, eps=1e-8):\n    \n    # PPMI行列の受け皿を作成\n    M = np.zeros_like(C, dtype=np.float32)\n    \n    # PPMIに用いる値を計算\n    N = np.sum(C) # 総単語数\n    S = np.sum(C, axis=0) # 各単語の出現回数\n    \n    # 進行状況確認用の値を計算\n    total = C.shape[0] * C.shape[1]\n    cnt = 0 # 処理回数を初期化\n    \n    # 1語ずつ正の相互情報量を計算\n    for i in tqdm(range(C.shape[0])): # 各行\n        for j in range(C.shape[1]): # 各列\n            \n            # PPMIを計算\n            pmi = np.log2(C[i, j] * N / (S[j] * S[i]) + eps)\n            M[i, j] = max(0, pmi)\n    \n    return M\n\n\n\n# 共起行列を作成\nC = create_co_matrix(corpus, vocab_size, window_size=1)\nprint(C)\n\nW = ppmi(C, verbose=True)\n\n\n# 正の相互情報量行列\nprint(np.round(W, 2))\n\n\n\n\n\n\n\n","type":"content","url":"/word2vec-1#id-10","position":23},{"hierarchy":{"lvl1":"単語分散表現","lvl3":"次元削減","lvl2":"カウントベースの手法の改善"},"type":"lvl3","url":"/word2vec-1#id-11","position":24},{"hierarchy":{"lvl1":"単語分散表現","lvl3":"次元削減","lvl2":"カウントベースの手法の改善"},"content":"次元削減は、ベクトルの次元を重要な情報を保持しながら削減する手法を指します。\n\n","type":"content","url":"/word2vec-1#id-11","position":25},{"hierarchy":{"lvl1":"単語分散表現","lvl4":"SVD","lvl3":"次元削減","lvl2":"カウントベースの手法の改善"},"type":"lvl4","url":"/word2vec-1#svd","position":26},{"hierarchy":{"lvl1":"単語分散表現","lvl4":"SVD","lvl3":"次元削減","lvl2":"カウントベースの手法の改善"},"content":"次元削減を行う方法はいくつかあります。ここでは特異値分解(Singular Value Decomposition:SVD)を使って次元削減を行います。\n\nSVDは、任意の行列を3つの行列の積へ分解します。\n\nA = U\\Sigma V^T\n\nここで、Aはn \\times dの行列、Uはn \\times n、Vはd \\times dの直交行列です。\n\n直交行列\n\n直交行列とは、転置行列A^Tと逆行列A^{-1}が等しくなる行列。直交行列の列ベクトル（または行ベクトル）は互いに直交し、かつ正規化されています（つまり、それぞれのベクトルの長さが1）。これは、直交行列が基底変換を行う際に、ベクトルの長さや角度を保持することを意味します。そのため、直交行列は主にベクトル空間の基底を変換する際や、回転や反射などの幾何学的変換を表現するのに用いられます。\n\n\n\nUは何らかの空間の基底を形成しています。ここは、Uという行列を「単語空間」として扱うことができます。\n\n\\Sigmaは対角行列で、この対角成分には、「特異値」というものが大きい順で並んでいます。ここで、「特異値」は対応する軸の重要度とみなすことができます。次元削減は、この情報によって重要でない要素を削除することが考えれます。\n\nSVDによって、疎なベクトルが密なベクトルへ変換されています。この密なベクトルから、先頭の二つの要素を取り出すことで、元のベクトルの「情報」をできるだけ保持しながら次元を削減することができます。\n\nU, S, V = np.linalg.svd(W)\n\nprint(C[0]) # 共起行列\nprint(W[0]) # PPMI行列\nprint(U[0]) # SVD\n\n\n\n","type":"content","url":"/word2vec-1#svd","position":27},{"hierarchy":{"lvl1":"単語分散表現","lvl2":"カウントベースの手法による単語分散の作成"},"type":"lvl2","url":"/word2vec-1#id-12","position":28},{"hierarchy":{"lvl1":"単語分散表現","lvl2":"カウントベースの手法による単語分散の作成"},"content":"\n\nimport re\nimport pickle\n\nwith open(\"./Data/dokujo-tsushin.txt\", mode=\"r\",encoding=\"utf-8\") as f:\n    corpus = []\n    for line in f:\n        cleaned_line = line.replace('\\u3000', '').replace('\\n', '')\n        if cleaned_line!=\"\":\n            corpus.append(cleaned_line)\n\n\n\ncorpus[:5]\n\n\n\n#!pip install mecab-python3\n#!pip install unidic-lite\n\n\n\nimport MeCab\nfrom tqdm.notebook import tqdm\ndef tokenize_with_mecab(sentences):\n    # Initialize MeCab with the specified dictionary\n    corpus = []\n    for sentence in sentences:\n        sentence = re.sub(\"http://news.livedoor.com/article/detail/[0-9]{7}/\",\"\", sentence) # 注2）\n        sentence = re.sub(\"[0-9]{4}-[0-9]{2}-[0-9]{2}T[0-9]{2}:[0-9]{2}:[0-9]{2}\\+[0-9]{4}\",\"\", sentence) # 注3）\n        sentence = re.sub(\"[「」]\",\"\", sentence)\n        # Parse the sentence\n        node = mecab.parseToNode(sentence)\n        # Iterate over all nodes\n        while node:\n            # Extract the surface form of the word\n            word = node.surface\n            # Skip empty words and add to the corpus\n            if word:\n                corpus.append(word)\n            node = node.next\n    return corpus\n\n\n# Initialize the MeCab tokenizer\nmecab = MeCab.Tagger()\n#path = \"-d /opt/homebrew/lib/mecab/dic/mecab-ipadic-neologd\"\n#mecab = MeCab.Tagger(path)\ncorpus = tokenize_with_mecab(corpus)\n\n\n\ncorpus[:10]\n\n\n\nword_to_id = {}\nid_to_word = {}\n\nfor word in corpus:\n    if word not in word_to_id:\n        new_id = len(word_to_id)\n        word_to_id[word] = new_id\n        id_to_word[new_id] = word\n        \nprint('id_to_word[0]:', id_to_word[0])\nprint('id_to_word[1]:', id_to_word[1])\nprint('id_to_word[2]:', id_to_word[2])\nprint()\nprint(\"word_to_id['女']:\", word_to_id['女'])\nprint(\"word_to_id['結婚']:\", word_to_id['結婚'])\nprint(\"word_to_id['夫']:\", word_to_id['夫'])\n\n\n\n# リストに変換\ncorpus = [word_to_id[word] for word in corpus]\n\n# NumPy配列に変換\ncorpus = np.array(corpus)\n\n\n\nvocab_size=len(word_to_id)\nvocab_size\n\n\n\nwindow_size = 2\nwordvec_size = 100\nvocab_size = len(word_to_id)\n\n\n\nC=create_co_matrix(corpus,vocab_size,window_size=window_size)\n\n\n\n#W = ppmi(C)\n#np.save('./Data/W.npy', W)\nW = np.load('./Data/W.npy')\n\n\n\nfrom sklearn.utils.extmath import randomized_svd\nU, S, V= randomized_svd(W,n_components=wordvec_size,n_iter=5,random_state=None)\n\n\n\nword_vecs=U[:, :wordvec_size]\n\n\n\nquerys = ['女', '結婚', '彼女', \"秋\"]\n\nfor query in querys:\n    most_similar(query, word_to_id, id_to_word, word_vecs, top=5)\n\n\n\n","type":"content","url":"/word2vec-1#id-12","position":29},{"hierarchy":{"lvl1":"単語分散表現","lvl3":"おまけ：Pytorchでの実装","lvl2":"カウントベースの手法による単語分散の作成"},"type":"lvl3","url":"/word2vec-1#id-pytorch","position":30},{"hierarchy":{"lvl1":"単語分散表現","lvl3":"おまけ：Pytorchでの実装","lvl2":"カウントベースの手法による単語分散の作成"},"content":"\n\nimport torch\ntorch.backends.mps.is_available()\n\ndef ppmi_torch(C, eps=1e-8):\n    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n    C = torch.tensor(C, dtype=torch.float32, device=device)\n    \n    # 総単語数と各単語の出現頻度を計算\n    N = torch.sum(C)\n    S = torch.sum(C, dim=0, keepdim=True)  # 各単語の出現回数（列方向）\n    \n    # PPMIの計算（ループなし）\n    pmi = torch.log2(C * N / (S.t() * S) + eps)  # PMI行列を一括計算\n    M = torch.max(pmi, torch.tensor(0.0, device=device))  # PPMI行列を計算\n    \n    return M  # 必要であればCPUに戻す\n\n\n\nW = ppmi_torch(C)\n\n\n\nfrom sklearn.utils.extmath import randomized_svd\nU, S, V= randomized_svd(W.cpu().numpy(),n_components=wordvec_size,n_iter=5,random_state=None)\n\n\n\nword_vecs=U[:, :wordvec_size]\n\n\n\nquerys = ['女', '結婚', '彼女', \"秋\"]\n\nfor query in querys:\n    most_similar(query, word_to_id, id_to_word, word_vecs, top=5)\n\n","type":"content","url":"/word2vec-1#id-pytorch","position":31},{"hierarchy":{"lvl1":"word2vec"},"type":"lvl1","url":"/word2vec-2-embedding","position":0},{"hierarchy":{"lvl1":"word2vec"},"content":"\n\n前章では、「カウントベースの手法」によって単語分散表現を得ました。具体的には、単語の共起行列を作り、その行列に対してSVDを適用することで、密なベクトくー 単語分散表現ーを獲得したのです。\n\nしかし、カウントベースの手法にはいくつかの問題点があります。\n\n大規模なコーパスを扱う場合、巨大な共起行列に対してSVDを計算することが難しい。\n\nコーパスの全体から一回の学習で単語分散表現を獲得していますので、新しい単語が追加される場合、再度最初から学習を行う必要があり、単語分散表現更新の効率が低い。\n\n「カウントベースの手法」に代わる強力な手法として「推論ベース」の手法が挙げられます。特に、\n\nMikolov et al. (2013)によって提案されたword2vecの有用性が多くの自然言語処理タスクにおいて示されてきたのです。\n\n本章では、word2vecの仕組みについて説明し、それを実装することで理解を深めます。\n\n","type":"content","url":"/word2vec-2-embedding","position":1},{"hierarchy":{"lvl1":"word2vec","lvl2":"推論ベース手法とニューラルネットワーク"},"type":"lvl2","url":"/word2vec-2-embedding#id","position":2},{"hierarchy":{"lvl1":"word2vec","lvl2":"推論ベース手法とニューラルネットワーク"},"content":"推論ベースの手法は、ミニバッチで学習する形で、ニューラルネットワークを用いて、重みを繰り返し更新することで単語分散表現を獲得します。","type":"content","url":"/word2vec-2-embedding#id","position":3},{"hierarchy":{"lvl1":"word2vec","lvl3":"推論ベース手法の設計","lvl2":"推論ベース手法とニューラルネットワーク"},"type":"lvl3","url":"/word2vec-2-embedding#id-1","position":4},{"hierarchy":{"lvl1":"word2vec","lvl3":"推論ベース手法の設計","lvl2":"推論ベース手法とニューラルネットワーク"},"content":"推論ベース手法では、you 【？】 goodbye and I say hello .のような、周囲の単語が与えられたときに、【？】にどのような単語が出現するのかを推測する推論問題を繰り返し解くことで、単語の出現バターンを学習します。\n\nつまり、コンテキスト情報を入力として受け取り、各単語の出現する確率を出力する「モデル」を作成することは目標になります。ここで、正しい推測ができるように、コーパスを使って、ニューラルネットワークモデルの学習を行います。そして、その学習の結果として、単語の分散表現を得られます。\n\n推論タスク\n\n \n\n","type":"content","url":"/word2vec-2-embedding#id-1","position":5},{"hierarchy":{"lvl1":"word2vec","lvl3":"one-hot表現","lvl2":"推論ベース手法とニューラルネットワーク"},"type":"lvl3","url":"/word2vec-2-embedding#one-hot","position":6},{"hierarchy":{"lvl1":"word2vec","lvl3":"one-hot表現","lvl2":"推論ベース手法とニューラルネットワーク"},"content":"ニューラルネットワークで単語を処理するには、それを「固定長のベクトル」に変換する必要があります。\n\nそのための方法の一つは、単語をone-hot表現へと変換することです。one-hot表現とは、ベクトルの要素の中で一つだけが1で、残りは全て0であるようなベクトルと言います。\n\n単語をone-hot表現に変換するには、語彙数分の要素を持つベクトルを用意して、単語IDの該当する箇所を1に、残りは全て0に設定します。\n\n","type":"content","url":"/word2vec-2-embedding#one-hot","position":7},{"hierarchy":{"lvl1":"word2vec","lvl3":"CBOW（continuous bag-of-words）モデル","lvl2":"推論ベース手法とニューラルネットワーク"},"type":"lvl3","url":"/word2vec-2-embedding#cbow-continuous-bag-of-words","position":8},{"hierarchy":{"lvl1":"word2vec","lvl3":"CBOW（continuous bag-of-words）モデル","lvl2":"推論ベース手法とニューラルネットワーク"},"content":"CBOWモデルは、コンテキストからターゲットを推測することを目的としたニューラルネットワークです。このCBOWモデルで、できるだけ正確な推測ができるように訓練することで、単語の分散表現を取得することができます。\n\nここで、例として、コンテキスト[\"you\",\"goodbye\"]からターゲット\"say\"を予測するタスクを考えます。\n\n","type":"content","url":"/word2vec-2-embedding#cbow-continuous-bag-of-words","position":9},{"hierarchy":{"lvl1":"word2vec","lvl4":"入力層から中間層(エンコード)","lvl3":"CBOW（continuous bag-of-words）モデル","lvl2":"推論ベース手法とニューラルネットワーク"},"type":"lvl4","url":"/word2vec-2-embedding#id-2","position":10},{"hierarchy":{"lvl1":"word2vec","lvl4":"入力層から中間層(エンコード)","lvl3":"CBOW（continuous bag-of-words）モデル","lvl2":"推論ベース手法とニューラルネットワーク"},"content":"one-hotエンコーディングで、単語を固定長のベクトルに変換するすることができます。\n\n単語をベクトルで表すことができれば、そのベクトルはニューラルネットワークを構成する「レイヤ」によって処理することができるようになりました。\n\nコンテキストを\\mathbf{c}、重みを\\mathbf{W}とし、それぞれ次の形状とします。\n\n\\mathbf{c}\n    = \\begin{pmatrix}\n          c_{\\mathrm{you}} & c_{\\mathrm{say}} & c_{\\mathrm{goodbye}} & c_{\\mathrm{and}} & c_{\\mathrm{I}} & c_{\\mathrm{hello}} & c_{\\mathrm{period}}\n      \\end{pmatrix}\n,\\ \n\\mathbf{W}\n    = \\begin{pmatrix}\n          w_{\\mathrm{you},1} & w_{\\mathrm{you},2} & w_{\\mathrm{you},3} \\\\\n          w_{\\mathrm{say},1} & w_{\\mathrm{say},2} & w_{\\mathrm{say},3} \\\\\n          w_{\\mathrm{goodbye},1} & w_{\\mathrm{goodbye},2} & w_{\\mathrm{goodbye},3} \\\\\n          w_{\\mathrm{and},1} & w_{\\mathrm{and},2} & w_{\\mathrm{and},3} \\\\\n          w_{\\mathrm{I},1} & w_{\\mathrm{I},2} & w_{\\mathrm{I},3} \\\\\n          w_{\\mathrm{hello},1} & w_{\\mathrm{hello},2} & w_{\\mathrm{hello},3} \\\\\n          w_{\\mathrm{period},1} & w_{\\mathrm{period},2} & w_{\\mathrm{period},3} \\\\\n      \\end{pmatrix}\n\nコンテキストの要素数(列数)と重みの行数が、単語の種類数に対応します。\n\nコンテキスト(単語)はone-hot表現として扱うため、例えば「you」の場合は\\mathbf{c}_{\\mathrm{you}}\n    = \\begin{pmatrix}\n          1 & 0 & 0 & 0 & 0 & 0 & 0\n      \\end{pmatrix}\n\nとすることで、単語「you」を表現できます。\n\n重み付き和\\mathbf{h}は、行列の積で求められます。\\begin{aligned}\n\\mathbf{h}\n   &= \\mathbf{c}_{\\mathrm{you}}\n      \\mathbf{W}\n\\\\\n   &= \\begin{pmatrix}\n          h_1 & h_2 & h_3\n      \\end{pmatrix}\n\\end{aligned}\n\nh_1の計算を詳しく見ると、次のようになります。\\begin{aligned}\nh_1\n   &= c_{\\mathrm{you}} w_{\\mathrm{you},1}\n      + c_{\\mathrm{say}} w_{\\mathrm{say},1}\n      + c_{\\mathrm{goodbye}} w_{\\mathrm{goodbye},1}\n      + c_{\\mathrm{and}} w_{\\mathrm{and},1}\n      + c_{\\mathrm{I}} w_{\\mathrm{I},1}\n      + c_{\\mathrm{hello}} w_{\\mathrm{hello},1}\n      + c_{\\mathrm{period}} w_{\\mathrm{period},1}\n\\\\\n   &= 1 w_{\\mathrm{you},1}\n      + 0 w_{\\mathrm{say},1}\n      + 0 w_{\\mathrm{goodbye},1}\n      + 0 w_{\\mathrm{and},1}\n      + 0 w_{\\mathrm{I},1}\n      + 0 w_{\\mathrm{hello},1}\n      + 0 w_{\\mathrm{period},1}\n\\\\\n   &= w_{\\mathrm{you},1}\n\\end{aligned}\n\nコンテキストと重みの対応する(同じ単語に関する)要素を掛けて、全ての単語で和をとります。しかしコンテキストは、c_{you}以外の要素が0なので、対応する重みの値の影響は消えていまします。またc_{you}は1なので、対応する重みの値w_{\\mathrm{you},1}がそのまま中間層のニューロンに伝播します。\n\n残りの2つの要素も同様に計算できるので、重み付き和\\mathbf{h}\n    = \\begin{pmatrix}\n          w_{\\mathrm{you},1} & w_{\\mathrm{you},2} & w_{\\mathrm{you},3}\n      \\end{pmatrix}\n\nは、単語「you」に関する重みの値となります。\n\nimport numpy as np\n\n# 適当にコンテキスト(one-hot表現)を指定\nc = np.array([[1, 0, 0, 0, 0, 0, 0]])\nprint(f\"コンテキストの形状：{c.shape}\")\n\n# 重みをランダムに生成\nW = np.random.randn(7, 3)\nprint(f\"重み\\n{W}\")\n\n# 重み付き和を計算\nh = np.dot(c, W)\nprint(f\"重み付き和\\n{h}\")\nprint(f\"重み付き和の形状：{h.shape}\")\n\n\n\nコンテキストに複数な単語がある場合、入力層も複数になります。このとき、中間層にあるニューロンは、各入力層の全結合による変換後の値が平均されたものになります。\n\n中間層のニューロンの数を入力層よりも減らすことによって、中間層には、単語を予測するために必要な情報が\"コンパクト\"に収められて、結果としては密なベクトル表現が得られます。このとき、この中間層の情報は、人間には理解できない「ブラックボックス」ような状態になります。この作業は、「エンコード」と言います。\n\n","type":"content","url":"/word2vec-2-embedding#id-2","position":11},{"hierarchy":{"lvl1":"word2vec","lvl4":"中間層から出力層(デコード)","lvl3":"CBOW（continuous bag-of-words）モデル","lvl2":"推論ベース手法とニューラルネットワーク"},"type":"lvl4","url":"/word2vec-2-embedding#id-3","position":12},{"hierarchy":{"lvl1":"word2vec","lvl4":"中間層から出力層(デコード)","lvl3":"CBOW（continuous bag-of-words）モデル","lvl2":"推論ベース手法とニューラルネットワーク"},"content":"\n\n中間層の情報から目的の結果を得る作業は、「デコード」と言います。ここでは、中間層のニューロンの値\\mathbf{h}を各単語に対応した値になるように、つまり要素(行)数が単語の種類数となるように再度変換したものを、CBOWモデルの出力とします。\n\n出力層の重みを\\mathbf{W}_{\\mathrm{out}}\n    = \\begin{pmatrix}\n          w_{1,\\mathrm{you}} & w_{1,\\mathrm{say}} & w_{1,\\mathrm{goodbye}} & w_{1,\\mathrm{and}} &\n          w_{1,\\mathrm{I}} & w_{1,\\mathrm{hello}} & w_{1,\\mathrm{period}} \\\\\n          w_{2,\\mathrm{you}} & w_{2,\\mathrm{say}} & w_{2,\\mathrm{goodbye}} & w_{2,\\mathrm{and}} &\n          w_{2,\\mathrm{I}} & w_{2,\\mathrm{hello}} & w_{2,\\mathrm{period}} \\\\\n          w_{3,\\mathrm{you}} & w_{3,\\mathrm{say}} & w_{3,\\mathrm{goodbye}} & w_{3,\\mathrm{and}} &\n          w_{3,\\mathrm{I}} & w_{3,\\mathrm{hello}} & w_{3,\\mathrm{period}} \\\\\n      \\end{pmatrix}\n\nとします。行数が中間層のニューロン数、列数が単語の種類数になります。\n\n出力層も全結合層とすると、最終的な出力は\\begin{aligned}\n\\mathbf{s}\n   &= \\mathbf{h}\n      \\mathbf{W}_{\\mathrm{out}}\n\\\\\n   &= \\begin{pmatrix}\n          s_{\\mathrm{you}} & s_{\\mathrm{say}} & s_{\\mathrm{goodbye}} & s_{\\mathrm{and}} &\n          s_{\\mathrm{I}} & s_{\\mathrm{hello}} & s_{\\mathrm{period}}\n      \\end{pmatrix}\n\\end{aligned}\n\n例えば、「you」に関する要素の計算は、\\begin{aligned}\ns_{\\mathrm{you}}\n   &= \\frac{1}{2} (w_{\\mathrm{you},1} + w_{\\mathrm{goodbye},1}) w_{1,\\mathrm{you}}\n      + \\frac{1}{2} (w_{\\mathrm{you},2} + w_{\\mathrm{goodbye},2}) w_{2,\\mathrm{you}}\n      + \\frac{1}{2} (w_{\\mathrm{you},3} + w_{\\mathrm{goodbye},3}) w_{3,\\mathrm{you}}\n\\\\\n   &= \\frac{1}{2}\n      \\sum_{i=1}^3\n          (w_{\\mathrm{you},i} + w_{\\mathrm{goodbye},i}) w_{i,\\mathrm{you}}\n\\end{aligned}\n\nコンテキストに対応する入力層の重みの平均と「you」に関する出力の重みの積になります。\n\n他の要素(単語)についても同様に計算できるので、最終的な出力は\\begin{aligned}\n\\mathbf{s}\n   &= \\begin{pmatrix}\n          s_{\\mathrm{you}} & s_{\\mathrm{say}} & s_{\\mathrm{goodbye}} & s_{\\mathrm{and}} &\n          s_{\\mathrm{I}} & s_{\\mathrm{hello}} & s_{\\mathrm{period}}\n      \\end{pmatrix}\n\\\\\n   &= \\begin{pmatrix}\n          \\frac{1}{2} \\sum_{i=1}^3 (w_{\\mathrm{you},i} + w_{\\mathrm{goodbye},i}) w_{i,\\mathrm{you}} &\n          \\frac{1}{2} \\sum_{i=1}^3 (w_{\\mathrm{you},i} + w_{\\mathrm{goodbye},i}) w_{i,\\mathrm{say}} &\n          \\cdots &\n          \\frac{1}{2} \\sum_{i=1}^3 (w_{\\mathrm{you},i} + w_{\\mathrm{goodbye},i}) w_{i,\\mathrm{hello}} &\n          \\frac{1}{2} \\sum_{i=1}^3 (w_{\\mathrm{you},i} + w_{\\mathrm{goodbye},i}) w_{i,\\mathrm{period}}\n      \\end{pmatrix}\n\\end{aligned}\n\nとなります。\n\nここで、出力層のニューロンは各単語に対応し、各単語の「スコア」と言います。\n\n「スコア」の値が高ければ高いほど、それに対応する単語の出現確率も高くなり、ターゲットの単語であるとして採用します。そのため、スコアを求める処理を推論処理と言います。\n\nimport torch\nimport torch.nn as nn\nimport numpy as np\n\n# Define the context data\nc0 = torch.tensor([[1, 0, 0, 0, 0, 0, 0]], dtype=torch.float32) # you\nc1 = torch.tensor([[0, 0, 1, 0, 0, 0, 0]], dtype=torch.float32) # goodbye\n\n# Initialize weights randomly\nW_in = torch.randn(7, 3, requires_grad=False)  # Input layer weights\nW_out = torch.randn(3, 7, requires_grad=False) # Output layer weights\n\n# Define the layers using PyTorch's functional API\ndef in_layer(x, W):\n    return torch.matmul(x, W)\n\ndef out_layer(h, W):\n    return torch.matmul(h, W)\n\n# Forward pass through the input layers\nh0 = in_layer(c0, W_in) # you\nh1 = in_layer(c1, W_in) # goodbye\nh = 0.5 * (h0 + h1)\n\n# Forward pass through the output layer (scores)\ns = out_layer(h, W_out)\n\n# Print the outputs\nh0, h1, h, torch.round(s, decimals=3)\n\n\n\n\n\n正解は「say」として、Softmax関数によってスコアsを確率として扱えるように変換し、そして、正規化した値と教師ラベルを用いて損失を求めなさい。\n\n正解は「say」の場合、教師ラベルは[0, 1, 0, 0, 0, 0, 0]になります。\n\n","type":"content","url":"/word2vec-2-embedding#id-3","position":13},{"hierarchy":{"lvl1":"word2vec","lvl4":"word2vecの重みと分散表現","lvl3":"CBOW（continuous bag-of-words）モデル","lvl2":"推論ベース手法とニューラルネットワーク"},"type":"lvl4","url":"/word2vec-2-embedding#word2vec","position":14},{"hierarchy":{"lvl1":"word2vec","lvl4":"word2vecの重みと分散表現","lvl3":"CBOW（continuous bag-of-words）モデル","lvl2":"推論ベース手法とニューラルネットワーク"},"content":"\n\n与えられたコンテキストに対して単語を予測するときに、「良い重み」のネットワークがあれば、「確率」を表すニューロンにおいて、正解に対応するニューロンが高くなっていることが期待できます。そして、大規模コーパスを使って得られる単語の分散表現は、単語の意味や文法のルールにおいて、人間の直感と合致するケースが多く見られます。\n\nword2vecモデルの学習で行うことが、正しい予測ができるように重みを調整することです。つまり、「コンテキストから出現単語」を予測するという偽タスクをニューラルネットで解いてきましたが、目的はニューラルネットの重みを求めることになります。\n\nもっと具体的に言えば、word2vecで使用されるネットワークには二つの重みがあります。それは、入力層の重み\\mathbf{W_{in}}と、出力層の重み\\mathbf{W_{out}}です。それでは、どちらの重みを使えば良いでしょうか？\n\n入力側の重みを利用する\n\n出力側の重みを利用する\n\n二つの重みの両方を利用する\n\nWord2Vecモデルに関しては、多くの研究や応用例で、入力層の重みを単語のベクトル表現として使用さており、良好なパフォーマンスを示しています。\n\n","type":"content","url":"/word2vec-2-embedding#word2vec","position":15},{"hierarchy":{"lvl1":"word2vec","lvl2":"Word2Vecモデルの実装"},"type":"lvl2","url":"/word2vec-2-embedding#word2vec-1","position":16},{"hierarchy":{"lvl1":"word2vec","lvl2":"Word2Vecモデルの実装"},"content":"\n\n","type":"content","url":"/word2vec-2-embedding#word2vec-1","position":17},{"hierarchy":{"lvl1":"word2vec","lvl3":"学習データの準備","lvl2":"Word2Vecモデルの実装"},"type":"lvl3","url":"/word2vec-2-embedding#id-4","position":18},{"hierarchy":{"lvl1":"word2vec","lvl3":"学習データの準備","lvl2":"Word2Vecモデルの実装"},"content":"","type":"content","url":"/word2vec-2-embedding#id-4","position":19},{"hierarchy":{"lvl1":"word2vec","lvl4":"コンテキストとターゲット","lvl3":"学習データの準備","lvl2":"Word2Vecモデルの実装"},"type":"lvl4","url":"/word2vec-2-embedding#id-5","position":20},{"hierarchy":{"lvl1":"word2vec","lvl4":"コンテキストとターゲット","lvl3":"学習データの準備","lvl2":"Word2Vecモデルの実装"},"content":"Word2Vecモデルためのニューラルネットワークでは、「コンテキスト」を入力した時に、「ターゲット」が出現する確率を高くになるように学習を行います。\n\nそのため、コーパスから「コンテキスト」と「ターゲット」が対応するデータを作成する必要があります。\n\n# 前処理関数の実装\ndef preprocess(text):\n    # 前処理\n    text = text.lower() # 小文字に変換\n    text = text.replace('.', ' .') # ピリオドの前にスペースを挿入\n    words = text.split(' ') # 単語ごとに分割\n    \n    # ディクショナリを初期化\n    word_to_id = {}\n    id_to_word = {}\n    \n    # 未収録の単語をディクショナリに格納\n    for word in words:\n        if word not in word_to_id: # 未収録の単語のとき\n            # 次の単語のidを取得\n            new_id = len(word_to_id)\n            \n            # 単語をキーとして単語IDを格納\n            word_to_id[word] = new_id\n            \n            # 単語IDをキーとして単語を格納\n            id_to_word[new_id] = word\n    \n    # 単語IDリストを作成\n    corpus = [word_to_id[w] for w in words]\n    \n    return corpus, word_to_id, id_to_word\n\n\n\n# テキストを設定\ntext = 'You say goodbye and I say hello.'\n\n# 前処理\ncorpus, word_to_id, id_to_word = preprocess(text)\nprint(word_to_id)\nprint(id_to_word)\nprint(corpus)\n\n\n\nテキストの単語を単語IDに変換したcorpusからターゲットを抽出します。\n\nターゲットはコンテキストの中央の単語なので、corpusの始めと終わりのウインドウサイズ分の単語は含めません。\n\n# ウインドウサイズを指定\nwindow_size = 1\n\n# ターゲットを抽出\ntarget = corpus[window_size:-window_size]\nprint(target)\n\n\n\nターゲットの単語に対して、for文で前後ウィンドウサイズの範囲の単語を順番に抽出しcsに格納します。\n\nつまりウィンドウサイズを1とすると、corpusにおけるターゲットのインデックスidxに対して、1つ前(idx - window_size)から1つ後(idx + window_size)までの範囲の単語を順番にcs格納します。ただしターゲット自体の単語はコンテキストに含めません。\n\n# コンテキストを初期化(受け皿を作成)\ncontexts = []\n\n# 1つ目のターゲットのインデックス\nidx = window_size\n\n# 1つ目のターゲットのコンテキストを初期化(受け皿を作成)\ncs = []\n\n# 1つ目のターゲットのコンテキストを1単語ずつ格納\nfor t in range(-window_size, window_size + 1):\n    \n    # tがターゲットのインデックスのとき処理しない\n    if t == 0:\n        continue\n    \n    # コンテキストを格納\n    cs.append(corpus[idx + t])\n    print(cs)\n\n# 1つ目のターゲットのコンテキストを格納\ncontexts.append(cs)\nprint(contexts)\n\n\n\n# コンテキストとターゲットの作成関数の実装\ndef create_contexts_target(corpus, window_size=1):\n    \n    # ターゲットを抽出\n    target = corpus[window_size:-window_size]\n    \n    # コンテキストを初期化\n    contexts = []\n    \n    # ターゲットごとにコンテキストを格納\n    for idx in range(window_size, len(corpus) - window_size):\n        \n        # 現在のターゲットのコンテキストを初期化\n        cs = []\n        \n        # 現在のターゲットのコンテキストを1単語ずつ格納\n        for t in range(-window_size, window_size + 1):\n            \n            # 0番目の要素はターゲットそのものなので処理を省略\n            if t == 0:\n                continue\n            \n            # コンテキストを格納\n            cs.append(corpus[idx + t])\n            \n        # 現在のターゲットのコンテキストのセットを格納\n        contexts.append(cs)\n    \n    # NumPy配列に変換\n    return np.array(contexts), np.array(target) \n\n\n\n# コンテキストとターゲットを作成\ncontexts, targets = create_contexts_target(corpus, window_size=1)\nprint(contexts)\nprint(targets)\n\n\n\n","type":"content","url":"/word2vec-2-embedding#id-5","position":21},{"hierarchy":{"lvl1":"word2vec","lvl3":"PytorchでCBOWモデルの実装","lvl2":"Word2Vecモデルの実装"},"type":"lvl3","url":"/word2vec-2-embedding#pytorch-cbow","position":22},{"hierarchy":{"lvl1":"word2vec","lvl3":"PytorchでCBOWモデルの実装","lvl2":"Word2Vecモデルの実装"},"content":"\n\nimport numpy as np\nimport torch\nimport torch.optim as optim\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n\n\n","type":"content","url":"/word2vec-2-embedding#pytorch-cbow","position":23},{"hierarchy":{"lvl1":"word2vec","lvl4":"Embeddingレイヤ","lvl3":"PytorchでCBOWモデルの実装","lvl2":"Word2Vecモデルの実装"},"type":"lvl4","url":"/word2vec-2-embedding#embedding","position":24},{"hierarchy":{"lvl1":"word2vec","lvl4":"Embeddingレイヤ","lvl3":"PytorchでCBOWモデルの実装","lvl2":"Word2Vecモデルの実装"},"content":"\n\n先ほど、理解しやすいone-hot表現でコンテキストを変換する方法を説明しましたが、大規模なコーパスで学習する際、one-hot表現の次元数も大きくになって、非効率な学習の原因になります。\n\nただ、one-hot表現による計算は、単に行列の特定の行を抜き出すことだけですから、同じ機能を持つレイヤで入れ替えることは可能です。このような、重みパラメータから「単語IDに該当する行(ベクトル)」を抜き出すためのレイヤは「Embeddingレイヤ」と言います。\n\nPyTorchで提供されるモジュールnn.Embeddingを使うと、簡単にEmbeddingレイヤを実装することができます。\n\n例えば、語彙に6つの単語があり、各埋め込みベクトルの次元数を3に設定した場合、nn.Embeddingの定義は以下のようになります。\n\nそして、\n\nembedding_layer = nn.Embedding(6, 3)\n\n\n\nもしインデックス2のトークンの埋め込みを取得したい場合、次のようにします：\n\ninputs = torch.tensor([[1,2]], dtype=torch.long)\nembedding = embedding_layer(inputs)\nembedding\n\n\n\n埋め込みベクトルの和を取って、入力層から中間層までにエンコードの機能を実装できます。\n\nout=torch.sum(embedding, dim=1)\nout\n\n\n\nlinear1 = nn.Linear(3, 6)\n\n\n\nF.log_softmax(linear1(out), dim=1)\n\n\n\n","type":"content","url":"/word2vec-2-embedding#embedding","position":25},{"hierarchy":{"lvl1":"word2vec","lvl4":"ミニバッチ化データセットの作成","lvl3":"PytorchでCBOWモデルの実装","lvl2":"Word2Vecモデルの実装"},"type":"lvl4","url":"/word2vec-2-embedding#id-6","position":26},{"hierarchy":{"lvl1":"word2vec","lvl4":"ミニバッチ化データセットの作成","lvl3":"PytorchでCBOWモデルの実装","lvl2":"Word2Vecモデルの実装"},"content":"Word2Vecも含めて、深層学習によって学習を行う際には、ミニバッチ化して学習させることが一般的です。\n\npytorchで提供されているDataSetとDataLoaderという機能を用いてミニバッチ化を簡単に実現できます。","type":"content","url":"/word2vec-2-embedding#id-6","position":27},{"hierarchy":{"lvl1":"word2vec","lvl5":"DataSet","lvl4":"ミニバッチ化データセットの作成","lvl3":"PytorchでCBOWモデルの実装","lvl2":"Word2Vecモデルの実装"},"type":"lvl5","url":"/word2vec-2-embedding#dataset","position":28},{"hierarchy":{"lvl1":"word2vec","lvl5":"DataSet","lvl4":"ミニバッチ化データセットの作成","lvl3":"PytorchでCBOWモデルの実装","lvl2":"Word2Vecモデルの実装"},"content":"DataSetは，元々のデータを全て持っていて、ある番号を指定されると、その番号の入出力のペアをただ一つ返します。クラスを使って実装します。\n\nDataSetを実装する際には、クラスのメンバ関数として__len__()と__getitem__()を必ず作ります．\n\n__len__()は、len()を使ったときに呼ばれる関数です。\n\n__getitem__()は、array[i]のようにインデックスを使って要素を参照するときに呼ばれる関数です。\n\nclass CBOWDataset(Dataset):\n    def __init__(self, contexts, targets):\n        self.contexts = contexts\n        self.targets = targets\n    \n    def __len__(self):\n        return len(self.targets)\n    \n    def __getitem__(self, idx):\n        return self.contexts[idx], self.targets[idx]\n\n\n\n# Convert contexts and targets to tensors\ncontexts_tensor = torch.tensor(contexts, dtype=torch.long).to(device)\ntargets_tensor = torch.tensor(targets, dtype=torch.long).to(device)\n\n# Create the dataset\ndataset = CBOWDataset(contexts_tensor, targets_tensor)\n\n\n\nprint('全データ数:',len(dataset))\nprint('4番目のデータ:',dataset[3]) \nprint('4~5番目のデータ:',dataset[3:5])\n\n\n\n","type":"content","url":"/word2vec-2-embedding#dataset","position":29},{"hierarchy":{"lvl1":"word2vec","lvl5":"DataLoader","lvl4":"ミニバッチ化データセットの作成","lvl3":"PytorchでCBOWモデルの実装","lvl2":"Word2Vecモデルの実装"},"type":"lvl5","url":"/word2vec-2-embedding#dataloader","position":30},{"hierarchy":{"lvl1":"word2vec","lvl5":"DataLoader","lvl4":"ミニバッチ化データセットの作成","lvl3":"PytorchでCBOWモデルの実装","lvl2":"Word2Vecモデルの実装"},"content":"torch.utils.dataモジュールには、データのシャッフとミニバッチの整形に役立つDataLoaderというクラスが用意されます。\n\n# Create the DataLoader\nbatch_size = 2  # You can adjust the batch size\ndata_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n\n\n\nfor data in data_loader:\n    print(data)\n\n\n\n","type":"content","url":"/word2vec-2-embedding#dataloader","position":31},{"hierarchy":{"lvl1":"word2vec","lvl4":"CBOWモデルの構築","lvl3":"PytorchでCBOWモデルの実装","lvl2":"Word2Vecモデルの実装"},"type":"lvl4","url":"/word2vec-2-embedding#cbow","position":32},{"hierarchy":{"lvl1":"word2vec","lvl4":"CBOWモデルの構築","lvl3":"PytorchでCBOWモデルの実装","lvl2":"Word2Vecモデルの実装"},"content":"\n\nimport numpy as np\nimport torch\nimport torch.optim as optim\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\n\n\n\nself.embeddings = nn.Embedding(vocab_size, embedding_size): 語彙の各単語に対してembedding_size次元のベクトルを割り当てる埋め込み層を作成します。\n\nself.linear1 = nn.Linear(embedding_size, vocab_size): 埋め込みベクトルを受け取り、語彙のサイズに対応する出力を生成します。\n\nembeds = self.embeddings(inputs):入力された単語のインデックスに基づいて、埋め込み層から対応するベクトルを取得します。\n\nclass SimpleCBOW(nn.Module):\n    def __init__(self, vocab_size, embedding_size):\n        super(SimpleCBOW, self).__init__()\n        self.embeddings = nn.Embedding(vocab_size, embedding_size)\n        self.linear1 = nn.Linear(embedding_size, vocab_size)\n\n    def forward(self, inputs):\n        embeds = self.embeddings(inputs)\n        out = torch.sum(embeds, dim=1)\n        out = self.linear1(out)\n        log_probs = F.log_softmax(out, dim=1)\n        return log_probs\n\n\n\n# パラメータの設定\nembedding_size = 10\nlearning_rate = 0.01\nepochs = 100\nvocab_size = len(word_to_id)\n\n# モデルのインスタンス化\nmodel = SimpleCBOW(vocab_size, embedding_size).to(device)\nloss_function = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=learning_rate)\n\n\n# Training loop with batch processing\nfor epoch in range(epochs):\n    total_loss = 0\n    for i, (context_batch, target_batch) in enumerate(data_loader):\n        # Zero out the gradients from the last step\n        model.zero_grad()\n        # Forward pass through the model\n        log_probs = model(context_batch)\n        # Compute the loss\n        loss = loss_function(log_probs, target_batch)\n        # Backward pass to compute gradients\n        loss.backward()\n        # Update the model parameters\n        optimizer.step()\n        # Accumulate the loss\n        total_loss += loss.item()\n    # Log the total loss for the epoch\n    if epoch % 10 == 0:\n        print(f'Epoch {epoch}, Total loss: {total_loss}')\n\n\n\nnn.CrossEntropyLossの使い方\n\nnn.CrossEntropyLossターゲットラベルをクラスのインデックスとして受け取り、内部で必要な変換を行いますので、ターゲットをワンホットエンコーディングに変換する必要はありません。\n\nモデルの入力層の重みが単語分散表現であり、単語 \\times 埋め込み次元数の形の行列になります。\n\nmodel.embeddings.weight.shape\n\n\n\nword_embeddings = model.embeddings.weight.data\n\n# 各単語とそれに対応する分散表現を表示\nfor word, idx in word_to_id.items():\n    vector = word_embeddings[idx].cpu().numpy()\n    print(f\"Word: {word}\")\n    print(f\"Vector: {vector}\\n\")\n\n\n\n与えられたテキストを用いて、単語分散表現を学習しなさい。\n\nwindow_sizeを2に設定します\n\nbatch_sizeを10に設定します\n\n“When forty winters shall besiege thy brow,\nAnd dig deep trenches in thy beauty’s field,\nThy youth’s proud livery so gazed on now,\nWill be a totter’d weed of small worth held:\nThen being asked, where all thy beauty lies,\nWhere all the treasure of thy lusty days;\nTo say, within thine own deep sunken eyes,\nWere an all-eating shame, and thriftless praise.\nHow much more praise deserv’d thy beauty’s use,\nIf thou couldst answer ‘This fair child of mine\nShall sum my count, and make my old excuse,’\nProving his beauty by succession thine!\nThis were to be new made when thou art old,\nAnd see thy blood warm when thou feel’st it cold.”\n\n","type":"content","url":"/word2vec-2-embedding#cbow","position":33},{"hierarchy":{"lvl1":"word2vec","lvl2":"補足"},"type":"lvl2","url":"/word2vec-2-embedding#id-7","position":34},{"hierarchy":{"lvl1":"word2vec","lvl2":"補足"},"content":"","type":"content","url":"/word2vec-2-embedding#id-7","position":35},{"hierarchy":{"lvl1":"word2vec","lvl3":"学習データの作成","lvl2":"補足"},"type":"lvl3","url":"/word2vec-2-embedding#id-8","position":36},{"hierarchy":{"lvl1":"word2vec","lvl3":"学習データの作成","lvl2":"補足"},"content":" \n\n","type":"content","url":"/word2vec-2-embedding#id-8","position":37},{"hierarchy":{"lvl1":"word2vec","lvl3":"Negative Sampling","lvl2":"補足"},"type":"lvl3","url":"/word2vec-2-embedding#negative-sampling","position":38},{"hierarchy":{"lvl1":"word2vec","lvl3":"Negative Sampling","lvl2":"補足"},"content":"今まで紹介した学習の仕組みでは、正例(正しい答え)について学習を行いました。ここで、「良い重み」があれば、ターゲット単語についてSigmoidレイヤの出力は1に近づくことになります。\n\n　　 \n\nそれだけでなく、本当に行いたいのは、コンテキストが与えられるときに、間違った単語を予測してしまう確率も低いことが望まれます。ただ、全ての誤った単語(負例)を対象として、学習を行うことは非効率であるので、そこで、負例をいくつかピックアップします。これが「Negative Sampling」という手法の意味です。\n\n　　\n\nNegative Samplingでは、正例をターゲットとした場合の損失を求めると同時に、負例をいくつかサンプリングし、その負例に対しても同様に損失を求めます。そして、両者を足し合わせ、最終的な損失とします。このプロセスは、モデルが正しい単語を予測するだけでなく、不適切な単語を予測しない能力を同時に学習することを目的としています。\n\n　　\n\nそれでは、負例をどのようにサンプリングすべきですか？単純にランダムサンプリングの場合、高頻度の単語はサンプリングされやすく、低頻度の単語はサンプリングされにくく、珍しい単語や文脈に適切に対応できない原因になります。\n\nこの問題点を克服するために、word2vecで提案されるNegative Samplingでは、元となる確率分布に対して以下のように改装しましたP'(w_i)= \\frac{P(w_i)^{0.75}}{\\sum_{i=j}^n P(w_i)^{0.75}}\n\n0.75乗して調整することで、確率の低い単語に対してその確率を少しだけ高くすることができます。これにより、モデルはより現実的な言語パターンを学習し、実際の使用状況においてより正確な予測を行うことができます。\n\n","type":"content","url":"/word2vec-2-embedding#negative-sampling","position":39},{"hierarchy":{"lvl1":"word2vec","lvl3":"Skip-gram","lvl2":"補足"},"type":"lvl3","url":"/word2vec-2-embedding#skip-gram","position":40},{"hierarchy":{"lvl1":"word2vec","lvl3":"Skip-gram","lvl2":"補足"},"content":"これまで見てきたCBOWモデル以外、word2vecを学習する方法として、Skip-gramと呼ばれる言語モデルが提案されます。\n\nSkip-gramは、CBOWで扱うコンテキストとターゲットを逆転させて、ターゲットから、周囲の複数ある単語(コンテキスト)を推測します。\n\n実に、単語の分散表現の精度の点において、多くの場合、Skip-gramモデルの方が良い結果が得られています。特に、コーパスが大規模になるにつれて、低頻出の単語や類推問題の性能の点において、Skip-gramモデルの方が優れている傾向にあります。\n\n　　","type":"content","url":"/word2vec-2-embedding#skip-gram","position":41},{"hierarchy":{"lvl1":"Word2Vecが人文・社会科学研究における応用"},"type":"lvl1","url":"/word2vec-application","position":0},{"hierarchy":{"lvl1":"Word2Vecが人文・社会科学研究における応用"},"content":"\n\n","type":"content","url":"/word2vec-application","position":1},{"hierarchy":{"lvl1":"Word2Vecが人文・社会科学研究における応用","lvl2":"The Geometry of Culture: Analyzing the Meanings of Class through Word Embeddings  Kozlowski et al., 2019"},"type":"lvl2","url":"/word2vec-application#the-geometry-of-culture-analyzing-the-meanings-of-class-through-word-embeddings","position":2},{"hierarchy":{"lvl1":"Word2Vecが人文・社会科学研究における応用","lvl2":"The Geometry of Culture: Analyzing the Meanings of Class through Word Embeddings  Kozlowski et al., 2019"},"content":"\n\n","type":"content","url":"/word2vec-application#the-geometry-of-culture-analyzing-the-meanings-of-class-through-word-embeddings","position":3},{"hierarchy":{"lvl1":"Word2Vecが人文・社会科学研究における応用","lvl3":"問題関心","lvl2":"The Geometry of Culture: Analyzing the Meanings of Class through Word Embeddings  Kozlowski et al., 2019"},"type":"lvl3","url":"/word2vec-application#id","position":4},{"hierarchy":{"lvl1":"Word2Vecが人文・社会科学研究における応用","lvl3":"問題関心","lvl2":"The Geometry of Culture: Analyzing the Meanings of Class through Word Embeddings  Kozlowski et al., 2019"},"content":"「階層」という概念は多次元な側面より構成されています\n\nAffluence\n\nOccupation\n\nSymbolic Manifestations(social honor and prestige)\n\nGender\n\n「階層」という概念は時間とともに変化しています\n\n","type":"content","url":"/word2vec-application#id","position":5},{"hierarchy":{"lvl1":"Word2Vecが人文・社会科学研究における応用","lvl3":"方法","lvl2":"The Geometry of Culture: Analyzing the Meanings of Class through Word Embeddings  Kozlowski et al., 2019"},"type":"lvl3","url":"/word2vec-application#id-1","position":6},{"hierarchy":{"lvl1":"Word2Vecが人文・社会科学研究における応用","lvl3":"方法","lvl2":"The Geometry of Culture: Analyzing the Meanings of Class through Word Embeddings  Kozlowski et al., 2019"},"content":"","type":"content","url":"/word2vec-application#id-1","position":7},{"hierarchy":{"lvl1":"Word2Vecが人文・社会科学研究における応用","lvl4":"Cultural Dimensionsの測定","lvl3":"方法","lvl2":"The Geometry of Culture: Analyzing the Meanings of Class through Word Embeddings  Kozlowski et al., 2019"},"type":"lvl4","url":"/word2vec-application#cultural-dimensions","position":8},{"hierarchy":{"lvl1":"Word2Vecが人文・社会科学研究における応用","lvl4":"Cultural Dimensionsの測定","lvl3":"方法","lvl2":"The Geometry of Culture: Analyzing the Meanings of Class through Word Embeddings  Kozlowski et al., 2019"},"content":"Cultural Dimensionsの測定は、単語分散表現を用いて類推問題を解決する能力を活用しています。\n\n文化的な概念を反映する単語のペア間のベクトル計算でCultural Dimensionsを測定することが可能です(A)。\n\n\\vec{male}-\\vec{female}はGenderという概念反映しています。同じロジックで、\\vec{king}-\\vec{queen}のような単語のペアもGenderという概念を反映できると考えられます。\n\n\\vec{rich}-\\vec{poor}、\\vec{affluence}-\\vec{poverty}のような単語のペアはAffluenceという概念を反映できると考えられます。\n\nimport gensim.downloader\nmodel = gensim.downloader.load('word2vec-google-news-300')\n\n\n\nrich_list=[\"rich\",\"richer\",\"affluence\",\"luxury\"]\npoor_list=[\"poor\",\"poorer\",\"poverty\",\"cheap\"]\n\n\n\nimport numpy as np\naffluence_vec=[]\nfor i,j in zip(rich_list,poor_list):\n    affluence_vec.append(model[i]-model[j])\naffluence_vec=np.array(affluence_vec)\naffluence_vec=np.mean(affluence_vec,axis=0)\n\n\n\nなぜ複数の単語ペアで計算する必要がありますか？\n\n単語ペアでベクトル減法を行なって、結果の平均を取る方法以外、Dimensionsを測定する方法がありますか？\n\n","type":"content","url":"/word2vec-application#cultural-dimensions","position":9},{"hierarchy":{"lvl1":"Word2Vecが人文・社会科学研究における応用","lvl4":"Cultural Dimensionsで概念の「理解」","lvl3":"方法","lvl2":"The Geometry of Culture: Analyzing the Meanings of Class through Word Embeddings  Kozlowski et al., 2019"},"type":"lvl4","url":"/word2vec-application#cultural-dimensions-1","position":10},{"hierarchy":{"lvl1":"Word2Vecが人文・社会科学研究における応用","lvl4":"Cultural Dimensionsで概念の「理解」","lvl3":"方法","lvl2":"The Geometry of Culture: Analyzing the Meanings of Class through Word Embeddings  Kozlowski et al., 2019"},"content":"ある単語を「Cultural Dimensions」でどのように解釈されるかを、その単語のベクトルと文化的次元ベクトルとの間の角度を計算することで求めるのです。(B)\n\nこの角度が小さいほど、その単語はその文化的次元に強く関連していると言えます。この方法により、単語が持つ文化的な意味合いやニュアンスを数値的に分析することが可能になります。\n\ncos(\\theta))=\\frac{D \\cdot V}{|D||V|}\\theta = \\arccos(cos(\\theta))\n\ndef get_consine(vector, dimension):\n    \"\"\"\n    Calculate the angle between the vector and the given dimension\n    \"\"\"\n    v_dot_d = np.dot(vector, dimension)\n    v_d = np.linalg.norm(vector) * np.linalg.norm(dimension)\n    return v_dot_d / v_d\n\n\n\nget_consine(model[\"tennis\"],affluence_vec)\n\n\n\nfrom sklearn.metrics.pairwise import cosine_similarity\ncosine_similarity(model[\"tennis\"].reshape(1,-1),affluence_vec.reshape(1,-1))\n\n\n\ndef get_angle(vector, dimension,degree=False):\n    \"\"\"\n    Calculate the angle between the vector and the given dimension\n    \"\"\"\n    c = get_consine(vector, dimension)\n    if degree:\n        return np.degrees(np.arccos(np.clip(c, -1, 1)))\n    else:\n        return np.arccos(np.clip(c, -1, 1)) #return radian\n\n\n\nsports=[\"tennis\",\"soccer\",\"basketball\",\"boxing\",\"golf\",\"swimming\",\"volleyball\",\"camping\",\"weightlifting\",\"hiking\",\"hockey\"]\n\n\n\nfor sport in sports:\n    print(sport,get_angle(model[sport],affluence_vec,degree=True))\n\n\n\nこの結果をどのように解釈すべきですか？\n\n性別に関するDimensionを作成しなさい\n\n性別Dimensionで運動の位置付けを確認し、その結果を解釈しなさい\n\n","type":"content","url":"/word2vec-application#cultural-dimensions-1","position":11},{"hierarchy":{"lvl1":"Word2Vecが人文・社会科学研究における応用","lvl4":"Class意味の推移","lvl3":"方法","lvl2":"The Geometry of Culture: Analyzing the Meanings of Class through Word Embeddings  Kozlowski et al., 2019"},"type":"lvl4","url":"/word2vec-application#class","position":12},{"hierarchy":{"lvl1":"Word2Vecが人文・社会科学研究における応用","lvl4":"Class意味の推移","lvl3":"方法","lvl2":"The Geometry of Culture: Analyzing the Meanings of Class through Word Embeddings  Kozlowski et al., 2019"},"content":"階層という概念は多次元な側面より構成されて、さらにその構成は時間とともに変化している。\n\nCultural Dimensionsを構築することで、「階層」の各構成の「意味」を定量的に測定する\n\n「Affluence」が他の要素とどのように関係していることは、階層の意味構成を説明している\n\ndef create_vector(word_pair):\n    vec=[]\n    for i in word_pair:\n        vec.append(model[i[0]]-model[i[1]])\n    vec=np.array(vec)\n    vec=np.mean(vec,axis=0)\n    return vec\n\n\n\neducation_pair=[(\"educated\",\"uneducated\"),(\"learned\",\"unlearned\"),(\"taught\",\"untaught\"),\n                (\"schooled\",\"unschooled\"),(\"trained\",\"untrained\"),(\"lettered\",\"unlettered\"),\n                (\"tutored\",\"untutored\"),(\"literate\",\"illiterate\")]\n\n\n\neducation_vec=create_vector(education_pair)\n\n\n\ngender_pair=[(\"man\",\"woman\"),(\"men\",\"women\"),(\"he\",\"she\"),(\"him\",\"her\"),\n             (\"his\",\"her\"),(\"boy\",\"girl\"),(\"male\",\"female\"),(\"masculine\",\"feminine\")]\n\n\n\ngender_vec=create_vector(gender_pair)\n\n\n\ncosine_similarity(gender_vec.reshape(1,-1),affluence_vec.reshape(1,-1))\n\n\n\ncosine_similarity(education_vec.reshape(1,-1),affluence_vec.reshape(1,-1))\n\n\n\nこの結果をどのように解釈すべきですか？\n\n言語モデルは学習コーパスに含まれている「バイアス」をそのまま反映しています。例えば、言語モデルでは、エンジニア、トラック運転手は男性、モデル、看護師は女性というような、職業と性別の関係についての典型的なステレオタイプを学習していることがわかります。\n\n同様に、異なる時期のコーパスには、特定な時期の考え方や認識に関する情報が含められますので、そのコーパスで学習した言語モデルも特定な時期の考え方や認識を反映できると考えられます。\n\nこの図の結果をどのように解釈すべきですか？\n\n","type":"content","url":"/word2vec-application#class","position":13},{"hierarchy":{"lvl1":"Word2Vecが人文・社会科学研究における応用","lvl2":"Word2Vecを応用する研究"},"type":"lvl2","url":"/word2vec-application#word2vec","position":14},{"hierarchy":{"lvl1":"Word2Vecが人文・社会科学研究における応用","lvl2":"Word2Vecを応用する研究"},"content":"","type":"content","url":"/word2vec-application#word2vec","position":15},{"hierarchy":{"lvl1":"Word2Vecが人文・社会科学研究における応用","lvl3":"Semantic projection recovers rich human knowledge of multiple object features from word embeddings Grand et al., 2022","lvl2":"Word2Vecを応用する研究"},"type":"lvl3","url":"/word2vec-application#semantic-projection-recovers-rich-human-knowledge-of-multiple-object-features-from-word-embeddings","position":16},{"hierarchy":{"lvl1":"Word2Vecが人文・社会科学研究における応用","lvl3":"Semantic projection recovers rich human knowledge of multiple object features from word embeddings Grand et al., 2022","lvl2":"Word2Vecを応用する研究"},"content":"単語分散表現は表出する意味的特徴が人間の評価を近似することができることを体系的に説明しました。\n\n","type":"content","url":"/word2vec-application#semantic-projection-recovers-rich-human-knowledge-of-multiple-object-features-from-word-embeddings","position":17},{"hierarchy":{"lvl1":"Word2Vecが人文・社会科学研究における応用","lvl3":"Word embeddings quantify 100 years of gender and ethnic stereotypes Garg et al., 2018","lvl2":"Word2Vecを応用する研究"},"type":"lvl3","url":"/word2vec-application#word-embeddings-quantify-100-years-of-gender-and-ethnic-stereotypes","position":18},{"hierarchy":{"lvl1":"Word2Vecが人文・社会科学研究における応用","lvl3":"Word embeddings quantify 100 years of gender and ethnic stereotypes Garg et al., 2018","lvl2":"Word2Vecを応用する研究"},"content":"単語分散表現が、性別ステレオタイプと民族マイノリティに対する態度の変化を定量化するのにどのように役立つかを示しています。\n\n性別と民族に関する単語で、形容詞や職業などの中立的な単語(neutral words )と比較する手法\n\n性別(男性、女性)と職業に関連する単語リストをまとめます\n\n女性を代表する単語（例：she, female）と職業の単語（例：teacher, lawyer）との間の平均埋め込み距離を計算します\n\n男性を代表する単語と同じ職業の単語との平均埋め込み距離も計算します\n\n女性の平均距離から男性の平均距離を引く結果は「性別バイアス」と考えます。\\to 値がマイナスの場合、該当する職業は男性とより密接に関連付けていることを意味しています\\text{relative norm distance} = \\sum_{v_m \\in M} \\left( \\|v_m - v_1\\|_2 - \\|v_m - v_2\\|_2 \\right).\n\n\n\n「engineer」、「nurse」、「housekeeper」のGender Biasを計算しなさい\n\nNorm distanceはNumpyを使って実装できます\n\nEthnic Biasはどのように計算すべきのかを考えなさい。Ethnicのグループでは2以上であることを注意してください。","type":"content","url":"/word2vec-application#word-embeddings-quantify-100-years-of-gender-and-ethnic-stereotypes","position":19},{"hierarchy":{"lvl1":"GensimによるWord2Vecの学習と使用"},"type":"lvl1","url":"/word2vec-gensim","position":0},{"hierarchy":{"lvl1":"GensimによるWord2Vecの学習と使用"},"content":"\n\n前章でCBOWモデルを実装することでword2vecの仕組みを学びました。実際に、その以外、word2vecの関して様々な取り組みがあります。\n\nSkip-gramモデルでは、ターゲットからコンテキストを推測するタスクを構築しています\n\nNegative Samplingという新しい損失関数を導入することで学習の高速化を図る\n\n応用の視点から、これらの手法をすべでゼロから実装することが難しいので、\n\ngensimというライブラリを使って、Word2Vecモデルを学習、管理、使用することは、多くの自然言語処理タスクにおいて効果的な選択肢となります。\n\n#!pip install --upgrade gensim\n#!pip install mecab-python3\n#!pip install unidic-lite\n\n\n\n","type":"content","url":"/word2vec-gensim","position":1},{"hierarchy":{"lvl1":"GensimによるWord2Vecの学習と使用","lvl2":"Gensimの使い方"},"type":"lvl2","url":"/word2vec-gensim#gensim","position":2},{"hierarchy":{"lvl1":"GensimによるWord2Vecの学習と使用","lvl2":"Gensimの使い方"},"content":"","type":"content","url":"/word2vec-gensim#gensim","position":3},{"hierarchy":{"lvl1":"GensimによるWord2Vecの学習と使用","lvl3":"Gensimによる学習","lvl2":"Gensimの使い方"},"type":"lvl3","url":"/word2vec-gensim#gensim-1","position":4},{"hierarchy":{"lvl1":"GensimによるWord2Vecの学習と使用","lvl3":"Gensimによる学習","lvl2":"Gensimの使い方"},"content":"\n\nfrom gensim.models import Word2Vec\n\nsample_sents = [['you', 'say', 'goodbye', 'and', 'I', \"say\", \"hello\" '.']]\nmodel = Word2Vec(sentences=sample_sents, vector_size=5, window=1, min_count=1)\n\n\n\nmodel.wv['you']\n\n\n\nオプション\n\n説明\n\nsentences\n\n元となるコーパス．単語リストのリスト．\n\ncorpus_file\n\nコーパスをファイル読み込みする場合に指定．1行1文の形式で，単語は空白区切りで認識される．\n\nvector_size\n\n分散表現の次元．リファレンスではvector_sizeと書いてあるように見えるが，sizeでないと動かない．\n\nwindow\n\n学習時に利用されるコンテキストの長さ．\n\nmin_count\n\n分散表現を獲得する単語の最小頻度\n\nworkers\n\n学習時の使用スレッド数．\n\nsg\n\n学習アルゴリズムの選択．1ならskip-gram，0ならCBOW．\n\nwith open('./Data/lee_background.cor', 'r') as file:\n    corpus = file.readlines()\n\n\n\nprocessed_corpus = [line.lower().split() for line in corpus]\n\n\n\nmodel = Word2Vec(\n    sentences=processed_corpus,  # 学習対象のコーパス\n    vector_size=100,             # 埋め込みベクトルの次元数\n    window=5,                    # コンテキストウィンドウのサイズ\n    min_count=5,                 # 登場頻度が5未満の単語は無視\n    sg=1,                        # 1でskip-gramを使用（0ならCBOW）\n    negative=5                   # negative samplingのサンプル数\n)\n\n\n\n","type":"content","url":"/word2vec-gensim#gensim-1","position":5},{"hierarchy":{"lvl1":"GensimによるWord2Vecの学習と使用","lvl3":"Gensimによる日本語モデル学習","lvl2":"Gensimの使い方"},"type":"lvl3","url":"/word2vec-gensim#gensim-2","position":6},{"hierarchy":{"lvl1":"GensimによるWord2Vecの学習と使用","lvl3":"Gensimによる日本語モデル学習","lvl2":"Gensimの使い方"},"content":"\n\nimport MeCab\ndef tokenize(text):\n    \"\"\" テキストを形態素解析して、トークンのリストを返す \"\"\"\n    mecab = MeCab.Tagger(\"-Owakati\")\n    return mecab.parse(text).strip().split()\n\n\n\ndocuments = [\"これはサンプルの文書です。\", \"Word2Vecの学習を行います。\"]\n\n# 形態素解析を行い、単語リストに変換\ntokenized_documents = [tokenize(doc) for doc in documents]\n\n\n\n\n# Word2Vecモデルの訓練\n\nmodel_jp = Word2Vec(sentences=tokenized_documents, vector_size=100, window=5, min_count=1, workers=4)\n\n\n\n日本語のコーパスデータを用いて、Word2Vecモデルを学習してください。\n\n","type":"content","url":"/word2vec-gensim#gensim-2","position":7},{"hierarchy":{"lvl1":"GensimによるWord2Vecの学習と使用","lvl3":"モデルの使い方","lvl2":"Gensimの使い方"},"type":"lvl3","url":"/word2vec-gensim#id","position":8},{"hierarchy":{"lvl1":"GensimによるWord2Vecの学習と使用","lvl3":"モデルの使い方","lvl2":"Gensimの使い方"},"content":"モデルには,wvというオブジェクトに単語と単語分散表現の情報が格納されています。さらに、学習済みの単語ベクトルにアクセスし、それらを操作するための主要なインターフェースを提供します。\n\n単語ベクトルの取得: model.wv['word'] で特定の単語のベクトルを取得できます。\n\nmodel.wv['you']\n\n\n\n類似度の計算: model.wv.similarity('word1', 'word2') で2つの単語間の類似度を計算できます。\n\nmodel.wv.similarity(\"you\", \"your\")\n\n\n\n最も類似した単語の取得: model.wv.most_similar('word') で特定の単語に最も類似した単語を取得できます\n\nmodel.wv.most_similar(\"you\")\n\n\n\nNote\n\nGensimで学習済みモデルを使用する方法は、モデルの種類と読み込み方法によって異なります。通常は、通常はwvを介してベクトルにアクセスしますが、KeyedVectorsを使用する場合、KeyedVectors自体が単語ベクトルへの直接アクセスを提供するので、wvは不要です。\n\n","type":"content","url":"/word2vec-gensim#id","position":9},{"hierarchy":{"lvl1":"GensimによるWord2Vecの学習と使用","lvl3":"モデルの管理","lvl2":"Gensimの使い方"},"type":"lvl3","url":"/word2vec-gensim#id-1","position":10},{"hierarchy":{"lvl1":"GensimによるWord2Vecの学習と使用","lvl3":"モデルの管理","lvl2":"Gensimの使い方"},"content":"\n\n# モデルの保存と読み込み\n#model.save(\"word2vec.model\")\n#model = Word2Vec.load(\"word2vec.model\")\n\n\n\n","type":"content","url":"/word2vec-gensim#id-1","position":11},{"hierarchy":{"lvl1":"GensimによるWord2Vecの学習と使用","lvl4":"学習済みモデルの読み込み","lvl3":"モデルの管理","lvl2":"Gensimの使い方"},"type":"lvl4","url":"/word2vec-gensim#id-2","position":12},{"hierarchy":{"lvl1":"GensimによるWord2Vecの学習と使用","lvl4":"学習済みモデルの読み込み","lvl3":"モデルの管理","lvl2":"Gensimの使い方"},"content":"Gensimはいくつかの学習済みモデルを提供して、簡単に読み込むことができます。\n\nimport gensim.downloader\n\nprint(list(gensim.downloader.info()['models'].keys()))\n\n\n\nmodel = gensim.downloader.load('word2vec-google-news-300')\n\n\n\nsimilarity = model.similarity('woman', 'man')\nsimilarity\n\n\n\nmodel['king']\n\n\n\n学習済みモデルを読み込み、vec(king) - vec(man) + vec(woman)を計算し，そのベクトルと類似度の高い10語とその類似度を出力せよ．\n\nその他、各言語の学習済みモデルが多数公開されています。\n\nName\n\nModel\n\nData\n\nDim\n\nTokenizer\n\nDict\n\nWikiEntVec\n\nSkip-gram\n\nWikipedia\n\n100,200,300\n\nmecab\n\nmecab-ipadic-NEologd\n\n白ヤギ\n\nCBOW\n\nWikipedia\n\n50\n\nmecab\n\nmecab-ipadic-NEologd\n\nchiVe\n\nSkip-gram\n\nNWJC\n\n300\n\nSudachi\n\n\n\nbizreach\n\nSkip-gram\n\n求人データ\n\n100, 200\n\nmecab\n\nipadic\n\ndependency​-based​-japanese​-word​-embeddings\n\nDependency-Based Word Embeddings\n\nWikipedia\n\n100, 200, 300\n\nGinza\n\n\n\nfastText\n\nCBOW\n\nCommon Crawl, Wikipedia\n\n300\n\nmecab\n\n?\n\nwikipedia2vec\n\nSkip-gram\n\nWikipedia\n\n100, 300\n\nmecab\n\n?\n\nwordvectors\n\nSkip-gram, fastText\n\nWikipedia\n\n300\n\nmecab\n\n?\n\n","type":"content","url":"/word2vec-gensim#id-2","position":13},{"hierarchy":{"lvl1":"GensimによるWord2Vecの学習と使用","lvl3":"単語分散表現の可視化","lvl2":"Gensimの使い方"},"type":"lvl3","url":"/word2vec-gensim#id-3","position":14},{"hierarchy":{"lvl1":"GensimによるWord2Vecの学習と使用","lvl3":"単語分散表現の可視化","lvl2":"Gensimの使い方"},"content":"\n\nimport matplotlib.pyplot as plt\nfrom sklearn.manifold import TSNE\nimport numpy as np\n\n\n# List of words to visualize\nwords = ['woman', 'women', 'man', 'men', 'king', 'queen', 'prince', 'princess']\n\n# Check if the words are in the model to avoid KeyError\nvectors = [model[word] for word in words if word in model]\n\n# Converting list of vectors to a numpy array\nvectors_array = np.array(vectors)\n\n# Applying t-SNE for dimensionality reduction\ntsne = TSNE(n_components=2, random_state=0, perplexity=3)\nvectors_tsne = tsne.fit_transform(vectors_array)\n\n# Visualization\nplt.figure(figsize=(4, 4))\nfor i, word in enumerate(words):\n    if word in model:\n        plt.scatter(vectors_tsne[i, 0], vectors_tsne[i, 1])\n        plt.annotate(word, (vectors_tsne[i, 0], vectors_tsne[i, 1]))\n\nplt.xlabel('t-SNE Feature 0')\nplt.ylabel('t-SNE Feature 1')\nplt.title('t-SNE Visualization of Word Vectors')\nplt.show()\n\n\n\n","type":"content","url":"/word2vec-gensim#id-3","position":15},{"hierarchy":{"lvl1":"GensimによるWord2Vecの学習と使用","lvl3":"tensorboardで単語分散表現の可視化","lvl2":"Gensimの使い方"},"type":"lvl3","url":"/word2vec-gensim#tensorboard","position":16},{"hierarchy":{"lvl1":"GensimによるWord2Vecの学習と使用","lvl3":"tensorboardで単語分散表現の可視化","lvl2":"Gensimの使い方"},"content":"可視化の際に用いられるツールとしては、TensorFlowのツールの一つであるTensorBoardが、豊富な機能とインタラクティブな操作性を備えています。\n\nfrom tensorboardX import SummaryWriter\nimport torch\n\n\n\n# 分散表現・単語のリストを取得\nweights = model.vectors\nlabels = model.index_to_key\n\n\n\nweights = weights[:1000]\nlabels = labels[:1000]\n\n\n\nwriter = SummaryWriter('runs/google_embeddings')\nwriter.add_embedding(torch.FloatTensor(weights), metadata=labels)\n\n\n\n上記スクリプトを実行すると、実行されたディレクトリにデータが作成されます。TensorBoardの起動時にrunsディレクトリを指定することで、変換した単語の分散表現が可視化できます。tensorboard --logdir=runs\n\n上記コマンドを実行した状態で http://localhost:6006/ にアクセスすると、PROJECTORのページにてグラフが確認できます。","type":"content","url":"/word2vec-gensim#tensorboard","position":17}]}