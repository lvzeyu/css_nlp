<!DOCTYPE html><html lang="en" class="" style="scroll-padding:60px"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width,initial-scale=1"/><title>大規模言語モデルの基本 - 計算社会科学と自然言語処理</title><meta property="og:title" content="大規模言語モデルの基本 - 計算社会科学と自然言語処理"/><meta name="generator" content="mystmd"/><meta name="keywords" content=""/><meta name="image" content="/build/LLMs_parameter-ec157a4028abc601fb429210b7f4b07c.png"/><meta property="og:image" content="/build/LLMs_parameter-ec157a4028abc601fb429210b7f4b07c.png"/><link rel="stylesheet" href="/build/_assets/app-AIT5GAEP.css"/><link rel="stylesheet" href="/build/_assets/thebe-core-VKVHG5VY.css"/><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/jupyter-matplotlib@0.11.3/css/mpl_widget.css"/><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.css"/><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css" integrity="sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ" crossorigin="anonymous"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-85RFPTYEE3"></script><script>window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'G-85RFPTYEE3');</script><link rel="icon" href="/favicon.ico"/><link rel="stylesheet" href="/myst-theme.css"/><script>
  const savedTheme = localStorage.getItem("myst:theme");
  const theme = window.matchMedia("(prefers-color-scheme: light)").matches ? 'light' : 'dark';
  const classes = document.documentElement.classList;
  const hasAnyTheme = classes.contains('light') || classes.contains('dark');
  if (!hasAnyTheme) classes.add(savedTheme ?? theme);
</script></head><body class="m-0 transition-colors duration-500 bg-white dark:bg-stone-900"><div class="myst-skip-to-article fixed top-1 left-1 h-[0px] w-[0px] focus-within:z-40 focus-within:h-auto focus-within:w-auto bg-white overflow-hidden focus-within:p-2 focus-within:ring-1" aria-label="skip to content options"><a href="#skip-to-frontmatter" class="myst-skip-to-link block px-2 py-1 text-black underline">Skip to article frontmatter</a><a href="#skip-to-article" class="myst-skip-to-link block px-2 py-1 text-black underline">Skip to article content</a></div><dialog id="myst-no-css" style="position:fixed;left:0px;top:0px;width:100vw;height:100vh;font-size:4rem;padding:1rem;color:black;background:white"><strong>Site not loading correctly?</strong><p>This may be due to an incorrect <code>BASE_URL</code> configuration. See<!-- --> <a href="https://mystmd.org/guide/deployment#deploy-base-url">the MyST Documentation</a> <!-- -->for reference.</p><script>
    (() => {
            // Test for has-styling variable set by the MyST stylesheet
            const node = document.currentScript.parentNode;
            const hasCSS = window.getComputedStyle(node).getPropertyValue("--has-styling");
            if (hasCSS === ""){
                    node.showModal();
            }

    })()
</script></dialog><div class="myst-top-nav bg-white/80 backdrop-blur dark:bg-stone-900/80 shadow dark:shadow-stone-700 p-3 md:px-8 sticky w-screen top-0 z-30 h-[60px]"><nav class="myst-top-nav-bar flex items-center justify-between flex-nowrap max-w-[1440px] mx-auto"><div class="flex flex-row xl:min-w-[19.5rem] mr-2 sm:mr-7 justify-start items-center shrink-0"><div class="block xl:hidden"><button class="myst-top-nav-menu-button flex items-center justify-center border-stone-400 text-stone-800 hover:text-stone-900 dark:text-stone-200 hover:dark:text-stone-100 w-10 h-10"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" data-slot="icon" width="1.5rem" height="1.5rem"><path fill-rule="evenodd" d="M3 6.75A.75.75 0 0 1 3.75 6h16.5a.75.75 0 0 1 0 1.5H3.75A.75.75 0 0 1 3 6.75ZM3 12a.75.75 0 0 1 .75-.75h16.5a.75.75 0 0 1 0 1.5H3.75A.75.75 0 0 1 3 12Zm0 5.25a.75.75 0 0 1 .75-.75h16.5a.75.75 0 0 1 0 1.5H3.75a.75.75 0 0 1-.75-.75Z" clip-rule="evenodd"></path></svg><span class="sr-only">Open Menu</span></button></div><a class="myst-home-link flex items-center ml-3 dark:text-white w-fit md:ml-5 xl:ml-7" href="/"><div class="myst-home-link-logo mr-3 flex items-center dark:bg-white dark:rounded px-1"><img src="/build/tohoku-university-lo-68dede7d9dcb965aff25e8429e755296.svg" class="h-9" height="2.25rem"/></div><span class="text-md sm:text-xl tracking-tight sm:mr-5 sr-only">Made with MyST</span></a></div><div class="flex items-center flex-grow w-auto"><div class="flex-grow hidden text-md lg:block"></div><div class="flex-grow block"></div><button type="button" aria-haspopup="dialog" aria-expanded="false" aria-controls="radix-:R75cp:" data-state="closed" class="myst-search-bar flex items-center h-10 aspect-square sm:w-64 text-left text-gray-600 border border-gray-300 dark:border-gray-600 rounded-lg bg-gray-50 dark:bg-gray-700 myst-search-bar-disabled hover:ring-blue-500 dark:hover:ring-blue-500 hover:border-blue-500 dark:hover:border-blue-500"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" data-slot="icon" class="p-2.5 h-10 w-10 aspect-square"><path fill-rule="evenodd" d="M10.5 3.75a6.75 6.75 0 1 0 0 13.5 6.75 6.75 0 0 0 0-13.5ZM2.25 10.5a8.25 8.25 0 1 1 14.59 5.28l4.69 4.69a.75.75 0 1 1-1.06 1.06l-4.69-4.69A8.25 8.25 0 0 1 2.25 10.5Z" clip-rule="evenodd"></path></svg><span class="myst-search-text-placeholder hidden sm:block grow">Search</span><div aria-hidden="true" class="myst-search-shortcut items-center hidden mx-1 font-mono text-sm text-gray-600 sm:flex gap-x-1"><kbd class="px-2 py-1 border border-gray-300 dark:border-gray-600 rounded-md shadow-[0px_2px_0px_0px_rgba(0,0,0,0.08)] dark:shadow-none hide-mac">CTRL</kbd><kbd class="px-2 py-1 border border-gray-300 dark:border-gray-600 rounded-md shadow-[0px_2px_0px_0px_rgba(0,0,0,0.08)] dark:shadow-none show-mac">⌘</kbd><kbd class="px-2 py-1 border border-gray-300 dark:border-gray-600 rounded-md shadow-[0px_2px_0px_0px_rgba(0,0,0,0.08)] dark:shadow-none ">K</kbd><script>
;(() => {
const script = document.currentScript;
const root = script.parentElement;

const isMac = /mac/i.test(
      window.navigator.userAgentData?.platform ?? window.navigator.userAgent,
    );
root.querySelectorAll(".hide-mac").forEach(node => {node.classList.add(isMac ? "hidden" : "block")});
root.querySelectorAll(".show-mac").forEach(node => {node.classList.add(!isMac ? "hidden" : "block")});
})()</script></div></button><button class="myst-theme-button theme rounded-full aspect-square border border-stone-700 dark:border-white hover:bg-neutral-100 border-solid overflow-hidden text-stone-700 dark:text-white hover:text-stone-500 dark:hover:text-neutral-800 w-10 h-10 mx-3" title="Toggle theme between light and dark mode" aria-label="Toggle theme between light and dark mode"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" data-slot="icon" class="myst-theme-moon-icon h-full w-full p-0.5 hidden dark:block"><path fill-rule="evenodd" d="M9.528 1.718a.75.75 0 0 1 .162.819A8.97 8.97 0 0 0 9 6a9 9 0 0 0 9 9 8.97 8.97 0 0 0 3.463-.69.75.75 0 0 1 .981.98 10.503 10.503 0 0 1-9.694 6.46c-5.799 0-10.5-4.7-10.5-10.5 0-4.368 2.667-8.112 6.46-9.694a.75.75 0 0 1 .818.162Z" clip-rule="evenodd"></path></svg><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="myst-theme-sun-icon h-full w-full p-0.5 dark:hidden"><path stroke-linecap="round" stroke-linejoin="round" d="M12 3v2.25m6.364.386-1.591 1.591M21 12h-2.25m-.386 6.364-1.591-1.591M12 18.75V21m-4.773-4.227-1.591 1.591M5.25 12H3m4.227-4.773L5.636 5.636M15.75 12a3.75 3.75 0 1 1-7.5 0 3.75 3.75 0 0 1 7.5 0Z"></path></svg></button><div class="block sm:hidden"></div><div class="hidden sm:block"></div></div></nav></div><div class="myst-primary-sidebar fixed xl:article-grid grid-gap xl:w-screen xl:pointer-events-none overflow-auto max-xl:min-w-[300px] hidden z-10" style="top:60px"><div class="myst-primary-sidebar-pointer pointer-events-auto xl:col-margin-left flex-col overflow-hidden hidden xl:flex"><div class="myst-primary-sidebar-nav flex-grow py-6 overflow-y-auto primary-scrollbar"><nav aria-label="Navigation" class="myst-primary-sidebar-topnav overflow-y-hidden transition-opacity ml-3 xl:ml-0 mr-3 max-w-[350px] lg:hidden"><div class="w-full px-1 dark:text-white font-medium"></div></nav><div class="my-3 border-b-2 lg:hidden"></div><nav aria-label="Table of Contents" class="myst-primary-sidebar-toc flex-grow overflow-y-hidden transition-opacity ml-3 xl:ml-0 mr-3 max-w-[350px]"><div class="myst-toc w-full px-1 dark:text-white"><a title="計算社会科学と自然言語処理" class="block break-words focus:outline outline-blue-200 outline-2 rounded myst-toc-item p-2 my-1 rounded-lg hover:bg-slate-300/30 font-bold" href="/">計算社会科学と自然言語処理</a><div data-state="closed" class="w-full"><div class="myst-toc-item flex flex-row w-full gap-2 px-2 my-1 text-left rounded-lg outline-none hover:bg-slate-300/30"><div title="イントロダクション" class="block break-words rounded py-2 grow cursor-pointer">イントロダクション</div><button class="self-center flex-none rounded-md group hover:bg-slate-300/30 focus:outline outline-blue-200 outline-2" aria-label="Open Folder" type="button" aria-controls="radix-:Rmpsp:" aria-expanded="false" data-state="closed"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" data-slot="icon" class="transition-transform duration-300 group-data-[state=open]:rotate-90 text-text-slate-700 dark:text-slate-100" height="1.5rem" width="1.5rem"><path fill-rule="evenodd" d="M16.28 11.47a.75.75 0 0 1 0 1.06l-7.5 7.5a.75.75 0 0 1-1.06-1.06L14.69 12 7.72 5.03a.75.75 0 0 1 1.06-1.06l7.5 7.5Z" clip-rule="evenodd"></path></svg></button></div><div data-state="closed" id="radix-:Rmpsp:" hidden="" class="pl-3 pr-[2px] collapsible-content"></div></div><div data-state="closed" class="w-full"><div class="myst-toc-item flex flex-row w-full gap-2 px-2 my-1 text-left rounded-lg outline-none hover:bg-slate-300/30"><div title="基礎知識" class="block break-words rounded py-2 grow cursor-pointer">基礎知識</div><button class="self-center flex-none rounded-md group hover:bg-slate-300/30 focus:outline outline-blue-200 outline-2" aria-label="Open Folder" type="button" aria-controls="radix-:Rupsp:" aria-expanded="false" data-state="closed"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" data-slot="icon" class="transition-transform duration-300 group-data-[state=open]:rotate-90 text-text-slate-700 dark:text-slate-100" height="1.5rem" width="1.5rem"><path fill-rule="evenodd" d="M16.28 11.47a.75.75 0 0 1 0 1.06l-7.5 7.5a.75.75 0 0 1-1.06-1.06L14.69 12 7.72 5.03a.75.75 0 0 1 1.06-1.06l7.5 7.5Z" clip-rule="evenodd"></path></svg></button></div><div data-state="closed" id="radix-:Rupsp:" hidden="" class="pl-3 pr-[2px] collapsible-content"></div></div><div data-state="closed" class="w-full"><div class="myst-toc-item flex flex-row w-full gap-2 px-2 my-1 text-left rounded-lg outline-none hover:bg-slate-300/30"><div title="ニューラルネットワーク" class="block break-words rounded py-2 grow cursor-pointer">ニューラルネットワーク</div><button class="self-center flex-none rounded-md group hover:bg-slate-300/30 focus:outline outline-blue-200 outline-2" aria-label="Open Folder" type="button" aria-controls="radix-:R16psp:" aria-expanded="false" data-state="closed"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" data-slot="icon" class="transition-transform duration-300 group-data-[state=open]:rotate-90 text-text-slate-700 dark:text-slate-100" height="1.5rem" width="1.5rem"><path fill-rule="evenodd" d="M16.28 11.47a.75.75 0 0 1 0 1.06l-7.5 7.5a.75.75 0 0 1-1.06-1.06L14.69 12 7.72 5.03a.75.75 0 0 1 1.06-1.06l7.5 7.5Z" clip-rule="evenodd"></path></svg></button></div><div data-state="closed" id="radix-:R16psp:" hidden="" class="pl-3 pr-[2px] collapsible-content"></div></div><div data-state="closed" class="w-full"><div class="myst-toc-item flex flex-row w-full gap-2 px-2 my-1 text-left rounded-lg outline-none hover:bg-slate-300/30"><div title="PyTorch" class="block break-words rounded py-2 grow cursor-pointer">PyTorch</div><button class="self-center flex-none rounded-md group hover:bg-slate-300/30 focus:outline outline-blue-200 outline-2" aria-label="Open Folder" type="button" aria-controls="radix-:R1epsp:" aria-expanded="false" data-state="closed"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" data-slot="icon" class="transition-transform duration-300 group-data-[state=open]:rotate-90 text-text-slate-700 dark:text-slate-100" height="1.5rem" width="1.5rem"><path fill-rule="evenodd" d="M16.28 11.47a.75.75 0 0 1 0 1.06l-7.5 7.5a.75.75 0 0 1-1.06-1.06L14.69 12 7.72 5.03a.75.75 0 0 1 1.06-1.06l7.5 7.5Z" clip-rule="evenodd"></path></svg></button></div><div data-state="closed" id="radix-:R1epsp:" hidden="" class="pl-3 pr-[2px] collapsible-content"></div></div><div data-state="closed" class="w-full"><div class="myst-toc-item flex flex-row w-full gap-2 px-2 my-1 text-left rounded-lg outline-none hover:bg-slate-300/30"><div title="単語分散表現" class="block break-words rounded py-2 grow cursor-pointer">単語分散表現</div><button class="self-center flex-none rounded-md group hover:bg-slate-300/30 focus:outline outline-blue-200 outline-2" aria-label="Open Folder" type="button" aria-controls="radix-:R1mpsp:" aria-expanded="false" data-state="closed"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" data-slot="icon" class="transition-transform duration-300 group-data-[state=open]:rotate-90 text-text-slate-700 dark:text-slate-100" height="1.5rem" width="1.5rem"><path fill-rule="evenodd" d="M16.28 11.47a.75.75 0 0 1 0 1.06l-7.5 7.5a.75.75 0 0 1-1.06-1.06L14.69 12 7.72 5.03a.75.75 0 0 1 1.06-1.06l7.5 7.5Z" clip-rule="evenodd"></path></svg></button></div><div data-state="closed" id="radix-:R1mpsp:" hidden="" class="pl-3 pr-[2px] collapsible-content"></div></div><div data-state="closed" class="w-full"><div class="myst-toc-item flex flex-row w-full gap-2 px-2 my-1 text-left rounded-lg outline-none hover:bg-slate-300/30"><div title="RNN" class="block break-words rounded py-2 grow cursor-pointer">RNN</div><button class="self-center flex-none rounded-md group hover:bg-slate-300/30 focus:outline outline-blue-200 outline-2" aria-label="Open Folder" type="button" aria-controls="radix-:R1upsp:" aria-expanded="false" data-state="closed"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" data-slot="icon" class="transition-transform duration-300 group-data-[state=open]:rotate-90 text-text-slate-700 dark:text-slate-100" height="1.5rem" width="1.5rem"><path fill-rule="evenodd" d="M16.28 11.47a.75.75 0 0 1 0 1.06l-7.5 7.5a.75.75 0 0 1-1.06-1.06L14.69 12 7.72 5.03a.75.75 0 0 1 1.06-1.06l7.5 7.5Z" clip-rule="evenodd"></path></svg></button></div><div data-state="closed" id="radix-:R1upsp:" hidden="" class="pl-3 pr-[2px] collapsible-content"></div></div><div data-state="closed" class="w-full"><div class="myst-toc-item flex flex-row w-full gap-2 px-2 my-1 text-left rounded-lg outline-none hover:bg-slate-300/30"><div title="Transformer" class="block break-words rounded py-2 grow cursor-pointer">Transformer</div><button class="self-center flex-none rounded-md group hover:bg-slate-300/30 focus:outline outline-blue-200 outline-2" aria-label="Open Folder" type="button" aria-controls="radix-:R26psp:" aria-expanded="false" data-state="closed"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" data-slot="icon" class="transition-transform duration-300 group-data-[state=open]:rotate-90 text-text-slate-700 dark:text-slate-100" height="1.5rem" width="1.5rem"><path fill-rule="evenodd" d="M16.28 11.47a.75.75 0 0 1 0 1.06l-7.5 7.5a.75.75 0 0 1-1.06-1.06L14.69 12 7.72 5.03a.75.75 0 0 1 1.06-1.06l7.5 7.5Z" clip-rule="evenodd"></path></svg></button></div><div data-state="closed" id="radix-:R26psp:" hidden="" class="pl-3 pr-[2px] collapsible-content"></div></div><div data-state="open" class="w-full"><div class="myst-toc-item flex flex-row w-full gap-2 px-2 my-1 text-left rounded-lg outline-none hover:bg-slate-300/30"><div title="大規模言語モデル" class="block break-words rounded py-2 grow font-semibold text-blue-800 dark:text-blue-200 cursor-pointer">大規模言語モデル</div><button class="self-center flex-none rounded-md group hover:bg-slate-300/30 focus:outline outline-blue-200 outline-2" aria-label="Open Folder" type="button" aria-controls="radix-:R2epsp:" aria-expanded="true" data-state="open"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" data-slot="icon" class="transition-transform duration-300 group-data-[state=open]:rotate-90 text-text-slate-700 dark:text-slate-100" height="1.5rem" width="1.5rem"><path fill-rule="evenodd" d="M16.28 11.47a.75.75 0 0 1 0 1.06l-7.5 7.5a.75.75 0 0 1-1.06-1.06L14.69 12 7.72 5.03a.75.75 0 0 1 1.06-1.06l7.5 7.5Z" clip-rule="evenodd"></path></svg></button></div><div data-state="open" id="radix-:R2epsp:" class="pl-3 pr-[2px] collapsible-content"><a title="GPT" class="block break-words focus:outline outline-blue-200 outline-2 rounded myst-toc-item p-2 my-1 rounded-lg hover:bg-slate-300/30" href="/gpt">GPT</a><a title="大規模言語モデルの基本" aria-current="page" class="block break-words focus:outline outline-blue-200 outline-2 rounded myst-toc-item p-2 my-1 rounded-lg myst-toc-item-exact bg-blue-300/30 active" href="/llm">大規模言語モデルの基本</a><a title="LLMSの応用(1)：Langchainの基礎" class="block break-words focus:outline outline-blue-200 outline-2 rounded myst-toc-item p-2 my-1 rounded-lg hover:bg-slate-300/30" href="/langchain-basic">LLMSの応用(1)：Langchainの基礎</a></div></div></div></nav></div><div class="myst-primary-sidebar-footer flex-none py-6 transition-all duration-700 translate-y-6 opacity-0"><a class="myst-made-with-myst flex mx-auto text-gray-700 w-fit hover:text-blue-700 dark:text-gray-200 dark:hover:text-blue-400" href="https://mystmd.org/made-with-myst" target="_blank" rel="noreferrer"><svg style="width:24px;height:24px" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 100 100" stroke="none"><g id="icon"><path fill="currentColor" d="M23.8,54.8v-3.6l4.7-0.8V17.5l-4.7-0.8V13H36l13.4,31.7h0.2l13-31.7h12.6v3.6l-4.7,0.8v32.9l4.7,0.8v3.6h-15
          v-3.6l4.9-0.8V20.8H65L51.4,53.3h-3.8l-14-32.5h-0.1l0.2,17.4v12.1l5,0.8v3.6H23.8z"></path><path fill="#F37726" d="M47,86.9c0-5.9-3.4-8.8-10.1-8.8h-8.4c-5.2,0-9.4-1.3-12.5-3.8c-3.1-2.5-5.4-6.2-6.8-11l4.8-1.6
          c1.8,5.6,6.4,8.6,13.8,8.8h9.2c6.4,0,10.8,2.5,13.1,7.5c2.3-5,6.7-7.5,13.1-7.5h8.4c7.8,0,12.7-2.9,14.6-8.7l4.8,1.6
          c-1.4,4.9-3.6,8.6-6.8,11.1c-3.1,2.5-7.3,3.7-12.4,3.8H63c-6.7,0-10,2.9-10,8.8"></path></g></svg><span class="self-center ml-2 text-sm">Made with MyST</span></a></div></div></div><main class="article-grid grid-gap"><article class="article-grid subgrid-gap col-screen article content"><div class="hidden"></div><div id="skip-to-frontmatter" aria-label="article frontmatter" class="myst-fm-block mb-8 pt-9"><div class="myst-fm-block-header flex items-center mb-5 h-6 text-sm font-light"><div class="flex-grow"></div><div class="myst-fm-block-badges"><a href="https://github.com/lvzeyu/css_nlp" title="GitHub Repository: lvzeyu/css_nlp" target="_blank" rel="noopener noreferrer" class="myst-fm-github-link text-inherit hover:text-inherit"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" width="1.25rem" height="1.25rem" class="myst-fm-github-icon inline-block mr-1 opacity-60 hover:opacity-100"><path d="M12 2.5c-5.4 0-9.8 4.4-9.8 9.7 0 4.3 2.8 8 6.7 9.2.5.1.7-.2.7-.5v-1.8c-2.4.5-3.1-.6-3.3-1.1-.1-.3-.6-1.1-1-1.4-.3-.2-.8-.6 0-.6s1.3.7 1.5 1c.9 1.5 2.3 1.1 2.8.8.1-.6.3-1.1.6-1.3-2.2-.2-4.4-1.1-4.4-4.8 0-1.1.4-1.9 1-2.6-.1-.2-.4-1.2.1-2.6 0 0 .8-.3 2.7 1 .8-.2 1.6-.3 2.4-.3.8 0 1.7.1 2.4.3 1.9-1.3 2.7-1 2.7-1 .5 1.3.2 2.3.1 2.6.6.7 1 1.5 1 2.6 0 3.7-2.3 4.6-4.4 4.8.4.3.7.9.7 1.8V21c0 .3.2.6.7.5 3.9-1.3 6.6-4.9 6.6-9.2 0-5.4-4.4-9.8-9.8-9.8z"></path></svg></a></div><a href="https://github.com/lvzeyu/css_nlp/edit/master/notebook/llm.ipynb" title="Edit This Page" target="_blank" rel="noopener noreferrer" class="myst-fm-edit-link text-inherit hover:text-inherit"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="1.25rem" height="1.25rem" class="myst-fm-edit-icon inline-block mr-1 opacity-60 hover:opacity-100"><path stroke-linecap="round" stroke-linejoin="round" d="m16.862 4.487 1.687-1.688a1.875 1.875 0 1 1 2.652 2.652L10.582 16.07a4.5 4.5 0 0 1-1.897 1.13L6 18l.8-2.685a4.5 4.5 0 0 1 1.13-1.897l8.932-8.931Zm0 0L19.5 7.125M18 14v4.75A2.25 2.25 0 0 1 15.75 21H5.25A2.25 2.25 0 0 1 3 18.75V8.25A2.25 2.25 0 0 1 5.25 6H10"></path></svg></a><div class="myst-fm-downloads-dropdown relative flex inline-block mx-1 grow-0" data-headlessui-state=""><button class="myst-fm-downloads-button relative ml-2 -mr-1" id="headlessui-menu-button-:Rs8ucp:" type="button" aria-haspopup="menu" aria-expanded="false" data-headlessui-state=""><span class="sr-only">Downloads</span><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="1.25rem" height="1.25rem" class="myst-fm-downloads-icon"><title>Download</title><path stroke-linecap="round" stroke-linejoin="round" d="M3 16.5v2.25A2.25 2.25 0 0 0 5.25 21h13.5A2.25 2.25 0 0 0 21 18.75V16.5M16.5 12 12 16.5m0 0L7.5 12m4.5 4.5V3"></path></svg></button></div></div><h1 class="myst-fm-block-title mb-0">大規模言語モデルの基本</h1></div><div class="block my-10 lg:sticky lg:z-10 lg:h-0 lg:pt-0 lg:my-0 lg:ml-10 lg:col-margin-right" style="top:60px"><nav></nav></div><div id="skip-to-article"></div><div id="Pc5oQqogFc" class="myst-jp-nb-block relative group/block"><p>近年、GPTをはじめとする大規模言語モデルが自然言語処理分野において顕著な成果を挙げ、文章の生成や論理推論など高度な課題においても高い性能を示している。それに伴う、大規模言語モデルを活用することでより効率的に人間の行動や社会現象に関する理論や仮説の検証と発展し、新たな可能性をもたらすと期待されています。</p><p>今まで説明した通り、文脈を考慮した単語埋め込みである文脈化単語埋め込み(contextualized word embedding)を計算するTransformerを大規模コーパスで自己教師あり学習(Masked LMやNext sentence prediction)で事前学習し、そのモデルを下流タスクのテータセットを使って微調整する方法は、自然言語処理でよく用いられる手法です。</p><p>こうした事前学習した大規模なニューラルネットワークは、大規模言語モデルのように呼ばれます。</p><p>特に、2020年以降にパラメータ数およびテキストデータをさらに大規模することで、大規模言語モデルの性能も飛躍的に向上し、人間と自然にやりとりできるような能力を身につけました。これによって、ファインチューニングは行なわずに、事前学習された大規模言語モデルをプロンプト(prompt)と呼ばれるテキストを通じて制御することで下流タスク方法も一般的になりつつあります。</p><p>ここでは、大規模言語モデルの近年(2023年まで)の進展について解説します。進展が激しい分野のため、ここでの解説に基づいて適宜最新の情報も各自調べてほしいです。</p></div><div id="NQYcXVEkTS" class="myst-jp-nb-block relative group/block"><h2 id="id" class="relative group"><span class="mr-3 select-none">1</span><span class="heading-text">モデルの大規模化とその効果</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#id" title="Link to this Section" aria-label="Link to this Section">¶</a></h2><p>大規模言語モデルの開発が進むにつれて、モデルに含まれるパラメータ数が飛躍的に増加してきています。</p><p>2018年に発表されたBERTは3.4億個、2019年のGPT-2では15億個だったパラメータ数が2020年のGPT-3では1750億、そしてGPT-4はは100兆個のパラメータを持つと言われており、加速的に増加していることがわかります。</p></div><div id="JAofIklR5T" class="myst-jp-nb-block relative group/block"><img id="KNlwRGHfcZ" style="margin-left:auto;margin-right:auto" src="/build/LLMs_parameter-ec157a4028abc601fb429210b7f4b07c.png" data-canonical-url="./Figure/LLMs_parameter.png" class=""/><img id="vnvuYjnLRU" style="margin-left:auto;margin-right:auto" src="/build/gpt_para-66d806b5b79617ddbf36df25e9e6fef1.png" data-canonical-url="./Figure/gpt_para.png" class=""/></div><div id="pn12GHUZns" class="myst-jp-nb-block relative group/block"><p>こうした大規模化が行われている背景には、モデルの規模を大きくすることで性能が比例して改善していくという経験的な法則であるスケール則があります。</p><p>さらに、大規模言語モデルが一定の規模を超えると、タスクの性能が飛躍的に向上する現象も報告されています。こうした大規模化することで性能が改善し獲得される能力を創発的能力(emergent abilities)と呼ぶことがあります。</p><aside class="myst-aside myst-aside-${kind} text-sm lg:h-0 col-margin-right"><p>一方、大規模言語モデルが創発的能力を持つことに疑問視している声もあります。</p></aside><img id="cYDuJj13o1" style="margin-left:auto;margin-right:auto" src="/build/emergent-d2694b9351ce41c7dfd2034adf2cb385.png" data-canonical-url="./Figure/emergent.png" class=""/></div><div id="Lq7gXQhUXM" class="myst-jp-nb-block relative group/block"><h2 id="prompting" class="relative group"><span class="mr-3 select-none">2</span><span class="heading-text">Prompting</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#prompting" title="Link to this Section" aria-label="Link to this Section">¶</a></h2><h3 id="id-1" class="relative group"><span class="mr-3 select-none">2.1</span><span class="heading-text">文脈内学習</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#id-1" title="Link to this Section" aria-label="Link to this Section">¶</a></h3></div><div id="T7dB21D2Qw" class="myst-jp-nb-block relative group/block"><p>GPTのようなパラメータが非常に大きいモデルでは、学習時に入力される文章内には、様々なサブタスクが埋め込まれると考えられます。こうした文章の生成を学習することで、内包される様々な言語タスクへの処理能力の獲得が期待できます。
このアプローチはGPT-2から用いられており、文脈内学習(in-context learning)と呼ばれます。</p><aside class="myst-aside myst-aside-${kind} text-sm lg:h-0 col-margin-right"><p>詳細はこちらの<a target="_blank" rel="noreferrer" href="https://arxiv.org/abs/2005.14165" class="">論文:Language Models are Few-Shot Learners</a>を参照してください。</p></aside><img id="NgF8Dx12Na" style="margin-left:auto;margin-right:auto" src="/build/meta_learning-0d9335f3b146c91ca09078f39fc34491.png" data-canonical-url="./Figure/meta_learning.png" class=""/><p>モデルは新しいタスクをこなすための追加のトレーニングデータなしに（zero-shot learning）、または非常に少量のデータで（few-shot learning）多様なタスクを達成することが可能になります。</p><ul><li><p>Few-shot learning: 推論時に少数（10から100）のデモンストレーションを与えます</p></li><li><p>One-shot learning: 推論時に一つのデモンストレーションを与えます</p></li><li><p>Zero-shot learning: 推論時にデモンストレーションは与えられず、自然言語によるタスク指示のみが与えられます</p></li></ul><img id="BsJMjwIuan" style="margin-left:auto;margin-right:auto" src="/build/shot-learning-ae6fca156f60571e643b613d4db391d0.png" data-canonical-url="./Figure/shot-learning.png" class=""/><p>このように、従来ファインチューニングが必要される多くのタスクは、モデルにプロンプト(prompt)と呼ばれるテキストを入力して後続けするテキストを予測するという形で解かせることが知られています。</p><ul><li><p>基本的には、タスクの説明と多くのデモンストレーションを与えることでモデルの性能が向上していく傾向が見られます。</p></li><li><p>zero, one-shotでも悪くない性能、few-shotでは一部ファインチューニングを用いたSOTAモデルに匹敵する性能を得ることが確認できました。</p></li></ul><img id="iffpVr4aQC" style="margin-left:auto;margin-right:auto" src="/build/in-context-200c05c5ac8b27c8efc40deafa5de088.png" data-canonical-url="./Figure/in-context.png" class=""/><p>現在、一般的には、few-shot learningモデルの性能がファインチューニングによる教師あり学習モデルを超えることはまだ少ないが、ラベル付きデータの生成に費やすコストと時間を効率すると、プロンプトはファインチューニングによる教師あり学習の効率的な代替手段と考えられます。</p><img id="pKbJpQ92wF" style="margin-left:auto;margin-right:auto" src="/build/shot-compare-31e1df16c91bc3dd7e1e5176a2d87a4a.png" data-canonical-url="./Figure/shot-compare.png" class=""/></div><div id="inEcI0SJcN" class="myst-jp-nb-block relative group/block"><h3 id="chain-of-thought" class="relative group"><span class="mr-3 select-none">2.2</span><span class="heading-text">chain-of-thought推論</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#chain-of-thought" title="Link to this Section" aria-label="Link to this Section">¶</a></h3><aside class="myst-aside myst-aside-${kind} text-sm lg:h-0 col-margin-right"><p>詳細はこちらの<a target="_blank" rel="noreferrer" href="https://arxiv.org/abs/2201.11903" class="">論文:Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</a>を参照してください。</p></aside><p>大規模言語モデルが苦手とされるタスクの一つに他段階の推論が必要となるマルチステップ推論(multi-step reasoning)があります。</p><p>複数の段階の推論が必要な際に、推論過程の例示を与えるchain-of-thought推論(chain-of-thought reasoning)を用いることで性能が改善することが報告されています。</p><p>具体的言えば、chain-of-thought推論では、回答を加えて推論過程を示す例示を与えて、モデルが回答を行う際に推論過程を含めて出力テキストを生成するようにします。</p><img id="REuo6qzNnZ" style="margin-left:auto;margin-right:auto" src="/build/chains_of_thought-6af15225868a72490f05a0a1c638f322.png" data-canonical-url="./Figure/chains_of_thought.png" class=""/></div><div id="uEs8N9Yads" class="myst-jp-nb-block relative group/block"><p>さらに、chain-of-thought推論の推論過程を人間が与えるのではなく、推論過程の生成を促す「Let’s think step by step」のような文字列をプロンプトの末尾に追加して、推論過程を生成されてから回答を抽出するzero-shot chain-of-thought reasoningも提案されています。</p><p>この方法を使うと、プロンプトを書き換えるだけで推論の性能を改善することができます。このことから、大規模言語モデルを使うあたってはプロンプトの与え方を工夫することが重要であることがわかります。</p><img id="ZsmzCA0ax0" style="margin-left:auto;margin-right:auto" src="/build/zero_CoT-2e1d99436695654f74540cb26ffc846a.png" data-canonical-url="./Figure/zero_CoT.png" class=""/><aside class="myst-aside myst-aside-${kind} text-sm lg:h-0 col-margin-right"><p>詳細はこちらの<a target="_blank" rel="noreferrer" href="https://arxiv.org/abs/2205.11916" class="">論文:Large Language Models are Zero-Shot Reasoners</a>を参照してください。</p></aside></div><div id="IZzFHeAfOc" class="myst-jp-nb-block relative group/block"><h2 id="id-2" class="relative group"><span class="mr-3 select-none">3</span><span class="heading-text">指示チューニング</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#id-2" title="Link to this Section" aria-label="Link to this Section">¶</a></h2></div><div id="pvuOt4Ho3f" class="myst-jp-nb-block relative group/block"><p>通常の事前学習では、モデルは膨大なデータセットから次の単語を予測する能力を学習します。しかし、そのままでは具体的なタスクを効率的にこなすことは難しい場合があります。</p><p>指示チューニング（instruction tuning）とは、様々なタスクのデータを指示と理想的な回答の組で構成されるデータセットを言語モデルに与え追加学習させることで、言語モデルの性能を向上させる技術です。</p><ul><li><p>指示チューニングデータセットの構築</p></li><li><p>ファインチューニング: 指示チューニングデータセットを用いて、既存のLLMを再トレーニングします。これにより、モデルが指示に応じた出力を生成する能力が向上します。</p></li></ul><img id="ekkQwJvQ10" style="margin-left:auto;margin-right:auto" src="/build/instrust-example-5c2e99563d08457de4497eb7533aaddb.png" data-canonical-url="./Figure/instrust-example.png" class=""/><p>Flan(Finetuned Lanaguage Net)では、大規模言語モデルをデータセットを集約してフィインチューニングした結果、多数のタスクにおけるzero-shot学習の性能が向上したことが報告されています。</p><p><a target="_blank" rel="noreferrer" href="https://huggingface.co/google/flan-t5-xxl?text=The+square+root+of+x+is+the+cube+root+of+y.+What+is+y+to+the+power+of+2%2C+if+x+%3D+4%3F" class="">「Flan-T5」</a>は、Google AI の新しいオープンソース言語モデルです。1,800 以上の言語タスクでファインチューニングされており、プロンプトとマルチステップの推論能力が劇的に向上しています。</p><img id="LZG6XvY00P" style="margin-left:auto;margin-right:auto" src="/build/flan-1-7a505e0e60ebed20f22312c621ec2af9.png" data-canonical-url="./Figure/flan-1.png" class=""/></div><div id="HuoNoo0tKc" class="myst-jp-nb-block relative group/block"><h2 id="id-3" class="relative group"><span class="mr-3 select-none">4</span><span class="heading-text">人間のフィードバックからの強化学習</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#id-3" title="Link to this Section" aria-label="Link to this Section">¶</a></h2></div><div id="L7kSTKY4Dl" class="myst-jp-nb-block relative group/block"><p>RLHF（Reinforcement Learning from Human Feedback）とは、「人間のフィードバックからの強化学習」という名前の通り、人間の価値基準に沿うように、人間のフィードバックを使ってAI（言語）モデルを強化学習で微調整（ファインチューニング）する手法である。</p><h3 id="id-4" class="relative group"><span class="mr-3 select-none">4.1</span><span class="heading-text">強化学習</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#id-4" title="Link to this Section" aria-label="Link to this Section">¶</a></h3><p>強化学習 (Reinforcement Learning) とは、機械学習の一種であり、エージェントが動的環境と、繰り返し試行錯誤のやりとりを重ねることによってタスクを実行できるようになる手法です。この学習手法により、エージェントは、タスクの報酬を最大化する一連の意思決定を行うことができます。教師付き学習とよく似た問題設定ですが、与えられた正解の出力をそのまま学習すれば良いわけではなく、もっと広い意味での「価値」を最大化する行動を学習しなければなりません。</p><p>例えば、犬を訓練すること例として考えてください。強化学習の用語を使用すると、この場合の学習の目的は、犬 (エージェント) のしつけ (学習) を行い、ある環境の中でタスクを完了させることです。これには、犬の周囲の環境や訓練士が含まれます。</p><ul><li><p>まず、訓練士が命令や合図を出し、それを犬が観察 (観測) します。続いて、犬は行動を起こすことで反応します。犬の行動が目的の行動に近い場合、訓練士は、おやつやおもちゃなどのごほうび (報酬) を与えますが、それ以外の場合、ごほうびは与えません。</p></li><li><p>しつけ (学習) を始めたばかりの頃は、犬はランダムな行動を取る傾向にあります。犬は観測した特定の状況を行動やごほうび (報酬) と関連付けようとするため、与えられた指示が「おすわり」であっても、ローリングなど別の行動を取る場合があります。</p></li><li><p>犬の立場から見ると、すべての合図に正しく反応して、おやつをできるだけ多くもらえるような状況が理想的です。</p></li></ul><p>強化学習のしつけ (学習) とは、犬が何らかのごぼうび (報酬) を最大化する理想的な行動を学習するように、犬の方策を「調整」することを指します。学習が完了すると、犬は飼い主を観察し、獲得した方策によって、その場にふさわしい行動 (「おすわり」と命令されたらおすわりをするなど) が取れるようになります。</p><p>まとめると、強化学習の目的は、与えられた「環境」における価値（あるいは「利益」と呼びます）を最大化するように「エージェント」を学習させます。</p></div><div id="klNLn6GNey" class="myst-jp-nb-block relative group/block"><h2 id="rlhf-reinforcement-learning-from-human-feedback" class="relative group"><span class="mr-3 select-none">5</span><span class="heading-text">RLHF（Reinforcement Learning from Human Feedback）</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#rlhf-reinforcement-learning-from-human-feedback" title="Link to this Section" aria-label="Link to this Section">¶</a></h2><p>RLHFの役割は、人間の好みや意図といった「人間の価値基準」がAIモデルに反映されることになります。具体的には、あるプロンプトに対してAIが生成した応答文の良し悪しを人間がランク付けし、そのランク付されたデータセットを使って「より望ましい応答文とはどんな感じの文章なのか」を評価できる報酬モデルを作成するわけです。</p><ul><li><p>教師あり学習で既存の言語モデルをfine-tuning</p></li><li><p>指示チューニング済みモデルが出力したテキストに対して人手で優劣に順位付けします</p></li><li><p>このデータセットを使って、報酬(テキストの優劣を反映したスカラー値)を予測する報酬モデルを学習します</p><ul><li><p>報酬モデルをこのように学習させることで、似たようなプロンプトが与えられた時に、より望ましいと評価された応答（＝よりランクが高かった応答）に近い応答文が、より報酬が高くなります。結果として、より好ましいと評価された応答文に近い応答文が生成される確率が高まります。</p></li></ul></li></ul><img id="qFkePFpnYE" style="margin-left:auto;margin-right:auto" src="/build/rlhf-799f02a1517ba0cc2ca803959684c685.png" data-canonical-url="./Figure/rlhf.png" class=""/><img id="g1LBuHZtni" style="margin-left:auto;margin-right:auto" src="/build/rlhf-performance-73858c45a8e87668ecaab06fcb89bf36.png" data-canonical-url="./Figure/rlhf-performance.png" class=""/></div><div id="GKRkU5OHh3" class="myst-jp-nb-block relative group/block"><h2 id="parameter-efficient-fine-tuning-peft" class="relative group"><span class="mr-3 select-none">6</span><span class="heading-text">Parameter efficient fine-tuning (PEFT)</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#parameter-efficient-fine-tuning-peft" title="Link to this Section" aria-label="Link to this Section">¶</a></h2><img id="V9Kwqp9fwW" style="margin-left:auto;margin-right:auto" src="/build/PEFT-99de48d7f1515ab4df9c25e6375bb4f1.png" data-canonical-url="./Figure/PEFT.png" class=""/></div><div id="sO3qJW9C3z" class="myst-jp-nb-block relative group/block"><table class=""><tbody><tr class=""><th class=""><strong>項目</strong></th><th class=""><strong>PEFT</strong></th><th class=""><strong>Full Fine-Tuning</strong></th></tr><tr class=""><td class=""><strong>概要</strong></td><td class="">モデル全体ではなく、追加的に設定したパラメータや、一部のパラメータを微調整する手法。</td><td class="">事前学習済みモデル全体のパラメータを調整する手法。</td></tr><tr class=""><td class=""><strong>計算リソース</strong></td><td class="">大幅に削減可能。学習中に更新するパラメータ数が少ないため、GPUメモリや計算時間が少なくて済む。 (例　GPT-3 LoRA: 350GB)</td><td class="">多大な計算リソースを必要とする。全パラメータを更新するため、GPUメモリや計算負荷が高い (例　GPT-3: 1.2TB)。</td></tr><tr class=""><td class=""><strong>保存領域</strong></td><td class="">調整されたパラメータのみを保存するため、小さな保存領域で十分(例　GPT-3 LoRA: 35MB)。</td><td class="">微調整後のモデル全体を保存する必要があるため、元モデルと同じサイズの保存領域は必要(例　GPT-3: 350GB)。</td></tr></tbody></table></div><div id="ydVi81jke0" class="myst-jp-nb-block relative group/block"><p>PEFTの手法は主に以下の2つのカテゴリに分類できます：</p><ul><li><p>Reparameterization: モデルの一部パラメータを特定の形式に再構成し、その部分のみを微調整します。</p><ul><li><p>LoRA (Low-Rank Adaptation)</p></li></ul></li><li><p>Additive: 元のモデルのパラメータに加算的に新しいパラメータ（追加の層など）を導入して微調整を行います。</p></li><li><p>Soft Prompt: 入力系列にタスクごとのベクトル(Soft Prompt)を付加し、学習を行います。</p></li></ul></div><div id="whrGQJMWCS" class="myst-jp-nb-block relative group/block"><img id="TMplLM1K1D" style="margin-left:auto;margin-right:auto" src="/build/PEFT_method-3d50c0b8874b94ca76d8d888f3213641.png" data-canonical-url="./Figure/PEFT_method.png" class=""/></div><div id="vRg9GD0Qw4" class="myst-jp-nb-block relative group/block"><h3 id="lora-low-rank-adaptation" class="relative group"><span class="mr-3 select-none">6.1</span><span class="heading-text">LoRA (Low-Rank Adaptation)</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#lora-low-rank-adaptation" title="Link to this Section" aria-label="Link to this Section">¶</a></h3><p>LoRA は、モデル内の大きな重み行列（例: 全結合層や注意メカニズムの線形変換）を低ランクの行列に分解し、その一部だけを学習することで、計算コストとストレージを大幅に削減します。</p><p><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>W</mi><mo>+</mo><mi mathvariant="normal">Δ</mi><mi>W</mi><mo>=</mo><mi>W</mi><mo>+</mo><mi>A</mi><mi>B</mi></mrow><annotation encoding="application/x-tex">W + \Delta W = W + AB</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord">Δ</span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal">A</span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span></span></span></span></span></p><ul><li><p><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>d</mi><mo>×</mo><mi>r</mi></mrow></msup><mo separator="true">,</mo><mtext> </mtext><mi>B</mi><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>r</mi><mo>×</mo><mi>k</mi></mrow></msup></mrow><annotation encoding="application/x-tex">A \in \mathbb{R}^{d \times r}, \, B \in \mathbb{R}^{r \times k}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7224em;vertical-align:-0.0391em;"></span><span class="mord mathnormal">A</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.0435em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span></span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8491em;"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span></span></span></span></span></span></span></span></span></span> は学習可能な低ランク行列。</p></li><li><p><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>r</mi></mrow><annotation encoding="application/x-tex">r</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span></span></span></span></span> はランクで、通常は <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>r</mi><mo>≪</mo><mi>min</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>d</mi><mo separator="true">,</mo><mi>k</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">r \ll \min(d, k)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5782em;vertical-align:-0.0391em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">≪</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop">min</span><span class="mopen">(</span><span class="mord mathnormal">d</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="mclose">)</span></span></span></span></span>。</p></li></ul><p>この近似により、学習すべきパラメータの総数は元の <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>W</mi></mrow><annotation encoding="application/x-tex">W</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span></span></span></span></span> の <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">O</mi><mo stretchy="false">(</mo><mi>d</mi><mo>⋅</mo><mi>k</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mathcal{O}(d \cdot k)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathcal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord mathnormal">d</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="mclose">)</span></span></span></span></span> から、低ランク部分のみの <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">O</mi><mo stretchy="false">(</mo><mi>r</mi><mo>⋅</mo><mo stretchy="false">(</mo><mi>d</mi><mo>+</mo><mi>k</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mathcal{O}(r \cdot (d + k))</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathcal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathnormal">d</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="mclose">))</span></span></span></span></span> に大幅に削減されます。</p><ul><li><p>Transformerのパラメータ数 <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>512</mn><mo>×</mo><mn>64</mn><mo>=</mo><mn>32768</mn></mrow><annotation encoding="application/x-tex">512 \times 64 = 32768</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">512</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">64</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">32768</span></span></span></span></span></p></li><li><p>LoRAのパラメータ数<span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mo>×</mo><mi>r</mi><mo>=</mo><mn>512</mn><mo>×</mo><mn>8</mn><mo>=</mo><mn>4096</mn><mo stretchy="false">(</mo><mi>r</mi><mo>=</mo><mn>8</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">d \times r = 512 \times 8 = 4096 (r=8)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7778em;vertical-align:-0.0833em;"></span><span class="mord mathnormal">d</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">512</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">8</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">4096</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">8</span><span class="mclose">)</span></span></span></span></span></p><ul><li><p><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>86</mn><mi mathvariant="normal">%</mi></mrow><annotation encoding="application/x-tex">86 \%</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8056em;vertical-align:-0.0556em;"></span><span class="mord">86%</span></span></span></span></span>削減</p></li></ul></li></ul></div><div id="g17HsTbeOe" class="myst-jp-nb-block relative group/block"><img id="uISRZlcD3k" style="margin-left:auto;margin-right:auto" src="/build/lora_overview-e9c3f7498b7537cab5fa8931814013ba.png" data-canonical-url="./Figure/lora_overview.png" class=""/></div><div id="LxdTRR3NgX" class="myst-jp-nb-block relative group/block"><img id="ntyLb7LAAK" style="margin-left:auto;margin-right:auto" src="/build/lora_calculation-e4c86dc99c5b589495059f01187bc3ee.png" data-canonical-url="./Figure/lora_calculation.png" class=""/></div><div id="ngMvzb2paW" class="myst-jp-nb-block relative group/block"><img id="s4OGY9oriN" style="margin-left:auto;margin-right:auto" src="/build/lora_performance-3d4cd142d775bfcd32a7c9a5cab6a81a.png" data-canonical-url="./Figure/lora_performance.png" class=""/></div><div class="myst-backmatter-parts"></div><div class="myst-footer-links flex pt-10 mb-10 space-x-4"><a class="myst-footer-link flex-1 block p-4 font-normal text-gray-600 no-underline border border-gray-200 rounded shadow-sm group hover:border-blue-600 dark:hover:border-blue-400 hover:text-blue-600 dark:hover:text-blue-400 dark:text-gray-100 dark:border-gray-500 hover:shadow-lg dark:shadow-neutral-700 myst-footer-link-prev" href="/gpt"><div class="flex h-full align-middle"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="1.5rem" height="1.5rem" class="myst-footer-link-icon self-center transition-transform group-hover:-translate-x-1 shrink-0"><path stroke-linecap="round" stroke-linejoin="round" d="M10.5 19.5 3 12m0 0 7.5-7.5M3 12h18"></path></svg><div class="flex-grow text-right"><div class="myst-footer-link-group text-xs text-gray-500 dark:text-gray-400">大規模言語モデル</div>GPT</div></div></a><a class="myst-footer-link flex-1 block p-4 font-normal text-gray-600 no-underline border border-gray-200 rounded shadow-sm group hover:border-blue-600 dark:hover:border-blue-400 hover:text-blue-600 dark:hover:text-blue-400 dark:text-gray-100 dark:border-gray-500 hover:shadow-lg dark:shadow-neutral-700 myst-footer-link-next" href="/langchain-basic"><div class="flex h-full align-middle"><div class="flex-grow"><div class="myst-footer-link-group text-xs text-gray-500 dark:text-gray-400">大規模言語モデル</div>LLMSの応用(1)：Langchainの基礎</div><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="1.5rem" height="1.5rem" class="myst-footer-link-icon self-center transition-transform group-hover:translate-x-1 shrink-0"><path stroke-linecap="round" stroke-linejoin="round" d="M13.5 4.5 21 12m0 0-7.5 7.5M21 12H3"></path></svg></div></a></div></article></main><script>((a,l)=>{if(!window.history.state||!window.history.state.key){let u=Math.random().toString(32).slice(2);window.history.replaceState({key:u},"")}try{let d=JSON.parse(sessionStorage.getItem(a)||"{}")[l||window.history.state.key];typeof d=="number"&&window.scrollTo(0,d)}catch(u){console.error(u),sessionStorage.removeItem(a)}})("positions", null)</script><link rel="modulepreload" href="/build/entry.client-PCJPW7TK.js"/><link rel="modulepreload" href="/build/_shared/chunk-AQ2CODAG.js"/><link rel="modulepreload" href="/build/_shared/chunk-JJXTQVMA.js"/><link rel="modulepreload" href="/build/_shared/chunk-OZE3FFNP.js"/><link rel="modulepreload" href="/build/_shared/chunk-OYMW4E3D.js"/><link rel="modulepreload" href="/build/_shared/chunk-C4DFGG5C.js"/><link rel="modulepreload" href="/build/_shared/chunk-J7TUH54J.js"/><link rel="modulepreload" href="/build/_shared/chunk-FZ2S7OYD.js"/><link rel="modulepreload" href="/build/_shared/chunk-JEM6JXYA.js"/><link rel="modulepreload" href="/build/_shared/chunk-34XIY2DH.js"/><link rel="modulepreload" href="/build/_shared/chunk-KQM5FBHR.js"/><link rel="modulepreload" href="/build/_shared/chunk-OCWQY3HK.js"/><link rel="modulepreload" href="/build/_shared/chunk-7HNKBP4B.js"/><link rel="modulepreload" href="/build/_shared/chunk-CUKUDK3R.js"/><link rel="modulepreload" href="/build/_shared/chunk-3EBOCCHJ.js"/><link rel="modulepreload" href="/build/_shared/chunk-O4VQNZ62.js"/><link rel="modulepreload" href="/build/_shared/chunk-4OEDG4JQ.js"/><link rel="modulepreload" href="/build/_shared/chunk-GUCIBHGO.js"/><link rel="modulepreload" href="/build/root-CXYA7X5D.js"/><link rel="modulepreload" href="/build/_shared/chunk-DATP5P2X.js"/><link rel="modulepreload" href="/build/routes/$-JRBPULBO.js"/><script>window.__remixContext = {"url":"/llm","state":{"loaderData":{"root":{"config":{"version":3,"myst":"1.7.0","options":{"favicon":"/build/tohoku-university-lo-68dede7d9dcb965aff25e8429e755296.svg","logo":"/build/tohoku-university-lo-68dede7d9dcb965aff25e8429e755296.svg","analytics_google":"G-85RFPTYEE3"},"nav":[],"actions":[],"projects":[{"numbering":{"heading_1":{"template":"enabled","enabled":true},"heading_2":{"template":"enabled","enabled":true},"heading_3":{"template":"enabled","enabled":true},"heading_4":{"template":"enabled","enabled":true},"heading_5":{"template":"enabled","enabled":true},"heading_6":{"template":"enabled","enabled":true}},"title":"計算社会科学と自然言語処理","github":"https://github.com/lvzeyu/css_nlp","id":"356ed62a-cf04-49ae-b7bc-d9f79f2ef8bb","exports":[],"bibliography":[],"index":"intro","pages":[{"title":"イントロダクション","level":1},{"slug":"introduction","title":"ガイダンス","description":"このページで、行動科学演習(LB63310)と計算人文社会学研究演習Ⅱ (LM23309)の授業資料を公開しています。","date":"","thumbnail":"/build/NLP_history-80702ee1d37040856a480174f5bcbbfc.png","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"title":"基礎知識","level":1},{"slug":"nlp-basis2","title":"自然言語処理の基礎","description":"","date":"","thumbnail":"/build/7e2db436150c38a00650f96925aa5581.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"ml-basis2","title":"機械学習の基本概念","description":"","date":"","thumbnail":"/build/supervised-learning-b5be7f5e73313e60f2c06d545c226553.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"math-basis2","title":"数学基礎","description":"","date":"","thumbnail":"/build/7e2db436150c38a00650f96925aa5581.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"title":"ニューラルネットワーク","level":1},{"slug":"nn2","title":"ニューラルネットワーク","description":"","date":"","thumbnail":"/build/7e2db436150c38a00650f96925aa5581.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"backpropagation","title":"誤差逆伝播法","description":"","date":"","thumbnail":"/build/7e2db436150c38a00650f96925aa5581.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"sgd","title":"学習に関するテクニック","description":"","date":"","thumbnail":"/build/7e2db436150c38a00650f96925aa5581.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"title":"PyTorch","level":1},{"slug":"pytorch2","title":"Pytorch","description":"","date":"","thumbnail":"/build/7e2db436150c38a00650f96925aa5581.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"title":"単語分散表現","level":1},{"slug":"word2vec-1","title":"単語分散表現","description":"","date":"","thumbnail":"/build/7e2db436150c38a00650f96925aa5581.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"word2vec-2-embedding","title":"word2vec","description":"","date":"","thumbnail":"/build/7e2db436150c38a00650f96925aa5581.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"word2vec-gensim","title":"GensimによるWord2Vecの学習と使用","description":"","date":"","thumbnail":"/build/7e2db436150c38a00650f96925aa5581.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"word2vec-application","title":"Word2Vecが人文・社会科学研究における応用","description":"","date":"","thumbnail":"/build/7e2db436150c38a00650f96925aa5581.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"title":"RNN","level":1},{"slug":"rnn","title":"RNNの基礎","description":"","date":"","thumbnail":"/build/rnn-4dbf33ec81abf2baa6d5bbe97bb61683.png","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"lstm","title":"LSTM","description":"","date":"","thumbnail":"/build/lstm1-4ea8525c2f72b4f2ff8298a9f7b9c0be.png","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"pytorch-lstm","title":"LSTMの実装","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"lstm-classification","title":"LSTMによる文書分類","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"seq2seq","title":"Seq2seq","description":"","date":"","thumbnail":"/build/7e2db436150c38a00650f96925aa5581.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"title":"Transformer","level":1},{"slug":"attention","title":"Attention","description":"","date":"","thumbnail":"/build/attention-eed7f8fafc28c8cb6bd5276bbd04714b.png","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"self-attention","title":"Self-Attention","description":"","date":"","thumbnail":"/build/7e2db436150c38a00650f96925aa5581.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"transformer","title":"Transformerアーキテクチャ","description":"","date":"","thumbnail":"/build/transformer-493148401fbb971a745b29ab5b44e10f.png","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"bert","title":"BERT","description":"","date":"","thumbnail":"/build/7e2db436150c38a00650f96925aa5581.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"bert-sentiment","title":"BERTによるセンチメント分析","description":"","date":"","thumbnail":"/build/7e2db436150c38a00650f96925aa5581.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"bert-topic","title":"BERTopic","description":"","date":"","thumbnail":"/build/7e2db436150c38a00650f96925aa5581.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"title":"大規模言語モデル","level":1},{"slug":"gpt","title":"GPT","description":"","date":"","thumbnail":"/build/gpt_history-c08ab6ce749d3aed300b023e318cb96c.png","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"llm","title":"大規模言語モデルの基本","description":"","date":"","thumbnail":"/build/LLMs_parameter-ec157a4028abc601fb429210b7f4b07c.png","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"langchain-basic","title":"LLMSの応用(1)：Langchainの基礎","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2}]}]},"CONTENT_CDN_PORT":"3100","MODE":"static"},"routes/$":{"config":{"version":3,"myst":"1.7.0","options":{"favicon":"/build/tohoku-university-lo-68dede7d9dcb965aff25e8429e755296.svg","logo":"/build/tohoku-university-lo-68dede7d9dcb965aff25e8429e755296.svg","analytics_google":"G-85RFPTYEE3"},"nav":[],"actions":[],"projects":[{"numbering":{"heading_1":{"template":"enabled","enabled":true},"heading_2":{"template":"enabled","enabled":true},"heading_3":{"template":"enabled","enabled":true},"heading_4":{"template":"enabled","enabled":true},"heading_5":{"template":"enabled","enabled":true},"heading_6":{"template":"enabled","enabled":true}},"title":"計算社会科学と自然言語処理","github":"https://github.com/lvzeyu/css_nlp","id":"356ed62a-cf04-49ae-b7bc-d9f79f2ef8bb","exports":[],"bibliography":[],"index":"intro","pages":[{"title":"イントロダクション","level":1},{"slug":"introduction","title":"ガイダンス","description":"このページで、行動科学演習(LB63310)と計算人文社会学研究演習Ⅱ (LM23309)の授業資料を公開しています。","date":"","thumbnail":"/build/NLP_history-80702ee1d37040856a480174f5bcbbfc.png","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"title":"基礎知識","level":1},{"slug":"nlp-basis2","title":"自然言語処理の基礎","description":"","date":"","thumbnail":"/build/7e2db436150c38a00650f96925aa5581.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"ml-basis2","title":"機械学習の基本概念","description":"","date":"","thumbnail":"/build/supervised-learning-b5be7f5e73313e60f2c06d545c226553.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"math-basis2","title":"数学基礎","description":"","date":"","thumbnail":"/build/7e2db436150c38a00650f96925aa5581.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"title":"ニューラルネットワーク","level":1},{"slug":"nn2","title":"ニューラルネットワーク","description":"","date":"","thumbnail":"/build/7e2db436150c38a00650f96925aa5581.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"backpropagation","title":"誤差逆伝播法","description":"","date":"","thumbnail":"/build/7e2db436150c38a00650f96925aa5581.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"sgd","title":"学習に関するテクニック","description":"","date":"","thumbnail":"/build/7e2db436150c38a00650f96925aa5581.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"title":"PyTorch","level":1},{"slug":"pytorch2","title":"Pytorch","description":"","date":"","thumbnail":"/build/7e2db436150c38a00650f96925aa5581.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"title":"単語分散表現","level":1},{"slug":"word2vec-1","title":"単語分散表現","description":"","date":"","thumbnail":"/build/7e2db436150c38a00650f96925aa5581.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"word2vec-2-embedding","title":"word2vec","description":"","date":"","thumbnail":"/build/7e2db436150c38a00650f96925aa5581.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"word2vec-gensim","title":"GensimによるWord2Vecの学習と使用","description":"","date":"","thumbnail":"/build/7e2db436150c38a00650f96925aa5581.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"word2vec-application","title":"Word2Vecが人文・社会科学研究における応用","description":"","date":"","thumbnail":"/build/7e2db436150c38a00650f96925aa5581.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"title":"RNN","level":1},{"slug":"rnn","title":"RNNの基礎","description":"","date":"","thumbnail":"/build/rnn-4dbf33ec81abf2baa6d5bbe97bb61683.png","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"lstm","title":"LSTM","description":"","date":"","thumbnail":"/build/lstm1-4ea8525c2f72b4f2ff8298a9f7b9c0be.png","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"pytorch-lstm","title":"LSTMの実装","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"lstm-classification","title":"LSTMによる文書分類","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"seq2seq","title":"Seq2seq","description":"","date":"","thumbnail":"/build/7e2db436150c38a00650f96925aa5581.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"title":"Transformer","level":1},{"slug":"attention","title":"Attention","description":"","date":"","thumbnail":"/build/attention-eed7f8fafc28c8cb6bd5276bbd04714b.png","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"self-attention","title":"Self-Attention","description":"","date":"","thumbnail":"/build/7e2db436150c38a00650f96925aa5581.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"transformer","title":"Transformerアーキテクチャ","description":"","date":"","thumbnail":"/build/transformer-493148401fbb971a745b29ab5b44e10f.png","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"bert","title":"BERT","description":"","date":"","thumbnail":"/build/7e2db436150c38a00650f96925aa5581.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"bert-sentiment","title":"BERTによるセンチメント分析","description":"","date":"","thumbnail":"/build/7e2db436150c38a00650f96925aa5581.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"bert-topic","title":"BERTopic","description":"","date":"","thumbnail":"/build/7e2db436150c38a00650f96925aa5581.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"title":"大規模言語モデル","level":1},{"slug":"gpt","title":"GPT","description":"","date":"","thumbnail":"/build/gpt_history-c08ab6ce749d3aed300b023e318cb96c.png","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"llm","title":"大規模言語モデルの基本","description":"","date":"","thumbnail":"/build/LLMs_parameter-ec157a4028abc601fb429210b7f4b07c.png","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"langchain-basic","title":"LLMSの応用(1)：Langchainの基礎","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2}]}]},"page":{"version":3,"kind":"Notebook","sha256":"3b675c7742d3b78f7f4761f1f69acd326a140448e28558ae6e158d2f99538a58","slug":"llm","location":"/notebook/llm.ipynb","dependencies":[],"frontmatter":{"title":"大規模言語モデルの基本","content_includes_title":false,"kernelspec":{"name":"python3","display_name":"jupyterbook","language":"python"},"github":"https://github.com/lvzeyu/css_nlp","numbering":{"heading_1":{"enabled":true,"template":"enabled"},"heading_2":{"enabled":true,"template":"enabled"},"heading_3":{"enabled":true,"template":"enabled"},"heading_4":{"enabled":true,"template":"enabled"},"heading_5":{"enabled":true,"template":"enabled"},"heading_6":{"enabled":true,"template":"enabled"},"title":{"offset":1}},"source_url":"https://github.com/lvzeyu/css_nlp/blob/master/notebook/llm.ipynb","edit_url":"https://github.com/lvzeyu/css_nlp/edit/master/notebook/llm.ipynb","thumbnail":"/build/LLMs_parameter-ec157a4028abc601fb429210b7f4b07c.png","exports":[{"format":"ipynb","filename":"llm.ipynb","url":"/build/llm-83b731980afacb372a53c7f294eda9c0.ipynb"}]},"widgets":{},"mdast":{"type":"root","children":[{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"近年、GPTをはじめとする大規模言語モデルが自然言語処理分野において顕著な成果を挙げ、文章の生成や論理推論など高度な課題においても高い性能を示している。それに伴う、大規模言語モデルを活用することでより効率的に人間の行動や社会現象に関する理論や仮説の検証と発展し、新たな可能性をもたらすと期待されています。","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"YvRoiEpTis"}],"key":"rPrgQHGeqR"},{"type":"paragraph","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"今まで説明した通り、文脈を考慮した単語埋め込みである文脈化単語埋め込み(contextualized word embedding)を計算するTransformerを大規模コーパスで自己教師あり学習(Masked LMやNext sentence prediction)で事前学習し、そのモデルを下流タスクのテータセットを使って微調整する方法は、自然言語処理でよく用いられる手法です。","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"lmkOPGc2sS"}],"key":"DRNXPIa8Ml"},{"type":"paragraph","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"text","value":"こうした事前学習した大規模なニューラルネットワークは、大規模言語モデルのように呼ばれます。","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"dO79r6B605"}],"key":"OVfb4ciI69"},{"type":"paragraph","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"text","value":"特に、2020年以降にパラメータ数およびテキストデータをさらに大規模することで、大規模言語モデルの性能も飛躍的に向上し、人間と自然にやりとりできるような能力を身につけました。これによって、ファインチューニングは行なわずに、事前学習された大規模言語モデルをプロンプト(prompt)と呼ばれるテキストを通じて制御することで下流タスク方法も一般的になりつつあります。","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"q6PHMIMS2A"}],"key":"SVOL5mEdmU"},{"type":"paragraph","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"text","value":"ここでは、大規模言語モデルの近年(2023年まで)の進展について解説します。進展が激しい分野のため、ここでの解説に基づいて適宜最新の情報も各自調べてほしいです。","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"UDbeSwUptM"}],"key":"KqKFLjd47J"}],"key":"Pc5oQqogFc"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"モデルの大規模化とその効果","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"EOP4dcMrg1"}],"identifier":"id","label":"モデルの大規模化とその効果","html_id":"id","implicit":true,"enumerator":"1","key":"h1AQGuAs2L"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"大規模言語モデルの開発が進むにつれて、モデルに含まれるパラメータ数が飛躍的に増加してきています。","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"W2WariUXwy"}],"key":"U549dy8brD"},{"type":"paragraph","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"2018年に発表されたBERTは3.4億個、2019年のGPT-2では15億個だったパラメータ数が2020年のGPT-3では1750億、そしてGPT-4はは100兆個のパラメータを持つと言われており、加速的に増加していることがわかります。","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"mUgvKqKSe7"}],"key":"CKEUpOHiPs"}],"key":"NQYcXVEkTS"},{"type":"block","kind":"notebook-content","children":[{"type":"image","url":"/build/LLMs_parameter-ec157a4028abc601fb429210b7f4b07c.png","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"KNlwRGHfcZ","urlSource":"./Figure/LLMs_parameter.png"},{"type":"image","url":"/build/gpt_para-66d806b5b79617ddbf36df25e9e6fef1.png","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"vnvuYjnLRU","urlSource":"./Figure/gpt_para.png"}],"key":"JAofIklR5T"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"こうした大規模化が行われている背景には、モデルの規模を大きくすることで性能が比例して改善していくという経験的な法則であるスケール則があります。","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"gIR44XtklU"}],"key":"XhBUvhptJt"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"さらに、大規模言語モデルが一定の規模を超えると、タスクの性能が飛躍的に向上する現象も報告されています。こうした大規模化することで性能が改善し獲得される能力を創発的能力(emergent abilities)と呼ぶことがあります。","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"y4t2kioFVw"}],"key":"ebI8HrLdXh"},{"type":"aside","children":[{"type":"paragraph","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"text","value":"一方、大規模言語モデルが創発的能力を持つことに疑問視している声もあります。","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"awVLpex6J2"}],"key":"A9mNXxIqyp"}],"key":"hsBAceSRJO"},{"type":"image","url":"/build/emergent-d2694b9351ce41c7dfd2034adf2cb385.png","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"cYDuJj13o1","urlSource":"./Figure/emergent.png"}],"key":"pn12GHUZns"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Prompting","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"kwxL2IXRXc"}],"identifier":"prompting","label":"Prompting","html_id":"prompting","implicit":true,"enumerator":"2","key":"JqH9y0vWOp"},{"type":"heading","depth":3,"position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"文脈内学習","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"BiICJJ4smx"}],"identifier":"id","label":"文脈内学習","html_id":"id-1","implicit":true,"enumerator":"2.1","key":"JEHTweszX8"}],"key":"Lq7gXQhUXM"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"text","value":"GPTのようなパラメータが非常に大きいモデルでは、学習時に入力される文章内には、様々なサブタスクが埋め込まれると考えられます。こうした文章の生成を学習することで、内包される様々な言語タスクへの処理能力の獲得が期待できます。\nこのアプローチはGPT-2から用いられており、文脈内学習(in-context learning)と呼ばれます。","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"rlpv40qKpp"}],"key":"g7TWR7By4l"},{"type":"aside","children":[{"type":"paragraph","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"詳細はこちらの","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"pTbjRBpe2j"},{"type":"link","url":"https://arxiv.org/abs/2005.14165","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"論文:Language Models are Few-Shot Learners","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"SmsgHnE4XA"}],"urlSource":"https://arxiv.org/abs/2005.14165","key":"K9xedAatBQ"},{"type":"text","value":"を参照してください。","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"t4PWIkLgX3"}],"key":"APBNQdDOBB"}],"key":"kBbJAYC7zp"},{"type":"image","url":"/build/meta_learning-0d9335f3b146c91ca09078f39fc34491.png","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"NgF8Dx12Na","urlSource":"./Figure/meta_learning.png"},{"type":"paragraph","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"text","value":"モデルは新しいタスクをこなすための追加のトレーニングデータなしに（zero-shot learning）、または非常に少量のデータで（few-shot learning）多様なタスクを達成することが可能になります。","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"I7qouloGX2"}],"key":"RiLBtueoja"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":14,"column":1},"end":{"line":17,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Few-shot learning: 推論時に少数（10から100）のデモンストレーションを与えます","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"key":"XuFrIZit0G"}],"key":"ovxliaXHsD"}],"key":"UGEvqwts0Z"},{"type":"listItem","spread":true,"position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"One-shot learning: 推論時に一つのデモンストレーションを与えます","position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"key":"j50k6BVCsC"}],"key":"FxENiQbF5r"}],"key":"xg3tqHWXbG"},{"type":"listItem","spread":true,"position":{"start":{"line":16,"column":1},"end":{"line":17,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Zero-shot learning: 推論時にデモンストレーションは与えられず、自然言語によるタスク指示のみが与えられます","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"key":"OaQ0zdcQmx"}],"key":"BkjlOfDx8K"}],"key":"gMGdm5Pn6D"}],"key":"pzqL6DyOhq"},{"type":"image","url":"/build/shot-learning-ae6fca156f60571e643b613d4db391d0.png","position":{"start":{"line":18,"column":1},"end":{"line":18,"column":1}},"key":"BsJMjwIuan","urlSource":"./Figure/shot-learning.png"},{"type":"paragraph","position":{"start":{"line":22,"column":1},"end":{"line":22,"column":1}},"children":[{"type":"text","value":"このように、従来ファインチューニングが必要される多くのタスクは、モデルにプロンプト(prompt)と呼ばれるテキストを入力して後続けするテキストを予測するという形で解かせることが知られています。","position":{"start":{"line":22,"column":1},"end":{"line":22,"column":1}},"key":"KK9OvmbiWd"}],"key":"rKXZht4P25"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":24,"column":1},"end":{"line":27,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":24,"column":1},"end":{"line":25,"column":1}},"children":[{"type":"paragraph","position":{"start":{"line":24,"column":1},"end":{"line":24,"column":1}},"children":[{"type":"text","value":"基本的には、タスクの説明と多くのデモンストレーションを与えることでモデルの性能が向上していく傾向が見られます。","position":{"start":{"line":24,"column":1},"end":{"line":24,"column":1}},"key":"cP6h069DXP"}],"key":"QfoYhUo9cZ"}],"key":"nu0KbqA6JT"},{"type":"listItem","spread":true,"position":{"start":{"line":26,"column":1},"end":{"line":27,"column":1}},"children":[{"type":"paragraph","position":{"start":{"line":26,"column":1},"end":{"line":26,"column":1}},"children":[{"type":"text","value":"zero, one-shotでも悪くない性能、few-shotでは一部ファインチューニングを用いたSOTAモデルに匹敵する性能を得ることが確認できました。","position":{"start":{"line":26,"column":1},"end":{"line":26,"column":1}},"key":"eRAW6ege3o"}],"key":"OQsdDWEnKR"}],"key":"Hag6lwQUqA"}],"key":"sUg7EsvfD6"},{"type":"image","url":"/build/in-context-200c05c5ac8b27c8efc40deafa5de088.png","position":{"start":{"line":28,"column":1},"end":{"line":28,"column":1}},"key":"iffpVr4aQC","urlSource":"./Figure/in-context.png"},{"type":"paragraph","position":{"start":{"line":31,"column":1},"end":{"line":31,"column":1}},"children":[{"type":"text","value":"現在、一般的には、few-shot learningモデルの性能がファインチューニングによる教師あり学習モデルを超えることはまだ少ないが、ラベル付きデータの生成に費やすコストと時間を効率すると、プロンプトはファインチューニングによる教師あり学習の効率的な代替手段と考えられます。","position":{"start":{"line":31,"column":1},"end":{"line":31,"column":1}},"key":"Q88rKUJIKc"}],"key":"gGurDh4Ms5"},{"type":"image","url":"/build/shot-compare-31e1df16c91bc3dd7e1e5176a2d87a4a.png","position":{"start":{"line":34,"column":1},"end":{"line":34,"column":1}},"key":"pKbJpQ92wF","urlSource":"./Figure/shot-compare.png"}],"key":"T7dB21D2Qw"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"chain-of-thought推論","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"J9VSdIgJZf"}],"identifier":"chain-of-thought","label":"chain-of-thought推論","html_id":"chain-of-thought","implicit":true,"enumerator":"2.2","key":"ps8bobFQMQ"},{"type":"aside","children":[{"type":"paragraph","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"詳細はこちらの","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"uduX5amCu3"},{"type":"link","url":"https://arxiv.org/abs/2201.11903","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"論文:Chain-of-Thought Prompting Elicits Reasoning in Large Language Models","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"otLfkaDKDJ"}],"urlSource":"https://arxiv.org/abs/2201.11903","key":"GBMjmeYSIc"},{"type":"text","value":"を参照してください。","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"UzFDq7ctzy"}],"key":"J5JPRCDavh"}],"key":"JzQWetWQdM"},{"type":"paragraph","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"text","value":"大規模言語モデルが苦手とされるタスクの一つに他段階の推論が必要となるマルチステップ推論(multi-step reasoning)があります。","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"key":"NRVYL7jMIN"}],"key":"enizo6XiFK"},{"type":"paragraph","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"children":[{"type":"text","value":"複数の段階の推論が必要な際に、推論過程の例示を与えるchain-of-thought推論(chain-of-thought reasoning)を用いることで性能が改善することが報告されています。","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"IQpQKYhFdB"}],"key":"alWCzHrMr0"},{"type":"paragraph","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"children":[{"type":"text","value":"具体的言えば、chain-of-thought推論では、回答を加えて推論過程を示す例示を与えて、モデルが回答を行う際に推論過程を含めて出力テキストを生成するようにします。","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"hZWDVeURT5"}],"key":"BBFPMonXuj"},{"type":"image","url":"/build/chains_of_thought-6af15225868a72490f05a0a1c638f322.png","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"key":"REuo6qzNnZ","urlSource":"./Figure/chains_of_thought.png"}],"key":"inEcI0SJcN"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"さらに、chain-of-thought推論の推論過程を人間が与えるのではなく、推論過程の生成を促す「Let’s think step by step」のような文字列をプロンプトの末尾に追加して、推論過程を生成されてから回答を抽出するzero-shot chain-of-thought reasoningも提案されています。","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"qE78gOKUZf"}],"key":"WV0d9lh05A"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"この方法を使うと、プロンプトを書き換えるだけで推論の性能を改善することができます。このことから、大規模言語モデルを使うあたってはプロンプトの与え方を工夫することが重要であることがわかります。","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"jhVUYIOfHY"}],"key":"AVR77H0IK8"},{"type":"image","url":"/build/zero_CoT-2e1d99436695654f74540cb26ffc846a.png","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"ZsmzCA0ax0","urlSource":"./Figure/zero_CoT.png"},{"type":"aside","children":[{"type":"paragraph","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"text","value":"詳細はこちらの","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"iuo64xUCkJ"},{"type":"link","url":"https://arxiv.org/abs/2205.11916","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"text","value":"論文:Large Language Models are Zero-Shot Reasoners","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"InGJjxXFzM"}],"urlSource":"https://arxiv.org/abs/2205.11916","key":"ss1l6acE5E"},{"type":"text","value":"を参照してください。","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"pd1luXEKJz"}],"key":"H7OPfebwch"}],"key":"AdzHj5MNG6"}],"key":"uEs8N9Yads"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"指示チューニング","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Dq7fxeSxJ7"}],"identifier":"id","label":"指示チューニング","html_id":"id-2","implicit":true,"enumerator":"3","key":"LNtah9CDGr"}],"key":"IZzFHeAfOc"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"通常の事前学習では、モデルは膨大なデータセットから次の単語を予測する能力を学習します。しかし、そのままでは具体的なタスクを効率的にこなすことは難しい場合があります。","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"uM8X8geqyl"}],"key":"SeDUVh1hCa"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"指示チューニング（instruction tuning）とは、様々なタスクのデータを指示と理想的な回答の組で構成されるデータセットを言語モデルに与え追加学習させることで、言語モデルの性能を向上させる技術です。","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"OobiZBXElL"}],"key":"oYEpBx3BFR"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":5,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"指示チューニングデータセットの構築","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"v6zHkXzgTr"}],"key":"R4zjaCAaXg"}],"key":"wnD12AHoWO"},{"type":"listItem","spread":true,"position":{"start":{"line":6,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"ファインチューニング: 指示チューニングデータセットを用いて、既存のLLMを再トレーニングします。これにより、モデルが指示に応じた出力を生成する能力が向上します。","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"e2QmUFaZqZ"}],"key":"gdvfGVkYVW"}],"key":"l2io1gTSE7"}],"key":"Tl4n4OQFBO"},{"type":"image","url":"/build/instrust-example-5c2e99563d08457de4497eb7533aaddb.png","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"key":"ekkQwJvQ10","urlSource":"./Figure/instrust-example.png"},{"type":"paragraph","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"text","value":"Flan(Finetuned Lanaguage Net)では、大規模言語モデルをデータセットを集約してフィインチューニングした結果、多数のタスクにおけるzero-shot学習の性能が向上したことが報告されています。","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"s5KdxtW7F4"}],"key":"x3NjIQdfus"},{"type":"paragraph","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"link","url":"https://huggingface.co/google/flan-t5-xxl?text=The+square+root+of+x+is+the+cube+root+of+y.+What+is+y+to+the+power+of+2%2C+if+x+%3D+4%3F","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"text","value":"「Flan-T5」","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"szSnr5kxgk"}],"urlSource":"https://huggingface.co/google/flan-t5-xxl?text=The+square+root+of+x+is+the+cube+root+of+y.+What+is+y+to+the+power+of+2%2C+if+x+%3D+4%3F","key":"kvlyfXW5fH"},{"type":"text","value":"は、Google AI の新しいオープンソース言語モデルです。1,800 以上の言語タスクでファインチューニングされており、プロンプトとマルチステップの推論能力が劇的に向上しています。","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"JHWcDL2yFe"}],"key":"lY5eNf6GxO"},{"type":"image","url":"/build/flan-1-7a505e0e60ebed20f22312c621ec2af9.png","position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"key":"LZG6XvY00P","urlSource":"./Figure/flan-1.png"}],"key":"pvuOt4Ho3f"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"人間のフィードバックからの強化学習","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"luhMhmdn4a"}],"identifier":"id","label":"人間のフィードバックからの強化学習","html_id":"id-3","implicit":true,"enumerator":"4","key":"gRpR8VeZOM"}],"key":"HuoNoo0tKc"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"RLHF（Reinforcement Learning from Human Feedback）とは、「人間のフィードバックからの強化学習」という名前の通り、人間の価値基準に沿うように、人間のフィードバックを使ってAI（言語）モデルを強化学習で微調整（ファインチューニング）する手法である。","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"h9RouRVDpy"}],"key":"caPm9uoSku"},{"type":"heading","depth":3,"position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"強化学習","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"uTPvF3j5K5"}],"identifier":"id","label":"強化学習","html_id":"id-4","implicit":true,"enumerator":"4.1","key":"fTf9Mby7T7"},{"type":"paragraph","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"強化学習 (Reinforcement Learning) とは、機械学習の一種であり、エージェントが動的環境と、繰り返し試行錯誤のやりとりを重ねることによってタスクを実行できるようになる手法です。この学習手法により、エージェントは、タスクの報酬を最大化する一連の意思決定を行うことができます。教師付き学習とよく似た問題設定ですが、与えられた正解の出力をそのまま学習すれば良いわけではなく、もっと広い意味での「価値」を最大化する行動を学習しなければなりません。","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"ywBOgYuEL5"}],"key":"wvdN8JtsrH"},{"type":"paragraph","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"text","value":"例えば、犬を訓練すること例として考えてください。強化学習の用語を使用すると、この場合の学習の目的は、犬 (エージェント) のしつけ (学習) を行い、ある環境の中でタスクを完了させることです。これには、犬の周囲の環境や訓練士が含まれます。","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"IMgwCUHurm"}],"key":"mY5cHv7zks"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":9,"column":1},"end":{"line":14,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":9,"column":1},"end":{"line":10,"column":1}},"children":[{"type":"paragraph","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"text","value":"まず、訓練士が命令や合図を出し、それを犬が観察 (観測) します。続いて、犬は行動を起こすことで反応します。犬の行動が目的の行動に近い場合、訓練士は、おやつやおもちゃなどのごほうび (報酬) を与えますが、それ以外の場合、ごほうびは与えません。","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"fFVZp3uCXN"}],"key":"VKZPg7h5Ml"}],"key":"tIMjaNdb9b"},{"type":"listItem","spread":true,"position":{"start":{"line":11,"column":1},"end":{"line":12,"column":1}},"children":[{"type":"paragraph","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"text","value":"しつけ (学習) を始めたばかりの頃は、犬はランダムな行動を取る傾向にあります。犬は観測した特定の状況を行動やごほうび (報酬) と関連付けようとするため、与えられた指示が「おすわり」であっても、ローリングなど別の行動を取る場合があります。","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"KvptEbf5VW"}],"key":"nXykqFc1jh"}],"key":"zUQydNObo4"},{"type":"listItem","spread":true,"position":{"start":{"line":13,"column":1},"end":{"line":14,"column":1}},"children":[{"type":"paragraph","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"text","value":"犬の立場から見ると、すべての合図に正しく反応して、おやつをできるだけ多くもらえるような状況が理想的です。","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"xKZz5lxO4b"}],"key":"gZSdPpenLh"}],"key":"otkTC0y2qN"}],"key":"GtP728JzdM"},{"type":"paragraph","position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"children":[{"type":"text","value":"強化学習のしつけ (学習) とは、犬が何らかのごぼうび (報酬) を最大化する理想的な行動を学習するように、犬の方策を「調整」することを指します。学習が完了すると、犬は飼い主を観察し、獲得した方策によって、その場にふさわしい行動 (「おすわり」と命令されたらおすわりをするなど) が取れるようになります。","position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"key":"Fliz7XAbJB"}],"key":"F3V0r3ICWl"},{"type":"paragraph","position":{"start":{"line":17,"column":1},"end":{"line":17,"column":1}},"children":[{"type":"text","value":"まとめると、強化学習の目的は、与えられた「環境」における価値（あるいは「利益」と呼びます）を最大化するように「エージェント」を学習させます。","position":{"start":{"line":17,"column":1},"end":{"line":17,"column":1}},"key":"g73yXAIB46"}],"key":"jZdaUYELzW"}],"key":"L7kSTKY4Dl"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"RLHF（Reinforcement Learning from Human Feedback）","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"tjx4myBqSb"}],"identifier":"rlhf-reinforcement-learning-from-human-feedback","label":"RLHF（Reinforcement Learning from Human Feedback）","html_id":"rlhf-reinforcement-learning-from-human-feedback","implicit":true,"enumerator":"5","key":"B1ZHlD3kez"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"RLHFの役割は、人間の好みや意図といった「人間の価値基準」がAIモデルに反映されることになります。具体的には、あるプロンプトに対してAIが生成した応答文の良し悪しを人間がランク付けし、そのランク付されたデータセットを使って「より望ましい応答文とはどんな感じの文章なのか」を評価できる報酬モデルを作成するわけです。","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"i98QQFpRED"}],"key":"uj9cbyWfSU"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":5,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"教師あり学習で既存の言語モデルをfine-tuning","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"TKONjxPdc4"}],"key":"wyPpmNfWc5"}],"key":"CmnBonuwz0"},{"type":"listItem","spread":true,"position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"指示チューニング済みモデルが出力したテキストに対して人手で優劣に順位付けします","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"yWrd1nOYvt"}],"key":"s8FZIk56zr"}],"key":"x5baTJe420"},{"type":"listItem","spread":true,"position":{"start":{"line":7,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"このデータセットを使って、報酬(テキストの優劣を反映したスカラー値)を予測する報酬モデルを学習します","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"bhcd94Imjb"}],"key":"MVeBgIXQLQ"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":8,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":8,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"報酬モデルをこのように学習させることで、似たようなプロンプトが与えられた時に、より望ましいと評価された応答（＝よりランクが高かった応答）に近い応答文が、より報酬が高くなります。結果として、より好ましいと評価された応答文に近い応答文が生成される確率が高まります。","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"key":"ZfdIyE7edv"}],"key":"nLUezQIKf9"}],"key":"VpeJRkzcZW"}],"key":"MZiylpG3gA"}],"key":"v3kQBMmAvy"}],"key":"gBUhTkLipj"},{"type":"image","url":"/build/rlhf-799f02a1517ba0cc2ca803959684c685.png","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"qFkePFpnYE","urlSource":"./Figure/rlhf.png"},{"type":"image","url":"/build/rlhf-performance-73858c45a8e87668ecaab06fcb89bf36.png","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"g1LBuHZtni","urlSource":"./Figure/rlhf-performance.png"}],"key":"klNLn6GNey"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Parameter efficient fine-tuning (PEFT)","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"tNKzH3jmjS"}],"identifier":"parameter-efficient-fine-tuning-peft","label":"Parameter efficient fine-tuning (PEFT)","html_id":"parameter-efficient-fine-tuning-peft","implicit":true,"enumerator":"6","key":"aMSBbUFeBK"},{"type":"image","url":"/build/PEFT-99de48d7f1515ab4df9c25e6375bb4f1.png","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"V9Kwqp9fwW","urlSource":"./Figure/PEFT.png"}],"key":"GKRkU5OHh3"},{"type":"block","kind":"notebook-content","children":[{"type":"table","position":{"start":{"line":1,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"tableRow","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"tableCell","header":true,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"項目","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"avl45PfdTt"}],"key":"sIUhbUhunw"}],"key":"oAPvqa1QET"},{"type":"tableCell","header":true,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"PEFT","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"NEWtoeHJTP"}],"key":"aLXFo7xi0o"}],"key":"AqBHFUbMwK"},{"type":"tableCell","header":true,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Full Fine-Tuning","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"z9AILoSl8t"}],"key":"ce7M4ee68L"}],"key":"XZtuArCMbG"}],"key":"PrsxbaKc2r"},{"type":"tableRow","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"tableCell","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"strong","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"概要","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"QJGlS1lbC9"}],"key":"YwRyLC9ZMP"}],"key":"auRO0qBU8U"},{"type":"tableCell","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"モデル全体ではなく、追加的に設定したパラメータや、一部のパラメータを微調整する手法。","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"T1KsLlgpbV"}],"key":"vt6wciGApZ"},{"type":"tableCell","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"事前学習済みモデル全体のパラメータを調整する手法。","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"aWuJwXYRyh"}],"key":"J1yPjdJIcb"}],"key":"NXuLNQRez4"},{"type":"tableRow","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"tableCell","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"strong","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"text","value":"計算リソース","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"DzINzyyhBD"}],"key":"PxwAALlXXu"}],"key":"r8kUJkYGss"},{"type":"tableCell","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"text","value":"大幅に削減可能。学習中に更新するパラメータ数が少ないため、GPUメモリや計算時間が少なくて済む。 (例　GPT-3 LoRA: 350GB)","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"YVzj9DDibS"}],"key":"jA3Nx1oyeX"},{"type":"tableCell","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"text","value":"多大な計算リソースを必要とする。全パラメータを更新するため、GPUメモリや計算負荷が高い (例　GPT-3: 1.2TB)。","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"aUrVk3Rywl"}],"key":"FOY4X59WrL"}],"key":"jdDh6fFeoR"},{"type":"tableRow","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"tableCell","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"strong","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"保存領域","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"YTEjVaCk3O"}],"key":"BffbVHN42n"}],"key":"hjzhnhda9n"},{"type":"tableCell","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"調整されたパラメータのみを保存するため、小さな保存領域で十分(例　GPT-3 LoRA: 35MB)。","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"POnyUIIT2X"}],"key":"KB6a4pPClV"},{"type":"tableCell","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"微調整後のモデル全体を保存する必要があるため、元モデルと同じサイズの保存領域は必要(例　GPT-3: 350GB)。","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"WNxqd9PieT"}],"key":"VeOTmGWE5x"}],"key":"REwCWNNL6E"}],"key":"LKM9bHmQoz"}],"key":"sO3qJW9C3z"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"PEFTの手法は主に以下の2つのカテゴリに分類できます：","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"xXhLjjpSBv"}],"key":"ktXan7hWnU"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":3,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":3,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Reparameterization: モデルの一部パラメータを特定の形式に再構成し、その部分のみを微調整します。","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"B762r8Vf0A"}],"key":"fUN7fFLABz"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"LoRA (Low-Rank Adaptation)","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"I9FmbEowHg"}],"key":"lw5xTDvfTi"}],"key":"vgkAFo2D3G"}],"key":"jslxguxMSl"}],"key":"mvWCHfJglg"},{"type":"listItem","spread":true,"position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Additive: 元のモデルのパラメータに加算的に新しいパラメータ（追加の層など）を導入して微調整を行います。","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"G4C8eX6RGy"}],"key":"cDHfrI3os4"}],"key":"QtGDXtvWPO"},{"type":"listItem","spread":true,"position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Soft Prompt: 入力系列にタスクごとのベクトル(Soft Prompt)を付加し、学習を行います。","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"lPeoMhN8rk"}],"key":"F6cILg97dh"}],"key":"eiQF4OejaP"}],"key":"KqurUB7vY1"}],"key":"ydVi81jke0"},{"type":"block","kind":"notebook-content","children":[{"type":"image","url":"/build/PEFT_method-3d50c0b8874b94ca76d8d888f3213641.png","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"TMplLM1K1D","urlSource":"./Figure/PEFT_method.png"}],"key":"whrGQJMWCS"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"LoRA (Low-Rank Adaptation)","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"qsRaRVOBXO"}],"identifier":"lora-low-rank-adaptation","label":"LoRA (Low-Rank Adaptation)","html_id":"lora-low-rank-adaptation","implicit":true,"enumerator":"6.1","key":"BaLx0dFdEe"},{"type":"paragraph","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"text","value":"LoRA は、モデル内の大きな重み行列（例: 全結合層や注意メカニズムの線形変換）を低ランクの行列に分解し、その一部だけを学習することで、計算コストとストレージを大幅に削減します。","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"J4RYvVZkzP"}],"key":"iEfDYbA3h5"},{"type":"paragraph","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"inlineMath","value":"W + \\Delta W = W + AB","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"html":"\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eW\u003c/mi\u003e\u003cmo\u003e+\u003c/mo\u003e\u003cmi mathvariant=\"normal\"\u003eΔ\u003c/mi\u003e\u003cmi\u003eW\u003c/mi\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmi\u003eW\u003c/mi\u003e\u003cmo\u003e+\u003c/mo\u003e\u003cmi\u003eA\u003c/mi\u003e\u003cmi\u003eB\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eW + \\Delta W = W + AB\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.7667em;vertical-align:-0.0833em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.13889em;\"\u003eW\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003cspan class=\"mbin\"\u003e+\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.6833em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003eΔ\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.13889em;\"\u003eW\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003e\u003c/span\u003e\u003cspan class=\"mrel\"\u003e=\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.7667em;vertical-align:-0.0833em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.13889em;\"\u003eW\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003cspan class=\"mbin\"\u003e+\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.6833em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003eA\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.05017em;\"\u003eB\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e","key":"M8yOB00Cg2"}],"key":"ghiqTWujk5"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":8,"column":1},"end":{"line":10,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"paragraph","children":[{"type":"inlineMath","value":"A \\in \\mathbb{R}^{d \\times r}, \\, B \\in \\mathbb{R}^{r \\times k}","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"html":"\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eA\u003c/mi\u003e\u003cmo\u003e∈\u003c/mo\u003e\u003cmsup\u003e\u003cmi mathvariant=\"double-struck\"\u003eR\u003c/mi\u003e\u003cmrow\u003e\u003cmi\u003ed\u003c/mi\u003e\u003cmo\u003e×\u003c/mo\u003e\u003cmi\u003er\u003c/mi\u003e\u003c/mrow\u003e\u003c/msup\u003e\u003cmo separator=\"true\"\u003e,\u003c/mo\u003e\u003cmtext\u003e \u003c/mtext\u003e\u003cmi\u003eB\u003c/mi\u003e\u003cmo\u003e∈\u003c/mo\u003e\u003cmsup\u003e\u003cmi mathvariant=\"double-struck\"\u003eR\u003c/mi\u003e\u003cmrow\u003e\u003cmi\u003er\u003c/mi\u003e\u003cmo\u003e×\u003c/mo\u003e\u003cmi\u003ek\u003c/mi\u003e\u003c/mrow\u003e\u003c/msup\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eA \\in \\mathbb{R}^{d \\times r}, \\, B \\in \\mathbb{R}^{r \\times k}\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.7224em;vertical-align:-0.0391em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003eA\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003e\u003c/span\u003e\u003cspan class=\"mrel\"\u003e∈\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:1.0435em;vertical-align:-0.1944em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e\u003cspan class=\"mord mathbb\"\u003eR\u003c/span\u003e\u003cspan class=\"msupsub\"\u003e\u003cspan class=\"vlist-t\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.8491em;\"\u003e\u003cspan style=\"top:-3.063em;margin-right:0.05em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:2.7em;\"\u003e\u003c/span\u003e\u003cspan class=\"sizing reset-size6 size3 mtight\"\u003e\u003cspan class=\"mord mtight\"\u003e\u003cspan class=\"mord mathnormal mtight\"\u003ed\u003c/span\u003e\u003cspan class=\"mbin mtight\"\u003e×\u003c/span\u003e\u003cspan class=\"mord mathnormal mtight\" style=\"margin-right:0.02778em;\"\u003er\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"mpunct\"\u003e,\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.1667em;\"\u003e\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.1667em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.05017em;\"\u003eB\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003e\u003c/span\u003e\u003cspan class=\"mrel\"\u003e∈\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.8491em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e\u003cspan class=\"mord mathbb\"\u003eR\u003c/span\u003e\u003cspan class=\"msupsub\"\u003e\u003cspan class=\"vlist-t\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.8491em;\"\u003e\u003cspan style=\"top:-3.063em;margin-right:0.05em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:2.7em;\"\u003e\u003c/span\u003e\u003cspan class=\"sizing reset-size6 size3 mtight\"\u003e\u003cspan class=\"mord mtight\"\u003e\u003cspan class=\"mord mathnormal mtight\" style=\"margin-right:0.02778em;\"\u003er\u003c/span\u003e\u003cspan class=\"mbin mtight\"\u003e×\u003c/span\u003e\u003cspan class=\"mord mathnormal mtight\" style=\"margin-right:0.03148em;\"\u003ek\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e","key":"kZesaRX5Ol"},{"type":"text","value":" は学習可能な低ランク行列。","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"key":"BkyKimJefe"}],"key":"gcKitshHsg"}],"key":"als7N6NYtw"},{"type":"listItem","spread":true,"position":{"start":{"line":9,"column":1},"end":{"line":10,"column":1}},"children":[{"type":"paragraph","children":[{"type":"inlineMath","value":"r","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"html":"\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003er\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003er\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.4306em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.02778em;\"\u003er\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e","key":"YzRM2dhWIY"},{"type":"text","value":" はランクで、通常は ","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"D4c8qIgyYc"},{"type":"inlineMath","value":"r \\ll \\min(d, k)","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"html":"\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003er\u003c/mi\u003e\u003cmo\u003e≪\u003c/mo\u003e\u003cmi\u003emin\u003c/mi\u003e\u003cmo\u003e⁡\u003c/mo\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmi\u003ed\u003c/mi\u003e\u003cmo separator=\"true\"\u003e,\u003c/mo\u003e\u003cmi\u003ek\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003er \\ll \\min(d, k)\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.5782em;vertical-align:-0.0391em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.02778em;\"\u003er\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003e\u003c/span\u003e\u003cspan class=\"mrel\"\u003e≪\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"\u003e\u003c/span\u003e\u003cspan class=\"mop\"\u003emin\u003c/span\u003e\u003cspan class=\"mopen\"\u003e(\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003ed\u003c/span\u003e\u003cspan class=\"mpunct\"\u003e,\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.1667em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.03148em;\"\u003ek\u003c/span\u003e\u003cspan class=\"mclose\"\u003e)\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e","key":"Nkzvl3jT29"},{"type":"text","value":"。","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"jo767wWeWO"}],"key":"qINSodqr2X"}],"key":"hbr419vKuR"}],"key":"svoig0lz0L"},{"type":"paragraph","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"text","value":"この近似により、学習すべきパラメータの総数は元の ","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"Ec5XU25RYX"},{"type":"inlineMath","value":"W","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"html":"\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eW\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eW\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.6833em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.13889em;\"\u003eW\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e","key":"m7FqPZfu08"},{"type":"text","value":" の ","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"QkpBcJQqvb"},{"type":"inlineMath","value":"\\mathcal{O}(d \\cdot k)","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"html":"\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi mathvariant=\"script\"\u003eO\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmi\u003ed\u003c/mi\u003e\u003cmo\u003e⋅\u003c/mo\u003e\u003cmi\u003ek\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003e\\mathcal{O}(d \\cdot k)\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathcal\" style=\"margin-right:0.02778em;\"\u003eO\u003c/span\u003e\u003cspan class=\"mopen\"\u003e(\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003ed\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003cspan class=\"mbin\"\u003e⋅\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.03148em;\"\u003ek\u003c/span\u003e\u003cspan class=\"mclose\"\u003e)\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e","key":"WtJVctIjCR"},{"type":"text","value":" から、低ランク部分のみの ","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"Y3w14sbdzr"},{"type":"inlineMath","value":"\\mathcal{O}(r \\cdot (d + k))","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"html":"\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi mathvariant=\"script\"\u003eO\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmi\u003er\u003c/mi\u003e\u003cmo\u003e⋅\u003c/mo\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmi\u003ed\u003c/mi\u003e\u003cmo\u003e+\u003c/mo\u003e\u003cmi\u003ek\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003e\\mathcal{O}(r \\cdot (d + k))\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathcal\" style=\"margin-right:0.02778em;\"\u003eO\u003c/span\u003e\u003cspan class=\"mopen\"\u003e(\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.02778em;\"\u003er\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003cspan class=\"mbin\"\u003e⋅\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"\u003e\u003c/span\u003e\u003cspan class=\"mopen\"\u003e(\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003ed\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003cspan class=\"mbin\"\u003e+\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.03148em;\"\u003ek\u003c/span\u003e\u003cspan class=\"mclose\"\u003e))\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e","key":"ilIxweTIfx"},{"type":"text","value":" に大幅に削減されます。","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"HPmlYZ8kXx"}],"key":"EbUnekTL28"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":12,"column":1},"end":{"line":14,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Transformerのパラメータ数 ","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"fNcNDFzyVH"},{"type":"inlineMath","value":"512 \\times 64 = 32768","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"html":"\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmn\u003e512\u003c/mn\u003e\u003cmo\u003e×\u003c/mo\u003e\u003cmn\u003e64\u003c/mn\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmn\u003e32768\u003c/mn\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003e512 \\times 64 = 32768\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.7278em;vertical-align:-0.0833em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e512\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003cspan class=\"mbin\"\u003e×\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.6444em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e64\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003e\u003c/span\u003e\u003cspan class=\"mrel\"\u003e=\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.6444em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e32768\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e","key":"DXYKOgrglz"}],"key":"xOKx70xi8f"}],"key":"kSgGpUnL2N"},{"type":"listItem","spread":true,"position":{"start":{"line":13,"column":1},"end":{"line":14,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"LoRAのパラメータ数","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"iWie6SWvG5"},{"type":"inlineMath","value":"d \\times r = 512 \\times 8 = 4096 (r=8)","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"html":"\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003ed\u003c/mi\u003e\u003cmo\u003e×\u003c/mo\u003e\u003cmi\u003er\u003c/mi\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmn\u003e512\u003c/mn\u003e\u003cmo\u003e×\u003c/mo\u003e\u003cmn\u003e8\u003c/mn\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmn\u003e4096\u003c/mn\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmi\u003er\u003c/mi\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmn\u003e8\u003c/mn\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003ed \\times r = 512 \\times 8 = 4096 (r=8)\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.7778em;vertical-align:-0.0833em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003ed\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003cspan class=\"mbin\"\u003e×\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.4306em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.02778em;\"\u003er\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003e\u003c/span\u003e\u003cspan class=\"mrel\"\u003e=\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.7278em;vertical-align:-0.0833em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e512\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003cspan class=\"mbin\"\u003e×\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.6444em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e8\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003e\u003c/span\u003e\u003cspan class=\"mrel\"\u003e=\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e4096\u003c/span\u003e\u003cspan class=\"mopen\"\u003e(\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.02778em;\"\u003er\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003e\u003c/span\u003e\u003cspan class=\"mrel\"\u003e=\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e8\u003c/span\u003e\u003cspan class=\"mclose\"\u003e)\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e","key":"Vmu1vYy5nw"}],"key":"gV77hbjbBH"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"children":[{"type":"paragraph","children":[{"type":"inlineMath","value":"86 \\%","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"html":"\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmn\u003e86\u003c/mn\u003e\u003cmi mathvariant=\"normal\"\u003e%\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003e86 \\%\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.8056em;vertical-align:-0.0556em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e86%\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e","key":"XKytuiK7Qd"},{"type":"text","value":"削減","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"key":"Wz3iYy9GVW"}],"key":"v2HcusCRM5"}],"key":"mu3c7JnfH7"}],"key":"J3Ft2cHkxa"}],"key":"EYmXfvUYBV"}],"key":"vxUOZ08qM5"}],"key":"vRg9GD0Qw4"},{"type":"block","kind":"notebook-content","children":[{"type":"image","url":"/build/lora_overview-e9c3f7498b7537cab5fa8931814013ba.png","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"uISRZlcD3k","urlSource":"./Figure/lora_overview.png"}],"key":"g17HsTbeOe"},{"type":"block","kind":"notebook-content","children":[{"type":"image","url":"/build/lora_calculation-e4c86dc99c5b589495059f01187bc3ee.png","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ntyLb7LAAK","urlSource":"./Figure/lora_calculation.png"}],"key":"LxdTRR3NgX"},{"type":"block","kind":"notebook-content","children":[{"type":"image","url":"/build/lora_performance-3d4cd142d775bfcd32a7c9a5cab6a81a.png","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"s4OGY9oriN","urlSource":"./Figure/lora_performance.png"}],"key":"ngMvzb2paW"}],"key":"xGmx0X7cUq"},"references":{"cite":{"order":[],"data":{}}},"footer":{"navigation":{"prev":{"title":"GPT","url":"/gpt","group":"大規模言語モデル"},"next":{"title":"LLMSの応用(1)：Langchainの基礎","url":"/langchain-basic","group":"大規模言語モデル"}}},"domain":"http://localhost:3000"},"project":{"numbering":{"heading_1":{"template":"enabled","enabled":true},"heading_2":{"template":"enabled","enabled":true},"heading_3":{"template":"enabled","enabled":true},"heading_4":{"template":"enabled","enabled":true},"heading_5":{"template":"enabled","enabled":true},"heading_6":{"template":"enabled","enabled":true}},"title":"計算社会科学と自然言語処理","github":"https://github.com/lvzeyu/css_nlp","id":"356ed62a-cf04-49ae-b7bc-d9f79f2ef8bb","exports":[],"bibliography":[],"index":"intro","pages":[{"title":"イントロダクション","level":1},{"slug":"introduction","title":"ガイダンス","description":"このページで、行動科学演習(LB63310)と計算人文社会学研究演習Ⅱ (LM23309)の授業資料を公開しています。","date":"","thumbnail":"/build/NLP_history-80702ee1d37040856a480174f5bcbbfc.png","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"title":"基礎知識","level":1},{"slug":"nlp-basis2","title":"自然言語処理の基礎","description":"","date":"","thumbnail":"/build/7e2db436150c38a00650f96925aa5581.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"ml-basis2","title":"機械学習の基本概念","description":"","date":"","thumbnail":"/build/supervised-learning-b5be7f5e73313e60f2c06d545c226553.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"math-basis2","title":"数学基礎","description":"","date":"","thumbnail":"/build/7e2db436150c38a00650f96925aa5581.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"title":"ニューラルネットワーク","level":1},{"slug":"nn2","title":"ニューラルネットワーク","description":"","date":"","thumbnail":"/build/7e2db436150c38a00650f96925aa5581.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"backpropagation","title":"誤差逆伝播法","description":"","date":"","thumbnail":"/build/7e2db436150c38a00650f96925aa5581.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"sgd","title":"学習に関するテクニック","description":"","date":"","thumbnail":"/build/7e2db436150c38a00650f96925aa5581.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"title":"PyTorch","level":1},{"slug":"pytorch2","title":"Pytorch","description":"","date":"","thumbnail":"/build/7e2db436150c38a00650f96925aa5581.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"title":"単語分散表現","level":1},{"slug":"word2vec-1","title":"単語分散表現","description":"","date":"","thumbnail":"/build/7e2db436150c38a00650f96925aa5581.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"word2vec-2-embedding","title":"word2vec","description":"","date":"","thumbnail":"/build/7e2db436150c38a00650f96925aa5581.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"word2vec-gensim","title":"GensimによるWord2Vecの学習と使用","description":"","date":"","thumbnail":"/build/7e2db436150c38a00650f96925aa5581.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"word2vec-application","title":"Word2Vecが人文・社会科学研究における応用","description":"","date":"","thumbnail":"/build/7e2db436150c38a00650f96925aa5581.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"title":"RNN","level":1},{"slug":"rnn","title":"RNNの基礎","description":"","date":"","thumbnail":"/build/rnn-4dbf33ec81abf2baa6d5bbe97bb61683.png","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"lstm","title":"LSTM","description":"","date":"","thumbnail":"/build/lstm1-4ea8525c2f72b4f2ff8298a9f7b9c0be.png","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"pytorch-lstm","title":"LSTMの実装","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"lstm-classification","title":"LSTMによる文書分類","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"seq2seq","title":"Seq2seq","description":"","date":"","thumbnail":"/build/7e2db436150c38a00650f96925aa5581.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"title":"Transformer","level":1},{"slug":"attention","title":"Attention","description":"","date":"","thumbnail":"/build/attention-eed7f8fafc28c8cb6bd5276bbd04714b.png","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"self-attention","title":"Self-Attention","description":"","date":"","thumbnail":"/build/7e2db436150c38a00650f96925aa5581.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"transformer","title":"Transformerアーキテクチャ","description":"","date":"","thumbnail":"/build/transformer-493148401fbb971a745b29ab5b44e10f.png","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"bert","title":"BERT","description":"","date":"","thumbnail":"/build/7e2db436150c38a00650f96925aa5581.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"bert-sentiment","title":"BERTによるセンチメント分析","description":"","date":"","thumbnail":"/build/7e2db436150c38a00650f96925aa5581.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"bert-topic","title":"BERTopic","description":"","date":"","thumbnail":"/build/7e2db436150c38a00650f96925aa5581.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"title":"大規模言語モデル","level":1},{"slug":"gpt","title":"GPT","description":"","date":"","thumbnail":"/build/gpt_history-c08ab6ce749d3aed300b023e318cb96c.png","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"llm","title":"大規模言語モデルの基本","description":"","date":"","thumbnail":"/build/LLMs_parameter-ec157a4028abc601fb429210b7f4b07c.png","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"langchain-basic","title":"LLMSの応用(1)：Langchainの基礎","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2}]}}},"actionData":null,"errors":null},"future":{"unstable_dev":false,"unstable_postcss":false,"unstable_tailwind":false,"v2_errorBoundary":true,"v2_headers":true,"v2_meta":true,"v2_normalizeFormMethod":true,"v2_routeConvention":true}};</script><script type="module" async="">import "/build/manifest-86A905A4.js";
import * as route0 from "/build/root-CXYA7X5D.js";
import * as route1 from "/build/routes/$-JRBPULBO.js";
window.__remixRouteModules = {"root":route0,"routes/$":route1};

import("/build/entry.client-PCJPW7TK.js");</script></body></html>