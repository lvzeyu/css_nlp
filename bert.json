{"version":3,"kind":"Notebook","sha256":"efec5eb0e3637438a430ccde5018beba55ab5bb58e2f2ab0d2dfc2f5754f1a7d","slug":"bert","location":"/notebook/BERT.ipynb","dependencies":[],"frontmatter":{"title":"BERT","content_includes_title":false,"kernelspec":{"name":"python3","display_name":"jupyterbook","language":"python"},"github":"https://github.com/lvzeyu/css_nlp","numbering":{"heading_1":{"enabled":true,"template":"enabled"},"heading_2":{"enabled":true,"template":"enabled"},"heading_3":{"enabled":true,"template":"enabled"},"heading_4":{"enabled":true,"template":"enabled"},"heading_5":{"enabled":true,"template":"enabled"},"heading_6":{"enabled":true,"template":"enabled"},"title":{"offset":1}},"source_url":"https://github.com/lvzeyu/css_nlp/blob/master/notebook/BERT.ipynb","edit_url":"https://github.com/lvzeyu/css_nlp/edit/master/notebook/BERT.ipynb","thumbnail":"/build/7e2db436150c38a00650f96925aa5581.svg","exports":[{"format":"ipynb","filename":"BERT.ipynb","url":"/build/BERT-7832dbc82174d9ce269840739f8a0705.ipynb"}]},"widgets":{},"mdast":{"type":"root","children":[{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"link","url":"https://colab.research.google.com/github/lvzeyu/css_nlp/blob/master/notebook/BERT.ipynb","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"image","url":"/build/7e2db436150c38a00650f96925aa5581.svg","alt":"Open In Colab","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"ASNPnjTnG7","urlSource":"https://colab.research.google.com/assets/colab-badge.svg"}],"urlSource":"https://colab.research.google.com/github/lvzeyu/css_nlp/blob/master/notebook/BERT.ipynb","key":"yjphgZ9BXn"}],"key":"NsLW9qbC6E"},{"type":"paragraph","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"text","value":"2018年Googleが発表したBERT（Bidirectional Encoder Representations from Transformers）は、エンコーダ構成のTransformersを採用し、先行するトークン列と後続するトークン列の双方向性から文脈を捉えます。","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"pDrTbGXPoI"}],"key":"lLg697NB29"},{"type":"paragraph","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"text","value":"BERTは、Wikipediaと7,000冊の書籍を合わせた大規模なコーパスを使って事前学習されたモデルを、下流タスクのデータセットでファインチューングすることで、様々なタスクの性能を大幅に改善できることが示されました。","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"yXMlfrvv4D"}],"key":"peQEW1ZL4z"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":11,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"事前学習: 大規模なコーパスを用いて、特定なタスクを学習することで、広範な言語データからパターンを学習し、汎用的な言語理解の能力を身につける。","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"f3G1ZP78bW"}],"key":"f4H3hcKISU"}],"key":"xmxPIRWrc7"},{"type":"listItem","spread":true,"position":{"start":{"line":12,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"ファインチューング：、特定のタスクや領域に特化した小さなデータセットを用いて、事前学習したモデルを微調整します。この微調整により、モデルは特定のタスクや領域に適応し、高い精度を達成することが可能です。","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"JXEgHJJB4X"}],"key":"BNv0DlzTUR"}],"key":"RVEG5qcMS2"}],"key":"wiedAuKKex"},{"type":"image","url":"/build/transfer_learning-fab84a1b72c9948cd22f51ce047d6966.png","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"key":"wKqljrjMhd","urlSource":"./Figure/transfer_learning.png"}],"key":"PlAJ2IeQe2"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Contextualized Embedding","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"BHbKuiPsLJ"}],"identifier":"contextualized-embedding","label":"Contextualized Embedding","html_id":"contextualized-embedding","implicit":true,"enumerator":"1","key":"XzxZ9Nt3cK"}],"key":"NPigJfAMgH"},{"type":"block","kind":"notebook-content","children":[{"type":"image","url":"/build/bert_embedding-0526bca2865a2e6067113d4812fad34e.png","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"FlUrWppfIO","urlSource":"./Figure/bert_embedding.png"}],"key":"sJqgT0tIL1"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"BERTの事前学習","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"AeZA9tJouB"}],"identifier":"bert","label":"BERTの事前学習","html_id":"bert","implicit":true,"enumerator":"2","key":"Lbq0fNiXsC"},{"type":"heading","depth":3,"position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"BERTの構造","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"gn32Wkpsm0"}],"identifier":"bert","label":"BERTの構造","html_id":"bert-1","implicit":true,"enumerator":"2.1","key":"vVKBGpGHjc"},{"type":"image","url":"/build/bert-base-bert-large-fed58a17a2474952bb33515d15373330.png","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"OOxJJs73Q3","urlSource":"./Figure/bert-base-bert-large-encoders.png"},{"type":"paragraph","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"text","value":"BERTは、Transformerのエンコーダ部分をベースにしたモデルになります。","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"NMmAAOuB8p"}],"key":"h4eR7vDqxO"},{"type":"paragraph","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"text","value":"複数のエンコーダ層を積み重ねることで、より複雑で長い文脈を処理できるようになり、幅広い自然言語処理タスクで高い性能を発揮しています。","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"CehwVHu1WK"}],"key":"emkWqe8PQu"},{"type":"image","url":"/build/bert-contexualized-e-75a833157513561a5b6e30d86f7758db.png","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"CxSwVN6Tp0","urlSource":"./Figure/bert-contexualized-embeddings.png"}],"key":"Rd7HpOAvZT"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"入力表現","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"JLZyJnX9ne"}],"identifier":"id","label":"入力表現","html_id":"id","implicit":true,"enumerator":"2.2","key":"uDx0xrXXkA"},{"type":"image","url":"/build/bert_input-733c4b03d8c0337848f70f7bacfa588e.png","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"VaLnA6Dr4n","urlSource":"./Figure/bert_input.png"},{"type":"paragraph","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"BERTで入力を作成する際、入力の開始を表す","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"RrRdQsN3sJ"},{"type":"inlineCode","value":"[CLS]","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"wkAkZcxCMG"},{"type":"text","value":"トークンと、入力の区切りを表す","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"lRIb7LbNW9"},{"type":"inlineCode","value":"[SEP]","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"VvIue0fRF6"},{"type":"text","value":"トークンという二つの特殊トークンが使われます。","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"iVBWo2y73b"}],"key":"foGx6fmDRA"},{"type":"paragraph","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"text","value":"またよく使われる特殊トークンとして、マスクタスクための","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"MBHmU4FP45"},{"type":"inlineCode","value":"[MASK]","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"a4Pj9mp1jZ"},{"type":"text","value":"トークン、vocabularyに含まれていないことを示す","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"yy3PngA4QL"},{"type":"inlineCode","value":"[UNK]","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"W35z7ugP77"},{"type":"text","value":"トークンがあります。","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"K0wnHJ65nU"}],"key":"UTafiGlWM6"},{"type":"paragraph","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"text","value":"入力トークン埋め込みと位置埋め込み以外、それぞれのテキストの範囲を区別しやくするためにsegment embeddingという埋め込みが導入されています。","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"rsLIaHTtY4"}],"key":"YQ5S8mynaE"},{"type":"paragraph","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"text","value":"まとめると、BERTの入力埋め込み","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"wISXXdhu8v"},{"type":"inlineMath","value":"x_i","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>x</mi><mi>i</mi></msub></mrow><annotation encoding=\"application/x-tex\">x_i</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.5806em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3117em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>","key":"lKwOyMwjlY"},{"type":"text","value":"は、トークン埋め込み、位置埋め込み、segment埋め込みより加算されます。","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"rlHWdHOnt6"}],"key":"gqzApOOkaA"}],"key":"GgsYWdtAKJ"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"BERTの出力","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"XAe1EQqo6O"}],"identifier":"bert","label":"BERTの出力","html_id":"bert-2","implicit":true,"enumerator":"2.3","key":"IXTJ51Ha73"},{"type":"image","url":"/build/bert-output-vector-4f36ad946b4a43590a83e7bc07c7123c.png","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"L7EiQT4o62","urlSource":"./Figure/bert-output-vector.png"},{"type":"paragraph","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"BERTの出力は、入力文の各トークンに対応する文脈化された埋め込みベクトル（contextual embeddings）です。これらの埋め込みは、モデルが入力文全体の文脈を考慮しながら、各トークンの意味を表現したものです。","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"oNjoMWYR0b"}],"key":"wTVBlgLVhy"}],"key":"Q4yZgAXJcr"},{"type":"block","kind":"notebook-content","children":[{"type":"admonition","kind":"note","children":[{"type":"admonitionTitle","children":[{"type":"text","value":"Note","key":"kBKpcc3EdT"}],"key":"jNvOvHkLwU"},{"type":"paragraph","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"text","value":"BERTの出力の中で、入力文の最初に挿入される特殊トークン ","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"AZtIXboQ6c"},{"type":"inlineCode","value":"[CLS]","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"nCCq9ld88e"},{"type":"text","value":" に対応する埋め込みは、文全体の情報を要約するように設計されています。","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"EYShUyUTMz"}],"key":"Np3OQUbaAI"}],"key":"XOShFWNJLS"}],"key":"gjN3H1Rcpf"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"マスク言語モデリング","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ZsoVMhgEY8"}],"identifier":"id","label":"マスク言語モデリング","html_id":"id-1","implicit":true,"enumerator":"2.4","key":"rX9sCftDmj"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"マスク言語モデリングは、トークンの穴埋め問題を解けるタスクです。具体的には、ランダムにトークン列中のトークンを隠して、その周辺の単語からマスクされた単語を予測することが求められます。","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"DahWnzrmDF"}],"key":"FKXxgh4C0A"},{"type":"paragraph","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"ここで、先行するトークンと後続トークンの双方の情報が使われていますので、全体の文脈を捉える学習を実現しています。","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"FqV1KpD9Uo"}],"key":"ZBAa4yCwuB"},{"type":"image","url":"/build/BERT-language-modeli-0d1934130a1e36c3307c7ef266c7b29c.png","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"psan9s6Muh","urlSource":"./Figure/BERT-language-modeling-masked-lm.png"}],"key":"CW4sstETbJ"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"次文予測","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"WS5ySGFkOi"}],"identifier":"id","label":"次文予測","html_id":"id-2","implicit":true,"enumerator":"2.5","key":"pAPia7dP5g"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"次文予測タスクでは、2つの文が与えられ、一方が他方の直後に来るかどうかを判定することが求められます。","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"V7fYAJ9t7V"}],"key":"gHmkO4cDiJ"},{"type":"image","url":"/build/bert-next-sentence-p-32a370f50b97ee2353f708c030f0d6ec.png","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"cSjDyuag5a","urlSource":"./Figure/bert-next-sentence-prediction.png"}],"key":"A4bMJ7pVJH"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"ファインチューング(Fine-Tuning)","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"HpABVbN9ke"}],"identifier":"id-fine-tuning","label":"ファインチューング(Fine-Tuning)","html_id":"id-fine-tuning","implicit":true,"enumerator":"3","key":"LPX2NAykuY"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"様々なタスクにおいて、事前学習モデルをもとにしてそれぞれのタスクに特化したモデルを作るためのステップはファインチューングです。","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"evo6u86yw0"}],"key":"Ac6DJ4FoD5"},{"type":"paragraph","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"事前学習タスクから下流タスクに切り替える時、モデルの最後のレイヤーをタスクに適したものに置き換える必要があります。この最後の層はヘッドと呼ばれ、タスクに固有な部分です。","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"h7gMMtPsEp"}],"key":"A7T4Cc2gOV"},{"type":"paragraph","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"text","value":"残りの部分はボディと呼ばれ、タスクに依存しない事前学習された部分であり(トークン埋め込み層やTransformer層が含まれ)、一般的な言語の理解を行うための基本的な情報を含んでいます。","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"KbdWDInr8P"}],"key":"asd8qhcGiv"},{"type":"paragraph","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"text","value":"例えば、テキスト分類の場合、追加層としては、BERTの最後の隠れ層からの出力に冒頭のSpecial token","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"JmBDgbXrYP"},{"type":"inlineCode","value":"[CLS]","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"aAc3jaeTxw"},{"type":"text","value":"を全結合層（Dense Layer）に経由し、各カテゴリーに属する確率を出力します。それは、[CLS]トークンは入力テキスト全体の文脈情報を集約すると考えるためです。","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"LG5wv4Gz9a"}],"key":"HHkTRLvcmB"},{"type":"admonition","kind":"note","children":[{"type":"admonitionTitle","children":[{"type":"text","value":"Note","key":"Rp2ep9xQjM"}],"key":"r7NE4E2KO0"},{"type":"paragraph","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"children":[{"type":"text","value":"テキスト分類タスクにおいて、[CLS]トークンと全結合層（Dense Layer）を使用するのが一般的ですが、他の追加層の設計を用いることももちろん可能です。","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"lq3Ph4P4uz"}],"key":"m584zSsXMK"}],"key":"bk0fRTiZ5m"},{"type":"image","url":"/build/bert_based_model-28e7006a9de208e9f074c5e5df95b023.png","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"key":"TyLs0NSiHG","urlSource":"./Figure/bert_based_model.png"}],"key":"eJGmExL2J8"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Huggingface transformerを使う","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"VUPqoSQNSX"}],"identifier":"huggingface-transformer","label":"Huggingface transformerを使う","html_id":"huggingface-transformer","implicit":true,"enumerator":"3.1","key":"U4xJQPwYoa"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"転移学習は事前学習済みモデルを新しいタスクに再利用するといった強みがあります。そのため、事前学習済みのモデルを素早く共有、ロードすることは重要です。","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"LlIOu8BbY4"}],"key":"ofIHUNUM6P"},{"type":"paragraph","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"link","url":"https://huggingface.co/","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"Hugging Face Hub","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"ArdaHuaj6V"}],"urlSource":"https://huggingface.co/","key":"HSITZS9da7"},{"type":"text","value":"は、モデル、データセットとデモを備えたプラットフォームです。","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"zy7BtBFyHF"}],"key":"ZeUwIH0fPJ"},{"type":"image","url":"/build/hf-ecosystem-6680c5be90b07a733baabb4de6aaa31e.png","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"faKPjyzf8j","urlSource":"./Figure/hf-ecosystem.png"},{"type":"paragraph","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"children":[{"type":"link","url":"https://huggingface.co/docs/transformers/index","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"children":[{"type":"text","value":"Huggingface transformer","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"AF2DKaNce2"}],"urlSource":"https://huggingface.co/docs/transformers/index","key":"eVFkkhObWt"},{"type":"text","value":"は、自然言語処理を中心に最先端のTransformerベースのモデルを効率に利用するためのオープンソースライブラリです。","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"f8qEfgbKw1"}],"key":"z5qQjccNMD"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":12,"column":1},"end":{"line":16,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"多数の事前学習済みモデル: ライブラリは、BERT、GPT-2、RoBERTa、T5、DistilBERTなど、さまざまな有名なNLPモデルの事前学習済みバージョンを提供しています。","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"DleTuvQc9P"}],"key":"KZYHsxTT8n"}],"key":"y0AdvDaRF8"},{"type":"listItem","spread":true,"position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"モデルの利用の簡易化: 事前学習済みのモデルを簡単にダウンロードし、特定のタスクにファインチューニングするための高レベルのAPIを提供しています。","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"ER8bVklwCN"}],"key":"ef34MSuUuU"}],"key":"u7k1kRUwaY"},{"type":"listItem","spread":true,"position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Tokenizers: ほとんどのモデルには、テキストデータをモデルが扱える形式に変換するためのトークナイザが付属しています。これはテキストの前処理を簡単に行うためのツールです。","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"key":"pPYu8p2ufp"}],"key":"iCHqeYKFLG"}],"key":"KNqXHHfnsu"},{"type":"listItem","spread":true,"position":{"start":{"line":15,"column":1},"end":{"line":16,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Model Hubの統合: Hugging FaceのModel Hubと直接統合されており、コミュニティによって共有されている数千もの事前学習済みモデルに簡単にアクセスできます。","position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"key":"UzOK1q4Kxj"}],"key":"qhf4SWMpLD"}],"key":"JOTDj9dUlF"}],"key":"Zq3F33bLyE"},{"type":"heading","depth":4,"position":{"start":{"line":17,"column":1},"end":{"line":17,"column":1}},"children":[{"type":"text","value":"pipline","position":{"start":{"line":17,"column":1},"end":{"line":17,"column":1}},"key":"wD3FL9gJbp"}],"identifier":"pipline","label":"pipline","html_id":"pipline","implicit":true,"enumerator":"3.1.1","key":"wGnoDB2OSV"},{"type":"paragraph","position":{"start":{"line":20,"column":1},"end":{"line":20,"column":1}},"children":[{"type":"link","url":"https://huggingface.co/docs/transformers/main_classes/pipelines","position":{"start":{"line":20,"column":1},"end":{"line":20,"column":1}},"children":[{"type":"inlineCode","value":"pipeline","position":{"start":{"line":20,"column":1},"end":{"line":20,"column":1}},"key":"l36B8d3L72"}],"urlSource":"https://huggingface.co/docs/transformers/main_classes/pipelines","key":"TAH0R7d0Nt"},{"type":"text","value":"というクラスで、特定のタスクを実行するために事前学習されたモデルとトークンナイザーを統合し、簡単に使用することができます。","position":{"start":{"line":20,"column":1},"end":{"line":20,"column":1}},"key":"aQ1YOJFPQV"}],"key":"KNkfvFWsZ9"}],"key":"vnkJBkqula"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"#!pip install fugashi\n#!pip install unidic-lite","key":"ZlSmrOF5Wx"},{"type":"outputs","id":"XqUbWeltRMiwUT3OnNbDW","children":[],"key":"GeQBNcPGJ6"}],"key":"wruROdazdJ"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"import pandas as pd\nfrom transformers import pipeline\n\nfill_mask = pipeline(\n    \"fill-mask\",\n    model=\"cl-tohoku/bert-base-japanese-v3\"\n)","key":"X7epNiJyWN"},{"type":"outputs","id":"mrecwS9I02dUqcg-rw90W","children":[],"key":"vcWXuTvJDo"}],"key":"E2YTM8AzWd"},{"type":"block","kind":"notebook-content","children":[],"key":"o713vZVxQK"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"masked_text = \"東北大学は[MASK]市に位置しています。\"\noutput = fill_mask(masked_text)","key":"FUJ6MZB89c"},{"type":"outputs","id":"xV0qNVl5Mw3LZwCrU-vBz","children":[],"key":"Vfg54pe9kk"}],"key":"PS0vxjE0Fl"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"display(pd.DataFrame(output))","key":"ooRdhDRfL9"},{"type":"outputs","id":"spp3yl43T16v9HMm2MWMP","children":[{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"text/html":{"content":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>score</th>\n      <th>token</th>\n      <th>token_str</th>\n      <th>sequence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.600761</td>\n      <td>14424</td>\n      <td>仙台</td>\n      <td>東北 大学 は 仙台 市 に 位置 し て い ます 。</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.048195</td>\n      <td>19197</td>\n      <td>盛岡</td>\n      <td>東北 大学 は 盛岡 市 に 位置 し て い ます 。</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.037116</td>\n      <td>15135</td>\n      <td>青森</td>\n      <td>東北 大学 は 青森 市 に 位置 し て い ます 。</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.026598</td>\n      <td>15394</td>\n      <td>山形</td>\n      <td>東北 大学 は 山形 市 に 位置 し て い ます 。</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.025400</td>\n      <td>14062</td>\n      <td>福島</td>\n      <td>東北 大学 は 福島 市 に 位置 し て い ます 。</td>\n    </tr>\n  </tbody>\n</table>\n</div>","content_type":"text/html"},"text/plain":{"content":"      score  token token_str                      sequence\n0  0.600761  14424        仙台  東北 大学 は 仙台 市 に 位置 し て い ます 。\n1  0.048195  19197        盛岡  東北 大学 は 盛岡 市 に 位置 し て い ます 。\n2  0.037116  15135        青森  東北 大学 は 青森 市 に 位置 し て い ます 。\n3  0.026598  15394        山形  東北 大学 は 山形 市 に 位置 し て い ます 。\n4  0.025400  14062        福島  東北 大学 は 福島 市 に 位置 し て い ます 。","content_type":"text/plain"}}},"children":[],"key":"abnYDgsiOg"}],"key":"hgwqWk0yBC"}],"key":"W7rINfB2Dw"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")","key":"jJIf0Gki7j"},{"type":"outputs","id":"_3zeMKRBztc5P5aapV9m7","children":[],"key":"Bte8DFwWgR"}],"key":"RuSw7PpvkT"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"ARTICLE = \"\"\" New York (CNN)When Liana Barrientos was 23 years old, she got married in Westchester County, New York.\nA year later, she got married again in Westchester County, but to a different man and without divorcing her first husband.\nOnly 18 days after that marriage, she got hitched yet again. Then, Barrientos declared \"I do\" five more times, sometimes only within two weeks of each other.\nIn 2010, she married once more, this time in the Bronx. In an application for a marriage license, she stated it was her \"first and only\" marriage.\nBarrientos, now 39, is facing two criminal counts of \"offering a false instrument for filing in the first degree,\" referring to her false statements on the\n2010 marriage license application, according to court documents.\nProsecutors said the marriages were part of an immigration scam.\nOn Friday, she pleaded not guilty at State Supreme Court in the Bronx, according to her attorney, Christopher Wright, who declined to comment further.\nAfter leaving court, Barrientos was arrested and charged with theft of service and criminal trespass for allegedly sneaking into the New York subway through an emergency exit, said Detective\nAnnette Markowski, a police spokeswoman. In total, Barrientos has been married 10 times, with nine of her marriages occurring between 1999 and 2002.\nAll occurred either in Westchester County, Long Island, New Jersey or the Bronx. She is believed to still be married to four men, and at one time, she was married to eight men at once, prosecutors say.\nProsecutors said the immigration scam involved some of her husbands, who filed for permanent residence status shortly after the marriages.\nAny divorces happened only after such filings were approved. It was unclear whether any of the men will be prosecuted.\nThe case was referred to the Bronx District Attorney\\'s Office by Immigration and Customs Enforcement and the Department of Homeland Security\\'s\nInvestigation Division. Seven of the men are from so-called \"red-flagged\" countries, including Egypt, Turkey, Georgia, Pakistan and Mali.\nHer eighth husband, Rashid Rajput, was deported in 2006 to his native Pakistan after an investigation by the Joint Terrorism Task Force.\nIf convicted, Barrientos faces up to four years in prison.  Her next court appearance is scheduled for May 18.\n\"\"\"\nprint(summarizer(ARTICLE, max_length=130, min_length=30, do_sample=False))","key":"WHBdxhjHHp"},{"type":"outputs","id":"ZvWKSoWtiUPG2UoGwyrwt","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"[{'summary_text': 'Liana Barrientos, 39, is charged with two counts of \"offering a false instrument for filing in the first degree\" In total, she has been married 10 times, with nine of her marriages occurring between 1999 and 2002. She is believed to still be married to four men.'}]\n"},"children":[],"key":"SjKC8G3L2G"}],"key":"WnmGqTgh1D"}],"key":"CJpNWOWDiB"},{"type":"block","kind":"notebook-content","children":[{"type":"tabSet","children":[{"type":"tabItem","title":"課題 1","children":[{"type":"paragraph","position":{"start":{"line":4,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"Huggingface Hubで日本語のセンチメント分類ためのモデルを探し、piplineでセンチメント分類器を実装しなさい。\n以下のテキストに対する分類結果を確認しよう。","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"Wu4vTHFQF5"}],"key":"UNP3vLWd7N"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":7,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"paragraph","children":[{"type":"inlineCode","value":"この製品は全く役に立ちませんでした","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"KdznbDficc"}],"key":"Ztw7FJlfMF"}],"key":"JOrkAkq1J7"},{"type":"listItem","spread":true,"position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"paragraph","children":[{"type":"inlineCode","value":"今日はいい天気ですね","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"key":"vvghvuDFuY"}],"key":"XTDLlbpNyX"}],"key":"Li4QOheThJ"},{"type":"listItem","spread":true,"position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"paragraph","children":[{"type":"inlineCode","value":"世界経済も、米国が12月に続き３月にも追加利上げを実施するなど、先進国を中心に回復の動きとなりました","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"YbUP6ma3ev"}],"key":"aIJxUBU0R3"}],"key":"PDjOawg6K6"}],"key":"LIfB7aX74n"}],"key":"XXTPWuMJjQ"}],"key":"Xs6AXBtwGi"}],"key":"WZFod7Qpmz"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"まとめ : word2vecからBERTまで","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"TN6K7X5GOq"}],"identifier":"id-word2vec-bert","label":"まとめ : word2vecからBERTまで","html_id":"id-word2vec-bert","implicit":true,"enumerator":"4","key":"p2sW415pxR"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"人間が使う自然言語をコンピュータに処理させるため、言語を数値形式で表現するモデリングのプロセスが必要とされ、どのようなモデルを採用するかによって分析の方向性は異なっています。","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"I6NPkVqHDh"}],"key":"Er6REuRzHj"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":5,"column":1},"end":{"line":18,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":5,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"paragraph","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"最も基本的なモデリングアプローチとして、文書を単語の集合とそれぞれの単語の頻度情報に変換するバグオブワーズ(bag of words)があげられます。この手法は、文書の基本的な内容を捉えるのに有効であるが、","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"NVnreeEUcE"},{"type":"underline","children":[{"type":"text","value":"単語の順番や意味のニュアンスなどの情報はすべて捨象されています。","key":"trXuK494UH"}],"key":"nJgEo7dWi9"}],"key":"kVdQjROvjw"}],"key":"PtYACYkeYC"},{"type":"listItem","spread":true,"position":{"start":{"line":8,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"paragraph","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"text","value":"より複雑な言語の特性を捉えるために、大量のコーパスを使った学習により、言語の文法や意味構造など多くの情報を埋め込んだ高度なモデルが期待されています。word2vecをはじめとする単語分散表現モデルは、単語を「意味」情報を表現したベクトルにマッピングすることができます。","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"key":"fQ3dgxMeP0"}],"key":"a1xV1rQ1EI"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":9,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"word2vecに単語分散表現の学習では、「単語の意味は、その単語の周囲の単語（文脈）によって決まる」という分布仮説に基づく手法が用いられます。この仮説にしたがうモデルでは、ある単語がどのような文脈で生じやすいかということをある程度考慮し、単語間の関係をベクトルで表現することができます。","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"TFYy5wk5ju"}],"key":"aLuCj0deEv"}],"key":"IoQ6iwZ3g5"},{"type":"listItem","spread":true,"position":{"start":{"line":10,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"ただ、word2vecではいくつの欠点があります。特に、word2vecではあくまで","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"rGUeAj6wQ6"},{"type":"text","value":"1","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"D9DZfxGQzq"},{"type":"text","value":"単語","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"ExNJK4Uaui"},{"type":"text","value":"1","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"ffxTdJ1EiZ"},{"type":"text","value":"ベクトルでしたが、実際のケースでは、文脈によって単語の意味が変わることがありますので、","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"mhLtxbUZ3U"},{"type":"underline","children":[{"type":"text","value":"文脈に依存する分散表現が望ましいです。","key":"Y0wiwMIIHn"}],"key":"OJ9e90RLEt"}],"key":"wenzXCwZKR"}],"key":"ZarMFCjZwv"}],"key":"SrUelAQP1A"}],"key":"Gb1Uv8bXEw"},{"type":"listItem","spread":true,"position":{"start":{"line":12,"column":1},"end":{"line":17,"column":1}},"children":[{"type":"paragraph","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"children":[{"type":"text","value":"テキストの「文脈」を表現するための言語モデルが開発されました。","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"do4EIrZEVL"}],"key":"ibTmHT58Hg"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":13,"column":1},"end":{"line":17,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":13,"column":1},"end":{"line":15,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"RNNとLSTMではテキストデータの時系列的な性質を捉え、","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"OCdGr9GWrj"},{"type":"underline","children":[{"type":"text","value":"文中の単語の順序や時間的な関連性をモデルが学習できるようになります","key":"YysYyBJZKo"}],"key":"noAalutj34"},{"type":"text","value":"。ただ、","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"VAhrt97Mfd"}],"key":"RgF27o18vK"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":14,"column":1},"end":{"line":15,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"長距離の依存関係を効果的に対処できでいない","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"key":"oGGOWzx2og"}],"key":"ryHqMG0D8i"}],"key":"MWr6NLHq2M"},{"type":"listItem","spread":true,"position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"計算コストが高い","position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"key":"iIFRc7T0PQ"}],"key":"wwbeDix1kq"}],"key":"cR5pULWI5j"}],"key":"fyVFfFfWoR"}],"key":"ZPuGwGEsst"},{"type":"listItem","spread":true,"position":{"start":{"line":16,"column":1},"end":{"line":17,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Self-Attenttionでは、","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"key":"tCF4euGAbR"},{"type":"underline","children":[{"type":"text","value":"すべての単語間の関係を並列に計算することで、長距離の依存関係を効果的に捉えます。","key":"mILCOTK36U"}],"key":"nBWrspUfyC"}],"key":"OPIxZAP4Qh"}],"key":"xrVeKyEPLA"}],"key":"b4VMT2g5Ht"}],"key":"HOoSV6i3Un"},{"type":"listItem","spread":true,"position":{"start":{"line":18,"column":1},"end":{"line":18,"column":1}},"children":[{"type":"paragraph","position":{"start":{"line":18,"column":1},"end":{"line":18,"column":1}},"children":[{"type":"text","value":"Transformerをベースにしたモデルでは、単語が出現する具体的な文脈に基づいてその単語の埋め込みを生成することができます。","position":{"start":{"line":18,"column":1},"end":{"line":18,"column":1}},"key":"WgXIfe0ItV"}],"key":"EZybGe7ICc"}],"key":"kXvIohMvcI"}],"key":"ftQ7buTI6o"}],"key":"qoP1MsNLyv"}],"key":"q1hPRrBgMB"},"references":{"cite":{"order":[],"data":{}}},"footer":{"navigation":{"prev":{"title":"Transformerアーキテクチャ","url":"/transformer","group":"Transformer"},"next":{"title":"BERTによるセンチメント分析","url":"/bert-sentiment","group":"Transformer"}}},"domain":"http://localhost:3000"}