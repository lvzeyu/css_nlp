Traceback (most recent call last):
  File "/Users/ryozawau/anaconda3/envs/jupyterbook/lib/python3.12/site-packages/jupyter_cache/executors/utils.py", line 58, in single_nb_execution
    executenb(
  File "/Users/ryozawau/anaconda3/envs/jupyterbook/lib/python3.12/site-packages/nbclient/client.py", line 1305, in execute
    return NotebookClient(nb=nb, resources=resources, km=km, **kwargs).execute()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/ryozawau/anaconda3/envs/jupyterbook/lib/python3.12/site-packages/jupyter_core/utils/__init__.py", line 165, in wrapped
    return loop.run_until_complete(inner)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/ryozawau/anaconda3/envs/jupyterbook/lib/python3.12/asyncio/base_events.py", line 684, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/Users/ryozawau/anaconda3/envs/jupyterbook/lib/python3.12/site-packages/nbclient/client.py", line 705, in async_execute
    await self.async_execute_cell(
  File "/Users/ryozawau/anaconda3/envs/jupyterbook/lib/python3.12/site-packages/nbclient/client.py", line 1058, in async_execute_cell
    await self._check_raise_for_error(cell, cell_index, exec_reply)
  File "/Users/ryozawau/anaconda3/envs/jupyterbook/lib/python3.12/site-packages/nbclient/client.py", line 914, in _check_raise_for_error
    raise CellExecutionError.from_cell_and_msg(cell, exec_reply_content)
nbclient.exceptions.CellExecutionError: An error occurred while executing the following cell:
------------------
import pandas as pd
from transformers import pipeline
#!pip install unidic-lite
#!pip install fugashi 
fill_mask = pipeline(
    "fill-mask",
    model="cl-tohoku/bert-base-japanese-v3"
)
------------------

----- stderr -----
/Users/ryozawau/anaconda3/envs/jupyterbook/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html
  from .autonotebook import tqdm as notebook_tqdm
----- stderr -----
/Users/ryozawau/anaconda3/envs/jupyterbook/lib/python3.12/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
----- stderr -----
/Users/ryozawau/anaconda3/envs/jupyterbook/lib/python3.12/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
----- stderr -----
/Users/ryozawau/anaconda3/envs/jupyterbook/lib/python3.12/site-packages/transformers/modeling_utils.py:519: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  return torch.load(checkpoint_file, map_location=map_location)
------------------

[0;31m---------------------------------------------------------------------------[0m
[0;31mModuleNotFoundError[0m                       Traceback (most recent call last)
File [0;32m~/anaconda3/envs/jupyterbook/lib/python3.12/site-packages/transformers/models/bert_japanese/tokenization_bert_japanese.py:457[0m, in [0;36mMecabTokenizer.__init__[0;34m(self, do_lower_case, never_split, normalize_text, mecab_dic, mecab_option)[0m
[1;32m    456[0m [38;5;28;01mtry[39;00m:
[0;32m--> 457[0m     [38;5;28;01mimport[39;00m [38;5;21;01mfugashi[39;00m
[1;32m    458[0m [38;5;28;01mexcept[39;00m [38;5;167;01mModuleNotFoundError[39;00m [38;5;28;01mas[39;00m error:

[0;31mModuleNotFoundError[0m: No module named 'fugashi'

During handling of the above exception, another exception occurred:

[0;31mModuleNotFoundError[0m                       Traceback (most recent call last)
Cell [0;32mIn[1], line 5[0m
[1;32m      2[0m [38;5;28;01mfrom[39;00m [38;5;21;01mtransformers[39;00m [38;5;28;01mimport[39;00m pipeline
[1;32m      3[0m [38;5;66;03m#!pip install unidic-lite[39;00m
[1;32m      4[0m [38;5;66;03m#!pip install fugashi [39;00m
[0;32m----> 5[0m fill_mask [38;5;241m=[39m [43mpipeline[49m[43m([49m
[1;32m      6[0m [43m    [49m[38;5;124;43m"[39;49m[38;5;124;43mfill-mask[39;49m[38;5;124;43m"[39;49m[43m,[49m
[1;32m      7[0m [43m    [49m[43mmodel[49m[38;5;241;43m=[39;49m[38;5;124;43m"[39;49m[38;5;124;43mcl-tohoku/bert-base-japanese-v3[39;49m[38;5;124;43m"[39;49m
[1;32m      8[0m [43m)[49m

File [0;32m~/anaconda3/envs/jupyterbook/lib/python3.12/site-packages/transformers/pipelines/__init__.py:967[0m, in [0;36mpipeline[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)[0m
[1;32m    964[0m             tokenizer_kwargs [38;5;241m=[39m model_kwargs[38;5;241m.[39mcopy()
[1;32m    965[0m             tokenizer_kwargs[38;5;241m.[39mpop([38;5;124m"[39m[38;5;124mtorch_dtype[39m[38;5;124m"[39m, [38;5;28;01mNone[39;00m)
[0;32m--> 967[0m         tokenizer [38;5;241m=[39m [43mAutoTokenizer[49m[38;5;241;43m.[39;49m[43mfrom_pretrained[49m[43m([49m
[1;32m    968[0m [43m            [49m[43mtokenizer_identifier[49m[43m,[49m[43m [49m[43muse_fast[49m[38;5;241;43m=[39;49m[43muse_fast[49m[43m,[49m[43m [49m[43m_from_pipeline[49m[38;5;241;43m=[39;49m[43mtask[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mhub_kwargs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mtokenizer_kwargs[49m
[1;32m    969[0m [43m        [49m[43m)[49m
[1;32m    971[0m [38;5;28;01mif[39;00m load_image_processor:
[1;32m    972[0m     [38;5;66;03m# Try to infer image processor from model or config name (if provided as str)[39;00m
[1;32m    973[0m     [38;5;28;01mif[39;00m image_processor [38;5;129;01mis[39;00m [38;5;28;01mNone[39;00m:

File [0;32m~/anaconda3/envs/jupyterbook/lib/python3.12/site-packages/transformers/models/auto/tokenization_auto.py:787[0m, in [0;36mAutoTokenizer.from_pretrained[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)[0m
[1;32m    783[0m     [38;5;28;01mif[39;00m tokenizer_class [38;5;129;01mis[39;00m [38;5;28;01mNone[39;00m:
[1;32m    784[0m         [38;5;28;01mraise[39;00m [38;5;167;01mValueError[39;00m(
[1;32m    785[0m             [38;5;124mf[39m[38;5;124m"[39m[38;5;124mTokenizer class [39m[38;5;132;01m{[39;00mtokenizer_class_candidate[38;5;132;01m}[39;00m[38;5;124m does not exist or is not currently imported.[39m[38;5;124m"[39m
[1;32m    786[0m         )
[0;32m--> 787[0m     [38;5;28;01mreturn[39;00m [43mtokenizer_class[49m[38;5;241;43m.[39;49m[43mfrom_pretrained[49m[43m([49m[43mpretrained_model_name_or_path[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[43minputs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m
[1;32m    789[0m [38;5;66;03m# Otherwise we have to be creative.[39;00m
[1;32m    790[0m [38;5;66;03m# if model is an encoder decoder, the encoder tokenizer class is used by default[39;00m
[1;32m    791[0m [38;5;28;01mif[39;00m [38;5;28misinstance[39m(config, EncoderDecoderConfig):

File [0;32m~/anaconda3/envs/jupyterbook/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2028[0m, in [0;36mPreTrainedTokenizerBase.from_pretrained[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, *init_inputs, **kwargs)[0m
[1;32m   2025[0m     [38;5;28;01melse[39;00m:
[1;32m   2026[0m         logger[38;5;241m.[39minfo([38;5;124mf[39m[38;5;124m"[39m[38;5;124mloading file [39m[38;5;132;01m{[39;00mfile_path[38;5;132;01m}[39;00m[38;5;124m from cache at [39m[38;5;132;01m{[39;00mresolved_vocab_files[file_id][38;5;132;01m}[39;00m[38;5;124m"[39m)
[0;32m-> 2028[0m [38;5;28;01mreturn[39;00m [38;5;28;43mcls[39;49m[38;5;241;43m.[39;49m[43m_from_pretrained[49m[43m([49m
[1;32m   2029[0m [43m    [49m[43mresolved_vocab_files[49m[43m,[49m
[1;32m   2030[0m [43m    [49m[43mpretrained_model_name_or_path[49m[43m,[49m
[1;32m   2031[0m [43m    [49m[43minit_configuration[49m[43m,[49m
[1;32m   2032[0m [43m    [49m[38;5;241;43m*[39;49m[43minit_inputs[49m[43m,[49m
[1;32m   2033[0m [43m    [49m[43mtoken[49m[38;5;241;43m=[39;49m[43mtoken[49m[43m,[49m
[1;32m   2034[0m [43m    [49m[43mcache_dir[49m[38;5;241;43m=[39;49m[43mcache_dir[49m[43m,[49m
[1;32m   2035[0m [43m    [49m[43mlocal_files_only[49m[38;5;241;43m=[39;49m[43mlocal_files_only[49m[43m,[49m
[1;32m   2036[0m [43m    [49m[43m_commit_hash[49m[38;5;241;43m=[39;49m[43mcommit_hash[49m[43m,[49m
[1;32m   2037[0m [43m    [49m[43m_is_local[49m[38;5;241;43m=[39;49m[43mis_local[49m[43m,[49m
[1;32m   2038[0m [43m    [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m,[49m
[1;32m   2039[0m [43m[49m[43m)[49m

File [0;32m~/anaconda3/envs/jupyterbook/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2260[0m, in [0;36mPreTrainedTokenizerBase._from_pretrained[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, *init_inputs, **kwargs)[0m
[1;32m   2258[0m [38;5;66;03m# Instantiate the tokenizer.[39;00m
[1;32m   2259[0m [38;5;28;01mtry[39;00m:
[0;32m-> 2260[0m     tokenizer [38;5;241m=[39m [38;5;28;43mcls[39;49m[43m([49m[38;5;241;43m*[39;49m[43minit_inputs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43minit_kwargs[49m[43m)[49m
[1;32m   2261[0m [38;5;28;01mexcept[39;00m [38;5;167;01mOSError[39;00m:
[1;32m   2262[0m     [38;5;28;01mraise[39;00m [38;5;167;01mOSError[39;00m(
[1;32m   2263[0m         [38;5;124m"[39m[38;5;124mUnable to load vocabulary from file. [39m[38;5;124m"[39m
[1;32m   2264[0m         [38;5;124m"[39m[38;5;124mPlease check that the provided vocabulary is accessible and not corrupted.[39m[38;5;124m"[39m
[1;32m   2265[0m     )

File [0;32m~/anaconda3/envs/jupyterbook/lib/python3.12/site-packages/transformers/models/bert_japanese/tokenization_bert_japanese.py:192[0m, in [0;36mBertJapaneseTokenizer.__init__[0;34m(self, vocab_file, spm_file, do_lower_case, do_word_tokenize, do_subword_tokenize, word_tokenizer_type, subword_tokenizer_type, never_split, unk_token, sep_token, pad_token, cls_token, mask_token, mecab_kwargs, sudachi_kwargs, jumanpp_kwargs, **kwargs)[0m
[1;32m    188[0m     [38;5;28mself[39m[38;5;241m.[39mword_tokenizer [38;5;241m=[39m BasicTokenizer(
[1;32m    189[0m         do_lower_case[38;5;241m=[39mdo_lower_case, never_split[38;5;241m=[39mnever_split, tokenize_chinese_chars[38;5;241m=[39m[38;5;28;01mFalse[39;00m
[1;32m    190[0m     )
[1;32m    191[0m [38;5;28;01melif[39;00m word_tokenizer_type [38;5;241m==[39m [38;5;124m"[39m[38;5;124mmecab[39m[38;5;124m"[39m:
[0;32m--> 192[0m     [38;5;28mself[39m[38;5;241m.[39mword_tokenizer [38;5;241m=[39m [43mMecabTokenizer[49m[43m([49m
[1;32m    193[0m [43m        [49m[43mdo_lower_case[49m[38;5;241;43m=[39;49m[43mdo_lower_case[49m[43m,[49m[43m [49m[43mnever_split[49m[38;5;241;43m=[39;49m[43mnever_split[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43m([49m[43mmecab_kwargs[49m[43m [49m[38;5;129;43;01mor[39;49;00m[43m [49m[43m{[49m[43m}[49m[43m)[49m
[1;32m    194[0m [43m    [49m[43m)[49m
[1;32m    195[0m [38;5;28;01melif[39;00m word_tokenizer_type [38;5;241m==[39m [38;5;124m"[39m[38;5;124msudachi[39m[38;5;124m"[39m:
[1;32m    196[0m     [38;5;28mself[39m[38;5;241m.[39mword_tokenizer [38;5;241m=[39m SudachiTokenizer(
[1;32m    197[0m         do_lower_case[38;5;241m=[39mdo_lower_case, never_split[38;5;241m=[39mnever_split, [38;5;241m*[39m[38;5;241m*[39m(sudachi_kwargs [38;5;129;01mor[39;00m {})
[1;32m    198[0m     )

File [0;32m~/anaconda3/envs/jupyterbook/lib/python3.12/site-packages/transformers/models/bert_japanese/tokenization_bert_japanese.py:459[0m, in [0;36mMecabTokenizer.__init__[0;34m(self, do_lower_case, never_split, normalize_text, mecab_dic, mecab_option)[0m
[1;32m    457[0m     [38;5;28;01mimport[39;00m [38;5;21;01mfugashi[39;00m
[1;32m    458[0m [38;5;28;01mexcept[39;00m [38;5;167;01mModuleNotFoundError[39;00m [38;5;28;01mas[39;00m error:
[0;32m--> 459[0m     [38;5;28;01mraise[39;00m error[38;5;241m.[39m[38;5;18m__class__[39m(
[1;32m    460[0m         [38;5;124m"[39m[38;5;124mYou need to install fugashi to use MecabTokenizer. [39m[38;5;124m"[39m
[1;32m    461[0m         [38;5;124m"[39m[38;5;124mSee https://pypi.org/project/fugashi/ for installation.[39m[38;5;124m"[39m
[1;32m    462[0m     )
[1;32m    464[0m mecab_option [38;5;241m=[39m mecab_option [38;5;129;01mor[39;00m [38;5;124m"[39m[38;5;124m"[39m
[1;32m    466[0m [38;5;28;01mif[39;00m mecab_dic [38;5;129;01mis[39;00m [38;5;129;01mnot[39;00m [38;5;28;01mNone[39;00m:

[0;31mModuleNotFoundError[0m: You need to install fugashi to use MecabTokenizer. See https://pypi.org/project/fugashi/ for installation.

