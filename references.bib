@article{DIMAGGIO2013570,
title = {Exploiting affinities between topic modeling and the sociological perspective on culture: Application to newspaper coverage of U.S. government arts funding},
journal = {Poetics},
volume = {41},
number = {6},
pages = {570-606},
year = {2013},
note = {Topic Models and the Cultural Sciences},
issn = {0304-422X},
doi = {https://doi.org/10.1016/j.poetic.2013.08.004},
author = {Paul DiMaggio and Manish Nag and David Blei},
keywords = {Topic models, Polysemy, Heteroglossia, Meaning, Content analysis, National Endowment for the Arts},
abstract = {Topic modeling provides a valuable method for identifying the linguistic contexts that surround social institutions or policy domains. This article uses Latent Dirichlet Allocation (LDA) to analyze how one such policy domain, government assistance to artists and arts organizations, was framed in almost 8000 articles. These comprised all articles that referred to government support for the arts in the U.S. published in five U.S. newspapers between 1986 and 1997—a period during which such assistance, once noncontroversial, became a focus of contention. We illustrate the strengths of topic modeling as a means of analyzing large text corpora, discuss the proper choice of models and interpretation of model results, describe means of validating topic-model solutions, and demonstrate the use of topic models in combination with other statistical tools to estimate differences between newspapers in the prevalence of different frames. Throughout, we emphasize affinities between the topic-modeling approach and such central concepts in the study of culture as framing, polysemy, heteroglossia, and the relationality of meaning.}
}

@article{
Garg2018,
author = {Nikhil Garg  and Londa Schiebinger  and Dan Jurafsky  and James Zou },
title = {Word embeddings quantify 100 years of gender and ethnic stereotypes},
journal = {Proceedings of the National Academy of Sciences},
volume = {115},
number = {16},
pages = {E3635-E3644},
year = {2018},
doi = {10.1073/pnas.1720347115},
URL = {https://www.pnas.org/doi/abs/10.1073/pnas.1720347115},
eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.1720347115},
abstract = {Word embeddings are a powerful machine-learning framework that represents each English word by a vector. The geometric relationship between these vectors captures meaningful semantic relationships between the corresponding words. In this paper, we develop a framework to demonstrate how the temporal dynamics of the embedding helps to quantify changes in stereotypes and attitudes toward women and ethnic minorities in the 20th and 21st centuries in the United States. We integrate word embeddings trained on 100 y of text data with the US Census to show that changes in the embedding track closely with demographic and occupation shifts over time. The embedding captures societal shifts—e.g., the women’s movement in the 1960s and Asian immigration into the United States—and also illuminates how specific adjectives and occupations became more closely associated with certain populations over time. Our framework for temporal analysis of word embedding opens up a fruitful intersection between machine learning and quantitative social science.}}

@article{Kozlowski2019,
author = {Austin C. Kozlowski and Matt Taddy and James A. Evans},
title ={The Geometry of Culture: Analyzing the Meanings of Class through Word Embeddings},
journal = {American Sociological Review},
volume = {84},
number = {5},
pages = {905-949},
year = {2019},
doi = {10.1177/0003122419877135},
abstract = { We argue word embedding models are a useful tool for the study of culture using a historical analysis of shared understandings of social class as an empirical case. Word embeddings represent semantic relations between words as relationships between vectors in a high-dimensional space, specifying a relational model of meaning consistent with contemporary theories of culture. Dimensions induced by word differences (rich – poor) in these spaces correspond to dimensions of cultural meaning, and the projection of words onto these dimensions reflects widely shared associations, which we validate with surveys. Analyzing text from millions of books published over 100 years, we show that the markers of class continuously shifted amidst the economic transformations of the twentieth century, yet the basic cultural dimensions of class remained remarkably stable. The notable exception is education, which became tightly linked to affluence independent of its association with cultivated taste. }
}


@inproceedings{Devlin2019, 
year = {2019}, 
author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina}, 
title = {{\{BERT\}: Pre-training of Deep Bidirectional Transformers for Language Understanding}}, 
booktitle = {Proceedings of the 2019 Conference of the North \{A\}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)}, 
doi = {10.18653/v1/n19-1423}, 
url = {https://www.aclweb.org/anthology/N19-1423}, 
abstract = {{We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\textbackslash\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).}}, 
pages = {4171--4186}, 
series = {Proceedings of the 2019 Conference of the North}, 
publisher = {Association for Computational Linguistics}, 
address = {Minneapolis, Minnesota}, 
keywords = {}
}

@article{laurer_van,
 title={Less Annotating, More Classifying: Addressing the Data Scarcity Issue of Supervised Machine Learning with Deep Transfer Learning and BERT-NLI},
  DOI={10.1017/pan.2023.20},
  journal={Political Analysis},
  author={Laurer, Moritz and van Atteveldt, Wouter and Casas, Andreu and Welbers, Kasper},
   year={2023},
    pages={1–17}}

@inproceedings{mikolov-etal-2013-linguistic,
    title = "Linguistic Regularities in Continuous Space Word Representations",
    author = "Mikolov, Tomas  and
      Yih, Wen-tau  and
      Zweig, Geoffrey",
    editor = "Vanderwende, Lucy  and
      Daum{\'e} III, Hal  and
      Kirchhoff, Katrin",
    booktitle = "Proceedings of the 2013 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2013",
    address = "Atlanta, Georgia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N13-1090",
    pages = "746--751",
}


@inproceedings{NIPS2013_9aa42b31,
	author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
	publisher = {Curran Associates, Inc.},
	title = {Distributed Representations of Words and Phrases and their Compositionality},
	url = {https://proceedings.neurips.cc/paper_files/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf},
	volume = {26},
	year = {2013},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf}}


@article{Grand2022, 
year = {2022}, 
title = {{Semantic projection recovers rich human knowledge of multiple object features from word embeddings}}, 
author = {Grand, Gabriel and Blank, Idan Asher and Pereira, Francisco and Fedorenko, Evelina}, 
journal = {Nature Human Behaviour}, 
doi = {10.1038/s41562-022-01316-8}, 
pmid = {35422527}, 
abstract = {{How is knowledge about word meaning represented in the mental lexicon? Current computational models infer word meanings from lexical co-occurrence patterns. They learn to represent words as vectors in a multidimensional space, wherein words that are used in more similar linguistic contexts—that is, are more semantically related—are located closer together. However, whereas inter-word proximity captures only overall relatedness, human judgements are highly context dependent. For example, dolphins and alligators are similar in size but differ in dangerousness. Here, we use a domain-general method to extract context-dependent relationships from word embeddings: ‘semantic projection’ of word-vectors onto lines that represent features such as size (the line connecting the words ‘small’ and ‘big’) or danger (‘safe’ to ‘dangerous’), analogous to ‘mental scales’. This method recovers human judgements across various object categories and properties. Thus, the geometry of word embeddings explicitly represents a wealth of context-dependent world knowledge. Grand, Blank, et al. show that context-dependent knowledge about objects, such as the similarities between animals in terms of size versus danger versus habitat, can be recovered from word embeddings via a simple, interpretable geometrical operation.}}, 
pages = {975--987}, 
number = {7}, 
volume = {6}, 
keywords = {}, 
local-url = {file://localhost/Users/ryozawau/Documents/Papers%20Library/Grand-Semantic%20projection%20recovers%20rich%20human%20knowledge%20of%20multiple%20object%20features%20from%20word%20embeddings-2022-Nature%20Human%20Behaviour.pdf}
}


@article{Garg2018, 
year = {2018}, 
title = {{Word embeddings quantify 100 years of gender and ethnic stereotypes}}, 
author = {Garg, Nikhil and Schiebinger, Londa and Jurafsky, Dan and Zou, James}, 
journal = {Proceedings of the National Academy of Sciences}, 
issn = {0027-8424}, 
doi = {10.1073/pnas.1720347115}, 
pmid = {29615513}, 
pmcid = {PMC5910851}, 
eprint = {1711.08412}, 
abstract = {{Word embeddings are a powerful machine-learning framework that represents each English word by a vector. The geometric relationship between these vectors captures meaningful semantic relationships between the corresponding words. In this paper, we develop a framework to demonstrate how the temporal dynamics of the embedding helps to quantify changes in stereotypes and attitudes toward women and ethnic minorities in the 20th and 21st centuries in the United States. We integrate word embeddings trained on 100 y of text data with the US Census to show that changes in the embedding track closely with demographic and occupation shifts over time. The embedding captures societal shifts—e.g., the women’s movement in the 1960s and Asian immigration into the United States—and also illuminates how specific adjectives and occupations became more closely associated with certain populations over time. Our framework for temporal analysis of word embedding opens up a fruitful intersection between machine learning and quantitative social science.}}, 
pages = {E3635--E3644}, 
number = {16}, 
volume = {115}, 
keywords = {}, 
local-url = {file://localhost/Users/ryozawau/Documents/Papers%20Library/Garg-Word%20embeddings%20quantify%20100%20years%20of%20gender%20and%20ethnic%20stereotypes-2018-Proceedings%20of%20the%20National%20Academy%20of%20Sciences.pdf}
}


@article{Jones2020, 
year = {2020}, 
title = {{Stereotypical Gender Associations in Language Have Decreased Over Time}}, 
author = {Jones, Jason J. and Amin, Mohammad Ruhul and Kim, Jessica and Skiena, Steven}, 
journal = {Sociological Science}, 
issn = {2330-6696}, 
doi = {10.15195/v7.a1}, 
abstract = {{Using a corpus of millions of digitized books, we document the presence and trajectory over time of stereotypical gender associations in the written English language from 1800 to 2000. We employ the novel methodology of word embeddings to quantify male gender bias: the tendency to associate a domain with the male gender. We measure male gender bias in four stereotypically gendered domains: career, family, science, and arts. We found that stereotypical gender associations in language have decreased over time but still remain, with career and science terms demonstrating positive male gender bias and family and arts terms demonstrating negative male gender bias. We also seek evidence of changing associations corresponding to the second shift and find partial support. Traditional gender ideology is latent within the text of published English-language books, yet the magnitude of traditionally gendered associations appears to be decreasing over time.}}, 
pages = {1--35}, 
volume = {7}, 
keywords = {}, 
local-url = {file://localhost/Users/ryozawau/Documents/Papers%20Library/Jones-Stereotypical%20Gender%20Associations%20in%20Language%20Have%20Decreased%20Over%20Time-2020-Sociological%20Science.pdf}
}


@article{Kozlowski2019, 
year = {2019}, 
title = {{The Geometry of Culture: Analyzing the Meanings of Class through Word Embeddings}}, 
author = {Kozlowski, Austin C. and Taddy, Matt and Evans, James A.}, 
journal = {American Sociological Review}, 
issn = {0003-1224}, 
doi = {10.1177/0003122419877135}, 
eprint = {1803.09288}, 
abstract = {{We argue word embedding models are a useful tool for the study of culture using a historical analysis of shared understandings of social class as an empirical case. Word embeddings represent semantic relations between words as relationships between vectors in a high-dimensional space, specifying a relational model of meaning consistent with contemporary theories of culture. Dimensions induced by word differences (rich – poor) in these spaces correspond to dimensions of cultural meaning, and the projection of words onto these dimensions reflects widely shared associations, which we validate with surveys. Analyzing text from millions of books published over 100 years, we show that the markers of class continuously shifted amidst the economic transformations of the twentieth century, yet the basic cultural dimensions of class remained remarkably stable. The notable exception is education, which became tightly linked to affluence independent of its association with cultivated taste.}}, 
pages = {905--949}, 
number = {5}, 
volume = {84}, 
keywords = {}, 
local-url = {file://localhost/Users/ryozawau/Documents/Papers%20Library/Kozlowski-The%20Geometry%20of%20Culture-%20Analyzing%20the%20Meanings%20of%20Class%20through%20Word%20Embeddings-2019-American%20Sociological%20Review.pdf}
}

@article{
lazer2009,
author = {David Lazer  and Alex Pentland  and Lada Adamic  and Sinan Aral  and Albert-László Barabási  and Devon Brewer  and Nicholas Christakis  and Noshir Contractor  and James Fowler  and Myron Gutmann  and Tony Jebara  and Gary King  and Michael Macy  and Deb Roy  and Marshall Van Alstyne },
title = {Computational Social Science},
journal = {Science},
volume = {323},
number = {5915},
pages = {721-723},
year = {2009},
doi = {10.1126/science.1167742}}

@article{hofman2021,
  title = {Integrating Explanation and Prediction in Computational Social Science},
  author = {Hofman, Jake M.and Watts},
  year = {2021},
  journal = {Nature},
  volume = {595},
  number = {7866},
  pages = {181--188},
  issn = {1476-4687},
  doi = {10.1038/s41586-021-03659-0},
  abstract = {Computational social science is more than just large repositories of digital data and the computational methods needed to construct and analyse them. It also represents a convergence of different fields with different ways of thinking about and doing science. The goal of this Perspective is to provide some clarity around how these approaches differ from one another and to propose how they might be productively integrated. Towards this end we make two contributions. The first is a schema for thinking about research activities along two dimensions---the extent to which work is explanatory, focusing on identifying and estimating causal effects, and the degree of consideration given to testing predictions of outcomes---and how these two priorities can complement, rather than compete with, one another. Our second contribution is to advocate that computational social scientists devote more attention to combining prediction and explanation, which we call integrative modelling, and to outline some practical suggestions for realizing this goal.},
  lccn = {1},
  keywords = {/unread},
  annotation = {TLDR: The goal of this Perspective is to provide some clarity around how computational social scientists' approaches to prediction and explanation differ from one another and to propose how they might be productively integrated.}
}
